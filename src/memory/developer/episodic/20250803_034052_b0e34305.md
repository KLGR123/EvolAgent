### Development Step 1: Antidisestablishmentarianism Wikipedia Revision Log Compilation Through June 2023

**Description**: Search for and access the Wikipedia page on 'Antidisestablishmentarianism' to locate its edit history or revision log. Extract comprehensive information about all edits made to this page from its creation until June 2023, including the total number of revisions, edit timestamps, and any available metadata about the page's editing activity over time.

**Use Cases**:
- Brand reputation monitoring and automated alerting for changes to a company’s Wikipedia page to detect unauthorized edits and respond proactively
- Academic research on the evolution of political terminology by extracting and analyzing the complete revision history of contested Wikipedia entries
- Digital humanities project mapping semantic shifts in philosophical concept articles by aggregating edit metadata and timestamps
- Journalism fact-checking and source verification by tracking contributor patterns and edit summaries on high-impact historical event pages
- SEO competitive analysis by monitoring revisions and backlink changes on industry-related Wikipedia pages to identify emerging trends
- Vandalism detection automation for educational resource pages by scheduling regular scrapes of the revision log and flagging anomalous edit patterns
- Historical data preservation and archiving for legislation pages by compiling all revisions into time-stamped snapshots in a secure database
- Machine learning model training for edit-prediction algorithms using user behavior and revision metadata harvested from comprehensive Wikipedia edit histories

```
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import os
from urllib.parse import urljoin, quote

print("=== ACCESSING ANTIDISESTABLISHMENTARIANISM WIKIPEDIA PAGE ===\n")
print("Objective: Extract comprehensive edit history and revision data until June 2023\n")

# Create workspace directory
workspace_dir = f"workspace_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
os.makedirs(workspace_dir, exist_ok=True)
print(f"Created workspace directory: {workspace_dir}\n")

# Define the target page
page_title = "Antidisestablishmentarianism"
base_url = "https://en.wikipedia.org"
page_url = f"{base_url}/wiki/{page_title}"
history_url = f"{base_url}/w/index.php?title={page_title}&action=history"

print(f"Target page: {page_title}")
print(f"Page URL: {page_url}")
print(f"History URL: {history_url}\n")

# Set up headers for requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# First, check if the main page exists and get basic information
print("=== CHECKING MAIN PAGE ACCESSIBILITY ===\n")

try:
    response = requests.get(page_url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        print(f"✅ Successfully accessed main page")
        print(f"Content length: {len(response.content):,} bytes")
        
        # Parse main page for basic info
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Get page title
        title_element = soup.find('title')
        actual_title = title_element.get_text().strip() if title_element else 'Unknown'
        print(f"Page title: {actual_title}")
        
        # Look for last modified information
        last_modified = soup.find('li', {'id': 'footer-info-lastmod'})
        if last_modified:
            print(f"Last modified: {last_modified.get_text().strip()}")
        
        # Save main page HTML for reference
        main_page_file = os.path.join(workspace_dir, 'antidisestablishmentarianism_main.html')
        with open(main_page_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"✅ Main page saved to: {os.path.basename(main_page_file)}")
        
    else:
        print(f"❌ Failed to access main page: HTTP {response.status_code}")
        if response.status_code == 404:
            print("Page may not exist or may have been moved")
        
except Exception as e:
    print(f"❌ Error accessing main page: {str(e)}")

# Now access the edit history page
print(f"\n=== ACCESSING EDIT HISTORY PAGE ===\n")

try:
    history_response = requests.get(history_url, headers=headers, timeout=30)
    
    if history_response.status_code == 200:
        print(f"✅ Successfully accessed edit history page")
        print(f"Content length: {len(history_response.content):,} bytes")
        
        # Parse history page
        history_soup = BeautifulSoup(history_response.content, 'html.parser')
        
        # Save history page HTML for detailed analysis
        history_page_file = os.path.join(workspace_dir, 'antidisestablishmentarianism_history.html')
        with open(history_page_file, 'w', encoding='utf-8') as f:
            f.write(history_response.text)
        print(f"✅ History page saved to: {os.path.basename(history_page_file)}")
        
        # Extract edit history information
        print(f"\n=== EXTRACTING EDIT HISTORY DATA ===\n")
        
        # Look for revision entries in the history page
        # Wikipedia history pages typically use <li> elements with revision data
        revision_entries = []
        
        # Find the revision list container
        revision_list = history_soup.find('ul', {'id': 'pagehistory'})
        if not revision_list:
            # Alternative selector
            revision_list = history_soup.find('ul', class_='mw-contributions-list')
        if not revision_list:
            # Look for any list that might contain revisions
            revision_list = history_soup.find('div', {'id': 'mw-content-text'})
        
        if revision_list:
            print(f"Found revision list container: {revision_list.name}")
            
            # Extract individual revision entries
            revisions = revision_list.find_all('li')
            print(f"Found {len(revisions)} potential revision entries")
            
            for i, revision in enumerate(revisions[:10], 1):  # Process first 10 for inspection
                revision_text = revision.get_text().strip()
                if revision_text:  # Skip empty entries
                    print(f"\nRevision {i}:")
                    print(f"  Text preview: {revision_text[:200]}..." if len(revision_text) > 200 else f"  Text: {revision_text}")
                    
                    # Look for specific elements within each revision
                    timestamp_elem = revision.find('a', class_='mw-changeslist-date')
                    if timestamp_elem:
                        print(f"  Timestamp found: {timestamp_elem.get_text().strip()}")
                    
                    user_elem = revision.find('a', class_='mw-userlink')
                    if user_elem:
                        print(f"  User found: {user_elem.get_text().strip()}")
                    
                    # Look for edit summary
                    summary_elem = revision.find('span', class_='comment')
                    if summary_elem:
                        print(f"  Summary: {summary_elem.get_text().strip()}")
        else:
            print("❌ Could not find revision list container")
            print("Attempting alternative extraction methods...")
            
            # Alternative: look for any elements that might contain revision data
            potential_revisions = history_soup.find_all(['li', 'tr', 'div'], class_=lambda x: x and ('revision' in x.lower() or 'history' in x.lower() or 'change' in x.lower()))
            print(f"Found {len(potential_revisions)} potential revision elements with alternative method")
            
            if potential_revisions:
                for i, elem in enumerate(potential_revisions[:5], 1):
                    print(f"\nAlternative revision {i}:")
                    text = elem.get_text().strip()
                    print(f"  Text: {text[:150]}..." if len(text) > 150 else f"  Text: {text}")
        
        # Look for pagination or "show more" links to get complete history
        print(f"\n=== CHECKING FOR PAGINATION ===\n")
        
        pagination_links = history_soup.find_all('a', href=True)
        next_links = []
        prev_links = []
        
        for link in pagination_links:
            href = link.get('href', '')
            link_text = link.get_text().strip().lower()
            
            if 'offset' in href and ('next' in link_text or 'older' in link_text):
                next_links.append({
                    'text': link.get_text().strip(),
                    'href': href,
                    'full_url': urljoin(base_url, href)
                })
            elif 'offset' in href and ('prev' in link_text or 'newer' in link_text):
                prev_links.append({
                    'text': link.get_text().strip(),
                    'href': href,
                    'full_url': urljoin(base_url, href)
                })
        
        print(f"Found {len(next_links)} 'next/older' pagination links")
        print(f"Found {len(prev_links)} 'prev/newer' pagination links")
        
        if next_links:
            print("\nNext/Older links:")
            for i, link in enumerate(next_links[:3], 1):
                print(f"  {i}. {link['text']} -> {link['full_url']}")
        
        # Extract any visible statistics about the page
        print(f"\n=== EXTRACTING PAGE STATISTICS ===\n")
        
        # Look for revision count information
        stats_text = history_soup.get_text()
        
        # Common patterns for revision counts
        import re
        
        # Look for patterns like "showing X of Y revisions"
        revision_count_patterns = [
            r'showing\s+(\d+)\s+of\s+(\d+)\s+revision',
            r'(\d+)\s+revision[s]?\s+total',
            r'page\s+has\s+(\d+)\s+revision'
        ]
        
        for pattern in revision_count_patterns:
            matches = re.findall(pattern, stats_text, re.IGNORECASE)
            if matches:
                print(f"Found revision count pattern: {pattern}")
                print(f"Matches: {matches}")
        
        # Look for date ranges
        date_patterns = [
            r'(\d{1,2}\s+\w+\s+\d{4})',  # "1 January 2023"
            r'(\d{4}-\d{2}-\d{2})',      # "2023-01-01"
        ]
        
        dates_found = []
        for pattern in date_patterns:
            matches = re.findall(pattern, stats_text)
            dates_found.extend(matches)
        
        if dates_found:
            print(f"\nDates found in history: {len(dates_found)} total")
            # Show first and last few dates
            unique_dates = list(set(dates_found))
            unique_dates.sort()
            print(f"Date range sample: {unique_dates[:3]} ... {unique_dates[-3:] if len(unique_dates) > 6 else unique_dates[3:]}")
        
    else:
        print(f"❌ Failed to access edit history page: HTTP {history_response.status_code}")
        
except Exception as e:
    print(f"❌ Error accessing edit history page: {str(e)}")

# Try Wikipedia API approach for more structured data
print(f"\n=== ATTEMPTING WIKIPEDIA API ACCESS ===\n")

# Wikipedia API endpoint for revision history
api_url = "https://en.wikipedia.org/w/api.php"

# Parameters for getting revision history
api_params = {
    'action': 'query',
    'format': 'json',
    'titles': page_title,
    'prop': 'revisions',
    'rvlimit': '50',  # Get first 50 revisions
    'rvprop': 'timestamp|user|comment|size|ids',
    'rvdir': 'older'  # Start from newest
}

try:
    print(f"API URL: {api_url}")
    print(f"Parameters: {api_params}")
    
    api_response = requests.get(api_url, params=api_params, headers=headers, timeout=30)
    
    if api_response.status_code == 200:
        print(f"✅ Successfully accessed Wikipedia API")
        
        api_data = api_response.json()
        print(f"API response keys: {list(api_data.keys())}")
        
        if 'query' in api_data and 'pages' in api_data['query']:
            pages = api_data['query']['pages']
            print(f"Pages in response: {list(pages.keys())}")
            
            for page_id, page_data in pages.items():
                print(f"\nPage ID: {page_id}")
                print(f"Page data keys: {list(page_data.keys())}")
                
                if 'revisions' in page_data:
                    revisions = page_data['revisions']
                    print(f"Found {len(revisions)} revisions in API response")
                    
                    # Save API data
                    api_data_file = os.path.join(workspace_dir, 'api_revisions_sample.json')
                    with open(api_data_file, 'w', encoding='utf-8') as f:
                        json.dump(api_data, f, indent=2, ensure_ascii=False)
                    print(f"✅ API data saved to: {os.path.basename(api_data_file)}")
                    
                    # Display sample revisions
                    print(f"\nSample revisions:")
                    for i, rev in enumerate(revisions[:5], 1):
                        print(f"\n  Revision {i}:")
                        for key, value in rev.items():
                            print(f"    {key}: {value}")
                    
                    # Check if we can get more revisions
                    if len(revisions) == 50:  # We hit the limit
                        print(f"\n⚠️ Retrieved maximum of 50 revisions. More data available.")
                        print(f"Need to implement pagination to get complete history.")
                    
                else:
                    print(f"❌ No revisions found in page data")
                    if 'missing' in page_data:
                        print(f"Page appears to be missing or doesn't exist")
        else:
            print(f"❌ Unexpected API response structure")
            print(f"Full response: {api_data}")
    
    else:
        print(f"❌ API request failed: HTTP {api_response.status_code}")
        print(f"Response: {api_response.text[:500]}")
        
except Exception as e:
    print(f"❌ Error accessing Wikipedia API: {str(e)}")

# Create initial summary of findings
print(f"\n=== INITIAL ANALYSIS SUMMARY ===\n")

summary_data = {
    'analysis_metadata': {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'target_page': page_title,
        'workspace_directory': workspace_dir,
        'objective': 'Extract comprehensive edit history until June 2023'
    },
    'access_results': {
        'main_page_accessible': 'response' in locals() and response.status_code == 200,
        'history_page_accessible': 'history_response' in locals() and history_response.status_code == 200,
        'api_accessible': 'api_response' in locals() and api_response.status_code == 200
    },
    'files_created': [
        f for f in os.listdir(workspace_dir) if os.path.isfile(os.path.join(workspace_dir, f))
    ],
    'next_steps': [
        'Parse HTML history page for detailed revision extraction',
        'Implement API pagination to get complete revision history',
        'Filter revisions to only include those before June 2023',
        'Extract comprehensive metadata for each revision'
    ]
}

# Save summary
summary_file = os.path.join(workspace_dir, 'initial_analysis_summary.json')
with open(summary_file, 'w', encoding='utf-8') as f:
    json.dump(summary_data, f, indent=2, ensure_ascii=False)

print(f"📊 Analysis Summary:")
print(f"  Target page: {page_title}")
print(f"  Main page accessible: {'✅' if summary_data['access_results']['main_page_accessible'] else '❌'}")
print(f"  History page accessible: {'✅' if summary_data['access_results']['history_page_accessible'] else '❌'}")
print(f"  API accessible: {'✅' if summary_data['access_results']['api_accessible'] else '❌'}")
print(f"  Files created: {len(summary_data['files_created'])}")

for filename in summary_data['files_created']:
    file_path = os.path.join(workspace_dir, filename)
    file_size = os.path.getsize(file_path)
    print(f"    - {filename} ({file_size:,} bytes)")

print(f"\n✅ Initial data collection completed!")
print(f"📁 All data saved to workspace: {workspace_dir}")
print(f"🔍 Ready for detailed revision history extraction and analysis")
```