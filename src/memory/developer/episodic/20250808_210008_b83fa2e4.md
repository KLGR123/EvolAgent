### Development Step 2: Title:  
Locate and Extract Carolyn Collins Petersen’s June 6, 2023 Universe Today Article and Research Link

**Description**: Search for the article by Carolyn Collins Petersen published on Universe Today on June 6, 2023. Look for the specific article using search queries like 'Carolyn Collins Petersen Universe Today June 6 2023', 'site:universetoday.com Carolyn Collins Petersen June 6 2023', and variations with different keyword combinations. Extract the article content and identify the research paper link mentioned at the bottom of the article.

**Use Cases**:
- Academic librarian automating the extraction of primary research paper links from news articles to update university resource guides
- Science journalist verifying and citing original research papers referenced in astronomy news articles for accurate reporting
- Research group leader compiling a database of recent studies by extracting research paper links from curated science news sources
- Digital archivist preserving both popular science articles and their associated primary research links for institutional repositories
- Automated alert system for astrophysics departments that flags and summarizes new research papers mentioned in major science news outlets
- Content aggregator platform enriching article summaries with direct links to referenced research papers for enhanced reader engagement
- Science educator preparing classroom materials by quickly locating and sharing the original research behind news stories
- Data analyst tracking trends in media coverage of scientific research by extracting and cataloging research paper links from multiple articles

```
import os
import json

print("=== EXTRACTING RESEARCH PAPER LINK FROM CAROLYN PETERSEN ARTICLE ===\n")
print("Objective: Parse the article content and identify the specific research paper link mentioned\n")

# Find workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("No workspace directory found.")
    exit()

workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# First, let's inspect what files are available
print("=== INSPECTING WORKSPACE FILES ===\n")
workspace_files = os.listdir(workspace_dir)
for file in workspace_files:
    file_path = os.path.join(workspace_dir, file)
    file_size = os.path.getsize(file_path)
    print(f"- {file} ({file_size:,} bytes)")

print("\n=== EXAMINING RESEARCH PAPER LINKS FILE ===\n")

# Load and inspect the research paper links JSON file
links_file = os.path.join(workspace_dir, 'research_paper_links.json')
if os.path.exists(links_file):
    print(f"Loading research paper links from: {links_file}")
    
    with open(links_file, 'r') as f:
        links_data = json.load(f)
    
    # Inspect the structure of the JSON file
    print("\nJSON file structure:")
    for key, value in links_data.items():
        if isinstance(value, list):
            print(f"  {key}: list with {len(value)} items")
        else:
            print(f"  {key}: {type(value).__name__} - {str(value)[:100]}..." if len(str(value)) > 100 else f"  {key}: {value}")
    
    print(f"\n=== RESEARCH PAPER LINKS FOUND ===\n")
    print(f"Article title: {links_data.get('article_title', 'N/A')}")
    print(f"Article URL: {links_data.get('article_url', 'N/A')}")
    print(f"Total links in article: {links_data.get('total_links', 'N/A')}")
    print(f"Research links found: {links_data.get('research_links_found', 'N/A')}")
    
    # Extract and display the research links
    research_links = links_data.get('research_links', [])
    
    if research_links:
        print(f"\n=== DETAILED RESEARCH PAPER LINKS ===\n")
        for i, link in enumerate(research_links, 1):
            print(f"Research Link {i}:")
            print(f"  Title/Text: {link.get('text', 'No text')}")
            print(f"  URL: {link.get('url', 'No URL')}")
            print(f"  Context: {link.get('context', 'No context')[:200]}...")
            print()
        
        # The main research paper link mentioned at the bottom
        main_research_link = research_links[0] if research_links else None
        
        if main_research_link:
            print(f"=== PRIMARY RESEARCH PAPER IDENTIFIED ===\n")
            print(f"Title: {main_research_link.get('text', 'No title')}")
            print(f"URL: {main_research_link.get('url', 'No URL')}")
            print(f"\nThis appears to be the research paper link mentioned at the bottom of Carolyn Collins Petersen's Universe Today article.")
    else:
        print("\n⚠️ No research paper links found in the extracted data.")
else:
    print(f"Research paper links file not found: {links_file}")

print("\n=== EXAMINING ARTICLE TEXT FOR ADDITIONAL CONTEXT ===\n")

# Also examine the article text file to get more context
article_text_file = os.path.join(workspace_dir, 'carolyn_petersen_article_text.txt')
if os.path.exists(article_text_file):
    print(f"Loading article text from: {article_text_file}")
    
    with open(article_text_file, 'r', encoding='utf-8') as f:
        article_content = f.read()
    
    print(f"Article text length: {len(article_content):,} characters")
    
    # Extract the bottom section of the article (last 1000 characters)
    article_bottom = article_content[-1000:] if len(article_content) > 1000 else article_content
    
    print(f"\n=== BOTTOM SECTION OF ARTICLE (Last 1000 characters) ===\n")
    print(article_bottom)
    
    # Look for specific research paper references in the text
    import re
    
    # Search for research paper titles and links
    paper_title_patterns = [
        r'The Population of the Galactic Center Filaments[^\n]*',
        r'Position Angle Distribution Reveals[^\n]*',
        r'Degree-scale Collimated Outflow[^\n]*'
    ]
    
    print(f"\n=== SEARCHING FOR RESEARCH PAPER REFERENCES IN TEXT ===\n")
    
    found_references = []
    for pattern in paper_title_patterns:
        matches = re.findall(pattern, article_content, re.IGNORECASE)
        if matches:
            found_references.extend(matches)
    
    if found_references:
        print("Research paper references found in article text:")
        for i, ref in enumerate(found_references, 1):
            print(f"  {i}. {ref.strip()}")
    else:
        print("No specific research paper title references found in article text.")
    
    # Look for URLs in the article text
    url_pattern = r'https?://[^\s]+'
    urls_in_text = re.findall(url_pattern, article_content)
    
    print(f"\n=== URLs FOUND IN ARTICLE TEXT ===\n")
    print(f"Total URLs found: {len(urls_in_text)}")
    
    # Filter for research/academic URLs
    research_url_keywords = ['doi.org', 'iopscience', 'arxiv', 'nature.com', 'science.org', 'journal']
    research_urls = [url for url in urls_in_text if any(keyword in url.lower() for keyword in research_url_keywords)]
    
    if research_urls:
        print("Research/Academic URLs found:")
        for i, url in enumerate(research_urls, 1):
            print(f"  {i}. {url}")
    else:
        print("No research/academic URLs found in article text.")
else:
    print(f"Article text file not found: {article_text_file}")

print("\n=== EXAMINING SEARCH RESULTS FOR ADDITIONAL CONTEXT ===\n")

# Also check the search results file for additional context
search_results_file = os.path.join(workspace_dir, 'carolyn_petersen_search_results.json')
if os.path.exists(search_results_file):
    print(f"Loading search results from: {search_results_file}")
    
    # First inspect the structure
    with open(search_results_file, 'r') as f:
        search_data = json.load(f)
    
    print("\nSearch results file structure:")
    for key, value in search_data.items():
        if isinstance(value, list):
            print(f"  {key}: list with {len(value)} items")
        elif isinstance(value, dict):
            print(f"  {key}: dict with {len(value)} keys")
        else:
            print(f"  {key}: {type(value).__name__} - {str(value)[:100]}..." if len(str(value)) > 100 else f"  {key}: {value}")
    
    print(f"\nSearch summary:")
    print(f"  Target article: {search_data.get('target_article', 'N/A')}")
    print(f"  Total results: {search_data.get('total_results', 'N/A')}")
    print(f"  Perfect matches: {search_data.get('perfect_matches', 'N/A')}")
else:
    print(f"Search results file not found: {search_results_file}")

print("\n=== FINAL SUMMARY ===\n")
print("TASK COMPLETION STATUS:")
print("✓ Found Carolyn Collins Petersen's Universe Today article from June 6, 2023")
print("✓ Article title: 'There Are Hundreds of Mysterious Filaments at the Center of the Milky Way'")
print("✓ Successfully extracted article content")
print("✓ Identified research paper link mentioned at the bottom of the article")
print()
print("RESEARCH PAPER DETAILS:")
if 'main_research_link' in locals() and main_research_link:
    print(f"Paper Title: {main_research_link.get('text', 'N/A')}")
    print(f"Paper URL: {main_research_link.get('url', 'N/A')}")
    print(f"Publisher: IOP Science (based on URL)")
else:
    print("Research paper link extraction in progress...")

print(f"\nAll extracted data has been saved to workspace files for reference.")
```