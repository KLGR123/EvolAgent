### Development Step 10: Map connections between LOTR and A Song of Ice and Fire via archived Wayback links

**Description**: Access the archived Wikipedia page for 'A Song of Ice and Fire' (book series) as it appeared at the end of July 3, 2023. Use the same Wayback Machine approach to retrieve the specific version from that date. Extract all inbound links and cross-reference them with the previously extracted outbound links from The Lord of the Rings page to identify direct connections. If no direct connection exists, systematically trace potential multi-step paths by examining intermediate pages that were linked from LOTR, particularly focusing on the high-priority fantasy/literature connections like 'International Fantasy Award', 'High fantasy', and other genre-related pages that might contain links to George R.R. Martin's work.

**Use Cases**:
- Academic comparative literature research mapping direct and multi-step Wikipedia linkages between “The Lord of the Rings” and “A Song of Ice and Fire” to quantify intertextual influences in fantasy studies
- Digital library network visualization for public libraries, automatically retrieving archived fantasy literature pages and cross-referencing inbound/outbound links to enhance catalog discovery tools
- Publishing house market analysis, extracting cross-franchise connections via archived Wikipedia snapshots to identify trending high-fantasy topics for co-marketing campaigns
- SEO and content strategy optimization for genre blogs, tracing inbound and outbound link networks around Tolkien’s and Martin’s works to target high-authority backlink opportunities
- Fan engagement platform development, building an interactive knowledge graph from archived and live Wikipedia data that shows character, award, and genre relationships across major fantasy series
- Digital preservation verification for archival teams, programmatically confirming the availability and integrity of July 2023 snapshots of key fantasy literature pages to ensure reliable historical records
- E-book retailer metadata enrichment, harvesting inbound link data from archived Wikipedia pages to automatically tag related works, awards, and genre attributes in recommendation engines

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime

print('=== ACCESSING ARCHIVED WIKIPEDIA PAGE: A SONG OF ICE AND FIRE ===\n')
print('Objective: Retrieve the Wikipedia page from around July 3, 2023 (trying multiple dates)')
print('Target URL: https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire')
print('Strategy: Try multiple dates around July 3, 2023 to find available snapshots\n')

# The URL of the Wikipedia page to retrieve
url = "https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire"

# Try multiple dates around July 3, 2023
candidate_dates = [
    "20230703",  # July 3, 2023 (original target)
    "20230704",  # July 4, 2023
    "20230702",  # July 2, 2023
    "20230705",  # July 5, 2023
    "20230701",  # July 1, 2023
    "20230706",  # July 6, 2023
    "20230630",  # June 30, 2023
    "20230710",  # July 10, 2023
    "20230625",  # June 25, 2023
    "20230715"   # July 15, 2023
]

print(f'Trying {len(candidate_dates)} different dates around July 3, 2023...')

archive_url = None
archive_date = None
working_date = None

for date in candidate_dates:
    print(f'\nTrying date: {date} ({date[:4]}-{date[4:6]}-{date[6:8]})')
    
    # Check if the webpage is available in the Wayback Machine
    api_url = f"https://archive.org/wayback/available?url={url}&timestamp={date}"
    
    try:
        avail_response = requests.get(api_url, timeout=20)
        
        if avail_response.status_code == 200:
            avail_data = avail_response.json()
            print(f'API Response: {avail_data}')
            
            if "archived_snapshots" in avail_data and "closest" in avail_data["archived_snapshots"]:
                closest = avail_data["archived_snapshots"]["closest"]
                
                if closest["available"]:
                    archive_url = closest["url"]
                    archive_date = closest["timestamp"]
                    working_date = date
                    
                    print(f'✓ SUCCESS! Found archived version:')
                    print(f'  Archive URL: {archive_url}')
                    print(f'  Archive timestamp: {archive_date}')
                    print(f'  Formatted date: {archive_date[:4]}-{archive_date[4:6]}-{archive_date[6:8]} {archive_date[8:10]}:{archive_date[10:12]}:{archive_date[12:14]}')
                    break
                else:
                    print(f'  ❌ No archived version available for this date')
            else:
                print(f'  ❌ No archived snapshots data found')
        else:
            print(f'  ❌ API Error: {avail_response.status_code}')
            
    except Exception as e:
        print(f'  ❌ Exception: {str(e)}')
        continue

if not archive_url:
    print(f'\n❌ FAILED: No archived version found for any of the candidate dates')
    print(f'Tried dates: {candidate_dates}')
    
    # Try a broader search approach
    print(f'\n=== TRYING BROADER SEARCH FOR JULY 2023 ===\n')
    
    # Try first and last day of July 2023
    broad_dates = ["20230731", "20230701", "20230715"]
    
    for date in broad_dates:
        print(f'Trying broader date: {date}')
        api_url = f"https://archive.org/wayback/available?url={url}&timestamp={date}"
        
        try:
            avail_response = requests.get(api_url, timeout=20)
            if avail_response.status_code == 200:
                avail_data = avail_response.json()
                if "archived_snapshots" in avail_data and "closest" in avail_data["archived_snapshots"]:
                    closest = avail_data["archived_snapshots"]["closest"]
                    if closest["available"]:
                        archive_url = closest["url"]
                        archive_date = closest["timestamp"]
                        working_date = date
                        print(f'✓ Found with broader search: {archive_url}')
                        break
        except Exception as e:
            print(f'Exception: {str(e)}')
            continue
    
    if not archive_url:
        print(f'\n❌ ULTIMATE FAILURE: No archived version found even with broader search')
        print(f'The A Song of Ice and Fire Wikipedia page may not have been archived around July 2023')
        exit()

print(f'\n=== DOWNLOADING ARCHIVED PAGE ===\n')

# Headers to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

try:
    print(f'Downloading archived page from: {archive_url}')
    response = requests.get(archive_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f'✓ Successfully downloaded archived page')
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    print(f'Content type: {response.headers.get("Content-Type", "unknown")}')
    
except Exception as e:
    print(f"❌ Error downloading archived page: {str(e)}")
    exit()

# Parse the HTML content
print(f'\n=== PARSING HTML CONTENT ===\n')

soup = BeautifulSoup(response.content, 'html.parser')

# Remove Wayback Machine navigation elements
print('Removing Wayback Machine navigation elements...')
for element in soup.find_all(class_=lambda x: x and 'wayback' in x.lower()):
    element.decompose()

# Remove script and style tags for cleaner text extraction
for element in soup(["script", "style"]):
    element.decompose()

# Get basic page information
title = soup.find('title')
if title:
    page_title = title.get_text().strip()
    print(f'Page Title: {page_title}')

# Find the main content area
main_content = soup.find('div', {'id': 'mw-content-text'}) or soup.find('div', {'class': 'mw-content-ltr'})
if main_content:
    print(f'✓ Found main content area')
else:
    print(f'⚠️ Main content area not found, using full page')
    main_content = soup

# Extract the page text for analysis
page_text = main_content.get_text()
lines = (line.strip() for line in page_text.splitlines())
chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
clean_text = ' '.join(chunk for chunk in chunks if chunk)

print(f'\nPage text length: {len(clean_text):,} characters')
print(f'First 500 characters: {clean_text[:500]}...')

# Extract all inbound links (links pointing TO other pages)
print(f'\n=== EXTRACTING INBOUND LINKS ===\n')

# Find all links in the main content
all_links = main_content.find_all('a', href=True)
print(f'Total links found: {len(all_links)}')

# Filter for Wikipedia article links
wikipedia_links = []
for link in all_links:
    href = link.get('href')
    if href:
        # Convert relative URLs to absolute
        if href.startswith('/'):
            href = urljoin('https://en.wikipedia.org', href)
        
        # Filter for Wikipedia article links
        if 'en.wikipedia.org/wiki/' in href and ':' not in href.split('/')[-1]:
            # Remove anchors and query parameters
            clean_href = href.split('#')[0].split('?')[0]
            
            # Get link text
            link_text = link.get_text().strip()
            
            # Extract article title from URL
            article_title = clean_href.split('/')[-1].replace('_', ' ')
            
            wikipedia_links.append({
                'url': clean_href,
                'article_title': article_title,
                'link_text': link_text,
                'original_href': link.get('href')
            })

# Remove duplicates while preserving order
seen_urls = set()
unique_links = []
for link in wikipedia_links:
    if link['url'] not in seen_urls:
        seen_urls.add(link['url'])
        unique_links.append(link)

print(f'Wikipedia article links found: {len(unique_links)}')

# Display first 20 links for verification
print(f'\nFirst 20 Wikipedia links:')
for i, link in enumerate(unique_links[:20], 1):
    print(f'{i:2d}. {link["article_title"]} -> {link["url"]}')

if len(unique_links) > 20:
    print(f'    ... and {len(unique_links) - 20} more links')

# Create comprehensive data structure
archived_page_data = {
    'extraction_info': {
        'extracted_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'source_url': url,
        'archive_url': archive_url,
        'archive_date': archive_date,
        'formatted_archive_date': f'{archive_date[:4]}-{archive_date[4:6]}-{archive_date[6:8]} {archive_date[8:10]}:{archive_date[10:12]}:{archive_date[12:14]}',
        'page_title': page_title if 'page_title' in locals() else 'Unknown',
        'content_length': len(clean_text),
        'total_links_found': len(all_links),
        'wikipedia_links_count': len(unique_links),
        'attempted_date': working_date,
        'search_strategy': 'Multiple date attempts around July 3, 2023'
    },
    'inbound_links': unique_links,
    'page_content': {
        'title': page_title if 'page_title' in locals() else 'Unknown',
        'text_preview': clean_text[:1000] + '...' if len(clean_text) > 1000 else clean_text,
        'full_text_length': len(clean_text)
    }
}

# Save the extracted data
output_file = 'workspace/asoiaf_wikipedia_archived_july_2023.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(archived_page_data, f, indent=2, ensure_ascii=False)

print(f'\n=== EXTRACTION COMPLETE ===\n')
print(f'✓ A Song of Ice and Fire Wikipedia page successfully archived and processed')
print(f'✓ Archive date: {archived_page_data["extraction_info"]["formatted_archive_date"]}')
print(f'✓ Total inbound links extracted: {len(unique_links)}')
print(f'✓ Page content length: {len(clean_text):,} characters')
print(f'✓ Data saved to: {output_file}')

# Now let's inspect the workspace to see what LOTR files we have
print(f'\n=== INSPECTING WORKSPACE FOR LOTR FILES ===\n')

# Check workspace directory
workspace_files = os.listdir('workspace')
print(f'All workspace files: {workspace_files}')

# Look for LOTR-related files
lotr_files = [f for f in workspace_files if any(keyword in f.lower() for keyword in ['lotr', 'lord', 'rings', 'tolkien'])]
print(f'LOTR-related files found: {lotr_files}')

if lotr_files:
    print(f'\n=== INSPECTING LOTR FILES ===\n')
    for lotr_file in lotr_files:
        file_path = os.path.join('workspace', lotr_file)
        file_size = os.path.getsize(file_path)
        print(f'File: {lotr_file}')
        print(f'Size: {file_size:,} bytes')
        
        # Try to inspect the structure if it's a JSON file
        if lotr_file.endswith('.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f'JSON structure - Top level keys: {list(data.keys()) if isinstance(data, dict) else "Not a dictionary"}')
                
                # Show more details about the structure
                if isinstance(data, dict):
                    for key, value in data.items():
                        if isinstance(value, list):
                            print(f'  {key}: List with {len(value)} items')
                        elif isinstance(value, dict):
                            print(f'  {key}: Dictionary with keys: {list(value.keys())}')
                        else:
                            print(f'  {key}: {type(value).__name__}')
                            
            except Exception as e:
                print(f'  Error reading JSON: {str(e)}')
        print()
        
else:
    print(f'\n⚠️ No LOTR files found in workspace')
    print(f'Will need to extract LOTR links first before cross-referencing')

print(f'\n=== SUMMARY ===\n')
print(f'✓ Successfully retrieved A Song of Ice and Fire Wikipedia page from {archived_page_data["extraction_info"]["formatted_archive_date"]}')
print(f'✓ Extracted {len(unique_links)} inbound links for cross-referencing')
print(f'✓ Page data saved to {output_file}')
print(f'✓ Ready for connection analysis with LOTR outbound links')
```