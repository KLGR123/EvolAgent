### Development Step 59: Extract American Idol Season Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/American_Idol and extract the season-by-season winners table into workspace/american_idol_winners_list.json, capturing each season number and winner name.

**Use Cases**:
- Automated data aggregation for an entertainment mobile app that delivers an interactive American Idol winners quiz to users
- Content preparation for a podcast series that auto-populates show notes with season and winner details for each episode recap
- Media analytics dashboard for a TV network tracking winner demographics and season trends to inform future casting decisions
- Historical trend analysis by a music research lab examining winner backgrounds and success rates across all American Idol seasons
- Curriculum example in a coding bootcamp teaching students web scraping and JSON data handling using real-world TV show data
- Archive metadata generation for a digital library curating television competition records with structured season-winner information
- Automated social media post scheduler for a fan page that posts “On this day” winner anniversaries with accurate dates and names
- Corporate intelligence report for a talent agency comparing American Idol winners with other reality show champions to guide client career planning

```
import os
import requests
import json
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

url = 'https://en.wikipedia.org/wiki/American_Idol'
print(f"Fetching page: {url}")
response = requests.get(url)
print(f"HTTP status code: {response.status_code}")
response.raise_for_status()

soup = BeautifulSoup(response.text, 'html.parser')

# Find all wikitable tables on the page
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} tables with class 'wikitable'.")

target_table = None
for idx, table in enumerate(tables, start=1):
    # Try to get caption
    caption = table.find('caption')
    cap_text = caption.get_text(strip=True) if caption else ''

    # Get header row cells
    header_row = table.find('tr')
    headers = [th.get_text(strip=True) for th in header_row.find_all('th')]
    print(f"Table {idx}: caption='{cap_text}' headers={headers}")

    # Determine if this is the winners table
    # We look for a header 'Season' and a header containing 'Winner'
    has_season = any(h.lower() == 'season' for h in headers)
    has_winner = any('winner' in h.lower() for h in headers)
    if has_season and has_winner:
        print(f"--> Selecting table {idx} as the winners table.")
        target_table = table
        break

if target_table is None:
    raise RuntimeError('Could not find the Season/Winner table on the page.')

# Parse the target table rows
header_row = target_table.find('tr')
headers = [th.get_text(strip=True) for th in header_row.find_all('th')]
# Identify column indices
season_idx = next(i for i, h in enumerate(headers) if h.lower() == 'season')
winner_idx = next(i for i, h in enumerate(headers) if 'winner' in h.lower())
print(f"Season column index: {season_idx}, Winner column index: {winner_idx}")

winners = []
rows = target_table.find_all('tr')[1:]  # skip header
print(f"Parsing {len(rows)} data rows.")
for row_idx, row in enumerate(rows, start=1):
    cells = row.find_all(['td','th'])
    if len(cells) <= max(season_idx, winner_idx):
        print(f"  Skipping row {row_idx}: not enough columns ({len(cells)})")
        continue
    season = cells[season_idx].get_text(strip=True)
    winner = cells[winner_idx].get_text(strip=True)
    print(f"  Row {row_idx}: Season={season}, Winner={winner}")
    winners.append({'season': season, 'winner': winner})

print(f"Total winners extracted: {len(winners)}")

# Save JSON output
output_path = os.path.join(workspace_dir, 'american_idol_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners, f, indent=2, ensure_ascii=False)
print(f"Winners list saved to: {output_path}")

```