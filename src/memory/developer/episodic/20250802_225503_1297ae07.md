### Development Step 8: Identify Oldest Closed numpy.polynomial Regression Issue and Label Addition Timestamp

**Description**: Search GitHub for numpy.polynomial issues to identify all closed issues that have the 'Regression' label. Focus on finding the oldest closed issue with this label and determine when the 'Regression' label was added to that specific issue. Use GitHub's issue search functionality with filters for repository 'numpy/numpy', path 'polynomial', status 'closed', and label 'Regression'. Extract the issue creation date, closure date, and label addition timestamp for the oldest matching issue.

**Use Cases**:
- Performance monitoring of polynomial regression functions in a quantitative trading platform to ensure recent NumPy updates didn’t introduce computational slowdowns or inaccuracies.
- Quality assurance for a scientific simulation software in aerospace engineering, auditing regression fix timelines in numpy.polynomial to confirm stability across multiple release versions.
- Open-source auditor conducting a security and stability review of numerical methods, pinpointing when the ‘Regression’ label was added to polynomial issues for compliance reporting.
- Data science team in pharmaceutical research verifying the historical integrity of polynomial fitting routines by extracting creation, closure, and label-addition dates to document reproducibility for clinical trial analyses.
- DevOps automation for a machine learning pipeline, integrating GitHub issue timeline checks to trigger alerts whenever new regressions are reported in the polynomial module impacting model training.
- Technical writer preparing detailed release notes for a computational mathematics library, using issue timelines to chronicle key regression fixes in numpy.polynomial.
- Product manager in an IoT analytics company assessing dependency risk by identifying the oldest regression issue in polynomial functions and its fix timeline to inform upgrade strategies.
- Academic software team developing a bioinformatics toolkit with polynomial approximations, tracking regression issue resolution history to support grant applications requiring evidence of software reliability.

```
import requests
import json
from datetime import datetime
import os
import time

print("=== NUMPY POLYNOMIAL REGRESSION ISSUES - SCOPING BUG COMPLETELY FIXED ===")
print("Objective: Eliminate all variable scoping issues with inline processing")
print("Focus: Find oldest closed issue with regression and polynomial content\n")

# Find workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
workspace_dir = workspace_dirs[0] if workspace_dirs else 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# GitHub API configuration
base_url = "https://api.github.com"
repo = "numpy/numpy"

headers = {
    'Accept': 'application/vnd.github.v3+json',
    'User-Agent': 'Python-GitHub-Search'
}

print(f"Using workspace directory: {workspace_dir}")
print(f"Repository: {repo}\n")

# Search for closed issues with 'regression' keyword and polynomial content
search_query = f"repo:{repo} is:issue is:closed regression polynomial"

print(f"=== TARGETED SEARCH: REGRESSION + POLYNOMIAL ISSUES ===")
print(f"Query: {search_query}")
print(f"Objective: Find oldest closed issue with regression and polynomial content\n")

search_url = f"{base_url}/search/issues"
params = {
    'q': search_query,
    'sort': 'created',  # Sort by creation date
    'order': 'asc',     # Ascending order (oldest first)
    'per_page': 100     # Get more results per page
}

print("Making GitHub API request...")
response = requests.get(search_url, headers=headers, params=params)

print(f"Response status: {response.status_code}")
if response.status_code != 200:
    print(f"Error response: {response.text}")
    exit()

search_results = response.json()
total_count = search_results['total_count']
items = search_results['items']

print(f"Total issues found: {total_count}")
print(f"Issues retrieved in this page: {len(items)}\n")

if not items:
    print("No issues found with the search criteria.")
    exit()

print("=== ANALYZING REGRESSION + POLYNOMIAL ISSUES ===")
print("Processing each issue with inline logic (no function scoping issues)...\n")

# Process each issue with completely inline logic to avoid ALL scoping issues
polynomial_regression_issues = []

for i, issue in enumerate(items, 1):
    # Get issue data safely
    title = issue.get('title', '') or ''
    body = issue.get('body', '') or ''
    
    # Convert to lowercase for comparison - inline to avoid scoping
    title_lower = title.lower()
    body_lower = body.lower()
    
    # Check polynomial relevance inline - no function calls
    poly_keywords = ['polynomial', 'poly', 'chebyshev', 'legendre', 'hermite', 'laguerre']
    is_poly_related = False
    for keyword in poly_keywords:
        if keyword in title_lower or keyword in body_lower:
            is_poly_related = True
            break
    
    # Check regression keyword inline - no function calls
    has_regression = 'regression' in title_lower or 'regression' in body_lower
    
    print(f"{i}. Issue #{issue['number']}: {title[:80]}...")
    print(f"   Created: {issue['created_at']}")
    print(f"   Closed: {issue.get('closed_at', 'N/A')}")
    print(f"   State: {issue['state']}")
    print(f"   Labels: {[label['name'] for label in issue.get('labels', [])]}")
    print(f"   Polynomial-related: {is_poly_related}")
    print(f"   Has regression keyword: {has_regression}")
    print(f"   URL: {issue['html_url']}")
    
    # Store all issues (since they already match our search criteria)
    issue_data = {
        'number': issue['number'],
        'title': title,
        'created_at': issue['created_at'],
        'closed_at': issue.get('closed_at'),
        'state': issue['state'],
        'labels': [label['name'] for label in issue.get('labels', [])],
        'html_url': issue['html_url'],
        'api_url': issue['url'],
        'is_polynomial_related': is_poly_related,
        'has_regression': has_regression,
        'body_preview': body[:500] if body else '',
        'relevance_score': (2 if is_poly_related else 0) + (1 if has_regression else 0)
    }
    polynomial_regression_issues.append(issue_data)
    print()

print(f"=== ANALYSIS SUMMARY ===")
print(f"Total issues analyzed: {len(items)}")
print(f"All issues stored (matched search criteria): {len(polynomial_regression_issues)}\n")

# Sort by creation date to find the oldest
polynomial_regression_issues.sort(key=lambda x: x['created_at'])

print("=== OLDEST ISSUES (sorted by creation date) ===")
for i, issue in enumerate(polynomial_regression_issues[:10], 1):  # Show top 10 oldest
    print(f"{i}. Issue #{issue['number']}: {issue['title'][:60]}...")
    print(f"   Created: {issue['created_at']}")
    print(f"   Closed: {issue['closed_at']}")
    print(f"   Labels: {issue['labels']}")
    print(f"   Polynomial: {issue['is_polynomial_related']}, Regression: {issue['has_regression']}")
    print(f"   Relevance Score: {issue['relevance_score']}")
    print(f"   URL: {issue['html_url']}")
    print()

# Identify the oldest issue
oldest_issue = polynomial_regression_issues[0]
print(f"=== OLDEST ISSUE IDENTIFIED ===")
print(f"Issue #{oldest_issue['number']}: {oldest_issue['title']}")
print(f"Created: {oldest_issue['created_at']}")
print(f"Closed: {oldest_issue['closed_at']}")
print(f"Current labels: {oldest_issue['labels']}")
print(f"Polynomial-related: {oldest_issue['is_polynomial_related']}")
print(f"Has regression: {oldest_issue['has_regression']}")
print(f"API URL: {oldest_issue['api_url']}")

# Analyze labels across all issues - inline processing
print(f"\n=== LABEL ANALYSIS ===")
all_labels = set()
regression_labeled_issues = []

for issue in polynomial_regression_issues:
    # Add labels to the set
    for label in issue['labels']:
        all_labels.add(label)
    
    # Check for regression-related labels inline
    regression_labels = []
    for label in issue['labels']:
        if 'regression' in label.lower() or 'regress' in label.lower():
            regression_labels.append(label)
    
    if regression_labels:
        regression_labeled_issues.append({
            'issue': issue,
            'regression_labels': regression_labels
        })

print(f"All unique labels found: {sorted(list(all_labels))}")
print(f"Issues with regression-related labels: {len(regression_labeled_issues)}")

if regression_labeled_issues:
    print("\nIssues with regression-related labels:")
    for item in regression_labeled_issues:
        issue = item['issue']
        print(f"  Issue #{issue['number']}: {issue['title'][:50]}...")
        print(f"    Regression labels: {item['regression_labels']}")
        print(f"    Created: {issue['created_at']}")
        print()
else:
    print("\nNo issues found with explicit 'Regression' labels.")
    print("This suggests we need to check issue timelines to see when labels were added.")

# Save comprehensive results
results_data = {
    'search_timestamp': datetime.now().isoformat(),
    'search_query': search_query,
    'repository': repo,
    'total_issues_found': total_count,
    'issues_analyzed': len(items),
    'all_issues': polynomial_regression_issues,
    'oldest_issue': oldest_issue,
    'unique_labels_found': sorted(list(all_labels)),
    'regression_labeled_issues_count': len(regression_labeled_issues),
    'regression_labeled_issues': regression_labeled_issues,
    'next_action': 'Get detailed timeline for oldest issue to find when Regression label was added'
}

with open(f'{workspace_dir}/numpy_polynomial_regression_complete_analysis.json', 'w') as f:
    json.dump(results_data, f, indent=2)

print(f"\nComprehensive analysis saved to: {workspace_dir}/numpy_polynomial_regression_complete_analysis.json")
print("\n=== READY FOR NEXT STEP ===")
print("Next step: Get detailed timeline/events for the oldest issue to determine when 'Regression' label was added")
print(f"Target issue for timeline analysis: #{oldest_issue['number']}")
print(f"Target issue title: {oldest_issue['title']}")
print(f"Target issue API URL: {oldest_issue['api_url']}")
```