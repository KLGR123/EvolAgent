### Development Step 14: Find Journal Named for Hreidmarâ€™s Son and Midkiffâ€™s June 2014 Dragon Depictions Article

**Description**: Search for information about Emily Midkiff's June 2014 article, focusing on identifying the journal named after one of Hreidmar's sons from Norse mythology. Research Hreidmar's sons (Fafnir, Regin, and Otr) to determine which one has a journal named after them, then locate Midkiff's specific article from June 2014 that discusses dragon depictions and contains quotes from two different authors expressing distaste.

**Use Cases**:
- Automated extraction of bibliographic metadata and quotes from Emily Midkiffâ€™s June 2014 dragon depiction article for a universityâ€™s digital humanities repository
- Periodic monitoring and archiving of new fiction and folklore analyses in the Fafnir journal for a specialist librarianâ€™s indexing workflow
- Integration of targeted author mentions and contextual excerpts into a Norse mythology research dashboard used by graduate students studying Hreidmarâ€™s lineage
- Auto-generation of structured summaries and direct quotations for a content curator managing an online dragon imagery exhibition in a museumâ€™s web portal
- Scheduled scraping and sentiment tagging of dragon artifact reviews cited in academic journals for a cultural studies research group tracking mythical creature discourse
- Real-time alert system for a fantasy literature podcast producer to detect when key authors like Midkiff publish new critical essays on dragon symbolism
- Bulk harvesting of author quote contexts and publication details for a scholarly publisherâ€™s citation database that powers in-house reference tools

```
import os
import json
import requests
from bs4 import BeautifulSoup

print('=== FAFNIR JOURNAL RESEARCH - SYNTAX FIXED ===') 
print('Objective: Find Emily Midkiff June 2014 article about dragon depictions')
print('Strategy: Fix all syntax errors and complete systematic navigation\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# The Fafnir journal URL
fafnir_journal_url = 'https://fafnir.journal.fi/'
print(f'Target URL: {fafnir_journal_url}')
print('âœ“ CONFIRMED: Fafnir - Nordic Journal of Science Fiction and Fantasy Research')
print('âœ“ Named after Fafnir, the dragon son of Hreidmar from Norse mythology')
print('âœ“ Hreidmar\'s sons: Fafnir (dragon), Regin, Otr - Fafnir has journal named after him\n')

# Set up headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

print('=== STEP 1: ACCESSING FAFNIR JOURNAL HOMEPAGE ===')
print()

try:
    print(f'Making request to: {fafnir_journal_url}')
    response = requests.get(fafnir_journal_url, headers=headers, timeout=30)
    
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    
    if response.status_code == 200:
        # Save the homepage
        homepage_path = os.path.join(workspace, 'fafnir_journal_homepage.html')
        with open(homepage_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f'âœ“ Homepage saved to: {homepage_path}')
        
        # Parse the homepage
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'\nPage title: {page_title.get_text().strip()}')
        
        print('\n=== STEP 2: SEARCHING HOMEPAGE FOR KEY TERMS ===')
        print()
        
        # Search homepage text for key terms
        homepage_text = soup.get_text().lower()
        
        key_terms = {
            '2014': homepage_text.count('2014'),
            'midkiff': homepage_text.count('midkiff'),
            'emily': homepage_text.count('emily'),
            'june': homepage_text.count('june'),
            'dragon': homepage_text.count('dragon')
        }
        
        print('Key term occurrences on homepage:')
        for term, count in key_terms.items():
            if count > 0:
                print(f'  {term}: {count} occurrences')
        
        print('\n=== STEP 3: FINDING NAVIGATION LINKS ===')
        print()
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Found {len(all_links)} total links on homepage')
        
        # Look for archive/navigation patterns
        navigation_keywords = ['archive', 'archives', 'issues', 'volumes', '2014', 'browse', 'past', 'current']
        
        relevant_links = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                full_url = f'https://fafnir.journal.fi{href}'
            elif not href.startswith('http'):
                full_url = f'https://fafnir.journal.fi/{href}'
            else:
                full_url = href
            
            # Check if link is relevant
            text_lower = text.lower()
            href_lower = href.lower()
            
            relevance_score = 0
            matched_keywords = []
            
            for keyword in navigation_keywords:
                if keyword in text_lower:
                    relevance_score += 2
                    matched_keywords.append(f'text:{keyword}')
                if keyword in href_lower:
                    relevance_score += 1
                    matched_keywords.append(f'url:{keyword}')
            
            if relevance_score > 0 and len(text) > 2:
                relevant_links.append({
                    'url': full_url,
                    'text': text[:100],
                    'score': relevance_score,
                    'keywords': matched_keywords,
                    'has_2014': '2014' in text_lower or '2014' in href_lower
                })
        
        # Sort by relevance score
        relevant_links.sort(key=lambda x: x['score'], reverse=True)
        
        print(f'Found {len(relevant_links)} relevant navigation links:')
        for i, link in enumerate(relevant_links[:10], 1):
            print(f'  {i}. Score {link["score"]}: "{link["text"]}"')
            print(f'     URL: {link["url"]}')
            print(f'     Keywords: {link["keywords"]}')
            if link['has_2014']:
                print('     *** CONTAINS 2014 REFERENCE ***')
            print()
        
        # Try to access the most promising link
        if relevant_links:
            # Prioritize links with 2014 references
            priority_links = [link for link in relevant_links if link['has_2014']]
            if not priority_links:
                priority_links = relevant_links[:3]  # Take top 3 by score
            
            target_link = priority_links[0]
            
            print(f'=== STEP 4: ACCESSING PRIORITY NAVIGATION LINK ===')
            print()
            print(f'Target: "{target_link["text"]}" (Score: {target_link["score"]})')
            print(f'URL: {target_link["url"]}')
            print(f'Keywords matched: {target_link["keywords"]}')
            
            try:
                print('\nMaking request to navigation page...')
                nav_response = requests.get(target_link['url'], headers=headers, timeout=30)
                print(f'Navigation page status: {nav_response.status_code}')
                
                if nav_response.status_code == 200:
                    # Save navigation page
                    nav_path = os.path.join(workspace, 'fafnir_navigation_page.html')
                    with open(nav_path, 'w', encoding='utf-8') as f:
                        f.write(nav_response.text)
                    print(f'âœ“ Navigation page saved to: {nav_path}')
                    
                    # Parse navigation page
                    nav_soup = BeautifulSoup(nav_response.content, 'html.parser')
                    nav_text = nav_soup.get_text().lower()
                    
                    print('\n=== STEP 5: ANALYZING NAVIGATION PAGE ===')
                    print()
                    
                    # Count key terms on navigation page
                    nav_key_terms = {
                        '2014': nav_text.count('2014'),
                        'midkiff': nav_text.count('midkiff'),
                        'emily': nav_text.count('emily'),
                        'june': nav_text.count('june'),
                        'dragon': nav_text.count('dragon')
                    }
                    
                    print('Key terms on navigation page:')
                    for term, count in nav_key_terms.items():
                        if count > 0:
                            print(f'  {term}: {count} occurrences')
                    
                    # Look for 2014 and Midkiff related links
                    nav_links = nav_soup.find_all('a', href=True)
                    target_links = []
                    
                    for link in nav_links:
                        href = link.get('href', '')
                        text = link.get_text().strip()
                        
                        # Convert to absolute URL
                        if href.startswith('/'):
                            full_url = f'https://fafnir.journal.fi{href}'
                        elif not href.startswith('http'):
                            full_url = f'https://fafnir.journal.fi/{href}'
                        else:
                            full_url = href
                        
                        # Check for target terms
                        text_lower = text.lower()
                        href_lower = href.lower()
                        
                        target_terms = ['2014', 'june', 'midkiff', 'emily', 'dragon']
                        matched_terms = []
                        
                        for term in target_terms:
                            if term in text_lower or term in href_lower:
                                matched_terms.append(term)
                        
                        if matched_terms and len(text) > 3:
                            target_links.append({
                                'url': full_url,
                                'text': text[:150],
                                'matched_terms': matched_terms,
                                'has_midkiff': 'midkiff' in matched_terms,
                                'has_2014': '2014' in matched_terms,
                                'has_june': 'june' in matched_terms
                            })
                    
                    if target_links:
                        print(f'\n*** FOUND {len(target_links)} TARGET LINKS ***')
                        for i, link in enumerate(target_links[:5], 1):
                            print(f'  {i}. "{link["text"]}"')
                            print(f'     URL: {link["url"]}')
                            print(f'     Matched terms: {link["matched_terms"]}')
                            if link['has_midkiff']:
                                print('     *** CONTAINS MIDKIFF ***')
                            if link['has_june'] and link['has_2014']:
                                print('     *** CONTAINS JUNE 2014 ***')
                            print()
                        
                        # Prioritize links with Midkiff, then June+2014, then 2014
                        best_link = None
                        for link in target_links:
                            if link['has_midkiff']:
                                best_link = link
                                break
                        
                        if not best_link:
                            for link in target_links:
                                if link['has_june'] and link['has_2014']:
                                    best_link = link
                                    break
                        
                        if not best_link:
                            for link in target_links:
                                if link['has_2014']:
                                    best_link = link
                                    break
                        
                        if not best_link:
                            best_link = target_links[0]
                        
                        print(f'=== STEP 6: ACCESSING BEST TARGET LINK ===')
                        print()
                        print(f'Selected: "{best_link["text"]}"')
                        print(f'URL: {best_link["url"]}')
                        print(f'Matched terms: {best_link["matched_terms"]}')
                        
                        try:
                            print('\nMaking request to target page...')
                            target_response = requests.get(best_link['url'], headers=headers, timeout=30)
                            print(f'Target page status: {target_response.status_code}')
                            
                            if target_response.status_code == 200:
                                # Save target page
                                target_path = os.path.join(workspace, 'fafnir_target_page.html')
                                with open(target_path, 'w', encoding='utf-8') as f:
                                    f.write(target_response.text)
                                print(f'âœ“ Target page saved to: {target_path}')
                                
                                # Parse target page
                                target_soup = BeautifulSoup(target_response.content, 'html.parser')
                                target_text = target_soup.get_text().lower()
                                
                                print('\n=== STEP 7: SEARCHING TARGET PAGE FOR EMILY MIDKIFF ===')
                                print()
                                
                                # Count key terms on target page
                                target_key_terms = {
                                    'midkiff': target_text.count('midkiff'),
                                    'emily': target_text.count('emily'),
                                    'june': target_text.count('june'),
                                    'dragon': target_text.count('dragon'),
                                    'depiction': target_text.count('depiction'),
                                    'distaste': target_text.count('distaste')
                                }
                                
                                print('Key terms on target page:')
                                for term, count in target_key_terms.items():
                                    if count > 0:
                                        print(f'  {term}: {count} occurrences')
                                
                                if target_key_terms['midkiff'] > 0:
                                    print('\nðŸŽ¯ *** EMILY MIDKIFF FOUND ON TARGET PAGE! ***')
                                    
                                    # Extract contexts around Midkiff mentions
                                    full_target_text = target_soup.get_text()
                                    midkiff_contexts = []
                                    
                                    # Find all positions of 'midkiff' in the text
                                    search_text = full_target_text.lower()
                                    start_pos = 0
                                    
                                    while True:
                                        pos = search_text.find('midkiff', start_pos)
                                        if pos == -1:
                                            break
                                        
                                        # Extract context around the match
                                        context_start = max(0, pos - 300)
                                        context_end = min(len(full_target_text), pos + 400)
                                        context = full_target_text[context_start:context_end].strip()
                                        midkiff_contexts.append(context)
                                        start_pos = pos + 1
                                    
                                    print(f'Found {len(midkiff_contexts)} Midkiff contexts:')
                                    for i, context in enumerate(midkiff_contexts, 1):
                                        print(f'\n{i}. Context around Midkiff mention:')
                                        print(f'   ...{context[:200]}...')
                                    
                                    # Look for quotes in the text (simple approach)
                                    if target_key_terms['dragon'] > 0:
                                        print('\n*** DRAGON CONTENT FOUND - SEARCHING FOR QUOTES ***')
                                        
                                        # Simple quote extraction without complex regex
                                        text_content = target_response.text
                                        
                                        # Look for quoted text using simple string operations
                                        quote_candidates = []
                                        
                                        # Split text by quote marks and look for substantial content
                                        parts = text_content.split('"')
                                        for i in range(1, len(parts), 2):  # Every other part should be quoted
                                            quote = parts[i].strip()
                                            if 30 <= len(quote) <= 300:  # Reasonable quote length
                                                quote_candidates.append(quote)
                                        
                                        # Also try smart quotes
                                        smart_parts = text_content.split('"')
                                        for part in smart_parts:
                                            if '"' in part:
                                                quote = part.split('
```