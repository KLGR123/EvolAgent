### Development Step 58: Find and download University of Leicester paper “Can Hiccup Supply Enough Fish…” to workspace/dragon_diet_fish_bag.pdf

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic librarians automating retrieval of targeted University of Leicester journal articles for building an open-access repository
- Data scientists scraping and downloading fishery ecology research PDFs to feed into model training pipelines for species population predictions
- Environmental consultants gathering dragon diet impact studies from academic websites to support biodiversity assessment reports
- Market intelligence analysts automatically collecting industry whitepapers in PDF format when keyword-searching for emerging aquaculture technologies
- Policy researchers aggregating full-text government and university PDFs on sustainable fishing practices for legislative review and policy drafting
- Technical writers harvesting product specification manuals in PDF from manufacturer portals via search redirects for inclusion in a central knowledge base
- Investigative journalists batch-downloading court and university research documents in PDF to verify claims and cite sources in in-depth news stories

```
import os
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)
html_path     = os.path.join(workspace, 'ddg_search.html')
pdf_list_path = os.path.join(workspace, 'ddg_pdf_urls.txt')
pdf_out_path  = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

# Verify we have the DuckDuckGo HTML
if not os.path.exists(html_path):
    print(f"[ERROR] Missing DuckDuckGo HTML at {html_path}. Run the fetch step first.")
    sys.exit(1)

print(f"[INFO] Parsing DuckDuckGo HTML from: {html_path}")
with open(html_path, 'r', encoding='utf-8') as f:
    soup = BeautifulSoup(f, 'html.parser')

# Step 1: Extract and decode redirect URLs (uddg) from <a class="result__a">
anchors = soup.find_all('a', class_='result__a')
print(f"[INFO] Found {len(anchors)} <a class='result__a'> anchors. Decoding uddg parameters...")

decoded_urls = []
for a in anchors:
    href = a.get('href', '')
    if not href:
        continue
    # Build full redirect URL
    if href.startswith('//'):
        redirect = 'https:' + href
    elif href.startswith('/'):
        redirect = 'https://duckduckgo.com' + href
    else:
        redirect = href

    # Parse out uddg parameter
    parsed = urlparse(redirect)
    qs = parse_qs(parsed.query)
    uddg_vals = qs.get('uddg')
    if not uddg_vals:
        continue
    decoded = unquote(uddg_vals[0])

    # Filter for our expected PDF endpoints (University of Leicester Journal downloads)
    if 'journals.le.ac.uk' in decoded and ('/download/' in decoded or decoded.lower().endswith('.pdf')):
        if decoded not in decoded_urls:
            decoded_urls.append(decoded)
            print(f"[FOUND] PDF candidate: {decoded}")

# Step 2: Save the filtered PDF URLs
if not decoded_urls:
    print("[ERROR] No PDF candidates found in results. Exiting.")
    sys.exit(1)

with open(pdf_list_path, 'w', encoding='utf-8') as out:
    for url in decoded_urls:
        out.write(url + '\n')
print(f"[SAVE] Extracted PDF URLs -> {pdf_list_path}")

# Step 3: Download the first valid PDF

def download_pdf(url, dest_path):
    headers = {'User-Agent': 'Mozilla/5.0'}
    print(f"[INFO] Attempting download from: {url}")
    try:
        r = requests.get(url, headers=headers, stream=True, timeout=60)
        print(f"[INFO] HTTP {r.status_code}, Content-Type: {r.headers.get('Content-Type')}")
    except Exception as e:
        print(f"[ERROR] Download failed: {e}")
        return False

    if r.status_code == 200:
        content_type = r.headers.get('Content-Type', '').lower()
        # Accept PDF or generic binary streams
        if 'pdf' in content_type or 'octet-stream' in content_type:
            total = 0
            with open(dest_path, 'wb') as f:
                for chunk in r.iter_content(8192):
                    if chunk:
                        f.write(chunk)
                        total += len(chunk)
            print(f"[SUCCESS] Saved PDF -> {dest_path} ({total} bytes)")
            return total > 10240  # ensure file >10 KB
        else:
            print(f"[WARN] Unexpected Content-Type ({content_type}). Skipping.")
            return False
    else:
        print(f"[WARN] HTTP {r.status_code} downloading PDF. Skipping.")
        return False

# Try each candidate URL
for url in decoded_urls:
    if download_pdf(url, pdf_out_path):
        print("[INFO] Download completed successfully. Exiting.")
        sys.exit(0)

print("[ERROR] All download attempts failed. See workspace/ddg_pdf_urls.txt for alternatives.")
sys.exit(1)

```