### Development Step 34: Extract Survivor US Seasons 1–44 winners into workspace/survivor_winners_list.json

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated content update for a reality TV fan site: scrape the Survivor winners list each season to refresh the “Past Champions” section without manual edits.
- Data journalism for entertainment reporting: extract season-by-season winners to analyze demographic trends and publish an interactive infographic on a news outlet.
- Machine learning feature pipeline in media analytics: incorporate historical Survivor winner names as a categorical input when training a model to predict future reality-show outcomes.
- Business intelligence dashboard for a TV network: integrate the JSON of Survivor winners with viewership and advertising revenue data to identify which champions drove audience spikes.
- Academic study on gender and diversity in reality television: gather winner names across 44 seasons, link to demographic profiles, and run statistical tests for a communications research paper.
- Podcast episode automation for Survivor recaps: pull the latest winner’s name from Wikipedia into show notes and social posts immediately after finale night.
- QA and regression testing for a mobile trivia app: validate the in-app Survivor winners database against the freshly scraped JSON to catch content drift or missing entries.
- Social media marketing campaign analysis: correlate announcement dates from the scraped winners list with Twitter hashtag volume to gauge fan engagement over time.

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch Survivor Wikipedia page
page_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/wiki/{page_title}"
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers=headers)
resp.raise_for_status()
print(f"Page fetched successfully (status {resp.status_code})\n")

# 3) Parse HTML
doc = BeautifulSoup(resp.text, 'html.parser')

# 4) Locate "Winners" section and its first following table
print("Locating 'Winners' section table...\n")
target = None
span = doc.find('span', {'id': 'Winners'})
if span and span.parent:
    # traverse siblings after the <h2> containing this span
    for sib in span.parent.next_siblings:
        if sib.name == 'table':
            target = sib
            print("→ Selected winners-only table from 'Winners' section\n")
            break

# 5) Fallback: scan all wikitable tables if not found
if not target:
    print("Could not find by section id; falling back to scanning .wikitable tables...\n")
    wikitables = doc.find_all('table', class_=lambda c: c and 'wikitable' in c)
    for idx, tbl in enumerate(wikitables, 1):
        # check if header row has exactly Season & Winner
        first = tbl.find('tr')
        if not first:
            continue
        hdrs = [th.get_text(strip=True).lower() for th in first.find_all(['th','td'], recursive=False)]
        if hdrs == ['season', 'winner']:
            target = tbl
            print(f"→ Fallback: selected simple 2-col table #{idx} with headers {hdrs}\n")
            break

if not target:
    print("❌ Could not find any suitable winners table. Exiting.")
    sys.exit(1)

# 6) Determine column indices
first = target.find('tr')
cols = [c.get_text(strip=True).lower() for c in first.find_all(['th','td'], recursive=False)]
season_idx = cols.index('season')
winner_idx = cols.index('winner')
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 7) Extract season→winner pairs
winners = []
for row in target.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    s = cells[season_idx].get_text(strip=True)
    if not s.isdigit():
        continue
    season = int(s)
    if not (1 <= season <= 44):
        continue
    cell = cells[winner_idx]
    a = cell.find('a')
    if a and re.search(r'[A-Za-z]', a.get_text()):
        name = a.get_text(strip=True)
    else:
        name = cell.get_text(strip=True)
    print(f"Parsed Season {season} → Winner: '{name}'")
    winners.append({'season': season, 'winner': name})

# 8) Sort and verify
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Verify table structure.")

# 9) Save to JSON
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {out_file}")

```