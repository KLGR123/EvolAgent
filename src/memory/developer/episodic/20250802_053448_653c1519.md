### Development Step 6: Locate and Download Leicester’s “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic research automation: a biology lab automates fetching PDFs of targeted journal articles by title to build a local literature database for gene-editing studies.
- Legal case management: a law firm auto-downloads court decision PDFs by case name and jurisdiction into a document management system for precedent analysis.
- Financial compliance monitoring: a banking compliance team retrieves the latest regulatory guideline PDFs (e.g., Basel III updates) by title into an audit workspace for risk assessment.
- Patent portfolio maintenance: a patent attorney’s office pulls official patent specification PDFs from USPTO/EPO using patent titles or numbers to update its in-house patent tracking system.
- Environmental data aggregation: an ecology research group collects government-issued PDF reports (e.g., environmental impact assessments) by title for inclusion in a geospatial analysis pipeline.
- Market intelligence gathering: a competitive intelligence team scrapes vendor whitepaper PDFs by product name from corporate websites to enrich a product feature comparison database.
- E-learning content updates: an online education platform auto-downloads module-specific PDF lecture notes by course title to sync its LMS content for new semesters.
- Data journalism archiving: an investigative newsroom archives municipal budget and policy PDF documents by exact title into a searchable repository for data-driven reporting.

```
import os
import sys
import time
import requests
from urllib.parse import urlparse, parse_qs, unquote

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("ERROR: BeautifulSoup4 is required. Install with 'pip install beautifulsoup4'.")
    sys.exit(1)

# 1. Prepare workspace directory
def ensure_workspace(dir_name='workspace'):
    os.makedirs(dir_name, exist_ok=True)
    return dir_name
workspace_dir = ensure_workspace()
print(f"[INFO] Using workspace directory: {workspace_dir}\n")

# 2. Define paper title and query
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?"
query = f'"{title}" University of Leicester filetype:pdf'
print(f"[INFO] Search query: {query}\n")

# 3. Manual override URL via environment variable
pdf_link = os.getenv('DRAGON_PDF_URL')
if pdf_link:
    print(f"[OVERRIDE] Using DRAGON_PDF_URL: {pdf_link}\n")

# 4. Attempt SerpAPI search (if no override and key present)
api_key = os.getenv('SERPAPI_API_KEY')
if not pdf_link and api_key:
    print("[SerpAPI] Starting primary search...")
    params = {
        'engine': 'google',
        'q': query,
        'api_key': api_key,
        'num': 10,
        'google_domain': 'google.com',
        'safe': 'off'
    }
    for attempt in range(1, 4):
        print(f"  Attempt {attempt}/3...")
        try:
            resp = requests.get('https://serpapi.com/search.json', params=params, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            for idx, item in enumerate(data.get('organic_results', []), 1):
                link = item.get('link', '')
                print(f"    [#{idx}] {link}")
                low = link.lower()
                if low.endswith('.pdf') or '.pdf?' in low:
                    pdf_link = link
                    print(f"    -> Selected PDF from SerpAPI: {pdf_link}\n")
                    break
            if pdf_link:
                break
        except requests.exceptions.HTTPError as e:
            code = e.response.status_code if e.response else None
            if code == 429:
                backoff = 2 ** (attempt - 1)
                print(f"    429 rate limit. Backing off {backoff}s...")
                time.sleep(backoff)
                continue
            else:
                print(f"    HTTPError {code}: {e}")
                break
        except Exception as e:
            print(f"    Error: {e}")
            break
    else:
        print("  SerpAPI exhausted.\n")

# 5. Fallback: DuckDuckGo HTML + decode uddg redirects
if not pdf_link:
    print("---\n[Fallback] DuckDuckGo HTML + uddg decode\n---")
    ddg_resp = requests.get(
        'https://duckduckgo.com/html/',
        params={'q': query},
        headers={'User-Agent': 'Mozilla/5.0'},
        timeout=30
    )
    ddg_resp.raise_for_status()
    soup = BeautifulSoup(ddg_resp.text, 'html.parser')
    candidates = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        # decode uddg redirect
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                real = unquote(q[0])
                if '.pdf' in real.lower():
                    candidates.append(real)
        # direct .pdf
        elif href.lower().endswith('.pdf'):
            candidates.append(href)
    candidates = list(dict.fromkeys(candidates))
    if candidates:
        for i, u in enumerate(candidates, 1):
            print(f"    {i}. {u}")
        pdf_link = candidates[0]
        print(f"    -> Selected PDF from DuckDuckGo fallback: {pdf_link}\n")

# 6. Fallback: Bing HTML search parsing
if not pdf_link:
    print("---\n[Fallback] Bing HTML search for PDF\n---")
    bing_url = 'https://www.bing.com/search'
    bing_params = {'q': query}
    bing_resp = requests.get(bing_url, params=bing_params,
                             headers={'User-Agent': 'Mozilla/5.0'},
                             timeout=30)
    bing_resp.raise_for_status()
    soup = BeautifulSoup(bing_resp.text, 'html.parser')
    bing_links = []
    for li in soup.select('li.b_algo h2 a'):
        u = li.get('href')
        if u and u.lower().endswith('.pdf'):
            bing_links.append(u)
    bing_links = list(dict.fromkeys(bing_links))
    if bing_links:
        for i, u in enumerate(bing_links, 1):
            print(f"    {i}. {u}")
        pdf_link = bing_links[0]
        print(f"    -> Selected PDF from Bing fallback: {pdf_link}\n")

# 7. Abort if still no link
if not pdf_link:
    print("ERROR: Unable to locate any .pdf URL via SerpAPI, DuckDuckGo, or Bing.\n"+
          "Please set the DRAGON_PDF_URL environment variable to the direct PDF link and retry.")
    sys.exit(1)

# 8. Download the PDF
pdf_path = os.path.join(workspace_dir, 'dragon_diet_fish_bag.pdf')
print(f"[DOWNLOAD] Fetching PDF from: {pdf_link}\n  Saving to: {pdf_path}\n")
headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/pdf'}
total = 0
with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as dl:
    dl.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in dl.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                total += len(chunk)
print(f"[DONE] Download complete: {total} bytes ({total/1024/1024:.2f} MB)")
if total < 10_000:
    print("WARNING: File size <10 KB; may be incomplete.")
else:
    print("SUCCESS: PDF appears valid.")

```