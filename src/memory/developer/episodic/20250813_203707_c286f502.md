### Development Step 3: Title: Identify US Survivor Winners Born in May Using Official Winner Lists and Birth Dates (Season 1–2023)

**Description**: Search for comprehensive information about US Survivor winners and their birth dates. Focus on finding official sources such as CBS Survivor databases, Wikipedia pages, or entertainment databases that list all winners from Season 1 through the most recent season available as of August 2023. Extract winner names, seasons, and birth dates to identify any winners born in May.

**Use Cases**:
- Entertainment journalists compiling a feature on Survivor winners with May birthdays for a special anniversary article
- Academic researchers conducting demographic studies on reality TV show winners, focusing on age and birth month trends
- TV show producers developing themed Survivor reunion episodes based on shared winner traits, such as birth month
- Data analysts at entertainment networks automating the extraction and validation of winner statistics for audience insights dashboards
- Fans creating interactive Survivor trivia games that include questions about winners’ birth dates and seasons
- Digital archivists maintaining up-to-date, structured databases of reality show winners for use in media encyclopedias and wikis

```
import os
import sys
import json
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time

# 1) Use existing workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Load the previously saved HTML page
html_path = os.path.join(workspace_dir, 'survivor_main_page.html')
if not os.path.exists(html_path):
    print("❌ survivor_main_page.html not found. Need to fetch page first.")
    sys.exit(1)

print("Loading previously saved Survivor main page HTML...")
with open(html_path, 'r', encoding='utf-8') as f:
    html_content = f.read()

# 3) Parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')
print("HTML parsed successfully\n")

# 4) Find the winners table and examine its structure carefully
print("Examining table structure in detail...")
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} wikitable(s)\n")

if len(tables) == 0:
    print("❌ No wikitables found")
    sys.exit(1)

# Let's examine the first few tables more carefully
for table_idx, table in enumerate(tables[:3]):
    print(f"=== TABLE {table_idx + 1} DETAILED ANALYSIS ===")
    
    # Get headers
    header_row = table.find('tr')
    if not header_row:
        print("No header row found")
        continue
        
    headers = []
    for cell in header_row.find_all(['th', 'td']):
        header_text = cell.get_text(strip=True)
        headers.append(header_text)
    
    print(f"Headers: {headers}")
    print(f"Number of columns: {len(headers)}")
    
    # Examine first 5 data rows in detail
    data_rows = table.find_all('tr')[1:6]  # Skip header, get first 5 data rows
    print(f"\nFirst 5 data rows:")
    
    for row_idx, row in enumerate(data_rows):
        cells = row.find_all(['td', 'th'])
        print(f"\n  Row {row_idx + 1} ({len(cells)} cells):")
        
        for cell_idx, cell in enumerate(cells):
            cell_text = cell.get_text(strip=True)
            # Check if cell contains a link
            link = cell.find('a')
            link_href = link.get('href', '') if link else ''
            
            print(f"    Col {cell_idx} ({headers[cell_idx] if cell_idx < len(headers) else 'N/A'}): '{cell_text}' {f'[LINK: {link_href}]' if link_href else ''}")
    
    print("\n" + "="*60 + "\n")

# 5) Based on the analysis, let's identify the correct table and columns
print("\n=== SELECTING CORRECT TABLE AND EXTRACTING WINNERS ===")

# Use the first table but let's be more careful about data extraction
winners_table = tables[0]
header_row = winners_table.find('tr')
headers = [cell.get_text(strip=True) for cell in header_row.find_all(['th', 'td'])]

print(f"Using table with headers: {headers}")

# Find column indices
try:
    season_idx = headers.index('Season')
    winner_idx = headers.index('Winner')
    runner_up_idx = headers.index('Runner(s)-up') if 'Runner(s)-up' in headers else -1
    print(f"Column indices -> Season: {season_idx}, Winner: {winner_idx}, Runner-up: {runner_up_idx}")
except ValueError as e:
    print(f"❌ Could not find required columns: {e}")
    sys.exit(1)

# 6) Extract winners more carefully
print("\nExtracting winners with improved logic...")
winners = []
rows = winners_table.find_all('tr')[1:]  # Skip header row

for i, row in enumerate(rows):
    cells = row.find_all(['td', 'th'])
    if len(cells) <= max(season_idx, winner_idx):
        print(f"  Skipping row {i+1}: insufficient columns ({len(cells)})")
        continue
    
    # Extract season number
    season_cell = cells[season_idx]
    season_text = season_cell.get_text(strip=True)
    
    # Handle season numbers that might have footnotes
    season_match = re.search(r'(\d+)', season_text)
    if not season_match:
        print(f"  Skipping row {i+1}: no valid season number in '{season_text}'")
        continue
    
    season_num = int(season_match.group(1))
    
    # Extract winner name more carefully
    winner_cell = cells[winner_idx]
    
    # Remove any sup tags (footnotes) before extracting text
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    
    # Look for a link first (more reliable)
    winner_link = winner_cell.find('a')
    if winner_link:
        winner_name = winner_link.get_text(strip=True)
        winner_wiki_link = winner_link.get('href', '')
        if winner_wiki_link.startswith('/'):
            winner_wiki_link = 'https://en.wikipedia.org' + winner_wiki_link
    else:
        winner_name = winner_cell.get_text(strip=True)
        winner_wiki_link = ''
    
    # Clean up winner name (remove any remaining footnotes)
    winner_name = re.sub(r'\[.*?\]', '', winner_name).strip()
    
    # Skip if winner name looks like a vote count or is empty
    if not winner_name or re.match(r'^\d+[–-]\d+([–-]\d+)?$', winner_name):
        print(f"  Skipping row {i+1}: invalid winner name '{winner_name}'")
        continue
    
    # Skip if season number is unreasonable (Survivor US has ~47 seasons as of 2023)
    if season_num < 1 or season_num > 50:
        print(f"  Skipping row {i+1}: unreasonable season number {season_num}")
        continue
    
    winner_info = {
        'season': season_num,
        'winner': winner_name,
        'wiki_link': winner_wiki_link
    }
    winners.append(winner_info)
    print(f"  Season {season_num:2d}: {winner_name}")
    if winner_wiki_link:
        print(f"    Link: {winner_wiki_link}")

print(f"\nExtracted {len(winners)} winners total\n")

# 7) Sort winners by season and validate
winners_sorted = sorted(winners, key=lambda x: x['season'])

# 8) Save the corrected winners list
winners_path = os.path.join(workspace_dir, 'survivor_winners_corrected.json')
with open(winners_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Saved corrected winners list to: {winners_path}")

# 9) Display summary and validation
print(f"\n=== CORRECTED WINNERS EXTRACTION SUMMARY ===")
print(f"Total winners extracted: {len(winners_sorted)}")
if winners_sorted:
    print(f"Season range: {min(w['season'] for w in winners_sorted)} - {max(w['season'] for w in winners_sorted)}")
    print(f"Winners with Wikipedia links: {sum(1 for w in winners_sorted if w['wiki_link'])}")
    print(f"Winners without Wikipedia links: {sum(1 for w in winners_sorted if not w['wiki_link'])}")

# 10) Show all winners for verification
print(f"\nAll extracted winners:")
for w in winners_sorted:
    link_status = "✓" if w['wiki_link'] else "✗"
    print(f"  Season {w['season']:2d}: {w['winner']} [{link_status}]")

print(f"\nNext step: Fetch individual winner Wikipedia pages to extract birth dates")
```