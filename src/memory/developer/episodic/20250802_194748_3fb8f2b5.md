### Development Step 31: Locate Shared Critical Term in Emily Midkiff’s June 2014 Fafnir Dragon Depiction Critique

**Description**: Access Emily Midkiff's June 2014 article in the Fafnir journal to extract the full text and identify the specific word that was quoted from two different authors expressing distaste for the nature of dragon depictions. Parse the article content systematically to locate quotes from multiple authors that contain the same critical word about dragon portrayals.

**Use Cases**:
- Comparative literary analysis for fantasy scholars: automate extraction of identical critical terms across multiple author quotes in Emily Midkiff’s dragon studies to demonstrate shifts in negative descriptors and support thematic research papers.
- Digital humanities archiving: systematically scrape and preserve full texts from Fafnir’s 2014 issues, then tag and index repeated terminology on dragon portrayals to enable cross‐issue keyword searches in a university’s online repository.
- Automated citation database enrichment: extract inline quotations about dragon depictions from academic articles, identify overlapping critique words, and populate a reference management system (e.g., Zotero or EndNote) with precise context and author attribution.
- Mythology blog content generation: pull standardized disdainful descriptors of dragons from Midkiff and other scholars, then repurpose these quotes in a niche blog post comparing historical and modern critiquing language about mythical creatures.
- Sentiment trend analysis for literary journals: track and quantify usage frequency of a specific negative adjective applied to dragons over time across multiple volumes of Fafnir, visualizing shifts in scholarly tone about fantasy imagery.
- AI training data compilation for genre critique: build a labeled corpus of critical quotes on dragon imagery by scraping Emily Midkiff’s work and peer authors, fueling NLP models that can detect and classify disdainful commentary in new fantasy scholarship.
- Humanities-driven automated reporting: integrate the scraping and quote‐matching pipeline into a departmental dashboard that alerts medieval studies faculty when two or more historians use the same pejorative term for dragons, flagging potential discussions on critical consensus.

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin
import time
import re

print('=== EMILY MIDKIFF ARTICLE EXTRACTION - SYNTAX FIXED ===\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Target URL for Fafnir 2/2014 issue
target_issue_url = 'https://journal.finfar.org/journal/archive/fafnir-22014/'
print(f'Target issue: Fafnir 2/2014')
print(f'URL: {target_issue_url}\n')

# Headers to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

print('=== STEP 1: ACCESSING FAFNIR 2/2014 ISSUE PAGE ===')

try:
    print(f'Requesting: {target_issue_url}')
    issue_response = requests.get(target_issue_url, headers=headers, timeout=30)
    print(f'Response status: {issue_response.status_code}')
    print(f'Content length: {len(issue_response.content):,} bytes')
    print(f'Content type: {issue_response.headers.get("Content-Type", "unknown")}\n')
    
    if issue_response.status_code == 200:
        # Parse the HTML content
        soup = BeautifulSoup(issue_response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'Page title: {page_title.get_text().strip()}')
        
        # Extract all text content for analysis
        page_text = soup.get_text()
        print(f'Total page text length: {len(page_text):,} characters\n')
        
        # Confirm this page contains Emily Midkiff
        if 'midkiff' in page_text.lower():
            print('✓ Confirmed: Page contains "Midkiff"')
            
            # Find the exact context where Midkiff appears
            midkiff_indices = []
            text_lower = page_text.lower()
            start = 0
            while True:
                index = text_lower.find('midkiff', start)
                if index == -1:
                    break
                midkiff_indices.append(index)
                start = index + 1
            
            print(f'Found "Midkiff" at {len(midkiff_indices)} positions in the text')
            
            # Show context around each occurrence
            for i, index in enumerate(midkiff_indices, 1):
                context_start = max(0, index - 150)
                context_end = min(len(page_text), index + 150)
                context = page_text[context_start:context_end].replace('\n', ' ').strip()
                print(f'\nOccurrence {i} context:')
                print(f'...{context}...')
        else:
            print('⚠ Warning: "Midkiff" not found in page text')
        
        print('\n=== STEP 2: EXTRACTING ALL ARTICLE LINKS FROM THE ISSUE PAGE ===')
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Total links found on page: {len(all_links)}')
        
        # Filter links that might be articles
        potential_article_links = []
        
        for link in all_links:
            href = link.get('href')
            text = link.get_text().strip()
            
            # Skip empty links or navigation links
            if not href or not text:
                continue
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                href = urljoin('https://journal.finfar.org', href)
            elif not href.startswith('http'):
                href = urljoin(target_issue_url, href)
            
            # Look for links that might be articles (contain meaningful text)
            if len(text) > 10 and not any(nav_word in text.lower() for nav_word in ['home', 'archive', 'about', 'contact', 'menu', 'navigation', 'search']):
                potential_article_links.append({
                    'text': text,
                    'url': href,
                    'has_midkiff': 'midkiff' in text.lower()
                })
        
        print(f'Potential article links found: {len(potential_article_links)}')
        
        # Show all potential article links
        print('\n--- All Potential Article Links ---')
        for i, link in enumerate(potential_article_links, 1):
            marker = '*** MIDKIFF ***' if link['has_midkiff'] else ''
            print(f'{i:2d}. {marker}')
            print(f'    Text: {link["text"][:100]}...' if len(link['text']) > 100 else f'    Text: {link["text"]}')
            print(f'    URL:  {link["url"]}')
            print()
        
        # Find Emily Midkiff's specific article
        midkiff_links = [link for link in potential_article_links if link['has_midkiff']]
        
        if midkiff_links:
            print(f'=== FOUND {len(midkiff_links)} MIDKIFF ARTICLE LINK(S) ===')
            
            # Use the first Midkiff link (should be the main one)
            target_article = midkiff_links[0]
            print(f'Selected article:')
            print(f'Title: {target_article["text"]}')
            print(f'URL: {target_article["url"]}\n')
            
            print('=== STEP 3: ACCESSING EMILY MIDKIFF\'S ARTICLE ===')
            
            try:
                print(f'Accessing article: {target_article["url"]}')
                article_response = requests.get(target_article['url'], headers=headers, timeout=30)
                print(f'Article response status: {article_response.status_code}')
                print(f'Article content length: {len(article_response.content):,} bytes\n')
                
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.content, 'html.parser')
                    
                    # Get article title from the page
                    article_title_elem = article_soup.find('title')
                    if article_title_elem:
                        article_title = article_title_elem.get_text().strip()
                        print(f'Article page title: {article_title}')
                    
                    # Remove scripts, styles, and navigation elements
                    for element in article_soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
                        element.decompose()
                    
                    # Try multiple selectors to find the main article content
                    content_selectors = [
                        '.article-content',
                        '.article-body', 
                        '.entry-content',
                        '.post-content',
                        '.content',
                        'main',
                        '#content',
                        '.text',
                        'article'
                    ]
                    
                    article_content = None
                    used_selector = None
                    
                    for selector in content_selectors:
                        content_elem = article_soup.select_one(selector)
                        if content_elem:
                            article_content = content_elem.get_text()
                            used_selector = selector
                            print(f'✓ Article content extracted using selector: {selector}')
                            break
                    
                    if not article_content:
                        # Fallback to full page text
                        article_content = article_soup.get_text()
                        used_selector = 'full_page_fallback'
                        print('Using full page text as fallback')
                    
                    # Clean up the extracted text
                    lines = (line.strip() for line in article_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))
                    clean_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    print(f'✓ Cleaned article text: {len(clean_content):,} characters\n')
                    
                    # Save the full article text
                    article_text_file = os.path.join(workspace, 'midkiff_fafnir_article_full_text.txt')
                    with open(article_text_file, 'w', encoding='utf-8') as f:
                        f.write(f'Title: {target_article["text"]}\n')
                        f.write(f'URL: {target_article["url"]}\n')
                        f.write(f'Extraction method: {used_selector}\n')
                        f.write(f'Extracted: {time.strftime("%Y-%m-%d %H:%M:%S")}\n')
                        f.write('=' * 80 + '\n\n')
                        f.write(clean_content)
                    
                    print(f'✓ Full article text saved to: {article_text_file}')
                    
                    # Save raw HTML for backup
                    article_html_file = os.path.join(workspace, 'midkiff_fafnir_article_raw.html')
                    with open(article_html_file, 'w', encoding='utf-8') as f:
                        f.write(article_response.text)
                    
                    print(f'✓ Raw article HTML saved to: {article_html_file}\n')
                    
                    print('=== STEP 4: ANALYZING ARTICLE FOR DRAGON CRITICISM QUOTES ===')
                    
                    # PROPERLY FORMATTED quote patterns - FIXED syntax errors
                    quote_patterns = [
                        r'"([^"]{15,400})"',  # Standard double quotes
                        r'"([^"]{15,400})
```