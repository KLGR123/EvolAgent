### Development Step 45: Extract Survivor (US) Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated media database enrichment for a streaming service: ingest the Survivor winners list JSON into the content catalog to tag and filter episodes by champion for personalized recommendations.
- Reality TV diversity research and statistical analysis for academic publications: extract season winners data to analyze demographic trends (gender, age, background) across 44 seasons.
- Continuous integration for a fan-run Survivor wiki: auto-refresh the champions page by importing the JSON whenever Wikipedia updates the winners table, eliminating manual edits.
- Social media marketing automation for a Survivor fan page: schedule “On this day” posts and trivia tweets by feeding the winners list into a scheduler that highlights past champions.
- Interactive TV trivia chatbot integration on Discord or Slack: load the JSON winners data to power Q&A sessions, random champion quizzes, and leaderboard tracking.
- Business intelligence reporting for a media analytics firm: correlate the winners timeline against viewership and advertising rates to advise networks on season performance.
- Content licensing and rights management automation: merge the winners JSON into an internal system that tracks talent contracts, ensuring all champion appearances are correctly licensed.

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching page: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
})
resp.raise_for_status()
soup = BeautifulSoup(resp.text, 'html.parser')

# 3) Find the <span id="Winners"> heading and select the first <table> after it
print("Locating the Winners table under the 'Winners' section...\n")
span_winners = soup.find('span', id='Winners')
if not span_winners:
    print("❌ Could not find a <span id='Winners'> element. Page structure may have changed.")
    sys.exit(1)
# Find its heading tag (likely <h2>)
heading = span_winners.find_parent(['h2','h3','h4'])
if not heading:
    print("❌ Could not find the parent heading for the Winners section.")
    sys.exit(1)
# Iterate siblings until we hit a table
target_table = None
for sib in heading.find_next_siblings():
    if sib.name in ['h2','h3','h4']:
        # Reached next section without finding table
        break
    if sib.name == 'table':
        target_table = sib
        print("→ Found table immediately after the 'Winners' heading.\n")
        break
if not target_table:
    print("❌ Could not find a table immediately after the 'Winners' heading.")
    sys.exit(1)

# 4) Verify the header row contains exactly two <th> cells: Season and Winner
header_tr = target_table.find('tr')
if not header_tr:
    print("❌ Winners table has no header row.\n")
    sys.exit(1)
th_cells = header_tr.find_all('th', recursive=False)
headers = [th.get_text(strip=True) for th in th_cells]
print(f"Detected header cells: {headers}\n")
if [h.lower() for h in headers] != ['season','winner']:
    print("❌ Table header does not match ['Season','Winner']. Found: {headers}")
    sys.exit(1)

# 5) Extract Season→Winner pairs for seasons 1 through 44
winners = []
for row in target_table.find_all('tr')[1:]:  # skip header row
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) < 2:
        continue
    # Season number
    season_txt = cells[0].get_text(strip=True)
    m = re.match(r"^(\d+)", season_txt)
    if not m:
        continue
    season = int(m.group(1))
    if not (1 <= season <= 44):
        continue
    # Winner name
    winner_cell = cells[1]
    links = winner_cell.find_all('a')
    if links:
        names = [a.get_text(strip=True) for a in links if re.search(r"[A-Za-z]", a.get_text())]
        winner_name = ' & '.join(names) if names else winner_cell.get_text(strip=True)
    else:
        winner_name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season} → Winner: '{winner_name}'")
    winners.append({'season': season, 'winner': winner_name})

# 6) Sort and verify count
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: Extracted count != 44. Verify the Winners table structure.")

# 7) Save to workspace/survivor_winners_list.json
output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {output_path}")
```