### Development Step 4: Year and Gold Medalist Birth Countries of Cortina d'Ampezzo Championships Without Finland or Norway

**Description**: Search for information about FIS Nordic World Ski Championships held in Cortina d'Ampezzo that did not include participants from Finland or Norway. Focus on identifying which specific year this championship took place, then find the complete list of gold medalists from that event. Extract the birth countries of all gold medalists to identify which country produced a gold medalist at this particular championship.

**Use Cases**:
- Sports analytics firm automating identification of FIS Nordic World Ski Championship editions missing Finland or Norway to flag anomalies and extract gold medalists’ birth countries for performance trend reports
- National ski federation validating historical archives by programmatically confirming years without Finnish/Norwegian participants and compiling the exact list of medal winners and their countries of origin
- Sports museum curators building a digital exhibit on Cortina d’Ampezzo championships by scraping Wikipedia entries to isolate the unique edition without Finland or Norway and constructing medalist biographies
- Data-driven journalism workflow that rapidly collects event editions with major participant absences and visualizes gold medalists’ birth countries in an interactive infographic
- Academic research project on geopolitical impacts in winter sports, using automated scripts to detect championship years lacking Nordic powerhouses and analyze shifts in medalist nationalities
- Betting and odds-making platform integrating historical anomalies—championships without Finland/Norway—and medalist origin data into predictive models to fine-tune ski event forecasts
- Travel and sports tourism operator designing themed heritage ski tours by pinpointing the Cortina d’Ampezzo championship year with no Finnish or Norwegian competitors and profiling its gold medalists’ home countries

```
import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== FIS NORDIC WORLD SKI CHAMPIONSHIPS RESEARCH (VARIABLE SCOPE FIXED) ===\n")
print("Objective: Find Cortina d'Ampezzo championship WITHOUT Finland or Norway participants")
print("Then: Extract all gold medalists and their birth countries\n")

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# First, let's search for information about FIS Nordic World Ski Championships in Cortina d'Ampezzo
print("Step 1: Searching for FIS Nordic World Ski Championships in Cortina d'Ampezzo...\n")

# Key search URLs - start with Wikipedia which usually has comprehensive championship data
search_urls = [
    "https://en.wikipedia.org/wiki/FIS_Nordic_World_Ski_Championships",
    "https://en.wikipedia.org/wiki/Cortina_d%27Ampezzo",
    "https://en.wikipedia.org/wiki/1956_FIS_Nordic_World_Ski_Championships",
    "https://en.wikipedia.org/wiki/1941_FIS_Nordic_World_Ski_Championships"
]

successful_sources = []
failed_sources = []

for url in search_urls:
    print(f"Accessing: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=20)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            # Save the content
            filename = url.split('/')[-1].replace('%27', '_').replace('%', '_') + '.html'
            filepath = f'workspace/{filename}'
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # Quick content analysis - CRITICAL FIX: Parse content first, THEN analyze
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find('title')
            title_text = title.get_text().strip() if title else 'No title found'
            
            # CRITICAL FIX: Get text content FIRST before any analysis
            page_text = soup.get_text().lower()
            
            # NOW analyze the content using the properly defined variable
            cortina_mentions = page_text.count("cortina")
            championship_indicators = ['championship', 'gold medal', 'winner', 'result']
            has_championship_info = any(indicator in page_text for indicator in championship_indicators)
            
            # Look for Finland/Norway mentions to identify potential exclusions
            finland_mentions = page_text.count('finland')
            norway_mentions = page_text.count('norway')
            
            successful_sources.append({
                'url': url,
                'title': title_text,
                'filename': filepath,
                'cortina_mentions': cortina_mentions,
                'has_championship_info': has_championship_info,
                'finland_mentions': finland_mentions,
                'norway_mentions': norway_mentions,
                'content_length': len(response.text)
            })
            
            print(f"  ✓ Title: {title_text}")
            print(f"  ✓ Cortina mentions: {cortina_mentions}")
            print(f"  ✓ Championship info: {has_championship_info}")
            print(f"  ✓ Finland mentions: {finland_mentions}")
            print(f"  ✓ Norway mentions: {norway_mentions}")
            print(f"  ✓ Content length: {len(response.text)} characters")
            print(f"  ✓ Saved to: {filepath}\n")
            
        else:
            failed_sources.append({'url': url, 'status': response.status_code})
            print(f"  ✗ Failed - Status: {response.status_code}\n")
            
    except Exception as e:
        failed_sources.append({'url': url, 'error': str(e)})
        print(f"  ✗ Error: {str(e)}\n")
    
    time.sleep(2)  # Be respectful to servers

print(f"=== INITIAL SEARCH RESULTS ===\n")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources\n")

# Analyze the most promising sources
if successful_sources:
    print("=== ANALYZING SUCCESSFUL SOURCES ===\n")
    
    # Prioritize sources with high Cortina mentions and championship info
    priority_sources = sorted(successful_sources, 
                            key=lambda x: (x['cortina_mentions'], x['has_championship_info']), 
                            reverse=True)
    
    for i, source in enumerate(priority_sources, 1):
        print(f"{i}. {source['url']}")
        print(f"   Title: {source['title']}")
        print(f"   Cortina mentions: {source['cortina_mentions']}")
        print(f"   Championship info: {source['has_championship_info']}")
        print(f"   Finland/Norway mentions: {source['finland_mentions']}/{source['norway_mentions']}")
        
        if source['cortina_mentions'] > 0 and source['has_championship_info']:
            print(f"   *** HIGH PRIORITY SOURCE ***")
        print()
    
    # Now let's examine the content of high-priority sources in detail
    print("=== DETAILED CONTENT ANALYSIS ===\n")
    
    for idx, source in enumerate(priority_sources[:3], 1):  # Analyze top 3 sources
        print(f"Analyzing: {source['url']}\n")
        
        with open(source['filename'], 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Look for specific years when championships were held in Cortina
        print("Searching for Cortina d'Ampezzo championship years...")
        
        # Find all text mentioning Cortina and extract surrounding context
        full_text = soup.get_text()
        cortina_contexts = []
        
        # Split text into sentences and find those mentioning Cortina
        sentences = re.split(r'[.!?]', full_text)
        for sentence in sentences:
            if 'cortina' in sentence.lower():
                cortina_contexts.append(sentence.strip())
        
        print(f"Found {len(cortina_contexts)} sentences mentioning Cortina:\n")
        
        for j, context in enumerate(cortina_contexts[:10], 1):  # Show first 10
            print(f"{j}. {context}")
            
            # Look for years in this context
            years = re.findall(r'\b(19\d{2}|20\d{2})\b', context)
            if years:
                print(f"   Years found: {years}")
            
            # Look for Finland/Norway absence indicators
            if 'finland' not in context.lower() and 'norway' not in context.lower():
                print(f"   *** NO FINLAND/NORWAY MENTIONED - POTENTIAL TARGET ***")
            print()
        
        # Look for tables that might contain championship results
        tables = soup.find_all('table')
        print(f"Found {len(tables)} tables in this source")
        
        # Analyze tables for championship data
        championship_tables = []
        for table_idx, table in enumerate(tables):
            table_text = table.get_text().lower()
            
            # Check if table contains championship/medal information
            medal_indicators = ['gold', 'medal', 'winner', 'champion', 'result']
            has_medal_info = any(indicator in table_text for indicator in medal_indicators)
            
            # Check for Cortina mention in table
            has_cortina = 'cortina' in table_text
            
            # Check for Finland/Norway absence
            has_finland = 'finland' in table_text
            has_norway = 'norway' in table_text
            
            if has_medal_info or has_cortina:
                championship_tables.append({
                    'table_index': table_idx,
                    'has_medal_info': has_medal_info,
                    'has_cortina': has_cortina,
                    'has_finland': has_finland,
                    'has_norway': has_norway,
                    'table_element': table
                })
                
                print(f"  Table {table_idx}: Medal info={has_medal_info}, Cortina={has_cortina}, Finland={has_finland}, Norway={has_norway}")
        
        print(f"Found {len(championship_tables)} tables with championship/medal information\n")
        
        # Save detailed analysis for this source
        source_analysis = {
            'url': source['url'],
            'cortina_contexts': cortina_contexts,
            'championship_tables_count': len(championship_tables),
            'total_tables': len(tables),
            'championship_table_details': [
                {
                    'table_index': ct['table_index'],
                    'has_medal_info': ct['has_medal_info'],
                    'has_cortina': ct['has_cortina'],
                    'has_finland': ct['has_finland'],
                    'has_norway': ct['has_norway']
                } for ct in championship_tables
            ]
        }
        
        analysis_filename = f'workspace/cortina_analysis_{idx}.json'
        with open(analysis_filename, 'w') as f:
            json.dump(source_analysis, f, indent=2)
        
        print(f"Detailed analysis saved to: {analysis_filename}\n")
        print("-" * 50 + "\n")
else:
    print("No sources successfully accessed. Need to try alternative search methods.\n")

# Save overall research summary
research_summary = {
    'research_target': 'FIS Nordic World Ski Championships in Cortina d\'Ampezzo without Finland/Norway',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'successful_sources': len(successful_sources),
    'failed_sources': len(failed_sources),
    'sources_data': successful_sources,
    'failed_attempts': failed_sources,
    'next_steps': [
        'Identify specific year of Cortina championship without Finland/Norway',
        'Extract complete gold medalist list from that championship',
        'Research birth countries of all gold medalists'
    ]
}

with open('workspace/cortina_research_summary.json', 'w') as f:
    json.dump(research_summary, f, indent=2)

print("=== PHASE 1 COMPLETE ===\n")
print(f"Research summary saved to: workspace/cortina_research_summary.json")
print(f"Successfully gathered {len(successful_sources)} sources for analysis")
print("Next phase: Deep analysis to identify the specific championship year and gold medalists")
```