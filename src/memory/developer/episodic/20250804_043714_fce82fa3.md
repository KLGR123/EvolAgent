### Development Step 4: Extract Survivor US Seasons 1–44 Winners from Wikipedia and Save to JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automate the creation of a Survivor fan website’s winner timeline, pulling updated season–winner data immediately after each finale airs
- Feed historical winner data into a data science pipeline to analyze trends in player demographics and build predictive models for future champions
- Populate a mobile trivia app with accurate season–winner pairs, enabling automatic generation and rotation of Survivor-themed quiz questions
- Integrate the winner list JSON into a social media bot that schedules “On this day” Survivor winner anniversary posts across Twitter and Facebook
- Provide a data foundation for an entertainment analytics report comparing gender, age, and geographic distribution of Reality TV winners from 2000 to 2023
- Support targeted marketing by embedding accurate Survivor winner details into personalized email campaigns for merchandise and subscription services
- Supply a training dataset for an AI chatbot tasked with answering fan questions about Survivor seasons, winners, and historical stats
- Enable event planners to compile Survivor winner leaderboards for themed viewing parties, fan competitions, and charity trivia nights

```
import os
import json
import requests
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# Fetch the Survivor page
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
print(f"Fetching page: {url}")
response = requests.get(url)
response.raise_for_status()
print("Page fetched successfully, parsing HTML...")

soup = BeautifulSoup(response.text, 'html.parser')

# DEBUG: List all section headlines to find the exact id/text
print("=== Inspecting section headlines ===")
for span in soup.find_all('span', class_='mw-headline'):
    hid = span.get('id') or ''
    text = span.get_text(strip=True)
    print(f"  id='{hid}' → '{text}'")
print("==================================\n")

# Try to locate the "Winners by season" section via its headline text
heading_span = soup.find(
    'span',
    class_='mw-headline',
    string=lambda s: s and s.strip().lower() == 'winners by season'
)
if not heading_span:
    raise RuntimeError("Could not find the 'Winners by season' headline. Check the id/text printed above.")
print(f"Found heading span id='{heading_span.get('id')}', text='{heading_span.get_text(strip=True)}'")

# The table is the first wikitable after this heading
heading_tag = heading_span.find_parent(['h2', 'h3', 'h4'])
winners_table = heading_tag.find_next('table', class_='wikitable')
if not winners_table:
    raise RuntimeError("Could not find the wikitable following the 'Winners by season' heading.")
print("Located the 'Winners by season' table. Parsing rows...")

# Parse the table rows
rows = winners_table.find_all('tr')
print(f"Total rows including header: {len(rows)}")
data_rows = rows[1:]  # skip the header row
winners = []

for row in data_rows:
    # Only direct children to respect any colspan/rowspan elsewhere
    cells = row.find_all(['th', 'td'], recursive=False)
    if len(cells) < 2:
        print("Skipping a row: fewer than 2 cells")
        continue
    # Season is in first cell
    season_cell = cells[0]
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    season_text = season_cell.get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"Skipping row: cannot parse season='{season_text}'")
        continue
    if not (1 <= season_num <= 44):
        print(f"Skipping season {season_num}: out of target range 1–44")
        continue
    # Winner is in second cell
    winner_cell = cells[1]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    winner_name = winner_cell.get_text(strip=True)

    print(f"Parsed Season {season_num}: Winner -> {winner_name}")
    winners.append({'season': season_num, 'winner': winner_name})

# Sort and save
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"Total winners extracted for seasons 1–44: {len(winners_sorted)}")

output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Winners list saved to: {output_path}")
```