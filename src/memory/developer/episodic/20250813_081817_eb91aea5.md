### Development Step 6: Abel Hugo’s Napoleonic service, Spanish camp maps, Madrid hospital link, and reigning monarch context

**Description**: Research Abel Hugo, Victor Hugo's brother, focusing on his military service during the Napoleonic Wars and his time in Madrid. Search for information about his authorship of works containing maps of Napoleon's camps in Spain, his connection to the Hospital de Saint Louis in Madrid (converted to military hospital in 1809), and identify during which ruler's reign these events occurred. Look for biographical details about Abel Hugo's military career, his publications, and the specific timeframe when he was in Madrid with Victor Hugo.

**Use Cases**:
- Napoleonic war historian automating extraction of Abel Hugo’s military service timeline and Spanish campaign details from Wikipedia and Google Books to build a digital research database
- Museum exhibit curator compiling references to the Hospital de Saint Louis in Madrid and its 1809 conversion into a military hospital for an interactive historical display
- Historical wargame developer sourcing authentic maps of Napoleon’s camps in Spain and troop deployment data for realistic Peninsular War scenario design
- Academic genealogist tracing the Hugo family lineage by aggregating biographical details on Abel and Victor Hugo from multilingual wiki pages and archival books for a family tree database
- Digital humanities researcher conducting keyword-driven analysis of 19th-century military publications to study authorship patterns and thematic trends across multiple sources
- Publisher verifying authorship, publication dates, and metadata of obscure 1809 military memoirs and map volumes before digitizing them into an online historical archive
- Travel writer building a heritage guide to Napoleonic-era Madrid by extracting geolocation data and historical context on military hospitals and camp sites from aggregated sources
- Online education platform integrating automated Wikipedia parsing and Google Books searches to generate multimedia lesson modules on the Peninsular War, complete with maps, timelines, and primary-source excerpts

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Abel Hugo research with ultra-simplified Wikipedia parsing...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Initialize results storage
all_results = []
search_errors = []

print("\n=== ULTRA-SIMPLIFIED WIKIPEDIA SEARCH FOR ABEL HUGO ===")

# Wikipedia URLs to search
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Abel_Hugo",
    "https://fr.wikipedia.org/wiki/Abel_Hugo",
    "https://en.wikipedia.org/wiki/Victor_Hugo",
    "https://en.wikipedia.org/wiki/Napoleonic_Wars",
    "https://en.wikipedia.org/wiki/Peninsular_War",
    "https://en.wikipedia.org/wiki/Madrid"
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

for url in wikipedia_urls:
    try:
        print(f"\nFetching: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        print(f"Page title: {title}")
        
        # Extract main content - ULTRA-SIMPLIFIED APPROACH
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove scripts and styles only - no complex filtering
            for script in content_div.find_all('script'):
                script.decompose()
            for style in content_div.find_all('style'):
                style.decompose()
            
            # Get text content directly without complex filtering
            content = content_div.get_text(separator=' ', strip=True)
            print(f"Raw content length: {len(content)} characters")
            
            # Target keywords for Abel Hugo research
            target_keywords = [
                'abel hugo', 'victor hugo', 'napoleonic wars', 'madrid', 'spain', 
                'military service', 'military hospital', 'hospital',
                'napoleon', 'camps', 'maps', 'publications', 'author', 'brother',
                '1809', 'peninsular war', 'french army', 'officer', 'career',
                'joseph bonaparte', 'ferdinand vii', 'saint louis', 'saint-louis'
            ]
            
            # Find matching keywords
            content_lower = content.lower()
            found_keywords = []
            for keyword in target_keywords:
                if keyword in content_lower:
                    found_keywords.append(keyword)
            
            print(f"Keywords found: {', '.join(found_keywords)}")
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content[:6000],  # First 6000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                all_results.append(result)
                print(f"✓ Added result with {len(found_keywords)} keyword matches")
                
                # Special analysis for Abel Hugo content
                if 'abel hugo' in content_lower:
                    print("\n*** ABEL HUGO CONTENT DETECTED ***")
                    
                    # Find Abel Hugo mentions in context
                    abel_contexts = []
                    content_parts = content.split('.')
                    
                    for i, part in enumerate(content_parts):
                        part_lower = part.lower().strip()
                        if 'abel hugo' in part_lower and len(part_lower) > 15:
                            # Get context (current + next sentence)
                            context = part.strip()
                            if i + 1 < len(content_parts):
                                context += '. ' + content_parts[i + 1].strip()
                            abel_contexts.append(context)
                    
                    print(f"Found {len(abel_contexts)} Abel Hugo context mentions:")
                    for j, context in enumerate(abel_contexts[:4], 1):
                        print(f"  {j}. {context[:400]}...")
                    
                    # Look for key information patterns
                    military_found = any(term in content_lower for term in ['military', 'army', 'officer', 'service', 'war'])
                    madrid_found = 'madrid' in content_lower
                    napoleon_found = 'napoleon' in content_lower
                    hospital_found = 'hospital' in content_lower
                    
                    print(f"\n  Key information detected:")
                    print(f"    Military context: {military_found}")
                    print(f"    Madrid context: {madrid_found}")
                    print(f"    Napoleon context: {napoleon_found}")
                    print(f"    Hospital context: {hospital_found}")
            
        time.sleep(2)  # Be respectful to Wikipedia
        
    except Exception as e:
        error_msg = f"Error fetching {url}: {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

# Load existing Google Books results
print(f"\n=== LOADING EXISTING RESEARCH DATA ===")
existing_files = [
    'abel_hugo_final_research.json', 
    'abel_hugo_complete_research.json',
    'abel_hugo_final_comprehensive_research.json'
]

existing_books = []
for filename in existing_files:
    filepath = f'workspace/{filename}'
    if os.path.exists(filepath):
        print(f"Loading from: {filename}")
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            results = data.get('all_results', [])
            books = [r for r in results if r.get('source') == 'Google Books']
            existing_books.extend(books)
            print(f"  Loaded {len(books)} Google Books results")
            
        except Exception as e:
            print(f"  Error loading {filename}: {str(e)}")

# Remove duplicates
seen_titles = set()
unique_books = []
for book in existing_books:
    title = book.get('title', '')
    if title and title not in seen_titles:
        seen_titles.add(title)
        unique_books.append(book)

print(f"Unique Google Books results: {len(unique_books)}")

# Combine all results
all_results.extend(unique_books)
all_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print(f"\n=== CONDUCTING ADDITIONAL TARGETED SEARCHES ===")

# Try Google Books API for more specific Abel Hugo searches
specific_queries = [
    "Abel Hugo France Spain 1809",
    "Abel Hugo military Madrid hospital",
    "Abel Hugo Napoleon Bonaparte maps",
    "Abel Hugo Victor Hugo brother army"
]

for query in specific_queries:
    try:
        print(f"\nSearching Google Books: {query}")
        api_url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': query,
            'maxResults': 8,
            'printType': 'books',
            'langRestrict': 'en'
        }
        
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            print(f"Found {len(data['items'])} books")
            
            for item in data['items']:
                volume_info = item.get('volumeInfo', {})
                title = volume_info.get('title', 'Unknown Title')
                authors = volume_info.get('authors', ['Unknown Author'])
                description = volume_info.get('description', '')
                
                # Check if this is a new result
                if title not in seen_titles:
                    # Analyze for relevance
                    book_text = (title + ' ' + ' '.join(authors) + ' ' + description).lower()
                    
                    abel_keywords = [
                        'abel hugo', 'napoleon', 'madrid', 'spain', 'military', 'maps',
                        'hospital', '1809', 'peninsular war', 'victor hugo', 'brother',
                        'napoleonic', 'french army', 'officer', 'joseph bonaparte'
                    ]
                    
                    found_keywords = [kw for kw in abel_keywords if kw in book_text]
                    
                    if len(found_keywords) >= 2:
                        new_result = {
                            'title': title,
                            'authors': authors,
                            'description': description,
                            'keywords_found': found_keywords,
                            'source': 'Google Books',
                            'search_query': query,
                            'relevance_score': len(found_keywords)
                        }
                        all_results.append(new_result)
                        seen_titles.add(title)
                        print(f"  ✓ Added: {title} (Score: {len(found_keywords)})")
        
        time.sleep(1)
        
    except Exception as e:
        error_msg = f"Error searching Google Books '{query}': {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

# Final analysis
print(f"\n=== COMPREHENSIVE ABEL HUGO ANALYSIS ===")

# Re-sort all results
all_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print(f"Total results: {len(all_results)}")
print(f"Wikipedia results: {len([r for r in all_results if r['source'] == 'Wikipedia'])}")
print(f"Google Books results: {len([r for r in all_results if r['source'] == 'Google Books'])}")

# Analyze for Abel Hugo specific information
abel_analysis = {
    'military_service': [],
    'madrid_connections': [],
    'napoleon_maps_camps': [],
    'hospital_references': [],
    'publications_authorship': [],
    'timeframe_1809': [],
    'ruler_context': [],
    'family_relations': [],
    'key_biographical_sources': []
}

for result in all_results:
    # Get text for analysis
    if 'content' in result:
        text = result['content'].lower()
        full_text = result['content']
    elif 'description' in result:
        text = result['description'].lower()
        full_text = result['description']
    else:
        text = result.get('title', '').lower()
        full_text = result.get('title', '')
    
    # Check for Abel Hugo relevance
    is_abel_relevant = ('abel hugo' in text or 
                       ('abel' in text and 'hugo' in text and 'victor' in text))
    
    if is_abel_relevant or result['relevance_score'] >= 3:
        print(f"\nAnalyzing: {result['title']} (Score: {result['relevance_score']})")
        
        # Categorize findings
        if any(term in text for term in ['military', 'army', 'officer', 'service', 'war']):
            abel_analysis['military_service'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Military service information")
        
        if 'madrid' in text:
            abel_analysis['madrid_connections'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Madrid connection")
        
        if 'napoleon' in text and ('maps' in text or 'camps' in text):
            abel_analysis['napoleon_maps_camps'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Napoleon maps/camps")
        
        if 'hospital' in text:
            abel_analysis['hospital_references'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Hospital reference")
        
        if any(term in text for term in ['author', 'wrote', 'published', 'publication']):
            abel_analysis['publications_authorship'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Publications/authorship")
        
        if '1809' in text:
            abel_analysis['timeframe_1809'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ 1809 timeframe")
        
        if any(ruler in text for ruler in ['joseph bonaparte', 'ferdinand', 'charles iv']):
            abel_analysis['ruler_context'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Ruler context")
        
        if 'victor hugo' in text and 'brother' in text:
            abel_analysis['family_relations'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'sample': full_text[:800],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Family relations")
        
        # Add to key sources
        abel_analysis['key_biographical_sources'].append({
            'source': result['title'],
            'url': result.get('url', ''),
            'sample': full_text[:1000],
            'keywords': result['keywords_found'],
            'score': result['relevance_score'],
            'source_type': result['source']
        })

# Save final comprehensive research
final_data = {
    'research_date': datetime.now().isoformat(),
    'research_objective': 'Abel Hugo research: military service during Napoleonic Wars, Madrid connections, Napoleon camp maps, Hospital de Saint Louis, publications, ruler context during 1809',
    'search_summary': {
        'total_results': len(all_results),
        'wikipedia_successful': len([r for r in all_results if r['source'] == 'Wikipedia']),
        'google_books_total': len([r for r in all_results if r['source'] == 'Google Books']),
        'search_errors': len(search_errors),
        'top_relevance_score': max([r['relevance_score'] for r in all_results]) if all_results else 0
    },
    'all_research_results': all_results,
    'abel_hugo_analysis': abel_analysis,
    'research_errors': search_errors
}

final_output = 'workspace/abel_hugo_ultimate_research.json'
with open(final_output, 'w') as f:
    json.dump(final_data, f, indent=2)

print(f"\n{'='*80}")
print("ABEL HUGO RESEARCH - ULTIMATE SUMMARY")
print(f"{'='*80}")
print(f"Complete research saved to: {final_output}")
print(f"Total sources analyzed: {len(all_results)}")
print(f"Wikipedia pages processed: {len([r for r in all_results if r['source'] == 'Wikipedia'])}")
print(f"Google Books sources: {len([r for r in all_results if r['source'] == 'Google Books'])}")

# Display key research findings
print(f"\n=== ABEL HUGO RESEARCH FINDINGS ===")
for category, findings in abel_analysis.items():
    if findings and category != 'key_biographical_sources':
        print(f"\n{category.replace('_', ' ').title()}: {len(findings)} sources")
        # Show highest scoring source
        top_source = max(findings, key=lambda x: x['score'])
        print(f"  Top source: {top_source['source']} (Score: {top_source['score']})")
        print(f"  Keywords: {', '.join(top_source['keywords'])}")
        print(f"  Sample: {top_source['sample'][:300]}...")

# Display top overall sources
print(f"\n=== TOP SOURCES FOR ABEL HUGO RESEARCH ===")
for i, result in enumerate(all_results[:8], 1):
    print(f"\n{i}. {result['title']} (Relevance: {result['relevance_score']})")
    print(f"   Source: {result['source']}")
    if 'authors' in result:
        print(f"   Authors: {', '.join(result['authors'])}")
    print(f"   Keywords: {', '.join(result['keywords_found'])}")
    
    if 'content' in result and len(result['content']) > 100:
        print(f"   Content: {result['content'][:250]}...")
    elif 'description' in result:
        print(f"   Description: {result['description'][:250]}...")

print(f"\n=== RESEARCH COMPLETION STATUS ===")
print(f"Search errors encountered: {len(search_errors)}")
if search_errors:
    print("Errors:")
    for error in search_errors[-3:]:  # Show last 3 errors
        print(f"  - {error}")

print(f"\nAbel Hugo research completed! Full results in {final_output}")
```