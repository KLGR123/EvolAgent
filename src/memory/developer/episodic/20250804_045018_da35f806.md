### Development Step 55: Extract Survivor (American TV Series) Season 1–44 Winners from Wikipedia into JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Media analytics for reality TV networks: Automatically extract season‐winner data to build predictive models on contestant demographics and gameplay strategies for upcoming Survivor installments.
- Fan‐driven web app generation: Power a Survivor fan site that dynamically renders interactive timelines, winner profiles, and trivia quizzes based on the JSON list of champions.
- Streaming recommendation engine enhancement: Ingest historical winner information to tag and recommend the most popular Survivor seasons to new subscribers on a video‐on‐demand platform.
- Sports‐betting odds calibration: Feed curated winner archives into a betting platform’s risk engine to adjust real‐time odds and payouts for Survivor‐themed contests.
- Academic research on competition outcomes: Provide social science researchers with ready‐to‐use JSON data for logistic regression studies linking game mechanics to winner attributes.
- Marketing sentiment analysis alignment: Correlate season winners with spikes in social media engagement to optimize ad spend and campaign timing for brands sponsoring Survivor episodes.
- Automated fact‐checking for entertainment journalism: Integrate periodic scripts that validate and update the list of Survivor winners in a newsroom’s content management system.
- Mobile trivia game content pipeline: Power a daily quiz app by pulling the JSON file of winners to generate fresh Survivor trivia questions and leaderboards.

```
import os
import sys
import json
import re
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch Survivor Wikipedia page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)',
    'Accept-Language': 'en-US,en;q=0.9'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP Status: {resp.status_code} OK")
    print(f"→ Final URL: {resp.url}\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Find the correct table: caption "Series overview" or fallback on headers
wikitables = soup.find_all('table', class_='wikitable')
print(f"Found {len(wikitables)} wikitables on the page. Looking for 'Series overview'...\n")
series_table = None
for idx, tbl in enumerate(wikitables, start=1):
    cap = tbl.find('caption')
    cap_text = cap.get_text(strip=True).lower() if cap else ''
    print(f"Table {idx}: caption = '{cap_text}'")
    if 'series overview' in cap_text:
        series_table = tbl
        print(f"→ Matched 'Series overview' table at index {idx}\n")
        break

if not series_table:
    print("No caption match found, trying header‐based fallback...\n")
    for idx, tbl in enumerate(wikitables, start=1):
        first_row = tbl.find('tr')
        if not first_row:
            continue
        headers = [th.get_text(strip=True).lower() for th in first_row.find_all(['th','td'])]
        print(f"Fallback check Table {idx}: headers = {headers[:7]}")
        if 'season' in headers and any('winner' in h for h in headers):
            series_table = tbl
            print(f"→ Fallback matched table at index {idx}\n")
            break

if not series_table:
    print("❌ Could not locate the Series overview table. Exiting.")
    sys.exit(1)

# 5) Determine column indices dynamically
header_cells = series_table.find('tr').find_all(['th','td'])
col_texts = [c.get_text(strip=True) for c in header_cells]
col_lower = [t.lower() for t in col_texts]
print(f"Series overview columns detected: {col_texts}\n")

# find season index
try:
    season_idx = col_lower.index('season')
except ValueError:
    season_idx = next((i for i,t in enumerate(col_lower) if 'season' in t), None)
# find winner index (first header containing 'winner')
winner_idx = next((i for i,t in enumerate(col_lower) if 'winner' in t), None)

if season_idx is None or winner_idx is None:
    print("❌ Could not find 'Season' or 'Winner' columns. Exiting.")
    sys.exit(1)
print(f"Detected season column at index {season_idx}, winner column at index {winner_idx}\n")

# 6) Extract rows and handle merged/empty cells robustly
data = []
numeric_pattern = re.compile(r'^[\d–\-\s]+$')
rows = series_table.find_all('tr')[1:]  # skip header
for i, row in enumerate(rows, start=1):
    cells = row.find_all(['td','th'])
    # quick debug for problematic rows
    if i > 40:  # seasons 41+ are most likely to misalign
        print(f"Row {i}: total cells = {len(cells)}; texts = {[c.get_text(' ',strip=True) for c in cells]}")
    # parse season number
    if len(cells) <= season_idx:
        continue
    season_text = cells[season_idx].get_text(' ', strip=True).split()[0]
    try:
        season_num = int(season_text)
    except ValueError:
        continue
    if not (1 <= season_num <= 44):
        continue
    # primary winner extraction
    primary_text = cells[winner_idx].get_text(' ', strip=True)
    # detect numeric/fallback cases
    if not primary_text or numeric_pattern.fullmatch(primary_text):
        # fallback: find first cell (not season cell) with letters or a link
        fallback_name = None
        for j, cell in enumerate(cells):
            if j == season_idx:
                continue
            text = cell.get_text(' ', strip=True)
            # prefer link text
            link = cell.find('a')
            candidate = link.get_text(strip=True) if link else text
            # ensure candidate has at least one letter
            if any(ch.isalpha() for ch in candidate):
                fallback_name = candidate
                print(f"  Season {season_num} fallback: picked cell {j} -> '{fallback_name}' (primary was '{primary_text}')")
                break
        winner_name = fallback_name or primary_text or ''
    else:
        winner_name = primary_text
    data.append({'season': season_num, 'winner': winner_name})
    print(f"Parsed Season {season_num}: {winner_name}")

# 7) Validate and save
if len(data) != 44:
    print(f"⚠️ Warning: Parsed {len(data)} seasons (expected 44). Check for missing entries.")

# sort by season
data_sorted = sorted(data, key=lambda x: x['season'])
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(data_sorted, f, indent=2, ensure_ascii=False)
print(f"\n✅ Successfully wrote {len(data_sorted)} entries to: {out_file}")
```