### Development Step 9: Alternative Approaches to Identify the Book and Protagonist via 1992 Soviet Paratrooper Crackdown Research

**Description**: Search for alternative approaches to identify the book and protagonist, including: (1) Search for '1992 crackdown Soviet paratrooper organization' to find historical events that might match the scenario, (2) Look for books about specific Soviet paratroopers who became organization co-founders and faced crackdowns, (3) Search for translated titles or alternative names for 'Sacred Desire' in different languages, (4) Investigate specific 1992 crackdowns on organizations co-founded by former Soviet military personnel, focusing on identifying the actual historical figures and events that might be fictionalized in the book.

**Use Cases**:
- Rare book discovery and metadata extraction for a specialty bookstore sourcing out-of-print Soviet military fiction using targeted search patterns
- Historical research automation for a military historian compiling firsthand accounts of 1992 Russian Airborne Forces events
- Literary rights verification for a publishing house legal team identifying original titles and translations of niche war novels
- Digital archive enrichment for a national library cataloging references to veteran organizations and related memoirs
- Market intelligence gathering for a documentary producer locating primary sources on post-Soviet paratrooper crackdowns
- Academic literature review support for Slavic studies scholars mapping alternate titles and translations of Russian military novels
- Automated content scouting for a film studio exploring adaptation opportunities from obscure Soviet-era fiction

```
import os
import json
from ddgs import DDGS
import time

# Create workspace directory
os.makedirs('workspace', exist_ok=True)

print("=== TARGETED SEARCH BASED ON IDENTIFIED PATTERNS ===")
print("Focus: Search for specific leads identified in previous analysis")
print("Goal: Find the 'Sacred Desire' book and protagonist using promising patterns")
print("\n" + "="*70)

# First, let's examine the existing JSON files to understand what data we have
print("üìÅ EXAMINING EXISTING WORKSPACE FILES:")
print("-" * 40)

workspace_files = []
for file in os.listdir('workspace'):
    if file.endswith('.json'):
        workspace_files.append(file)
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")

# Let's inspect the largest file first to understand the data structure
if workspace_files:
    largest_file = max(workspace_files, key=lambda f: os.path.getsize(os.path.join('workspace', f)))
    print(f"\nüîç INSPECTING LARGEST FILE: {largest_file}")
    print("-" * 50)
    
    try:
        with open(os.path.join('workspace', largest_file), 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print("File structure:")
        for key in data.keys():
            value = data[key]
            if isinstance(value, dict):
                print(f"  {key}: dict with {len(value)} keys")
                # Show first few subkeys
                subkeys = list(value.keys())[:3]
                for subkey in subkeys:
                    subvalue = value[subkey]
                    if isinstance(subvalue, list):
                        print(f"    {subkey}: list with {len(subvalue)} items")
                    elif isinstance(subvalue, dict):
                        print(f"    {subkey}: dict with {len(subvalue)} keys")
                    else:
                        print(f"    {subkey}: {type(subvalue).__name__}")
            elif isinstance(value, list):
                print(f"  {key}: list with {len(value)} items")
                if value and isinstance(value[0], dict):
                    print(f"    Sample item keys: {list(value[0].keys())[:5]}")
            else:
                print(f"  {key}: {type(value).__name__}")
        
        # Extract high-relevance findings if available
        findings = []
        if 'findings' in data:
            findings = data['findings']
        elif 'search_results' in data and isinstance(data['search_results'], dict):
            if 'findings' in data['search_results']:
                findings = data['search_results']['findings']
        elif 'searches_performed' in data:
            # Extract from searches_performed structure
            for search in data['searches_performed']:
                if 'results' in search and isinstance(search['results'], list):
                    for result in search['results']:
                        findings.append(result)
        
        print(f"\nüìä EXTRACTED {len(findings)} FINDINGS FROM EXISTING DATA")
        
        # Analyze findings for the most promising patterns identified
        if findings:
            print("\nüéØ ANALYZING FINDINGS FOR KEY PATTERNS:")
            print("-" * 40)
            
            # Pattern categories based on previous analysis
            pattern_categories = {
                'russian_airborne_1992': [],
                'soviet_military_books': [],
                'veterans_organizations': [],
                'sacred_desire_translations': [],
                'paratrooper_co_founders': []
            }
            
            for finding in findings:
                title = finding.get('title', '').lower()
                description = finding.get('body', finding.get('description', finding.get('snippet', ''))).lower()
                combined = f"{title} {description}"
                
                # Categorize findings
                if ('1992' in combined and any(term in combined for term in ['russian airborne', 'airborne forces', 'vdv'])):
                    pattern_categories['russian_airborne_1992'].append(finding)
                
                if (any(term in combined for term in ['book', 'novel', 'literature']) and 
                    any(term in combined for term in ['soviet', 'russian', 'military', 'paratrooper'])):
                    pattern_categories['soviet_military_books'].append(finding)
                
                if (any(term in combined for term in ['veterans', 'organization', 'association']) and
                    any(term in combined for term in ['1992', 'political', 'movement'])):
                    pattern_categories['veterans_organizations'].append(finding)
                
                if any(term in combined for term in ['sacred desire', '—Å–≤—è—â–µ–Ω–Ω–æ–µ –∂–µ–ª–∞–Ω–∏–µ', '—Å–∞–∫—Ä–∞–ª—å–Ω–æ–µ –∂–µ–ª–∞–Ω–∏–µ']):
                    pattern_categories['sacred_desire_translations'].append(finding)
                
                if (any(term in combined for term in ['co-founder', 'founder', 'established']) and
                    any(term in combined for term in ['paratrooper', 'military', 'organization'])):
                    pattern_categories['paratrooper_co_founders'].append(finding)
            
            # Display categorized findings
            for category, category_findings in pattern_categories.items():
                if category_findings:
                    print(f"\nüìã {category.upper().replace('_', ' ')}: {len(category_findings)} findings")
                    for i, finding in enumerate(category_findings[:2], 1):
                        print(f"\n  {i}. {finding.get('title', 'No title')}")
                        print(f"     URL: {finding.get('href', finding.get('url', finding.get('link', 'No URL')))}")
                        desc = finding.get('body', finding.get('description', finding.get('snippet', 'No description')))
                        print(f"     Description: {desc[:100]}...")
    
    except Exception as e:
        print(f"Error inspecting file: {e}")
        import traceback
        traceback.print_exc()

# Now perform targeted searches based on the most promising patterns
print("\n\nüîç PERFORMING TARGETED SEARCHES BASED ON IDENTIFIED PATTERNS:")
print("=" * 65)

# Initialize DDGS searcher
searcher = DDGS(timeout=15)

# Targeted search queries based on the four most promising leads
targeted_queries = [
    # Lead 1: Russian Airborne Forces establishment 1992
    '"Russian Airborne Forces" established "May 7 1992" book novel',
    '"May 7 1992" "Russian Airborne Forces" Boris Yeltsin decree book',
    'VDV "Russian Airborne Forces" 1992 establishment novel fiction',
    
    # Lead 2: Soviet military literature translations
    '"Sacred Desire" translated Russian military novel paratrooper',
    'Soviet paratrooper novel translated English "Sacred Desire"',
    'Russian military fiction translated paratrooper organization crackdown',
    
    # Lead 3: Soviet veterans organizations 1992
    'Soviet veterans organization 1992 co-founder paratrooper political',
    '1992 Soviet military veterans association political movement book',
    'Afghanistan veterans organization 1992 Russia political crackdown',
    
    # Lead 4: Specific historical events
    '"January 1992" Soviet officers political movement paratrooper',
    '1992 Mobile Troops creation Airborne Forces VDV book novel',
    'Soviet paratrooper organization disbanded 1992 co-founder book'
]

print(f"Executing {len(targeted_queries)} targeted searches...")
print("\n" + "-"*70)

# Store search results
targeted_results = {
    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'search_strategy': 'Targeted search based on identified promising patterns',
    'searches': [],
    'high_relevance_findings': [],
    'book_candidates': []
}

# Perform targeted searches
for i, query in enumerate(targeted_queries, 1):
    print(f"\nSearch {i}/{len(targeted_queries)}: {query}")
    print("-" * 50)
    
    try:
        results = searcher.text(
            query,
            max_results=5,
            page=1,
            backend=["google", "duckduckgo", "bing"],
            safesearch="off",
            region="en-us"
        )
        
        if results:
            print(f"Found {len(results)} results")
            
            search_data = {
                'query': query,
                'results_count': len(results),
                'results': results
            }
            targeted_results['searches'].append(search_data)
            
            # Analyze results for book candidates
            for j, result in enumerate(results, 1):
                title = result.get('title', 'No title')
                body = result.get('body', 'No description')
                href = result.get('href', 'No URL')
                
                print(f"\nResult {j}:")
                print(f"Title: {title}")
                print(f"URL: {href}")
                print(f"Description: {body}")
                
                # Check for high relevance indicators
                combined_text = f"{title.lower()} {body.lower()}"
                
                # Book-specific indicators
                book_indicators = ['book', 'novel', 'author', 'published', 'fiction', 'literature']
                military_indicators = ['paratrooper', 'military', 'soviet', 'russian', 'vdv', 'airborne']
                year_indicators = ['1992', 'crackdown', 'organization', 'co-founder', 'disbanded']
                title_indicators = ['sacred desire', '—Å–≤—è—â–µ–Ω–Ω–æ–µ –∂–µ–ª–∞–Ω–∏–µ']
                
                book_matches = sum(1 for indicator in book_indicators if indicator in combined_text)
                military_matches = sum(1 for indicator in military_indicators if indicator in combined_text)
                year_matches = sum(1 for indicator in year_indicators if indicator in combined_text)
                title_matches = sum(1 for indicator in title_indicators if indicator in combined_text)
                
                total_relevance = book_matches + military_matches + year_matches + (title_matches * 3)  # Weight title matches more
                
                if total_relevance >= 4:
                    print(f"üéØ HIGH RELEVANCE: {total_relevance} points")
                    print(f"   Book indicators: {book_matches}, Military: {military_matches}, Year: {year_matches}, Title: {title_matches}")
                    
                    targeted_results['high_relevance_findings'].append({
                        'query': query,
                        'title': title,
                        'url': href,
                        'description': body,
                        'relevance_score': total_relevance,
                        'book_matches': book_matches,
                        'military_matches': military_matches,
                        'year_matches': year_matches,
                        'title_matches': title_matches
                    })
                    
                    # Special detection for potential book candidates
                    if (book_matches >= 2 and military_matches >= 1) or title_matches > 0:
                        print(f"üìö POTENTIAL BOOK CANDIDATE DETECTED!")
                        targeted_results['book_candidates'].append({
                            'query': query,
                            'title': title,
                            'url': href,
                            'description': body,
                            'candidate_score': total_relevance
                        })
                
                print("-" * 30)
        else:
            print("No results found")
            targeted_results['searches'].append({
                'query': query,
                'results_count': 0,
                'results': []
            })
    
    except Exception as e:
        print(f"Error during search: {str(e)}")
        targeted_results['searches'].append({
            'query': query,
            'error': str(e),
            'results_count': 0
        })
        continue
    
    # Brief pause between searches
    time.sleep(2)
    print("\n" + "="*70)

# Analyze and summarize results
print("\n\nüìä TARGETED SEARCH ANALYSIS SUMMARY:")
print("=" * 45)

total_searches = len(targeted_queries)
successful_searches = len([s for s in targeted_results['searches'] if s.get('results_count', 0) > 0])
total_results = sum(s.get('results_count', 0) for s in targeted_results['searches'])
high_relevance_count = len(targeted_results['high_relevance_findings'])
book_candidates_count = len(targeted_results['book_candidates'])

print(f"Total targeted queries: {total_searches}")
print(f"Successful searches: {successful_searches}")
print(f"Total results collected: {total_results}")
print(f"High-relevance findings: {high_relevance_count}")
print(f"Book candidates identified: {book_candidates_count}")

# Display book candidates
if targeted_results['book_candidates']:
    print("\nüìö BOOK CANDIDATES IDENTIFIED:")
    print("-" * 35)
    
    # Sort by candidate score
    sorted_candidates = sorted(targeted_results['book_candidates'], key=lambda x: x['candidate_score'], reverse=True)
    
    for i, candidate in enumerate(sorted_candidates, 1):
        print(f"\n{i}. {candidate['title']}")
        print(f"   Query: {candidate['query']}")
        print(f"   URL: {candidate['url']}")
        print(f"   Score: {candidate['candidate_score']} points")
        print(f"   Description: {candidate['description'][:150]}...")
        
        # Check for specific Sacred Desire mentions
        desc_lower = candidate['description'].lower()
        title_lower = candidate['title'].lower()
        if 'sacred desire' in f"{title_lower} {desc_lower}":
            print(f"   üö® CONTAINS 'SACRED DESIRE' - POTENTIAL MATCH!")
else:
    print("\n‚ö†Ô∏è No specific book candidates identified")
    print("Analyzing high-relevance findings for clues...")

# Display top high-relevance findings
if targeted_results['high_relevance_findings']:
    print("\nüéØ TOP HIGH-RELEVANCE FINDINGS:")
    print("-" * 32)
    
    sorted_findings = sorted(targeted_results['high_relevance_findings'], key=lambda x: x['relevance_score'], reverse=True)
    
    for i, finding in enumerate(sorted_findings[:5], 1):
        print(f"\n{i}. {finding['title']}")
        print(f"   Query: {finding['query']}")
        print(f"   URL: {finding['url']}")
        print(f"   Relevance: {finding['relevance_score']} points")
        print(f"   Breakdown: Book({finding['book_matches']}) Military({finding['military_matches']}) Year({finding['year_matches']}) Title({finding['title_matches']})")
        print(f"   Description: {finding['description'][:120]}...")

# Save targeted search results
results_file = 'workspace/sacred_desire_targeted_search_results.json'
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(targeted_results, f, indent=2, ensure_ascii=False)

print(f"\nüíæ Targeted search results saved to: {results_file}")

# Create final assessment
print("\n\nüèÅ FINAL ASSESSMENT:")
print("=" * 20)

if book_candidates_count > 0:
    print("‚úÖ SUCCESS: Book candidates identified!")
    print(f"   Found {book_candidates_count} potential matches for 'Sacred Desire'")
    print("   Recommend detailed investigation of top candidates")
elif high_relevance_count > 0:
    print("üîç PROGRESS: High-relevance findings located")
    print(f"   Found {high_relevance_count} relevant results")
    print("   May contain clues to the book's identity")
else:
    print("‚ö†Ô∏è LIMITED RESULTS: No direct book matches found")
    print("   Consider expanding search terms or alternative approaches")

print("\nüìã NEXT STEPS:")
if book_candidates_count > 0:
    print("1. Investigate top book candidates in detail")
    print("2. Look for author information and publication details")
    print("3. Search for protagonist names in identified books")
else:
    print("1. Analyze high-relevance findings for additional clues")
    print("2. Search for specific authors of Soviet military fiction")
    print("3. Investigate Russian literature databases")
    print("4. Consider the book may be fictional or very obscure")

print("\n‚úÖ TARGETED SEARCH PHASE COMPLETE")
```