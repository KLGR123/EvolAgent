### Development Step 8: Identify Nonnative Clownfish (Finding Nemo) Occurrences in USGS Records Pre-2020 with Zip Codes

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting firms mapping pre-2020 nonnative clownfish occurrences by zip code to assess ecological impact for coastal development permit applications
- University marine biology departments extracting zip codeâ€“level invasive species records to support longitudinal studies of Amphiprion ocellaris distribution in Florida estuaries
- State wildlife management agencies automating detection of pre-2020 clownfish introductions to prioritize removal efforts and allocate monitoring budgets by county
- Conservation NGOs generating community outreach materials with local zip codes showing historical nonnative clownfish hotspots to engage citizen scientists in reporting sightings
- Government policy analysts compiling pre-2020 invasive fish data by zip code to draft targeted legislation restricting aquarium trade imports in high-risk areas
- GIS and urban planners integrating zip codeâ€“specific occurrence records into hazard maps for coastal municipalities to inform zoning regulations and emergency response plans
- Commercial aquarium trade compliance teams verifying historical nonnative release patterns to develop best-practice guidelines and prevent future introductions
- Environmental insurance underwriters assessing pre-2020 nonnative species presence at waterfront properties by zip code to adjust risk premiums and coverage terms

```
print("=== ACCESSING COLLECTION INFO FOR SPECIFIC OCCURRENCE RECORDS ===\n")

# Access the Collection Info link which showed promising FL state and 2018 year data
import requests
from bs4 import BeautifulSoup
import re
import json
import os
from urllib.parse import urljoin, urlparse

print("Target: USGS Collection Info for Amphiprion ocellaris (Clownfish)")
print("Objective: Extract specific occurrence records with location data and zip codes before 2020\n")

# Find the workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using workspace directory: {workspace_dir}")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created workspace directory: {workspace_dir}")

# First, let's inspect the complete USGS data file to understand the Collection Info links
usgs_data_file = os.path.join(workspace_dir, 'usgs_nas_clownfish_complete_data.json')
if os.path.exists(usgs_data_file):
    print(f"\n=== INSPECTING USGS DATA FILE STRUCTURE ===\n")
    print(f"File: {usgs_data_file}")
    
    with open(usgs_data_file, 'r') as f:
        usgs_data = json.load(f)
    
    print("Top-level keys in USGS data:")
    for key, value in usgs_data.items():
        if isinstance(value, dict):
            print(f"  {key}: Dictionary with {len(value)} keys")
        elif isinstance(value, list):
            print(f"  {key}: List with {len(value)} items")
        else:
            print(f"  {key}: {type(value).__name__} - {str(value)[:100]}...")
    
    # Extract Collection Info links from occurrence data sources
    if 'occurrence_data_sources' in usgs_data:
        occurrence_sources = usgs_data['occurrence_data_sources']
        print(f"\nOccurrence data sources available:")
        for key, value in occurrence_sources.items():
            if isinstance(value, list):
                print(f"  {key}: {len(value)} items")
            else:
                print(f"  {key}: {value}")
        
        # Look for Collection Info links specifically
        collection_links = []
        
        # Check all occurrence links for collection-related ones
        if 'all_occurrence_links' in occurrence_sources:
            all_links = occurrence_sources['all_occurrence_links']
            print(f"\nAnalyzing {len(all_links)} occurrence links for Collection Info:")
            
            for i, link in enumerate(all_links, 1):
                link_text = link.get('text', '').lower()
                link_url = link.get('url', '')
                keywords = link.get('keywords_found', [])
                
                # Look for collection-related links
                if 'collection' in keywords or 'collection' in link_text:
                    collection_links.append(link)
                    print(f"Collection Link {len(collection_links)}:")
                    print(f"  Text: {link.get('text', 'Unknown')}")
                    print(f"  URL: {link_url}")
                    print(f"  Keywords: {keywords}")
                    print(f"  {'-'*60}")
        
        print(f"\nTotal Collection Info links found: {len(collection_links)}")
        
        if collection_links:
            # Access the main Collection Info page first
            main_collection_link = None
            state_specific_links = []
            
            for link in collection_links:
                link_url = link.get('url', '')
                if 'State=' in link_url or 'Year' in link_url:
                    state_specific_links.append(link)
                elif 'CollectionInfo.aspx?SpeciesID=3243' in link_url and 'State=' not in link_url:
                    main_collection_link = link
            
            print(f"\nMain Collection Info link: {'Found' if main_collection_link else 'Not found'}")
            print(f"State-specific links: {len(state_specific_links)}")
            
            # Access the main Collection Info page
            if main_collection_link:
                collection_url = main_collection_link['url']
                print(f"\n=== ACCESSING MAIN COLLECTION INFO PAGE ===\n")
                print(f"Target URL: {collection_url}")
                
                try:
                    # Set headers to mimic browser request
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Connection': 'keep-alive'
                    }
                    
                    # Access the collection info page
                    response = requests.get(collection_url, headers=headers, timeout=30)
                    response.raise_for_status()
                    
                    print(f"Successfully accessed Collection Info page (Status: {response.status_code})")
                    print(f"Content length: {len(response.content):,} bytes\n")
                    
                    # Parse the HTML content
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extract page title
                    title_element = soup.find('title')
                    page_title = title_element.get_text().strip() if title_element else 'Unknown'
                    print(f"Collection Info Page Title: {page_title}")
                    
                    # Look for occurrence records table
                    print(f"\n=== SEARCHING FOR OCCURRENCE RECORDS TABLE ===\n")
                    
                    # Find all tables on the page
                    tables = soup.find_all('table')
                    print(f"Found {len(tables)} tables on the Collection Info page")
                    
                    occurrence_records = []
                    
                    for table_idx, table in enumerate(tables, 1):
                        print(f"\nAnalyzing Table {table_idx}:")
                        
                        # Extract table headers
                        headers = []
                        header_rows = table.find_all('tr')[:2]  # Check first 2 rows for headers
                        
                        for header_row in header_rows:
                            header_cells = header_row.find_all(['th', 'td'])
                            if header_cells:
                                row_headers = [cell.get_text().strip() for cell in header_cells]
                                if any(header.lower() in ['state', 'year', 'location', 'county', 'date', 'lat', 'lon'] for header in row_headers):
                                    headers = row_headers
                                    break
                        
                        print(f"  Headers: {headers[:8]}...")  # Show first 8 headers
                        
                        if headers:
                            # Extract data rows
                            data_rows = table.find_all('tr')[len(header_rows):]  # Skip header rows
                            print(f"  Data rows found: {len(data_rows)}")
                            
                            # Extract first few records to analyze structure
                            sample_records = []
                            for i, row in enumerate(data_rows[:10]):  # First 10 records
                                cells = row.find_all(['td', 'th'])
                                if cells:
                                    record_data = [cell.get_text().strip() for cell in cells]
                                    if len(record_data) >= len(headers):  # Valid record
                                        record_dict = {}
                                        for j, header in enumerate(headers):
                                            if j < len(record_data):
                                                record_dict[header] = record_data[j]
                                        sample_records.append(record_dict)
                                        
                                        print(f"    Record {i+1}: {record_data[:5]}...")  # Show first 5 fields
                            
                            occurrence_records.extend(sample_records)
                            print(f"  Sample records extracted: {len(sample_records)}")
                        
                        else:
                            print(f"  No relevant headers found in table {table_idx}")
                    
                    print(f"\nTotal occurrence records extracted: {len(occurrence_records)}")
                    
                    # Analyze extracted records for location data
                    if occurrence_records:
                        print(f"\n=== ANALYZING OCCURRENCE RECORDS FOR LOCATION DATA ===\n")
                        
                        location_data = {
                            'records_with_coordinates': [],
                            'records_with_states': [],
                            'records_with_years': [],
                            'pre_2020_records': []
                        }
                        
                        for i, record in enumerate(occurrence_records):
                            print(f"Record {i+1} analysis:")
                            record_has_location = False
                            
                            # Check each field in the record
                            for field_name, field_value in record.items():
                                field_lower = field_name.lower()
                                value_str = str(field_value).strip()
                                
                                print(f"  {field_name}: {value_str[:50]}...")  # Show first 50 chars
                                
                                # Check for coordinates
                                coord_pattern = r'-?\d+\.\d+'
                                if re.search(coord_pattern, value_str) and ('lat' in field_lower or 'lon' in field_lower):
                                    location_data['records_with_coordinates'].append({
                                        'record_index': i+1,
                                        'field': field_name,
                                        'value': value_str
                                    })
                                    record_has_location = True
                                
                                # Check for state codes
                                state_pattern = r'\b[A-Z]{2}\b'
                                if re.search(state_pattern, value_str) and ('state' in field_lower or len(value_str) == 2):
                                    location_data['records_with_states'].append({
                                        'record_index': i+1,
                                        'field': field_name,
                                        'value': value_str
                                    })
                                    record_has_location = True
                                
                                # Check for years (especially pre-2020)
                                year_pattern = r'\b(19\d{2}|20[01]\d)\b'
                                year_matches = re.findall(year_pattern, value_str)
                                if year_matches:
                                    for year in year_matches:
                                        location_data['records_with_years'].append({
                                            'record_index': i+1,
                                            'field': field_name,
                                            'year': year,
                                            'full_value': value_str
                                        })
                                        
                                        if int(year) < 2020:
                                            location_data['pre_2020_records'].append({
                                                'record_index': i+1,
                                                'field': field_name,
                                                'year': year,
                                                'full_record': record
                                            })
                            
                            if record_has_location:
                                print(f"  *** RECORD {i+1} HAS LOCATION DATA ***")
                            
                            print(f"  {'-'*40}")
                        
                        # Summary of location data found
                        print(f"\n=== LOCATION DATA SUMMARY ===\n")
                        print(f"Records with coordinates: {len(location_data['records_with_coordinates'])}")
                        print(f"Records with states: {len(location_data['records_with_states'])}")
                        print(f"Records with years: {len(location_data['records_with_years'])}")
                        print(f"Pre-2020 records: {len(location_data['pre_2020_records'])}")
                        
                        # Show pre-2020 records in detail
                        if location_data['pre_2020_records']:
                            print(f"\n=== PRE-2020 OCCURRENCE RECORDS ===\n")
                            for i, pre_2020_record in enumerate(location_data['pre_2020_records'], 1):
                                print(f"Pre-2020 Record {i}:")
                                print(f"  Year: {pre_2020_record['year']}")
                                print(f"  Year Field: {pre_2020_record['field']}")
                                print(f"  Full Record:")
                                for key, value in pre_2020_record['full_record'].items():
                                    print(f"    {key}: {value}")
                                print(f"  {'-'*60}")
                        
                        # Save the collection info analysis
                        collection_analysis = {
                            'collection_info_access': {
                                'url': collection_url,
                                'page_title': page_title,
                                'access_date': '2024',
                                'content_length': len(response.content),
                                'tables_found': len(tables)
                            },
                            'occurrence_records': occurrence_records,
                            'location_analysis': location_data,
                            'summary': {
                                'total_records_extracted': len(occurrence_records),
                                'records_with_coordinates': len(location_data['records_with_coordinates']),
                                'records_with_states': len(location_data['records_with_states']),
                                'pre_2020_records_found': len(location_data['pre_2020_records'])
                            },
                            'next_steps': [
                                'Extract coordinates for zip code conversion',
                                'Access state-specific collection pages',
                                'Convert coordinates to zip codes using geocoding',
                                'Compile final report of pre-2020 nonnative occurrences'
                            ]
                        }
                        
                        # Save the collection analysis
                        collection_file = os.path.join(workspace_dir, 'usgs_collection_info_analysis.json')
                        with open(collection_file, 'w') as f:
                            json.dump(collection_analysis, f, indent=2)
                        
                        print(f"\n=== COLLECTION INFO ANALYSIS COMPLETE ===\n")
                        print(f"Collection Info URL: {collection_url}")
                        print(f"Total Records Extracted: {len(occurrence_records)}")
                        print(f"Records with Location Data: {len(location_data['records_with_coordinates']) + len(location_data['records_with_states'])}")
                        print(f"Pre-2020 Records Found: {len(location_data['pre_2020_records'])}")
                        print(f"\nCollection analysis saved to: {collection_file}")
                        
                        if location_data['pre_2020_records']:
                            print(f"\n*** SUCCESS: PRE-2020 NONNATIVE CLOWNFISH RECORDS FOUND ***")
                            print(f"Found {len(location_data['pre_2020_records'])} occurrence records before 2020")
                            print(f"Ready to extract specific location data and convert to zip codes")
                        else:
                            print(f"\nNo pre-2020 records found. Will check state-specific collection pages.")
                    
                    else:
                        print(f"\nNo occurrence records extracted from tables.")
                        print(f"Will try alternative data extraction methods.")
                
                except requests.exceptions.RequestException as e:
                    print(f"Error accessing Collection Info page: {e}")
                    print("Will try alternative data sources.")
                
                except Exception as e:
                    print(f"Unexpected error during Collection Info analysis: {e}")
                    print("Error details for debugging:")
                    import traceback
                    traceback.print_exc()
            
            else:
                print(f"\nNo main Collection Info link found.")
                print(f"Will try state-specific collection links instead.")
        
        else:
            print(f"\nNo Collection Info links found in occurrence data sources.")
    
    else:
        print(f"\nNo occurrence_data_sources key found in USGS data.")
        print(f"Available keys: {list(usgs_data.keys())}")

else:
    print(f"USGS data file not found: {usgs_data_file}")
    print(f"Available files in workspace:")
    if os.path.exists(workspace_dir):
        for file in os.listdir(workspace_dir):
            print(f"  - {file}")
    else:
        print(f"  No workspace directory found.")
```