### Development Step 7: Identify Year and Gold Medalist Birth Countries for Cortina FIS Nordic Worlds Without Finland, Norway

**Description**: Search for information about FIS Nordic World Ski Championships held in Cortina d'Ampezzo that did not include participants from Finland or Norway. Focus on identifying which specific year this championship took place, then find the complete list of gold medalists from that event. Extract the birth countries of all gold medalists to identify which country produced a gold medalist at this particular championship.

**Use Cases**:
- Sports history research on Nordic skiing anomalies, identifying the year Cortina d’Ampezzo hosted a championship without Finland or Norway participants and profiling its gold medalists’ birth countries
- Broadcast commentary preparation for ski event documentaries, extracting and mapping gold medalist origins from the unique Cortina championship year to enrich on-air narratives
- Data journalism investigation into national representation in winter sports, using the Cortina case to illustrate how an event unfolded without traditional powerhouse nations
- Machine learning dataset curation for nationality prediction models, gathering athlete birthplace data from the specific Cortina championship to train geoclassification algorithms
- Cultural heritage archiving for winter sports museums, documenting medalists’ birth countries from the Cortina event to showcase global participation shifts over time
- Olympic committee compliance auditing, verifying historical records of country participation and medal distribution in the Cortina championship to ensure data integrity
- Sporting tourism itinerary design, creating themed travel experiences around Cortina d’Ampezzo’s historic ski events and highlighting medalists’ country origins for visitor engagement

```
import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== FIS NORDIC WORLD SKI CHAMPIONSHIPS RESEARCH (COMPLETELY RESTRUCTURED) ===\n")
print("Objective: Find Cortina d'Ampezzo championship WITHOUT Finland or Norway participants")
print("Then: Extract all gold medalists and their birth countries\n")

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# First, let's search for information about FIS Nordic World Ski Championships in Cortina d'Ampezzo
print("Step 1: Searching for FIS Nordic World Ski Championships in Cortina d'Ampezzo...\n")

# Key search URLs - start with Wikipedia which usually has comprehensive championship data
search_urls = [
    "https://en.wikipedia.org/wiki/FIS_Nordic_World_Ski_Championships",
    "https://en.wikipedia.org/wiki/Cortina_d%27Ampezzo",
    "https://en.wikipedia.org/wiki/1956_FIS_Nordic_World_Ski_Championships",
    "https://en.wikipedia.org/wiki/1941_FIS_Nordic_World_Ski_Championships"
]

successful_sources = []
failed_sources = []

for url in search_urls:
    print(f"Accessing: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=20)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            # Save the content first
            filename = url.split('/')[-1].replace('%27', '_').replace('%', '_') + '.html'
            filepath = f'workspace/{filename}'
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # COMPLETELY NEW APPROACH: Parse and analyze in separate steps
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Get title safely
            title_element = soup.find('title')
            if title_element:
                title_text = title_element.get_text().strip()
            else:
                title_text = 'No title found'
            
            # RESTRUCTURED: Get text content in a completely different way
            all_text = soup.get_text()
            lowercase_text = all_text.lower()
            
            # Now analyze the content step by step
            cortina_count = 0
            finland_count = 0
            norway_count = 0
            has_championship_data = False
            
            # Count occurrences manually to avoid variable scope issues
            for word in lowercase_text.split():
                if 'cortina' in word:
                    cortina_count += 1
                if 'finland' in word:
                    finland_count += 1
                if 'norway' in word:
                    norway_count += 1
            
            # Check for championship indicators
            championship_keywords = ['championship', 'gold medal', 'winner', 'result']
            for keyword in championship_keywords:
                if keyword in lowercase_text:
                    has_championship_data = True
                    break
            
            # Store results
            source_info = {
                'url': url,
                'title': title_text,
                'filename': filepath,
                'cortina_mentions': cortina_count,
                'has_championship_info': has_championship_data,
                'finland_mentions': finland_count,
                'norway_mentions': norway_count,
                'content_length': len(response.text)
            }
            
            successful_sources.append(source_info)
            
            print(f"  ✓ Title: {title_text}")
            print(f"  ✓ Cortina mentions: {cortina_count}")
            print(f"  ✓ Championship info: {has_championship_data}")
            print(f"  ✓ Finland mentions: {finland_count}")
            print(f"  ✓ Norway mentions: {norway_count}")
            print(f"  ✓ Content length: {len(response.text)} characters")
            print(f"  ✓ Saved to: {filepath}\n")
            
        else:
            failed_sources.append({'url': url, 'status': response.status_code})
            print(f"  ✗ Failed - Status: {response.status_code}\n")
            
    except Exception as e:
        failed_sources.append({'url': url, 'error': str(e)})
        print(f"  ✗ Error: {str(e)}\n")
    
    time.sleep(2)  # Be respectful to servers

print(f"=== INITIAL SEARCH RESULTS ===\n")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources\n")

# Analyze the most promising sources
if successful_sources:
    print("=== ANALYZING SUCCESSFUL SOURCES ===\n")
    
    # Sort sources by relevance
    def source_priority(source):
        return (source['cortina_mentions'], source['has_championship_info'])
    
    priority_sources = sorted(successful_sources, key=source_priority, reverse=True)
    
    for i, source in enumerate(priority_sources, 1):
        print(f"{i}. {source['url']}")
        print(f"   Title: {source['title']}")
        print(f"   Cortina mentions: {source['cortina_mentions']}")
        print(f"   Championship info: {source['has_championship_info']}")
        print(f"   Finland/Norway mentions: {source['finland_mentions']}/{source['norway_mentions']}")
        
        if source['cortina_mentions'] > 0 and source['has_championship_info']:
            print(f"   *** HIGH PRIORITY SOURCE ***")
        print()
    
    # Now let's examine the content of high-priority sources in detail
    print("=== DETAILED CONTENT ANALYSIS ===\n")
    
    analysis_results = []
    
    for idx, source in enumerate(priority_sources[:3], 1):  # Analyze top 3 sources
        print(f"Analyzing: {source['url']}\n")
        
        # Read the saved HTML file
        with open(source['filename'], 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        print("Searching for Cortina d'Ampezzo championship years...")
        
        # Get all text and find Cortina mentions
        page_content = soup.get_text()
        
        # Find sentences mentioning Cortina
        sentences = re.split(r'[.!?]', page_content)
        cortina_sentences = []
        
        for sentence in sentences:
            if 'cortina' in sentence.lower():
                clean_sentence = sentence.strip()
                if len(clean_sentence) > 10:  # Filter out very short fragments
                    cortina_sentences.append(clean_sentence)
        
        print(f"Found {len(cortina_sentences)} sentences mentioning Cortina:\n")
        
        potential_years = []
        for j, sentence in enumerate(cortina_sentences[:15], 1):  # Show first 15
            print(f"{j}. {sentence}")
            
            # Look for years in this sentence
            year_matches = re.findall(r'\b(19\d{2}|20\d{2})\b', sentence)
            if year_matches:
                print(f"   Years found: {year_matches}")
            
            # Check for Finland/Norway absence - this is the key criterion!
            sentence_lower = sentence.lower()
            has_finland = 'finland' in sentence_lower
            has_norway = 'norway' in sentence_lower
            
            if not has_finland and not has_norway and year_matches:
                print(f"   *** POTENTIAL TARGET: {year_matches} - NO FINLAND/NORWAY MENTIONED ***")
                for year in year_matches:
                    potential_years.append({
                        'year': year,
                        'context': sentence,
                        'sentence_number': j
                    })
            print()
        
        # Analyze tables for championship results
        tables = soup.find_all('table')
        print(f"Found {len(tables)} tables in this source")
        
        relevant_tables = []
        for table_idx, table in enumerate(tables):
            table_content = table.get_text().lower()
            
            # Check for championship/medal content
            medal_words = ['gold', 'medal', 'winner', 'champion', 'result']
            has_medals = any(word in table_content for word in medal_words)
            
            # Check for Cortina
            has_cortina_ref = 'cortina' in table_content
            
            # Check for Finland/Norway
            has_finland_ref = 'finland' in table_content
            has_norway_ref = 'norway' in table_content
            
            if has_medals or has_cortina_ref:
                table_info = {
                    'table_index': table_idx,
                    'has_medal_info': has_medals,
                    'has_cortina': has_cortina_ref,
                    'has_finland': has_finland_ref,
                    'has_norway': has_norway_ref
                }
                relevant_tables.append(table_info)
                
                print(f"  Table {table_idx}: Medal info={has_medals}, Cortina={has_cortina_ref}, Finland={has_finland_ref}, Norway={has_norway_ref}")
                
                # Highlight tables with Cortina but no Finland/Norway
                if has_cortina_ref and not has_finland_ref and not has_norway_ref:
                    print(f"    *** HIGHLY RELEVANT - Cortina mentioned, no Finland/Norway ***")
        
        print(f"Found {len(relevant_tables)} relevant tables\n")
        
        # Save analysis for this source
        source_analysis = {
            'url': source['url'],
            'cortina_sentences': cortina_sentences,
            'potential_target_years': potential_years,
            'relevant_tables_count': len(relevant_tables),
            'total_tables': len(tables),
            'table_details': relevant_tables
        }
        
        analysis_filename = f'workspace/cortina_detailed_analysis_{idx}.json'
        with open(analysis_filename, 'w') as f:
            json.dump(source_analysis, f, indent=2)
        
        analysis_results.append(source_analysis)
        
        print(f"Detailed analysis saved to: {analysis_filename}\n")
        print("-" * 50 + "\n")
    
    # Summary of findings
    print("=== SUMMARY OF FINDINGS ===\n")
    
    all_potential_years = []
    for analysis in analysis_results:
        all_potential_years.extend(analysis['potential_target_years'])
    
    if all_potential_years:
        print(f"Found {len(all_potential_years)} potential target years:")
        for target in all_potential_years:
            print(f"  Year: {target['year']} - Context: {target['context'][:100]}...")
    else:
        print("No clear target years identified in initial analysis.")
        print("May need to examine specific championship pages or use different search strategy.")
else:
    print("No sources successfully accessed. Need to try alternative search methods.\n")

# Save comprehensive research summary
research_summary = {
    'research_target': 'FIS Nordic World Ski Championships in Cortina d\'Ampezzo without Finland/Norway',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'successful_sources': len(successful_sources),
    'failed_sources': len(failed_sources),
    'sources_data': successful_sources,
    'failed_attempts': failed_sources,
    'analysis_completed': len(successful_sources) > 0,
    'next_steps': [
        'Identify specific year of Cortina championship without Finland/Norway',
        'Extract complete gold medalist list from that championship',
        'Research birth countries of all gold medalists'
    ]
}

with open('workspace/cortina_research_summary.json', 'w') as f:
    json.dump(research_summary, f, indent=2)

print("=== PHASE 1 COMPLETE ===\n")
print(f"Research summary saved to: workspace/cortina_research_summary.json")
print(f"Successfully gathered {len(successful_sources)} sources for analysis")
print("Next phase: Deep analysis to identify the specific championship year and gold medalists")
```