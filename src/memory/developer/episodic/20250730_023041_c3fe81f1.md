### Development Step 2: Archived July 3 2023 ‚ÄúThe Lord of the Rings‚Äù Wikipedia Links for ASOIAF Path-Finding

**Description**: Access the archived Wikipedia page for 'The Lord of the Rings' (book) as it appeared at the end of July 3, 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific version from that date. Extract all outbound links from the page content, focusing on internal Wikipedia links that could potentially lead toward 'A Song of Ice and Fire'. Create a comprehensive list of linked pages including literature, fantasy, author, publisher, and genre-related links that might serve as stepping stones in the path-finding process.

**Use Cases**:
- Academic digital humanities research mapping thematic networks by extracting and categorizing outbound links from the July 3, 2023 archived LOTR Wikipedia page to study how ‚ÄúThe Lord of the Rings‚Äù connects to other epic fantasy works.
- Library catalog enrichment in a university library system that uses the extracted internal links to automatically recommend related literature and author metadata for new acquisitions.
- Educational curriculum design for a high-school English literature course, where instructors leverage the categorized LOTR links to build a guided reading pathway through related fantasy novels, authors, and genres.
- Knowledge graph construction by a semantic web engineer who ingests the archived LOTR page links to establish relationships between literary works, publishers, and genre concepts in a linked data platform.
- SEO and content strategy for a fantasy book review blog, using the outbound link structure to identify high-traffic related articles (e.g., other fantasy sagas) to optimize internal cross-linking.
- AI-powered chatbot training for a virtual library assistant, using the detailed link dataset as input to recommend next-read suggestions from ‚ÄúThe Lord of the Rings‚Äù to ‚ÄúA Song of Ice and Fire‚Äù through multi-step literature connections.
- Intellectual property due diligence by a legal team, analyzing the network of related work links on the archived page to trace derivative content and verify rights-holder information for licensing negotiations.

```
import os
import json
from datetime import datetime

print("=== INSPECTING EXTRACTED LOTR WIKIPEDIA LINKS DATA ===\n")
print("Objective: Understand the structure of extracted links before processing for path-finding\n")

# First, locate the workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("‚ùå No workspace directory found")
    exit()

workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# List all files in workspace
print("Files in workspace:")
for file in os.listdir(workspace_dir):
    file_path = os.path.join(workspace_dir, file)
    file_size = os.path.getsize(file_path)
    print(f"  - {file} ({file_size:,} bytes)")

# Locate the LOTR links JSON file
lotr_file = os.path.join(workspace_dir, 'lotr_wikipedia_links_july_2023.json')
if not os.path.exists(lotr_file):
    print("\n‚ùå LOTR links file not found")
    exit()

print(f"\n=== INSPECTING FILE STRUCTURE: {os.path.basename(lotr_file)} ===\n")
print(f"File size: {os.path.getsize(lotr_file):,} bytes\n")

# Load and inspect the JSON structure without assuming contents
with open(lotr_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

print("Top-level keys in the JSON file:")
for key, value in data.items():
    if isinstance(value, dict):
        print(f"  {key}: Dictionary with {len(value)} keys")
        # Show nested structure
        for nested_key, nested_value in value.items():
            if isinstance(nested_value, list):
                print(f"    {nested_key}: List with {len(nested_value)} items")
            elif isinstance(nested_value, dict):
                print(f"    {nested_key}: Dictionary with {len(nested_value)} keys")
            else:
                print(f"    {nested_key}: {type(nested_value).__name__} = {nested_value}")
    elif isinstance(value, list):
        print(f"  {key}: List with {len(value)} items")
        if value:  # Show sample of first item structure
            first_item = value[0]
            if isinstance(first_item, dict):
                print(f"    Sample item keys: {list(first_item.keys())}")
            else:
                print(f"    Sample item type: {type(first_item).__name__}")
    else:
        print(f"  {key}: {type(value).__name__} = {value}")

print(f"\n=== EXAMINING EXTRACTION METADATA ===\n")

if 'extraction_metadata' in data:
    metadata = data['extraction_metadata']
    print("Extraction details:")
    for key, value in metadata.items():
        print(f"  {key}: {value}")
else:
    print("No extraction metadata found")

print(f"\n=== EXAMINING LINK CATEGORIES ===\n")

if 'categorized_links' in data:
    categorized = data['categorized_links']
    print("Available link categories:")
    for category, links in categorized.items():
        print(f"  {category.upper()}: {len(links)} links")
        if links:  # Show sample links from each category
            print(f"    Sample links:")
            for i, link in enumerate(links[:3], 1):
                if isinstance(link, dict) and 'article_name' in link:
                    print(f"      {i}. {link['article_name']}")
                elif isinstance(link, dict):
                    print(f"      {i}. Keys: {list(link.keys())}")
                else:
                    print(f"      {i}. {link}")
            if len(links) > 3:
                print(f"      ... and {len(links) - 3} more")
        print()
else:
    print("No categorized links found")

print(f"=== EXAMINING ALL WIKIPEDIA LINKS STRUCTURE ===\n")

if 'wikipedia_links' in data:
    all_links = data['wikipedia_links']
    print(f"Total Wikipedia links: {len(all_links)}")
    
    if all_links:
        print("\nStructure of first link:")
        first_link = all_links[0]
        if isinstance(first_link, dict):
            for key, value in first_link.items():
                print(f"  {key}: {value}")
        
        print("\nSample of first 10 links:")
        for i, link in enumerate(all_links[:10], 1):
            if isinstance(link, dict) and 'article_name' in link:
                article_name = link.get('article_name', 'Unknown')
                link_text = link.get('text', 'No text')
                href = link.get('href', 'No URL')
                print(f"  {i:2d}. {article_name}")
                print(f"      Text: {link_text[:50]}{'...' if len(link_text) > 50 else ''}")
                print(f"      URL: {href}")
                print()
else:
    print("No wikipedia_links found")

print(f"=== LOOKING FOR FANTASY/LITERATURE CONNECTIONS ===\n")

# Now that we understand the structure, let's examine links that might connect to fantasy literature
if 'categorized_links' in data:
    relevant_categories = ['fantasy', 'literature', 'authors', 'related_works']
    
    print("Examining most relevant categories for path-finding to 'A Song of Ice and Fire':\n")
    
    for category in relevant_categories:
        if category in data['categorized_links']:
            links = data['categorized_links'][category]
            print(f"{category.upper()} CATEGORY ({len(links)} links):")
            
            for i, link in enumerate(links, 1):
                if isinstance(link, dict) and 'article_name' in link:
                    article_name = link['article_name']
                    link_text = link.get('text', '')
                    
                    # Highlight potentially interesting links
                    interesting_keywords = ['fantasy', 'epic', 'series', 'saga', 'author', 'writer', 'novel', 'fiction', 'literature', 'genre', 'medieval', 'dragon', 'magic']
                    is_interesting = any(keyword in article_name.lower() or keyword in link_text.lower() for keyword in interesting_keywords)
                    
                    marker = "üåü" if is_interesting else "  "
                    print(f"  {marker} {i:2d}. {article_name}")
                    if link_text != article_name:
                        print(f"         Link text: {link_text[:60]}{'...' if len(link_text) > 60 else ''}")
            print()

# Check if any links directly mention related fantasy works
print(f"=== SCANNING FOR DIRECT FANTASY CONNECTIONS ===\n")

if 'wikipedia_links' in data:
    all_links = data['wikipedia_links']
    fantasy_keywords = ['song of ice and fire', 'game of thrones', 'george r r martin', 'george martin', 'fantasy literature', 'epic fantasy', 'fantasy series', 'fantasy saga', 'fantasy genre', 'high fantasy']
    
    print("Scanning all links for direct fantasy connections...")
    direct_connections = []
    
    for link in all_links:
        if isinstance(link, dict):
            article_name = link.get('article_name', '').lower()
            link_text = link.get('text', '').lower()
            combined_text = f"{article_name} {link_text}"
            
            for keyword in fantasy_keywords:
                if keyword in combined_text:
                    direct_connections.append({
                        'keyword_matched': keyword,
                        'link': link
                    })
                    break
    
    if direct_connections:
        print(f"\nüéØ Found {len(direct_connections)} direct fantasy connections:")
        for i, connection in enumerate(direct_connections, 1):
            link = connection['link']
            keyword = connection['keyword_matched']
            print(f"  {i}. {link.get('article_name', 'Unknown')} (matched: '{keyword}')")
            print(f"     URL: {link.get('href', 'No URL')}")
            print()
    else:
        print("\n‚ùå No direct connections to 'A Song of Ice and Fire' found")
        print("    This is expected - we'll need to use multi-step path-finding")

print(f"\n=== ANALYSIS SUMMARY ===\n")
print("‚úÖ Successfully inspected the LOTR Wikipedia links data structure")
print(f"üìä Data contains {len(data.get('wikipedia_links', []))} total Wikipedia links")
print(f"üóÇÔ∏è Links are organized into {len(data.get('categorized_links', {}))} categories")
print(f"üîç Most promising categories for path-finding:")
if 'categorized_links' in data:
    for category, links in data['categorized_links'].items():
        if len(links) > 0:
            print(f"   - {category.upper()}: {len(links)} links")
print(f"\nüéØ Ready to begin systematic path-finding analysis")
print(f"üìù Next step: Implement breadth-first search algorithm using these links")
```