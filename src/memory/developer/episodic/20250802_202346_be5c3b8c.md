### Development Step 10: 2011 Bulgarian Census: Tertiary Education Completion by Gender

**Description**: Search for the 2011 Bulgarian census data on tertiary education completion by gender. Look for official census results from Bulgaria's National Statistical Institute or other authoritative sources that provide the specific breakdown of men and women who completed tertiary education in 2011.

**Use Cases**:
- Government education department automates extraction of 2011 Bulgarian census tertiary education completion rates by gender to inform national higher-education funding allocations and policy adjustments
- Academic researcher aggregates historical gender-balanced tertiary graduation data from Bulgaria’s NSI website for longitudinal studies on educational attainment trends across Eastern Europe
- Non-profit advocacy group monitors gender equality in higher education by periodically scraping and analyzing Bulgarian census tables, generating reports to support UNESCO gender parity initiatives
- Higher-education consultancy integrates automated parsing of Bulgaria’s 2011 tertiary education statistics into its market analysis tool to advise international universities on recruitment strategies
- Labor-market analytics firm mines gender-segmented graduate numbers from Bulgarian census HTML to forecast skill supply in STEM fields for corporate workforce planning
- EU statistical office builds a scraper pipeline to collate Bulgaria’s tertiary education gender breakdown alongside other member states, enabling comparative policy benchmarking in Brussels
- Data journalism team uses automated HTML parsing of Bulgaria’s 2011 census education tables to create interactive infographics and articles highlighting shifts in male vs. female university attainment

```
import os
import json
from bs4 import BeautifulSoup
import re
import time

# Let's start by inspecting the existing analysis files to understand what we have
print("=== INSPECTING EXISTING ANALYSIS FILES ===")
print("First, let's check what analysis files already exist in workspace\n")

workspace_path = 'workspace'
if os.path.exists(workspace_path):
    all_files = os.listdir(workspace_path)
    json_files = [f for f in all_files if f.endswith('.json')]
    
    print(f"JSON analysis files found: {len(json_files)}")
    for json_file in json_files:
        filepath = os.path.join(workspace_path, json_file)
        file_size = os.path.getsize(filepath)
        print(f"  {json_file} - {file_size:,} bytes")
    
    # Let's inspect the structure of the most recent analysis file
    if json_files:
        latest_json = json_files[-1]  # Take the last one
        print(f"\nInspecting structure of: {latest_json}")
        
        with open(os.path.join(workspace_path, latest_json), 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                print(f"Top-level keys: {list(data.keys())}")
                
                if 'analysis_results' in data:
                    print(f"Analysis results count: {len(data.get('analysis_results', []))}")
                    if data['analysis_results']:
                        print(f"Sample result keys: {list(data['analysis_results'][0].keys())}")
            except json.JSONDecodeError as e:
                print(f"Error reading JSON: {e}")

print("\n=== DIRECT HTML FILE ANALYSIS ===")
print("Now let's directly analyze the HTML files with proper variable handling\n")

# Get all HTML files
html_files = [f for f in os.listdir(workspace_path) if f.endswith('.html')]
print(f"HTML files to analyze: {len(html_files)}")

# Focus on the most promising files based on names
high_priority_files = []
for filename in html_files:
    filepath = os.path.join(workspace_path, filename)
    file_size = os.path.getsize(filepath)
    
    # Skip very small files
    if file_size < 10000:  # Less than 10KB likely error pages
        continue
    
    filename_lower = filename.lower()
    
    # Priority scoring
    score = 0
    if 'demographics' in filename_lower:
        score += 4
    if 'education' in filename_lower:
        score += 4  
    if 'census' in filename_lower:
        score += 3
    if 'bulgaria' in filename_lower:
        score += 2
    if 'nsi' in filename_lower:
        score += 2
    if 'eurostat' in filename_lower:
        score += 1
    
    if score > 0:
        high_priority_files.append({
            'filename': filename,
            'score': score,
            'size': file_size
        })

# Sort by score
high_priority_files.sort(key=lambda x: x['score'], reverse=True)

print(f"High priority files identified: {len(high_priority_files)}")
for i, file_info in enumerate(high_priority_files[:8], 1):  # Show top 8
    print(f"{i}. {file_info['filename']} (Score: {file_info['score']}, Size: {file_info['size']:,} bytes)")

# Now let's analyze the top files with FIXED variable handling
print("\n=== ANALYZING TOP PRIORITY FILES ===\n")

successful_analyses = []
specific_findings = []

# Analyze top 3 files to avoid overwhelming output
for file_info in high_priority_files[:3]:
    filename = file_info['filename']
    print(f"Analyzing: {filename}")
    
    filepath = os.path.join(workspace_path, filename)
    
    try:
        # Read HTML content
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # Parse HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Get title
        title_element = soup.find('title')
        page_title = title_element.get_text().strip() if title_element else 'No title'
        
        # Extract text content
        full_text = soup.get_text()
        # CRITICAL FIX: Create lowercase version immediately
        full_text_lower = full_text.lower()
        
        print(f"  Title: {page_title}")
        print(f"  Content length: {len(full_text):,} characters")
        
        # Check for key indicators using the properly defined variable
        has_bulgaria = 'bulgaria' in full_text_lower or 'bulgarian' in full_text_lower
        has_2011 = '2011' in full_text_lower
        has_census = 'census' in full_text_lower
        has_tertiary = any(term in full_text_lower for term in ['tertiary', 'tertiary education', 'higher education'])
        has_gender = any(term in full_text_lower for term in ['men', 'women', 'male', 'female', 'gender', 'sex'])
        has_education = 'education' in full_text_lower
        
        relevance_score = sum([has_bulgaria, has_2011, has_census, has_tertiary, has_gender, has_education])
        
        print(f"  Bulgaria: {has_bulgaria} | 2011: {has_2011} | Census: {has_census}")
        print(f"  Tertiary: {has_tertiary} | Gender: {has_gender} | Education: {has_education}")
        print(f"  Relevance score: {relevance_score}/6")
        
        if relevance_score >= 4:  # High relevance threshold
            print(f"  *** HIGH RELEVANCE - DEEP ANALYSIS ***")
            
            # Search for specific patterns related to tertiary education by gender
            education_patterns = [
                r'tertiary education.*?(?:men|women|male|female).*?(\d+[.,]?\d*\s*%?)',
                r'(?:men|women|male|female).*?tertiary education.*?(\d+[.,]?\d*\s*%?)',
                r'higher education.*?(?:men|women|male|female).*?(\d+[.,]?\d*\s*%?)',
                r'university.*?(?:men|women|male|female).*?(\d+[.,]?\d*\s*%?)',
                r'completed.*?tertiary.*?(\d+[.,]?\d*\s*%?)',
                r'2011.*?education.*?(?:men|women|male|female).*?(\d+[.,]?\d*\s*%?)',
                r'census.*?2011.*?education.*?(\d+[.,]?\d*\s*%?)'
            ]
            
            pattern_matches = []
            for pattern in education_patterns:
                matches = re.finditer(pattern, full_text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    # Get context around the match
                    start = max(0, match.start() - 200)
                    end = min(len(full_text), match.end() + 200)
                    context = full_text[start:end].strip()
                    
                    # Ensure it's about Bulgaria
                    context_lower = context.lower()
                    if 'bulgaria' in context_lower or 'bulgarian' in context_lower:
                        pattern_matches.append({
                            'match': match.group(),
                            'context': context,
                            'pattern': pattern
                        })
            
            print(f"  Pattern matches found: {len(pattern_matches)}")
            
            # Look for relevant sentences with numbers
            relevant_sentences = []
            sentences = full_text.split('.')
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 50:  # Skip short sentences
                    continue
                
                sentence_lower = sentence.lower()
                
                # Check if sentence contains Bulgaria + education + numbers + gender
                if ('bulgaria' in sentence_lower and
                    any(edu in sentence_lower for edu in ['education', 'tertiary', 'university', 'degree']) and
                    re.search(r'\d+', sentence) and
                    any(gender in sentence_lower for gender in ['men', 'women', 'male', 'female'])):
                    relevant_sentences.append(sentence)
            
            print(f"  Relevant sentences: {len(relevant_sentences)}")
            
            # Analyze tables for structured data
            tables = soup.find_all('table')
            education_tables = []
            
            for table_idx, table in enumerate(tables):
                table_text = table.get_text().lower()
                
                # Check if table has education and gender data
                if (any(term in table_text for term in ['education', 'tertiary', 'university']) and
                    any(term in table_text for term in ['men', 'women', 'male', 'female', 'gender'])):
                    
                    # Extract table headers and sample data
                    headers = [th.get_text().strip() for th in table.find_all('th')]
                    rows = table.find_all('tr')
                    
                    # Get first few data rows
                    sample_data = []
                    for row in rows[1:4]:  # Skip header, take next 3 rows
                        cells = [td.get_text().strip() for td in row.find_all(['td', 'th'])]
                        if cells:
                            sample_data.append(cells)
                    
                    education_tables.append({
                        'index': table_idx,
                        'headers': headers,
                        'sample_data': sample_data,
                        'total_rows': len(rows)
                    })
            
            print(f"  Education tables: {len(education_tables)}")
            
            # Store comprehensive results
            analysis_result = {
                'filename': filename,
                'title': page_title,
                'content_length': len(full_text),
                'relevance_score': relevance_score,
                'pattern_matches': pattern_matches[:5],  # Top 5
                'relevant_sentences': relevant_sentences[:3],  # Top 3
                'education_tables': education_tables[:2],  # Top 2
                'indicators': {
                    'bulgaria': has_bulgaria,
                    '2011': has_2011,
                    'census': has_census,
                    'tertiary': has_tertiary,
                    'gender': has_gender,
                    'education': has_education
                }
            }
            
            successful_analyses.append(analysis_result)
            
            # Add findings to specific findings list
            for match in pattern_matches:
                specific_findings.append({
                    'type': 'pattern_match',
                    'source_file': filename,
                    'match': match['match'],
                    'context': match['context'],
                    'pattern': match['pattern']
                })
            
            for sentence in relevant_sentences:
                specific_findings.append({
                    'type': 'relevant_sentence',
                    'source_file': filename,
                    'content': sentence
                })
            
            # Show key findings
            if pattern_matches:
                print(f"  Key match: {pattern_matches[0]['match']}")
                print(f"  Context: {pattern_matches[0]['context'][:150]}...")
            
            if relevant_sentences:
                print(f"  Key sentence: {relevant_sentences[0][:200]}...")
            
            if education_tables:
                print(f"  Table headers: {education_tables[0]['headers'][:5]}")
        
        else:
            print(f"  Lower relevance - basic analysis only")
            successful_analyses.append({
                'filename': filename,
                'title': page_title,
                'content_length': len(full_text),
                'relevance_score': relevance_score,
                'basic_analysis_only': True
            })
        
        print()
        
    except Exception as e:
        print(f"  Error analyzing {filename}: {str(e)}")
        print()

# Save final results
print("=== FINAL RESULTS COMPILATION ===\n")

final_results = {
    'objective': 'Bulgarian 2011 census tertiary education completion by gender',
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'html_files_total': len(html_files),
    'high_priority_files': len(high_priority_files),
    'files_analyzed': len(successful_analyses),
    'specific_findings_count': len(specific_findings),
    'successful_analyses': successful_analyses,
    'specific_findings': specific_findings
}

results_file = os.path.join(workspace_path, 'bulgarian_tertiary_education_gender_analysis.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False)

print(f"Final results saved to: {results_file}")
print(f"Files successfully analyzed: {len(successful_analyses)}")
print(f"Specific findings extracted: {len(specific_findings)}")

# Display summary of key findings
if specific_findings:
    print("\n=== KEY FINDINGS SUMMARY ===\n")
    
    for i, finding in enumerate(specific_findings[:5], 1):  # Show top 5
        print(f"{i}. Type: {finding['type']}")
        print(f"   Source: {finding['source_file']}")
        if finding['type'] == 'pattern_match':
            print(f"   Match: {finding['match']}")
            print(f"   Context: {finding['context'][:200]}...")
        else:
            print(f"   Content: {finding['content'][:200]}...")
        print()

print("=== ANALYSIS COMPLETE ===\n")
print("Bulgarian 2011 census tertiary education data by gender extraction completed")
print("All findings compiled and saved for review")
```