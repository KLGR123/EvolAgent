### Development Step 37: Locate Shared Critical Word in Dragon Portrayal Quotes from Midkiff’s June 2014 Fafnir Article

**Description**: Access Emily Midkiff's June 2014 article in the Fafnir journal to extract the full text and identify the specific word that was quoted from two different authors expressing distaste for the nature of dragon depictions. Parse the article content systematically to locate quotes from multiple authors that contain the same critical word about dragon portrayals.

**Use Cases**:
- Literary comparative study of dragon imagery to help a medieval literature PhD candidate identify and analyze recurring critical descriptors across multiple scholarly articles in fantasy studies
- Editorial workflow automation for a niche fantasy magazine to extract and consolidate author quotes on dragon depictions from past issues, streamlining themed issue curation
- Catalog enrichment in academic libraries to automatically tag and index articles discussing dragon mythology by extracting shared critique terms, improving discoverability for researchers
- Product design feedback analysis in tabletop RPG development, enabling designers to gather consistent negative descriptors used by reviewers about dragon miniatures to inform design revisions
- Marketing sentiment tracking for a film studio to monitor and aggregate critical language used by scholars in journals about dragon portrayals, guiding promotional messaging
- Academic integrity verification for journal publishers to detect identical critical phrases across papers, ensuring proper citation and preventing unintentional self-plagiarism in mythological research
- AI knowledge base building for virtual assistants in fantasy lore applications, extracting unified critique vocabulary on dragons to power nuanced Q&A about mythical creature depictions
- Intellectual property case support for law firms to compile consistent descriptors used by experts in dragon imagery, strengthening arguments in cultural trademark and design rights disputes

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin
import time

print('=== EMILY MIDKIFF ARTICLE EXTRACTION - ROBUST APPROACH ===')
print('Objective: Extract full text and identify shared critical word about dragon depictions\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Target URL for Fafnir 2/2014 issue
target_issue_url = 'https://journal.finfar.org/journal/archive/fafnir-22014/'
print(f'Target issue: Fafnir 2/2014')
print(f'URL: {target_issue_url}\n')

# Headers to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

print('=== STEP 1: ACCESSING FAFNIR 2/2014 ISSUE PAGE ===')

try:
    print(f'Requesting: {target_issue_url}')
    issue_response = requests.get(target_issue_url, headers=headers, timeout=30)
    print(f'Response status: {issue_response.status_code}')
    print(f'Content length: {len(issue_response.content):,} bytes')
    print(f'Content type: {issue_response.headers.get("Content-Type", "unknown")}\n')
    
    if issue_response.status_code == 200:
        # Parse the HTML content
        soup = BeautifulSoup(issue_response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'Page title: {page_title.get_text().strip()}')
        
        # Extract all text content for analysis
        page_text = soup.get_text()
        print(f'Total page text length: {len(page_text):,} characters\n')
        
        # Confirm this page contains Emily Midkiff
        if 'midkiff' in page_text.lower():
            print('✓ Confirmed: Page contains "Midkiff"')
            
            # Find the exact context where Midkiff appears
            midkiff_indices = []
            text_lower = page_text.lower()
            start = 0
            while True:
                index = text_lower.find('midkiff', start)
                if index == -1:
                    break
                midkiff_indices.append(index)
                start = index + 1
            
            print(f'Found "Midkiff" at {len(midkiff_indices)} positions in the text')
            
            # Show context around each occurrence
            for i, index in enumerate(midkiff_indices, 1):
                context_start = max(0, index - 150)
                context_end = min(len(page_text), index + 150)
                context = page_text[context_start:context_end].replace('\n', ' ').strip()
                print(f'\nOccurrence {i} context:')
                print(f'...{context}...')
        else:
            print('⚠ Warning: "Midkiff" not found in page text')
        
        print('\n=== STEP 2: EXTRACTING ALL ARTICLE LINKS FROM THE ISSUE PAGE ===')
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Total links found on page: {len(all_links)}')
        
        # Filter links that might be articles - COMPLETELY REWRITTEN
        potential_article_links = []
        
        for link_element in all_links:
            href_value = link_element.get('href')
            text_value = link_element.get_text().strip()
            
            # Skip empty links or navigation links
            if not href_value or not text_value:
                continue
            
            # Convert relative URLs to absolute
            if href_value.startswith('/'):
                full_url = urljoin('https://journal.finfar.org', href_value)
            elif not href_value.startswith('http'):
                full_url = urljoin(target_issue_url, href_value)
            else:
                full_url = href_value
            
            # Look for links that might be articles (contain meaningful text)
            navigation_words = ['home', 'archive', 'about', 'contact', 'menu', 'navigation', 'search']
            is_navigation = any(nav_word in text_value.lower() for nav_word in navigation_words)
            
            if len(text_value) > 10 and not is_navigation:
                has_midkiff = 'midkiff' in text_value.lower()
                potential_article_links.append({
                    'text': text_value,
                    'url': full_url,
                    'has_midkiff': has_midkiff
                })
        
        print(f'Potential article links found: {len(potential_article_links)}')
        
        # Show all potential article links
        print('\n--- All Potential Article Links ---')
        for i, link_data in enumerate(potential_article_links, 1):
            marker = '*** MIDKIFF ***' if link_data['has_midkiff'] else ''
            print(f'{i:2d}. {marker}')
            if len(link_data['text']) > 100:
                print(f'    Text: {link_data["text"][:100]}...')
            else:
                print(f'    Text: {link_data["text"]}')
            print(f'    URL:  {link_data["url"]}')
            print()
        
        # Find Emily Midkiff's specific article
        midkiff_links = [link_data for link_data in potential_article_links if link_data['has_midkiff']]
        
        if midkiff_links:
            print(f'=== FOUND {len(midkiff_links)} MIDKIFF ARTICLE LINK(S) ===')
            
            # Use the first Midkiff link (should be the main one)
            target_article = midkiff_links[0]
            print(f'Selected article:')
            print(f'Title: {target_article["text"]}')
            print(f'URL: {target_article["url"]}\n')
            
            print('=== STEP 3: ACCESSING EMILY MIDKIFF\'S ARTICLE ===')
            
            try:
                print(f'Accessing article: {target_article["url"]}')
                article_response = requests.get(target_article['url'], headers=headers, timeout=30)
                print(f'Article response status: {article_response.status_code}')
                print(f'Article content length: {len(article_response.content):,} bytes\n')
                
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.content, 'html.parser')
                    
                    # Get article title from the page
                    article_title_elem = article_soup.find('title')
                    if article_title_elem:
                        article_title = article_title_elem.get_text().strip()
                        print(f'Article page title: {article_title}')
                    
                    # Remove scripts, styles, and navigation elements
                    for element in article_soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
                        element.decompose()
                    
                    # Try multiple selectors to find the main article content
                    content_selectors = [
                        '.article-content',
                        '.article-body', 
                        '.entry-content',
                        '.post-content',
                        '.content',
                        'main',
                        '#content',
                        '.text',
                        'article'
                    ]
                    
                    article_content = None
                    used_selector = None
                    
                    for selector in content_selectors:
                        content_elem = article_soup.select_one(selector)
                        if content_elem:
                            article_content = content_elem.get_text()
                            used_selector = selector
                            print(f'✓ Article content extracted using selector: {selector}')
                            break
                    
                    if not article_content:
                        # Fallback to full page text
                        article_content = article_soup.get_text()
                        used_selector = 'full_page_fallback'
                        print('Using full page text as fallback')
                    
                    # Clean up the extracted text
                    lines = (line.strip() for line in article_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))
                    clean_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    print(f'✓ Cleaned article text: {len(clean_content):,} characters\n')
                    
                    # Save the full article text
                    article_text_file = os.path.join(workspace, 'midkiff_fafnir_article_full_text.txt')
                    with open(article_text_file, 'w', encoding='utf-8') as f:
                        f.write(f'Title: {target_article["text"]}\n')
                        f.write(f'URL: {target_article["url"]}\n')
                        f.write(f'Extraction method: {used_selector}\n')
                        f.write(f'Extracted: {time.strftime("%Y-%m-%d %H:%M:%S")}\n')
                        f.write('=' * 80 + '\n\n')
                        f.write(clean_content)
                    
                    print(f'✓ Full article text saved to: {article_text_file}')
                    
                    # Save raw HTML for backup
                    article_html_file = os.path.join(workspace, 'midkiff_fafnir_article_raw.html')
                    with open(article_html_file, 'w', encoding='utf-8') as f:
                        f.write(article_response.text)
                    
                    print(f'✓ Raw article HTML saved to: {article_html_file}\n')
                    
                    # Save article metadata for next step
                    metadata = {
                        'article_info': {
                            'title': target_article['text'],
                            'url': target_article['url'],
                            'extraction_method': used_selector,
                            'content_length': len(clean_content),
                            'extracted_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        },
                        'files_created': {
                            'full_text': article_text_file,
                            'raw_html': article_html_file
                        }
                    }
                    
                    metadata_file = os.path.join(workspace, 'midkiff_article_metadata.json')
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    
                    print(f'✓ Article metadata saved to: {metadata_file}')
                    
                    # Summary
                    print('\n=== EXTRACTION SUMMARY ===')
                    print(f'✓ Successfully accessed Emily Midkiff\'s article in Fafnir 2/2014')
                    print(f'✓ Article title: "{target_article["text"]}"')
                    print(f'✓ Extracted {len(clean_content):,} characters of article text')
                    print(f'✓ Files saved to workspace:')
                    print(f'  - Full text: {article_text_file}')
                    print(f'  - Raw HTML: {article_html_file}')
                    print(f'  - Metadata: {metadata_file}')
                    print('\n✓ Ready for quote analysis in next step')
                    
                else:
                    print(f'✗ Failed to access article: HTTP {article_response.status_code}')
                    
            except Exception as e:
                print(f'Error accessing article: {str(e)}')
        
        else:
            print('⚠ No links containing "Midkiff" found on the issue page')
            print('\nLet me search more broadly in the page content...')
            
            # Alternative approach: search for Emily Midkiff in the page text more systematically
            text_lines = page_text.split('\n')
            midkiff_lines = [line.strip() for line in text_lines if 'midkiff' in line.lower() and line.strip()]
            
            print(f'\nLines containing "Midkiff": {len(midkiff_lines)}')
            for i, line in enumerate(midkiff_lines, 1):
                if len(line) > 200:
                    print(f'{i}. {line[:200]}...')
                else:
                    print(f'{i}. {line}')
    
    else:
        print(f'✗ Failed to access issue page: HTTP {issue_response.status_code}')
        print(f'Response content preview: {issue_response.text[:500]}...')

except Exception as e:
    print(f'Error during issue page access: {str(e)}')

print('\n=== EMILY MIDKIFF ARTICLE EXTRACTION COMPLETE ===')
print('Objective: Extract full text and identify shared critical word about dragon depictions')
print('Status: Article extraction completed - ready for quote analysis in next step')
```