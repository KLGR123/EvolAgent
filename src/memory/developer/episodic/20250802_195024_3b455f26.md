### Development Step 38: Identify duplicated word in multiple authors’ dragon portrayal quotes in Midkiff’s June 2014 Fafnir article

**Description**: Access Emily Midkiff's June 2014 article in the Fafnir journal to extract the full text and identify the specific word that was quoted from two different authors expressing distaste for the nature of dragon depictions. Parse the article content systematically to locate quotes from multiple authors that contain the same critical word about dragon portrayals.

**Use Cases**:
- Comparative literary analysis for medieval studies researchers collating critical terminology on dragon imagery across multiple Fafnir journal articles
- Automated archiving and metadata generation for digital libraries ingesting open-access literature criticism from the Fafnir 2/2014 issue
- Editorial quality assurance in academic publishing: verifying correct extraction of author names and article titles (e.g., Emily Midkiff) in journal issue archives
- NLP-driven sentiment analysis pipeline for fantasy literature scholars to identify recurring pejoratives (such as “hackneyed”) used by different critics
- Semantic keyword tracking for genre studies labs monitoring how specific negative descriptors of dragon portrayals evolve over successive journal publications
- Integration into a university’s knowledge management system to auto-populate summaries and direct quotes about dragon depiction critiques for medieval literature courses
- Automated alert system for librarians and archivists to detect new thematic overlaps in vocabulary (shared critical words) when fresh criticism articles are published in Fafnir journal
- Digital humanities teaching tool that automatically extracts primary quotes and metadata from open-access academic journals for student research assignments

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin
import time

print('=== EMILY MIDKIFF ARTICLE EXTRACTION - SIMPLE APPROACH ===')
print('Objective: Extract full text and identify shared critical word about dragon depictions\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Target URL for Fafnir 2/2014 issue
target_issue_url = 'https://journal.finfar.org/journal/archive/fafnir-22014/'
print(f'Target issue: Fafnir 2/2014')
print(f'URL: {target_issue_url}\n')

# Headers to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

print('=== STEP 1: ACCESSING FAFNIR 2/2014 ISSUE PAGE ===')

try:
    print(f'Requesting: {target_issue_url}')
    issue_response = requests.get(target_issue_url, headers=headers, timeout=30)
    print(f'Response status: {issue_response.status_code}')
    print(f'Content length: {len(issue_response.content):,} bytes')
    print(f'Content type: {issue_response.headers.get("Content-Type", "unknown")}\n')
    
    if issue_response.status_code == 200:
        # Parse the HTML content
        soup = BeautifulSoup(issue_response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'Page title: {page_title.get_text().strip()}')
        
        # Extract all text content for analysis
        page_text = soup.get_text()
        print(f'Total page text length: {len(page_text):,} characters\n')
        
        # Confirm this page contains Emily Midkiff
        if 'midkiff' in page_text.lower():
            print('✓ Confirmed: Page contains "Midkiff"')
            
            # Find the exact context where Midkiff appears
            midkiff_indices = []
            text_lower = page_text.lower()
            start = 0
            while True:
                index = text_lower.find('midkiff', start)
                if index == -1:
                    break
                midkiff_indices.append(index)
                start = index + 1
            
            print(f'Found "Midkiff" at {len(midkiff_indices)} positions in the text')
            
            # Show context around each occurrence
            for i, index in enumerate(midkiff_indices, 1):
                context_start = max(0, index - 150)
                context_end = min(len(page_text), index + 150)
                context = page_text[context_start:context_end].replace('\n', ' ').strip()
                print(f'\nOccurrence {i} context:')
                print(f'...{context}...')
        else:
            print('⚠ Warning: "Midkiff" not found in page text')
        
        print('\n=== STEP 2: EXTRACTING ALL ARTICLE LINKS FROM THE ISSUE PAGE ===')
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Total links found on page: {len(all_links)}')
        
        # Filter links that might be articles - SIMPLE APPROACH
        potential_article_links = []
        
        for link in all_links:
            link_href = link.get('href')
            link_text = link.get_text()
            
            if link_href and link_text:
                link_text = link_text.strip()
                
                # Convert relative URLs to absolute
                if link_href.startswith('/'):
                    full_url = 'https://journal.finfar.org' + link_href
                elif not link_href.startswith('http'):
                    full_url = target_issue_url + '/' + link_href
                else:
                    full_url = link_href
                
                # Look for links that might be articles
                if len(link_text) > 10:
                    nav_words = ['home', 'archive', 'about', 'contact', 'menu', 'navigation', 'search']
                    is_nav = False
                    for nav_word in nav_words:
                        if nav_word in link_text.lower():
                            is_nav = True
                            break
                    
                    if not is_nav:
                        has_midkiff = 'midkiff' in link_text.lower()
                        potential_article_links.append({
                            'text': link_text,
                            'url': full_url,
                            'has_midkiff': has_midkiff
                        })
        
        print(f'Potential article links found: {len(potential_article_links)}')
        
        # Show all potential article links
        print('\n--- All Potential Article Links ---')
        for i, link in enumerate(potential_article_links, 1):
            marker = '*** MIDKIFF ***' if link['has_midkiff'] else ''
            print(f'{i:2d}. {marker}')
            if len(link['text']) > 100:
                print(f'    Text: {link["text"][:100]}...')
            else:
                print(f'    Text: {link["text"]}')
            print(f'    URL:  {link["url"]}')
            print()
        
        # Find Emily Midkiff's specific article
        midkiff_links = []
        for link in potential_article_links:
            if link['has_midkiff']:
                midkiff_links.append(link)
        
        if midkiff_links:
            print(f'=== FOUND {len(midkiff_links)} MIDKIFF ARTICLE LINK(S) ===')
            
            # Use the first Midkiff link (should be the main one)
            target_article = midkiff_links[0]
            print(f'Selected article:')
            print(f'Title: {target_article["text"]}')
            print(f'URL: {target_article["url"]}\n')
            
            print('=== STEP 3: ACCESSING EMILY MIDKIFF\'S ARTICLE ===')
            
            try:
                print(f'Accessing article: {target_article["url"]}')
                article_response = requests.get(target_article['url'], headers=headers, timeout=30)
                print(f'Article response status: {article_response.status_code}')
                print(f'Article content length: {len(article_response.content):,} bytes\n')
                
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.content, 'html.parser')
                    
                    # Get article title from the page
                    article_title_elem = article_soup.find('title')
                    if article_title_elem:
                        article_title = article_title_elem.get_text().strip()
                        print(f'Article page title: {article_title}')
                    
                    # Remove scripts, styles, and navigation elements
                    for element in article_soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
                        element.decompose()
                    
                    # Try multiple selectors to find the main article content
                    content_selectors = [
                        '.article-content',
                        '.article-body', 
                        '.entry-content',
                        '.post-content',
                        '.content',
                        'main',
                        '#content',
                        '.text',
                        'article'
                    ]
                    
                    article_content = None
                    used_selector = None
                    
                    for selector in content_selectors:
                        content_elem = article_soup.select_one(selector)
                        if content_elem:
                            article_content = content_elem.get_text()
                            used_selector = selector
                            print(f'✓ Article content extracted using selector: {selector}')
                            break
                    
                    if not article_content:
                        # Fallback to full page text
                        article_content = article_soup.get_text()
                        used_selector = 'full_page_fallback'
                        print('Using full page text as fallback')
                    
                    # Clean up the extracted text
                    lines = (line.strip() for line in article_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))
                    clean_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    print(f'✓ Cleaned article text: {len(clean_content):,} characters\n')
                    
                    # Save the full article text
                    article_text_file = os.path.join(workspace, 'midkiff_fafnir_article_full_text.txt')
                    with open(article_text_file, 'w', encoding='utf-8') as f:
                        f.write(f'Title: {target_article["text"]}\n')
                        f.write(f'URL: {target_article["url"]}\n')
                        f.write(f'Extraction method: {used_selector}\n')
                        f.write(f'Extracted: {time.strftime("%Y-%m-%d %H:%M:%S")}\n')
                        f.write('=' * 80 + '\n\n')
                        f.write(clean_content)
                    
                    print(f'✓ Full article text saved to: {article_text_file}')
                    
                    # Save raw HTML for backup
                    article_html_file = os.path.join(workspace, 'midkiff_fafnir_article_raw.html')
                    with open(article_html_file, 'w', encoding='utf-8') as f:
                        f.write(article_response.text)
                    
                    print(f'✓ Raw article HTML saved to: {article_html_file}\n')
                    
                    # Save article metadata for next step
                    metadata = {
                        'article_info': {
                            'title': target_article['text'],
                            'url': target_article['url'],
                            'extraction_method': used_selector,
                            'content_length': len(clean_content),
                            'extracted_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        },
                        'files_created': {
                            'full_text': article_text_file,
                            'raw_html': article_html_file
                        }
                    }
                    
                    metadata_file = os.path.join(workspace, 'midkiff_article_metadata.json')
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    
                    print(f'✓ Article metadata saved to: {metadata_file}')
                    
                    # Summary
                    print('\n=== EXTRACTION SUMMARY ===')
                    print(f'✓ Successfully accessed Emily Midkiff\'s article in Fafnir 2/2014')
                    print(f'✓ Article title: "{target_article["text"]}"')
                    print(f'✓ Extracted {len(clean_content):,} characters of article text')
                    print(f'✓ Files saved to workspace:')
                    print(f'  - Full text: {article_text_file}')
                    print(f'  - Raw HTML: {article_html_file}')
                    print(f'  - Metadata: {metadata_file}')
                    print('\n✓ Ready for quote analysis in next step')
                    
                else:
                    print(f'✗ Failed to access article: HTTP {article_response.status_code}')
                    
            except Exception as e:
                print(f'Error accessing article: {str(e)}')
        
        else:
            print('⚠ No links containing "Midkiff" found on the issue page')
            print('\nLet me search more broadly in the page content...')
            
            # Alternative approach: search for Emily Midkiff in the page text more systematically
            text_lines = page_text.split('\n')
            midkiff_lines = []
            for line in text_lines:
                if 'midkiff' in line.lower() and line.strip():
                    midkiff_lines.append(line.strip())
            
            print(f'\nLines containing "Midkiff": {len(midkiff_lines)}')
            for i, line in enumerate(midkiff_lines, 1):
                if len(line) > 200:
                    print(f'{i}. {line[:200]}...')
                else:
                    print(f'{i}. {line}')
    
    else:
        print(f'✗ Failed to access issue page: HTTP {issue_response.status_code}')
        print(f'Response content preview: {issue_response.text[:500]}...')

except Exception as e:
    print(f'Error during issue page access: {str(e)}')

print('\n=== EMILY MIDKIFF ARTICLE EXTRACTION COMPLETE ===')
print('Objective: Extract full text and identify shared critical word about dragon depictions')
print('Status: Article extraction completed - ready for quote analysis in next step')
```