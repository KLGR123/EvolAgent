### Development Step 32: Extract Survivor (US) Seasons 1–44 Winners from Wikipedia into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- A TV analytics company automating the extraction of Survivor winners (seasons 1–44) to enrich their monthly audience engagement dashboards with up-to-date champion data
- A pop-culture blog pipeline that scrapes the official Survivor winners table and auto-generates an infographic ranking champions by season for weekly feature posts
- An academic media-studies researcher compiling a dataset of reality-show winners to analyze demographic trends and title diversity over 44 seasons
- An entertainment news aggregator that updates its mobile app’s “Recent Champions” feed by programmatically pulling the latest Survivor winner names from Wikipedia
- A data-science training workshop using the Survivor winners scraper as a hands-on example of HTML parsing, table detection, and JSON output for students
- A fan-community mobile app integrating season-by-season winner data to power a quiz module and historical “Who Won?” trivia game
- A chatbot developer building a Survivor Q&A assistant that references the JSON file of winners to answer user questions like “Who won season 27?”

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the standard (non-printable) Wikipedia page for Survivor
title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/wiki/{title}"
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers=headers)
resp.raise_for_status()
print(f"Page fetched successfully (status {resp.status_code})\n")

# 3) Parse HTML with BeautifulSoup
doc = BeautifulSoup(resp.text, 'html.parser')

# 4) First, scan ALL <table> tags for a simple 2-column winners-only table ['Season','Winner']
print("Scanning all tables for a simple 2-column Season→Winner table...\n")
target = None
all_tables = doc.find_all('table')
for idx, tbl in enumerate(all_tables, 1):
    first_row = tbl.find('tr')
    if not first_row:
        continue
    hdr_cells = first_row.find_all(['th', 'td'], recursive=False)
    hdr_texts = [c.get_text(strip=True).lower() for c in hdr_cells]
    # debug print of header row lengths
    print(f"Table {idx} headers: {hdr_texts}")
    if hdr_texts == ['season', 'winner']:
        target = tbl
        print(f"→ Selected simple 2-column winners-only table #{idx}. Headers match exactly ['Season','Winner']\n")
        break

# 5) Fallback: if no simple table, scan only wikitable-class tables for any containing both keywords
if not target:
    print("No exact 2-column table found; falling back to any .wikitable containing Season & Winner...\n")
    wikitables = doc.find_all('table', class_=lambda v: v and 'wikitable' in v)
    for idx, tbl in enumerate(wikitables, 1):
        first_row = tbl.find('tr')
        if not first_row:
            continue
        hdr_cells = first_row.find_all(['th', 'td'], recursive=False)
        hdr_texts = [c.get_text(strip=True).lower() for c in hdr_cells]
        if 'season' in hdr_texts and 'winner' in hdr_texts:
            target = tbl
            print(f"→ Fallback selected wikitable #{idx} with headers containing Season & Winner: {hdr_texts}\n")
            break

if not target:
    print("❌ Could not find any suitable table with Season & Winner. Exiting.")
    sys.exit(1)

# 6) Determine column indices for Season and Winner
def extract_header_indices(tbl):
    first_row = tbl.find('tr')
    hdr_cells = first_row.find_all(['th', 'td'], recursive=False)
    texts = [c.get_text(strip=True).lower() for c in hdr_cells]
    return texts.index('season'), texts.index('winner')

season_idx, winner_idx = extract_header_indices(target)
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 7) Extract Season→Winner entries
winners = []
for row in target.find_all('tr')[1:]:  # skip header row
    cells = row.find_all(['th', 'td'], recursive=False)
    # ensure enough cells
    if len(cells) <= max(season_idx, winner_idx):
        continue

    # parse season number
    season_text = cells[season_idx].get_text(strip=True)
    if not season_text.isdigit():
        continue
    season_num = int(season_text)
    if season_num < 1 or season_num > 44:
        continue

    # extract winner name: prefer <a> link text
    winner_cell = cells[winner_idx]
    a_tag = winner_cell.find('a')
    if a_tag and re.search(r'[A-Za-z]', a_tag.get_text()):
        name = a_tag.get_text(strip=True)
    else:
        name = winner_cell.get_text(strip=True)

    print(f"Parsed Season {season_num} → Winner: '{name}'")
    winners.append({'season': season_num, 'winner': name})

# 8) Sort & verify count
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: Extracted count != 44. Verify table structure or page updates.")
else:
    # flag any empty winners
    missing = [w['season'] for w in winners_sorted if not w['winner']]
    if missing:
        print(f"⚠️ Seasons with missing winner names: {missing}")

# 9) Save to JSON
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {out_file}")",
  
```