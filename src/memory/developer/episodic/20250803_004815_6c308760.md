### Development Step 7: Research Authoritative Land Area Data for All 39 Washington State County Seats

**Description**: Research and extract the land area data for all 39 Washington state county seats that were identified in the previous step. Use authoritative sources such as the U.S. Census Bureau, official city websites, or government geographic databases to obtain the land area measurements for each county seat city. Focus on finding the official land area figures in square miles or square kilometers for accurate comparison.

**Use Cases**:
- Local government GIS departments integrating county seats’ land area data into statewide mapping portals to enhance public map accuracy and land-use planning
- Real estate investment analysts comparing city land extents and zoning capacities across Washington county seats to identify high-growth development opportunities
- Environmental consulting firms using precise urban land footprints to model stormwater runoff, design green infrastructure, and comply with state environmental regulations
- Academic urban studies researchers correlating county seat land areas with historical population growth to analyze urban sprawl and regional development patterns
- Emergency management agencies mapping jurisdictional land areas of county seats to plan resource allocation, evacuation zones, and disaster response logistics
- Public health planners leveraging city land area metrics to optimize the placement of vaccination clinics and ensure equitable healthcare service coverage
- Transportation planners incorporating county seat land area figures into traffic simulation models to design efficient transit routes and reduce congestion
- State tourism boards comparing city land sizes to rank destinations, craft promotional itineraries, and highlight unique geographic characteristics of Washington’s county seats

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

print("=== WASHINGTON STATE COUNTY SEATS LAND AREA RESEARCH ===\n")
print("Objective: Extract land area data for all 39 Washington state county seats")
print("Sources: U.S. Census Bureau, official city websites, government databases\n")

# Load the county seats data from the JSON file
print("=== LOADING COUNTY SEATS DATA ===\n")

with open('workspace/wa_county_seats.json', 'r') as f:
    county_seats_data = json.load(f)

print(f"Loaded data for {len(county_seats_data)} county seats")
print("\nCounty seats to research:")
for i, seat in enumerate(county_seats_data, 1):
    print(f"  {i:2d}. {seat['county_seat']} ({seat['county']})")

# Initialize data structure for land area research
land_area_results = []

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print("\n=== BEGINNING LAND AREA RESEARCH ===\n")
print("Strategy: Use Wikipedia city pages as primary source for land area data")
print("Wikipedia typically contains official U.S. Census Bureau land area figures\n")

# Research land area for each county seat
for i, seat_data in enumerate(county_seats_data, 1):
    county_seat = seat_data['county_seat']
    county = seat_data['county']
    
    print(f"[{i:2d}/39] Researching {county_seat}, Washington...")
    
    # Construct Wikipedia URL for the city
    # Handle special cases for city names with spaces or special characters
    city_name_formatted = county_seat.replace(' ', '_')
    wikipedia_url = f"https://en.wikipedia.org/wiki/{city_name_formatted},_Washington"
    
    try:
        # Make request to Wikipedia page
        response = requests.get(wikipedia_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # Parse the HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for land area information in the infobox
        land_area_found = None
        area_unit = None
        
        # Method 1: Look for infobox with area information
        infobox = soup.find('table', class_='infobox')
        if infobox:
            # Look for rows containing area information
            rows = infobox.find_all('tr')
            for row in rows:
                row_text = row.get_text().lower()
                if 'area' in row_text and ('land' in row_text or 'total' in row_text):
                    # Extract the area value
                    cells = row.find_all(['th', 'td'])
                    if len(cells) >= 2:
                        area_cell = cells[1].get_text().strip()
                        
                        # Parse area value and unit
                        area_match = re.search(r'([0-9,.]+)\s*(sq\s*mi|km²|square miles|square kilometers)', area_cell, re.IGNORECASE)
                        if area_match:
                            land_area_found = area_match.group(1).replace(',', '')
                            unit_text = area_match.group(2).lower()
                            if 'sq mi' in unit_text or 'square miles' in unit_text:
                                area_unit = 'sq_miles'
                            elif 'km' in unit_text or 'square kilometers' in unit_text:
                                area_unit = 'sq_kilometers'
                            break
        
        # Method 2: Look for area information in the page text
        if not land_area_found:
            page_text = soup.get_text()
            # Look for patterns like "total area of X square miles" or "land area is X sq mi"
            area_patterns = [
                r'total area of ([0-9,.]+)\s*(square miles|sq\s*mi)',
                r'land area[^0-9]*([0-9,.]+)\s*(square miles|sq\s*mi)',
                r'area[^0-9]*([0-9,.]+)\s*(square miles|sq\s*mi)',
                r'([0-9,.]+)\s*(square miles|sq\s*mi)[^0-9]*total',
                r'([0-9,.]+)\s*(square miles|sq\s*mi)[^0-9]*land'
            ]
            
            for pattern in area_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    land_area_found = match.group(1).replace(',', '')
                    area_unit = 'sq_miles'
                    break
        
        # Store the results
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': land_area_found,
            'area_unit': area_unit,
            'wikipedia_url': wikipedia_url,
            'data_source': 'Wikipedia',
            'extraction_success': land_area_found is not None
        }
        
        land_area_results.append(result)
        
        if land_area_found:
            print(f"  ✓ Found: {land_area_found} {area_unit.replace('_', ' ')}")
        else:
            print(f"  ✗ No land area data found")
            
    except requests.RequestException as e:
        print(f"  ✗ Request failed: {str(e)}")
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': None,
            'area_unit': None,
            'wikipedia_url': wikipedia_url,
            'data_source': 'Wikipedia',
            'extraction_success': False,
            'error': str(e)
        }
        land_area_results.append(result)
    
    except Exception as e:
        print(f"  ✗ Error processing: {str(e)}")
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': None,
            'area_unit': None,
            'wikipedia_url': wikipedia_url,
            'data_source': 'Wikipedia',
            'extraction_success': False,
            'error': str(e)
        }
        land_area_results.append(result)
    
    # Be respectful to Wikipedia servers
    time.sleep(1)
    
    # Show progress every 10 cities
    if i % 10 == 0:
        successful = len([r for r in land_area_results if r['extraction_success']])
        print(f"\n  Progress: {i}/39 cities processed, {successful} successful extractions\n")

# Final results summary
print("\n=== LAND AREA RESEARCH COMPLETE ===\n")

successful_extractions = [r for r in land_area_results if r['extraction_success']]
failed_extractions = [r for r in land_area_results if not r['extraction_success']]

print(f"Total cities researched: {len(land_area_results)}")
print(f"Successful extractions: {len(successful_extractions)}")
print(f"Failed extractions: {len(failed_extractions)}")
print(f"Success rate: {len(successful_extractions)/len(land_area_results)*100:.1f}%")

# Show successful results
if successful_extractions:
    print(f"\nSuccessful land area extractions:")
    for result in successful_extractions:
        area_display = f"{result['land_area']} {result['area_unit'].replace('_', ' ')}"
        print(f"  {result['county_seat']}: {area_display}")

# Show failed extractions for troubleshooting
if failed_extractions:
    print(f"\nFailed extractions (need alternative sources):")
    for result in failed_extractions:
        print(f"  {result['county_seat']} ({result['county']})")

# Save complete results to JSON file
results_data = {
    'research_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'total_cities': len(land_area_results),
    'successful_extractions': len(successful_extractions),
    'failed_extractions': len(failed_extractions),
    'success_rate_percent': round(len(successful_extractions)/len(land_area_results)*100, 1),
    'data_source': 'Wikipedia (contains U.S. Census Bureau data)',
    'extraction_method': 'Web scraping with area pattern matching',
    'results': land_area_results
}

with open('workspace/wa_county_seats_land_area.json', 'w') as f:
    json.dump(results_data, f, indent=2)

print(f"\n✓ Complete results saved to: workspace/wa_county_seats_land_area.json")
print(f"\nNext step: For failed extractions, try alternative sources like U.S. Census Bureau API or official city websites")
```