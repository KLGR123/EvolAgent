### Development Step 1: Title: Identify US Survivor Winners Born in May Using Official Winner Lists and Birth Date Sources

**Description**: Search for comprehensive information about US Survivor winners and their birth dates. Focus on finding official sources such as CBS Survivor databases, Wikipedia pages, or entertainment databases that list all winners from Season 1 through the most recent season available as of August 2023. Extract winner names, seasons, and birth dates to identify any winners born in May.

**Use Cases**:
- Entertainment journalism fact-checking and profile creation for Survivor winners, ensuring accurate birth date and season information in articles
- Automated trivia database generation for fan websites, enabling users to search for Survivor winners by birth month or other criteria
- Data-driven casting analysis for TV production companies, studying demographic trends among past Survivor winners
- Academic research on reality TV contestant demographics, compiling comprehensive winner lists for sociological studies
- Birthday-based social media campaign planning for Survivor fan clubs, identifying winners born in May for celebratory posts
- Building interactive dashboards for Survivor analytics platforms, allowing users to filter and visualize winner data by birth date and season

```
import os
import sys
import json
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime

# 1) Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the main Survivor Wikipedia page
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
print(f"Fetching Survivor main page: {url}")
response = requests.get(url, headers=headers)
response.raise_for_status()
print(f"Page fetched successfully (status: {response.status_code})\n")

# 3) Parse the HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 4) Save the page for inspection
html_path = os.path.join(workspace_dir, 'survivor_main_page.html')
with open(html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"Saved main page HTML to: {html_path}\n")

# 5) Look for sections that might contain winner information
print("Scanning for relevant sections containing winner information...")
sections = soup.find_all('span', class_='mw-headline')
section_info = []
for i, section in enumerate(sections):
    section_id = section.get('id', '')
    section_text = section.get_text(strip=True)
    section_info.append({'id': section_id, 'text': section_text})
    print(f"{i+1:2d}. ID: '{section_id}' | Text: '{section_text}'")

# 6) Save section information
sections_path = os.path.join(workspace_dir, 'survivor_sections.json')
with open(sections_path, 'w', encoding='utf-8') as f:
    json.dump(section_info, f, indent=2)
print(f"\nSaved section information to: {sections_path}\n")

# 7) Look for tables that might contain winner data
print("Scanning for tables that might contain winner information...")
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} wikitable(s) on the page\n")

for i, table in enumerate(tables[:5]):  # Check first 5 tables
    print(f"--- Table {i+1} ---")
    # Get headers
    header_row = table.find('tr')
    if header_row:
        headers = []
        for cell in header_row.find_all(['th', 'td']):
            header_text = cell.get_text(strip=True)
            headers.append(header_text)
        print(f"Headers: {headers}")
        
        # Check if this looks like a winners table
        header_lower = [h.lower() for h in headers]
        if any('season' in h for h in header_lower) and any('winner' in h for h in header_lower):
            print("*** This table appears to contain season and winner information! ***")
            
            # Extract a few sample rows
            rows = table.find_all('tr')[1:6]  # First 5 data rows
            print("Sample data rows:")
            for j, row in enumerate(rows):
                cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]
                print(f"  Row {j+1}: {cells}")
    print()

# 8) Also check for links to individual season pages
print("Looking for links to individual Survivor season pages...")
season_links = []
for link in soup.find_all('a', href=True):
    href = link.get('href')
    text = link.get_text(strip=True)
    # Look for patterns like "Survivor: Season Name" or "Survivor (U.S. season X)"
    if href and ('/wiki/Survivor' in href or '/wiki/Survivor_' in href):
        if ('season' in text.lower() or 'survivor:' in text.lower()) and 'american' not in text.lower():
            season_links.append({'href': href, 'text': text})

print(f"Found {len(season_links)} potential season page links")
for i, link in enumerate(season_links[:10]):  # Show first 10
    print(f"{i+1:2d}. {link['text']} -> {link['href']}")

# 9) Save season links for further processing
links_path = os.path.join(workspace_dir, 'survivor_season_links.json')
with open(links_path, 'w', encoding='utf-8') as f:
    json.dump(season_links, f, indent=2)
print(f"\nSaved season links to: {links_path}")

print("\n=== Initial reconnaissance complete ===")
print(f"Files created in {workspace_dir}:")
print(f"- survivor_main_page.html (full page HTML)")
print(f"- survivor_sections.json (page sections)")
print(f"- survivor_season_links.json (links to season pages)")
print("\nNext step: Extract winner information from the main table or individual season pages")
```