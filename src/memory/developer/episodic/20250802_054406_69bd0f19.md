### Development Step 38: Download University of Leicester’s “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic literature aggregation for environmental science researchers: automatically scan Bing search results pages to locate and download full-text PDF studies on microplastic pollution for systematic review.
- Legal compliance document retrieval for corporate law teams: extract and compile PDF versions of updated financial regulations from government search results to ensure company policies remain up to date.
- Market intelligence gathering for tech analysts: automate discovery and download of vendor whitepapers in PDF format from search engine result pages to evaluate emerging AI solutions.
- Journalism research support for investigative reporters: identify and fetch PDF versions of leaked government reports and audits referenced in search results to corroborate stories.
- Patent portfolio development for intellectual property firms: scrape search result HTML to find and retrieve PDF patent filings relevant to a client’s technology domain for prior-art analysis.
- E-commerce competitive analysis for retail strategists: scan competitor sites via search listings, extract PDF catalogs and pricing sheets, and download them for automated price comparison.
- Academic conference monitoring for PhD students: automatically parse search engine outputs to detect newly published conference proceedings in PDF form on specific topics (e.g., neural networks) and store them in a research folder.

```
import os
from bs4 import BeautifulSoup

# --- Paths ---
workspace = 'workspace'
bing_html = os.path.join(workspace, 'bing_search.html')
inspection_out = os.path.join(workspace, 'search_results_inspection.txt')

# --- Ensure input exists ---
if not os.path.isfile(bing_html):
    print(f"ERROR: Cannot find Bing HTML at {bing_html}. Please run the search extraction first.")
    sys.exit(1)

print(f"[INFO] Inspecting Bing search HTML: {bing_html}\n")
with open(bing_html, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

# 1) Extract main result anchors (li.b_algo > h2 > a)
anchors = soup.select('li.b_algo h2 a')

# 2) Scan for any <a> elements whose text or href contains '.pdf'
pdf_links = []
for a in soup.find_all('a', href=True):
    href = a['href']
    txt = a.get_text() or ''
    if '.pdf' in href.lower() or '.pdf' in txt.lower():
        pdf_links.append((txt.strip(), href))

# 3) Summarize and write to inspection file
with open(inspection_out, 'w', encoding='utf-8') as out:
    out.write(f"SEARCH RESULTS INSPECTION\n")
    out.write(f"{'='*60}\n\n")
    # Main anchors
    out.write(f"1) Main result anchors (li.b_algo h2 a): {len(anchors)} found\n")
    out.write(f"{'-'*60}\n")
    for i, a in enumerate(anchors[:10], 1):
        title = a.get_text(strip=True)
        href = a['href']
        out.write(f"{i}. Title: {title}\n   URL: {href}\n\n")

    # PDF-like links
    out.write(f"2) <a> tags with '.pdf' in text or href: {len(pdf_links)} found\n")
    out.write(f"{'-'*60}\n")
    for i, (txt, href) in enumerate(pdf_links[:10], 1):
        display_txt = txt if txt else '[no link text]'
        out.write(f"{i}. Text: {display_txt}\n   Href: {href}\n\n")

    # Summary
    out.write(f"{'='*60}\n")
    out.write(f"SUMMARY:\n")
    out.write(f"- Total main anchors: {len(anchors)}\n")
    out.write(f"- Total PDF-like <a> tags: {len(pdf_links)}\n")

print(f"[SAVE] Inspection details written to: {inspection_out}")
print("[COMPLETE] Please open the inspection file to understand the HTML structure and adjust your extraction logic accordingly.")
```