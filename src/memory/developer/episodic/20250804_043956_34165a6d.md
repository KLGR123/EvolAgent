### Development Step 14: Extract Survivor (American TV) Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment trivia game content generation for mobile apps: automatically update question banks with the latest Survivor winners for quiz rounds
- Data journalism timeline visualization of Survivor champions: extract season‐by‐season winners to fuel interactive news graphics and online articles
- Academic social science research on reality TV demographics: compile winner data for statistical analysis of age, gender, and background trends across 44 seasons
- Fantasy Survivor league platform synchronization: pull the official winners list to update scoring algorithms and leaderboards in real time
- Marketing sponsorship performance analytics: correlate brand sponsors’ exposure with Survivor winners to measure campaign ROI
- Educational programming tutorials and workshops: use the Survivor winners scraping script to teach web scraping, HTML parsing, and JSON serialization in coding bootcamps
- Quality assurance monitoring for Wikipedia content: schedule periodic scraping and diffing of the winners table to detect and log unauthorized edits or vandalism

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# 1) Detect the workspace directory (handles dynamic suffixes)
candidates = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not candidates:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
# If multiple, pick the one with the most recent modification time
workspace_dir = max(candidates, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable version of the Survivor page with full headers
base_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={base_title}&printable=yes"
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/114.0.5735.199 Safari/537.36'
    ),
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching printable page:\n  {url}\n  with Accept-Language: {headers['Accept-Language']}\n")
response = requests.get(url, headers=headers)
response.raise_for_status()
print(f"Page fetched successfully (status code: {response.status_code})\n")

# 3) Save the printable HTML for inspection
html_path = os.path.join(workspace_dir, 'survivor_page_printable.html')
with open(html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"Saved printable HTML to: {html_path}\n")

# 4) Parse the printable HTML and extract all <span class="mw-headline">
soup = BeautifulSoup(response.text, 'html.parser')
headlines = soup.select('span.mw-headline')
print(f"Found {len(headlines)} <span class="mw-headline"> elements in printable view.\n")

# 5) Write each headline's id and text to a debug file
debug_path = os.path.join(workspace_dir, 'printable_spans.txt')
with open(debug_path, 'w', encoding='utf-8') as out:
    out.write("# All <span class='mw-headline'> in printable Survivor page\n")
    out.write("# Format: id=<span id>\ttext=<headline text>\n\n")
    for span in headlines:
        sid = span.get('id', '')
        txt = span.get_text(strip=True)
        out.write(f"id={sid}\ttext={txt}\n")

print(f"Wrote headlines to: {debug_path}\n")
print("Preview of first 10 entries:")
with open(debug_path, 'r', encoding='utf-8') as out:
    for i, line in enumerate(out):
        print(line.rstrip())
        if i >= 9:
            break
```