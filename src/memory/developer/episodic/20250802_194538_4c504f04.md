### Development Step 26: Identify repeated critical word describing dragon portrayals in Emily Midkiff’s June 2014 Fafnir article

**Description**: Access Emily Midkiff's June 2014 article in the Fafnir journal to extract the full text and identify the specific word that was quoted from two different authors expressing distaste for the nature of dragon depictions. Parse the article content systematically to locate quotes from multiple authors that contain the same critical word about dragon portrayals.

**Use Cases**:
- Automated literature review in mythological studies to extract and compare critical descriptors of dragon imagery across multiple academic articles
- Digital humanities indexing pipeline for tagging recurring critique vocabulary in Nordic folklore journals’ dragon portrayals
- Fantasy magazine editorial analytics that identify and consolidate author quotes expressing distaste for dragon character designs when planning themed issues
- Academic research assistant tool to locate and highlight shared negative descriptors of mythical creatures in peer-reviewed publications
- Gaming studio community feedback monitor that automatically extracts common critical terms about dragon aesthetics from forum discussions
- Cultural studies course resource that gathers dissenting opinions on dragon symbolism from online folklore archives for classroom debate
- Publishing house quality control script to scan fantasy manuscript peer reviews for overused or objectionable dragon-related adjectives
- Archival metadata enhancer for a digital journal repository to annotate articles with frequently occurring critical words about dragon depictions

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import time

print('=== SEARCHING FOR EMILY MIDKIFF ARTICLE IN FAFNIR JOURNAL ===')
print('Author: Emily Midkiff')
print('Journal: Fafnir')
print('Date: June 2014')
print('Objective: Extract full text to find quotes from multiple authors containing the same critical word about dragon depictions\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Headers to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# Step 1: Search for Fafnir journal website
print('=== STEP 1: LOCATING FAFNIR JOURNAL WEBSITE ===\n')

# Try direct access to common Fafnir journal URLs
fafnir_urls = [
    'https://journal.finfar.org/',
    'https://journal.fafnir.org/',
    'https://fafnir.journal.org/',
    'https://www.fafnir-journal.com/',
    'https://fafnir-journal.org/'
]

fafnir_base_url = None
for url in fafnir_urls:
    try:
        print(f'Trying: {url}')
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            print(f'✓ Successfully accessed: {url}')
            print(f'Status: {response.status_code}')
            print(f'Content length: {len(response.content):,} bytes')
            
            # Check if this looks like the Fafnir journal
            content_lower = response.text.lower()
            if 'fafnir' in content_lower and ('journal' in content_lower or 'article' in content_lower):
                fafnir_base_url = url
                print(f'✓ Confirmed Fafnir journal site: {url}\n')
                break
            else:
                print(f'⚠ Site found but may not be the journal\n')
        else:
            print(f'✗ Status {response.status_code}\n')
    except Exception as e:
        print(f'✗ Failed: {str(e)}\n')

# If direct URLs don't work, try web search
if not fafnir_base_url:
    print('=== CONDUCTING WEB SEARCH FOR FAFNIR JOURNAL ===\n')
    
    # Search for Fafnir journal
    search_queries = [
        'Fafnir journal Emily Midkiff 2014',
        '"Fafnir journal" academic',
        'Fafnir Nordic journal mythology'
    ]
    
    for query in search_queries:
        print(f'Searching: {query}')
        try:
            search_url = 'https://www.bing.com/search'
            search_response = requests.get(
                search_url,
                params={'q': query},
                headers=headers,
                timeout=30
            )
            
            if search_response.status_code == 200:
                search_soup = BeautifulSoup(search_response.content, 'html.parser')
                
                # Look for journal website links
                links = search_soup.find_all('a', href=True)
                potential_journal_links = []
                
                for link in links:
                    href = link.get('href')
                    text = link.get_text().strip()
                    
                    if href and 'fafnir' in href.lower():
                        # Clean up Bing redirect URLs
                        if 'bing.com' in href and 'u=a1' in href:
                            # Extract actual URL from Bing redirect
                            import urllib.parse
                            parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                            if 'u' in parsed:
                                actual_url = urllib.parse.unquote(parsed['u'][0])
                                if actual_url.startswith('a1'):
                                    actual_url = actual_url[2:]
                                href = actual_url
                        
                        potential_journal_links.append({
                            'url': href,
                            'text': text[:100]  # Truncate long text
                        })
                
                print(f'Found {len(potential_journal_links)} potential Fafnir links:')
                for i, link in enumerate(potential_journal_links[:5], 1):
                    print(f'  {i}. {link["text"]} -> {link["url"]}')
                
                # Try the most promising link
                if potential_journal_links:
                    test_url = potential_journal_links[0]['url']
                    try:
                        test_response = requests.get(test_url, headers=headers, timeout=15)
                        if test_response.status_code == 200:
                            fafnir_base_url = test_url
                            print(f'✓ Successfully accessed Fafnir journal: {test_url}\n')
                            break
                    except:
                        continue
                        
        except Exception as e:
            print(f'Search failed: {str(e)}')
            continue
        
        if fafnir_base_url:
            break

# Step 2: Search for Emily Midkiff's article
if fafnir_base_url:
    print('=== STEP 2: SEARCHING FOR EMILY MIDKIFF ARTICLE ===\n')
    
    try:
        # Get the main journal page
        main_response = requests.get(fafnir_base_url, headers=headers, timeout=30)
        main_soup = BeautifulSoup(main_response.content, 'html.parser')
        
        print(f'Journal main page title: {main_soup.find("title").get_text() if main_soup.find("title") else "Not found"}')
        
        # Look for archive, issues, or article links
        archive_selectors = [
            'a[href*="archive"]',
            'a[href*="issue"]',
            'a[href*="2014"]',
            'a[href*="volume"]',
            'a[href*="articles"]'
        ]
        
        archive_links = []
        for selector in archive_selectors:
            links = main_soup.select(selector)
            for link in links:
                href = link.get('href')
                if href:
                    if href.startswith('/'):
                        href = urljoin(fafnir_base_url, href)
                    archive_links.append({
                        'url': href,
                        'text': link.get_text().strip(),
                        'selector': selector
                    })
        
        # Remove duplicates
        unique_archive_links = []
        seen_urls = set()
        for link in archive_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_archive_links.append(link)
        
        print(f'Found {len(unique_archive_links)} potential archive/issue links:')
        for i, link in enumerate(unique_archive_links[:10], 1):
            print(f'  {i}. "{link["text"]}" -> {link["url"]}')
        
        # Search each archive link for Emily Midkiff
        midkiff_articles = []
        
        for link in unique_archive_links[:5]:  # Limit to first 5 to avoid too many requests
            print(f'\nSearching in: {link["text"]} ({link["url"]})')
            
            try:
                archive_response = requests.get(link['url'], headers=headers, timeout=20)
                if archive_response.status_code == 200:
                    archive_soup = BeautifulSoup(archive_response.content, 'html.parser')
                    archive_text = archive_soup.get_text().lower()
                    
                    # Check for Emily Midkiff and 2014
                    if 'midkiff' in archive_text and '2014' in archive_text:
                        print('✓ Found potential match - contains "midkiff" and "2014"')
                        
                        # Look for article links on this page
                        article_links = archive_soup.find_all('a', href=True)
                        for article_link in article_links:
                            article_text = article_link.get_text().strip()
                            article_href = article_link.get('href')
                            
                            if 'midkiff' in article_text.lower():
                                if article_href.startswith('/'):
                                    article_href = urljoin(fafnir_base_url, article_href)
                                
                                midkiff_articles.append({
                                    'title': article_text,
                                    'url': article_href,
                                    'found_on': link['url']
                                })
                                print(f'  ✓ Found Midkiff article: "{article_text}" -> {article_href}')
                    else:
                        print('✗ No match for "midkiff" and "2014"')
                else:
                    print(f'✗ Failed to access: {archive_response.status_code}')
                    
            except Exception as e:
                print(f'✗ Error accessing archive: {str(e)}')
        
        # Save search results
        search_results = {
            'journal_base_url': fafnir_base_url,
            'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'archive_links_found': unique_archive_links,
            'midkiff_articles_found': midkiff_articles
        }
        
        results_file = os.path.join(workspace, 'fafnir_search_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(search_results, f, indent=2, ensure_ascii=False)
        
        print(f'\n=== SEARCH RESULTS SUMMARY ===')
        print(f'✓ Fafnir journal base URL: {fafnir_base_url}')
        print(f'✓ Archive links explored: {len(unique_archive_links)}')
        print(f'✓ Midkiff articles found: {len(midkiff_articles)}')
        print(f'✓ Search results saved to: {results_file}')
        
        # If we found Midkiff articles, try to access the full text
        if midkiff_articles:
            print('\n=== STEP 3: ACCESSING MIDKIFF ARTICLE FULL TEXT ===\n')
            
            target_article = midkiff_articles[0]  # Use the first found article
            print(f'Target article: "{target_article["title"]}"')
            print(f'URL: {target_article["url"]}')
            
            try:
                article_response = requests.get(target_article['url'], headers=headers, timeout=30)
                print(f'Article response status: {article_response.status_code}')
                
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.content, 'html.parser')
                    
                    # Remove navigation, scripts, and other non-content elements
                    for element in article_soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                        element.decompose()
                    
                    # Try to find the main article content
                    content_selectors = [
                        '.article-content',
                        '.article-body',
                        '.full-text',
                        'main',
                        '.content',
                        '#content',
                        '.post-content'
                    ]
                    
                    article_content = None
                    for selector in content_selectors:
                        content_elem = article_soup.select_one(selector)
                        if content_elem:
                            article_content = content_elem.get_text()
                            print(f'✓ Article content extracted using selector: {selector}')
                            break
                    
                    if not article_content:
                        # Fallback to full page text
                        article_content = article_soup.get_text()
                        print('Using full page text as fallback')
                    
                    # Clean up the text
                    lines = (line.strip() for line in article_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    clean_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    print(f'✓ Extracted article text: {len(clean_content):,} characters')
                    
                    # Save the full article text
                    article_text_file = os.path.join(workspace, 'midkiff_fafnir_article.txt')
                    with open(article_text_file, 'w', encoding='utf-8') as f:
                        f.write(f'Title: {target_article["title"]}\n')
                        f.write(f'URL: {target_article["url"]}\n')
                        f.write(f'Extracted: {time.strftime("%Y-%m-%d %H:%M:%S")}\n')
                        f.write('='*80 + '\n\n')
                        f.write(clean_content)
                    
                    print(f'✓ Full article text saved to: {article_text_file}')
                    
                    # Now search for quotes and critical words about dragon depictions
                    print('\n=== STEP 4: ANALYZING ARTICLE FOR DRAGON CRITICISM QUOTES ===\n')
                    
                    # Look for common quote patterns
                    import re
                    
                    # Find quoted text (text within quotation marks)
                    quote_patterns = [
                        r'"([^"]{20,200})"',  # Double quotes
                        r'"([^"]{20,200})"',  # Smart quotes
                        r'"([^"]{20,200})"',  # Mixed smart quotes
                        r"'([^']{20,200})'",  # Single quotes for longer passages
                    ]
                    
                    all_quotes = []
                    for pattern in quote_patterns:
                        matches = re.findall(pattern, clean_content)
                        all_quotes.extend(matches)
                    
                    print(f'Found {len(all_quotes)} potential quotes in the article')
                    
                    # Filter quotes that might be about dragons or criticism
                    dragon_related_keywords = [
                        'dragon', 'dragons', 'wyrm', 'wyvern', 'serpent',
                        'beast', 'creature', 'monster', 'mythology', 'fantasy',
                        'depict', 'depiction', 'portrayal', 'representation',
                        'nature', 'character', 'image', 'vision'
                    ]
                    
                    critical_keywords = [
                        'distaste', 'dislike', 'criticism', 'critique', 'negative',
                        'poor', 'bad', 'terrible', 'awful', 'disappointing',
                        'problematic', 'troubling', 'concerning', 'objectionable'
                    ]
                    
                    relevant_quotes = []
                    for quote in all_quotes:
                        quote_lower = quote.lower()
                        has_dragon_ref = any(keyword in quote_lower for keyword in dragon_related_keywords)
                        has_critical_tone = any(keyword in quote_lower for keyword in critical_keywords)
                        
                        if has_dragon_ref or has_critical_tone:
                            relevant_quotes.append(quote)
                    
                    print(f'Found {len(relevant_quotes)} quotes potentially related to dragon criticism:')
                    
                    quotes_analysis = []
                    for i, quote in enumerate(relevant_quotes, 1):
                        print(f'\n--- Quote {i} ---')
                        print(f'"{quote}"')
                        
                        # Find context around this quote
                        quote_index = clean_content.find(quote)
                        if quote_index != -1:
                            context_start = max(0, quote_index - 200)
                            context_end = min(len(clean_content), quote_index + len(quote) + 200)
                            context = clean_content[context_start:context_end]
                            
                            print(f'Context: ...{context}...')
                            
                            quotes_analysis.append({
                                'quote': quote,
                                'context': context,
                                'index': quote_index
                            })
                    
                    # Look for repeated critical words across quotes
                    if len(relevant_quotes) >= 2:
                        print('\n=== ANALYZING FOR SHARED CRITICAL WORDS ===\n')
                        
                        # Extract words from each quote
                        quote_words = []
                        for quote in relevant_quotes:
                            words = re.findall(r'\b\w+\b', quote.lower())
                            quote_words.append(set(words))
                        
                        # Find words that appear in multiple quotes
                        if len(quote_words) >= 2:
                            common_words = quote_words[0]
                            for word_set in quote_words[1:]:
                                common_words = common_words.intersection(word_set)
                            
                            # Filter out common words (articles, prepositions, etc.)
                            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}
                            
                            meaningful_common_words = [word for word in common_words if word not in stop_words and len(word) > 3]
                            
                            print(f'Words appearing in multiple quotes: {meaningful_common_words}')
                            
                            if meaningful_common_words:
                                print(f'\n*** POTENTIAL SHARED CRITICAL WORD(S): {meaningful_common_words} ***')
                    
                    # Save detailed analysis
                    analysis_file = os.path.join(workspace, 'midkiff_quotes_analysis.json')
                    analysis_data = {
                        'article_title': target_article['title'],
                        'article_url': target_article['url'],
                        'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'total_quotes_found': len(all_quotes),
                        'relevant_quotes_count': len(relevant_quotes),
                        'quotes_analysis': quotes_analysis,
                        'potential_shared_words': meaningful_common_words if 'meaningful_common_words' in locals() else []
                    }
                    
                    with open(analysis_file, 'w', encoding='utf-8') as f:
                        json.dump(analysis_data, f, indent=2, ensure_ascii=False)
                    
                    print(f'\n✓ Detailed quotes analysis saved to: {analysis_file}')
                    
                else:
                    print(f'✗ Failed to access article: {article_response.status_code}')
                    
            except Exception as e:
                print(f'Error accessing article: {str(e)}')
        
        else:
            print('\n⚠ No Emily Midkiff articles found in the searched archives')
            print('The article may be in a different section or require different search terms')
    
    except Exception as e:
        print(f'Error during journal search: {str(e)}')

else:
    print('\n⚠ Could not locate the Fafnir journal website')
    print('Manual search or alternative access methods may be required')

print('\n=== EMILY MIDKIFF FAFNIR ARTICLE SEARCH COMPLETE ===')
print('Objective: Extract full text and identify shared critical word about dragon depictions')
print('Status: Search and analysis attempt completed')
print('Next steps: Review extracted content and analysis results')
```