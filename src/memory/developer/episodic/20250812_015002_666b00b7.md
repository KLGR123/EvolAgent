### Development Step 7: Identify South African IFP-Reform Party Agreement on Federalism and Bill of Rights, Ambassador to France

**Description**: Conduct a comprehensive web search to identify the agreement between a South African political figure and the leader of the Inkatha Freedom Party that led to the formation of the Reform Party and proposed a federal system and bill of rights. Focus on finding information about this specific political agreement, the parties involved, and identify who later served as South African ambassador to France. Search for keywords including 'Inkatha Freedom Party agreement Reform Party', 'South Africa federal system bill of rights agreement', 'South African ambassador France Reform Party', and 'IFP leader political agreement federal system'.

**Use Cases**:
- Academic research on South African constitutional transitions, extracting and analyzing the 1993 agreement between Roelf Meyer and Mangosuthu Buthelezi for a political science dissertation
- Legal compliance review for a human rights NGO, automatically identifying references to proposed federal structures and bill of rights clauses in transition-era accords
- Newsroom investigation into historical IFP‚ÄìNP alliances, using automated web scraping to compile timelines and direct quotes from primary Wikipedia and archival sources
- Diplomatic biography compilation tool for a history publisher, harvesting details on South Africa‚Äôs ambassadors to France and their prior involvement in the Reform Party formation
- University courseware development for modern African studies, generating lecture slides and reading lists based on keywords like ‚ÄúRecord of Understanding‚Äù and ‚Äúfederalism‚Äù negotiated by Inkatha
- Policy risk assessment dashboard for an international consultancy, monitoring mentions of federal system negotiations and IFP pacts to forecast regional autonomy trends
- Digital archiving pipeline for a national library, tagging and indexing web content related to 1990s political accords and ambassadorial appointments for future historical queries

```
import requests
import json
import os
from datetime import datetime
import time

print("Continuing comprehensive web search to identify South African political agreement involving Inkatha Freedom Party...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Check if we have any previous search results to build upon
previous_results_file = "workspace/ifp_agreement_search_results.json"
if os.path.exists(previous_results_file):
    print("Found previous search results, will build upon them...")
    with open(previous_results_file, 'r') as f:
        try:
            previous_data = json.load(f)
            print(f"Previous data contains {len(previous_data)} entries")
        except:
            previous_data = {}
else:
    previous_data = {}

# Search URLs for relevant information - comprehensive list
search_urls = [
    "https://en.wikipedia.org/wiki/Inkatha_Freedom_Party",
    "https://en.wikipedia.org/wiki/Reform_Party_(South_Africa)", 
    "https://en.wikipedia.org/wiki/Mangosuthu_Buthelezi",
    "https://en.wikipedia.org/wiki/South_African_transition_to_democracy",
    "https://en.wikipedia.org/wiki/Roelf_Meyer",
    "https://en.wikipedia.org/wiki/Convention_for_a_Democratic_South_Africa",
    "https://en.wikipedia.org/wiki/Record_of_Understanding",
    "https://en.wikipedia.org/wiki/List_of_ambassadors_of_South_Africa_to_France",
    "https://en.wikipedia.org/wiki/National_Party_(South_Africa)",
    "https://en.wikipedia.org/wiki/Multi-Party_Negotiating_Process"
]

# Enhanced keywords to search for
search_keywords = [
    'Reform Party',
    'federal system', 
    'bill of rights',
    'Inkatha Freedom Party',
    'IFP',
    'Mangosuthu Buthelezi',
    'ambassador to France',
    'political agreement',
    'Roelf Meyer',
    'CODESA',
    'National Party',
    'ANC',
    'Record of Understanding',
    'KwaZulu',
    'federalism',
    'constitutional negotiations',
    'Natal',
    'Zulu',
    'homeland',
    'accord',
    'pact',
    'alliance',
    'coalition'
]

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Storage for search results
search_results = {}
analysis_results = {}

print(f"\nStarting web search of {len(search_urls)} URLs...")

# Conduct comprehensive web search
for i, url in enumerate(search_urls, 1):
    page_name = url.split('/')[-1]
    print(f"\n[{i}/{len(search_urls)}] Fetching: {page_name}")
    print(f"URL: {url}")
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        content = response.text
        
        search_results[page_name] = {
            'url': url,
            'content_length': len(content),
            'content': content[:25000],  # Store first 25000 characters
            'fetch_time': datetime.now().isoformat()
        }
        
        print(f"‚úì Successfully retrieved {len(content):,} characters")
        
        # Analyze content for keywords immediately
        content_lower = content.lower()
        found_keywords = []
        relevant_sections = []
        
        print(f"Analyzing content for {len(search_keywords)} keywords...")
        
        for keyword in search_keywords:
            if keyword.lower() in content_lower:
                found_keywords.append(keyword)
                
                # Find section around keyword - get multiple occurrences
                start_pos = 0
                keyword_lower = keyword.lower()
                
                while True:
                    pos = content_lower.find(keyword_lower, start_pos)
                    if pos == -1:
                        break
                    
                    # Extract context around keyword
                    section_start = max(0, pos - 800)
                    section_end = min(len(content), pos + 800)
                    section = content[section_start:section_end]
                    
                    relevant_sections.append({
                        'keyword': keyword,
                        'section': section,
                        'position': pos,
                        'occurrence': len([s for s in relevant_sections if s['keyword'] == keyword]) + 1
                    })
                    
                    start_pos = pos + 1
                    
                    # Limit to 3 occurrences per keyword per page
                    if len([s for s in relevant_sections if s['keyword'] == keyword]) >= 3:
                        break
        
        analysis_results[page_name] = {
            'url': url,
            'found_keywords': found_keywords,
            'relevant_sections': relevant_sections,
            'keyword_count': len(found_keywords),
            'section_count': len(relevant_sections)
        }
        
        print(f"‚úì Found {len(found_keywords)} keywords, {len(relevant_sections)} relevant sections")
        if found_keywords:
            print(f"Keywords: {', '.join(found_keywords[:10])}{'...' if len(found_keywords) > 10 else ''}")
        
    except Exception as e:
        print(f"‚úó Error fetching {url}: {str(e)}")
        search_results[page_name] = {
            'url': url,
            'error': str(e),
            'content_length': 0,
            'content': '',
            'fetch_time': datetime.now().isoformat()
        }
        analysis_results[page_name] = {
            'url': url,
            'found_keywords': [],
            'relevant_sections': [],
            'keyword_count': 0,
            'section_count': 0,
            'error': str(e)
        }
    
    # Add delay between requests to be respectful
    time.sleep(1.5)

print(f"\n{'='*80}")
print("WEB SEARCH COMPLETED")
print(f"{'='*80}")

# Save comprehensive search results
output_file = "workspace/ifp_agreement_search_results.json"
with open(output_file, 'w') as f:
    json.dump(analysis_results, f, indent=2)
print(f"\nDetailed search results saved to {output_file}")

# Generate search summary
summary = {
    'search_date': datetime.now().isoformat(),
    'urls_searched': len(search_urls),
    'successful_fetches': len([r for r in search_results.values() if 'error' not in r]),
    'failed_fetches': len([r for r in search_results.values() if 'error' in r]),
    'total_keywords_found': sum(r.get('keyword_count', 0) for r in analysis_results.values()),
    'total_sections_found': sum(r.get('section_count', 0) for r in analysis_results.values())
}

print(f"\nSEARCH SUMMARY:")
print(f"URLs searched: {summary['urls_searched']}")
print(f"Successful fetches: {summary['successful_fetches']}")
print(f"Failed fetches: {summary['failed_fetches']}")
print(f"Total keywords found: {summary['total_keywords_found']}")
print(f"Total relevant sections: {summary['total_sections_found']}")

# Display results by page
print(f"\n{'='*80}")
print("SEARCH RESULTS BY PAGE")
print(f"{'='*80}")

for page_name, results in analysis_results.items():
    if results.get('keyword_count', 0) > 0:
        print(f"\nüìÑ {page_name}")
        print(f"   URL: {results['url']}")
        print(f"   Keywords found ({results['keyword_count']}): {', '.join(results['found_keywords'])}")
        print(f"   Relevant sections: {results['section_count']}")
    elif 'error' in results:
        print(f"\n‚ùå {page_name} - Error: {results['error']}")
    else:
        print(f"\n‚ö™ {page_name} - No relevant keywords found")

print(f"\n{'='*80}")
print("DETAILED ANALYSIS - SEARCHING FOR SPECIFIC PATTERNS")
print(f"{'='*80}")

# Analyze for specific patterns related to the question
potential_agreements = []
potential_ambassadors = []
reform_party_mentions = []
ifp_political_agreements = []
federal_bill_rights = []

for page_name, results in analysis_results.items():
    for section in results.get('relevant_sections', []):
        section_text = section['section'].lower()
        section_content = section['section']
        
        # Look for Reform Party mentions with context
        if 'reform party' in section_text:
            reform_party_mentions.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for IFP political agreements
        if ('inkatha' in section_text or 'ifp' in section_text) and ('agreement' in section_text or 'accord' in section_text or 'pact' in section_text):
            ifp_political_agreements.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for federal system + bill of rights combinations
        if ('federal' in section_text or 'federalism' in section_text) and ('bill of rights' in section_text or 'rights' in section_text):
            federal_bill_rights.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for ambassador to France references
        if 'ambassador' in section_text and 'france' in section_text:
            potential_ambassadors.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })

# Save comprehensive findings
findings = {
    'search_date': datetime.now().isoformat(),
    'search_summary': summary,
    'reform_party_mentions': reform_party_mentions,
    'ifp_political_agreements': ifp_political_agreements,
    'federal_bill_rights': federal_bill_rights,
    'potential_ambassadors': potential_ambassadors,
    'search_keywords': search_keywords,
    'pages_analyzed': list(analysis_results.keys())
}

findings_file = "workspace/ifp_agreement_findings.json"
with open(findings_file, 'w') as f:
    json.dump(findings, f, indent=2)
print(f"\nComprehensive findings saved to {findings_file}")

# Display key findings
print(f"\nüîç REFORM PARTY MENTIONS: {len(reform_party_mentions)}")
for i, mention in enumerate(reform_party_mentions[:3], 1):
    print(f"\n{i}. From {mention['source']}:")
    print(f"   {mention['section'][:500]}...")

print(f"\nü§ù IFP POLITICAL AGREEMENTS: {len(ifp_political_agreements)}")
for i, agreement in enumerate(ifp_political_agreements[:3], 1):
    print(f"\n{i}. From {agreement['source']}:")
    print(f"   {agreement['section'][:500]}...")

print(f"\nüìú FEDERAL SYSTEM + BILL OF RIGHTS: {len(federal_bill_rights)}")
for i, item in enumerate(federal_bill_rights[:3], 1):
    print(f"\n{i}. From {item['source']}:")
    print(f"   {item['section'][:500]}...")

print(f"\nüá´üá∑ AMBASSADOR TO FRANCE: {len(potential_ambassadors)}")
for i, ambassador in enumerate(potential_ambassadors[:3], 1):
    print(f"\n{i}. From {ambassador['source']}:")
    print(f"   {ambassador['section'][:500]}...")

print(f"\n{'='*80}")
print("COMPREHENSIVE SEARCH COMPLETED SUCCESSFULLY!")
print(f"{'='*80}")
print(f"üìä Final Statistics:")
print(f"   ‚Ä¢ Pages searched: {len(search_urls)}")
print(f"   ‚Ä¢ Keywords found: {summary['total_keywords_found']}")
print(f"   ‚Ä¢ Reform Party mentions: {len(reform_party_mentions)}")
print(f"   ‚Ä¢ IFP agreements: {len(ifp_political_agreements)}")
print(f"   ‚Ä¢ Federal/Bill of Rights: {len(federal_bill_rights)}")
print(f"   ‚Ä¢ Ambassador references: {len(potential_ambassadors)}")
print(f"\nüìÅ Results saved to workspace/ directory for further analysis")
```