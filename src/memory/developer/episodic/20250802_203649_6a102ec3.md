### Development Step 37: Search 2011 NSI Bulgarian Census for Tertiary Education Completion Numbers by Gender

**Description**: Search for the official 2011 Bulgarian census data from the Bulgarian National Statistical Institute (NSI) or other authoritative government sources that specifically contain the tertiary education completion numbers by gender. Focus on finding the exact counts or percentages of men and women who completed tertiary education according to the 2011 census results.

**Use Cases**:
- Government policy design and budget allocation for the Bulgarian Ministry of Education, using 2011 tertiary education completion numbers by gender to identify funding gaps between men and women
- Scholarship program targeting by an education-focused NGO, leveraging gender-disaggregated tertiary attainment data to prioritize underrepresented groups in specific regions
- Academic research on gender disparities in higher education, integrating precise male vs. female completion counts from the 2011 census into a comparative EU study
- Data journalism feature for a major Bulgarian news outlet, visualizing regional variations in tertiary graduation rates by gender to highlight social equity trends
- Market analysis for an edtech startup entering the Bulgarian market, assessing the male/female tertiary-educated population to tailor product offerings and marketing campaigns
- Workforce development planning by an international consultancy, using census-derived gender breakdowns of higher education attainment to advise corporate training initiatives
- UNESCO or World Bank report preparation, incorporating exact 2011 Bulgarian tertiary education statistics by gender into global education index assessments

```
import os
import json
from bs4 import BeautifulSoup
import re

workspace_dir = 'workspace'

print("=== PARSING HIGH-POTENTIAL NSI CENSUS FILES FOR TERTIARY EDUCATION DATA BY GENDER ===")
print("\nObjective: Extract 2011 Bulgarian census tertiary education completion statistics by gender\n")

# First, let's check what analysis results we have from the previous step
analysis_file = os.path.join(workspace_dir, 'nsi_priority_census_analysis_fixed.json')
if os.path.exists(analysis_file):
    print("--- Loading Previous Analysis Results ---")
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    print(f"Analysis timestamp: {analysis_data.get('analysis_timestamp', 'Unknown')}")
    print(f"Successful census sources: {analysis_data.get('successful_census_sources', 0)}")
    
    # Get the high-potential sources
    census_sources = analysis_data.get('census_sources_details', [])
    high_potential_sources = [s for s in census_sources if s.get('total_indicator_score', 0) > 15]
    
    print(f"\nHigh-potential sources identified: {len(high_potential_sources)}")
    for i, source in enumerate(high_potential_sources, 1):
        print(f"  {i}. {source['filename']} - Score: {source['total_indicator_score']}")
        print(f"     Original text: '{source['original_text']}'")
        print(f"     Tables found: {source['tables_found']}")
else:
    print("Previous analysis file not found. Will examine all priority census files.")
    high_potential_sources = []

# Define the priority files to analyze based on previous results
priority_files = [
    'priority_census_link_5.html',  # Education and Lifelong Learning (score: 91)
    'priority_census_link_2.html',  # Census (score: 80)
    'priority_census_link_7.html',  # Population (Demography, Migration and Projections) (score: 79)
    'priority_census_link_6.html',  # Supply Use Tables (score: 79)
    'priority_census_link_8.html'   # Population Grid (score: 79)
]

print(f"\n=== PHASE 1: EXAMINING PRIORITY FILE STRUCTURES ===\n")

file_analysis_results = []

for filename in priority_files:
    filepath = os.path.join(workspace_dir, filename)
    if os.path.exists(filepath):
        print(f"--- Analyzing: {filename} ---")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f"File size: {len(html_content):,} characters")
        
        # Parse with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Get page title
        title = soup.find('title')
        if title:
            print(f"Page title: {title.get_text().strip()}")
        
        # Look for main content areas
        main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('body')
        if main_content:
            content_text = main_content.get_text()
        else:
            content_text = soup.get_text()
        
        # Find all tables
        tables = soup.find_all('table')
        print(f"Tables found: {len(tables)}")
        
        # Look for links to data files or detailed census results
        data_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # Look for links to Excel files, PDFs, or census data
            if any(ext in href.lower() for ext in ['.xls', '.xlsx', '.pdf', '.csv']) or \
               any(term in text.lower() for term in ['download', 'data', 'table', 'census', '2011', 'education']):
                
                # Construct full URL
                if href.startswith('http'):
                    full_url = href
                elif href.startswith('/'):
                    full_url = f"https://www.nsi.bg{href}"
                else:
                    full_url = f"https://www.nsi.bg/en/{href}"
                
                data_links.append({
                    'text': text,
                    'href': href,
                    'full_url': full_url
                })
        
        print(f"Data file links found: {len(data_links)}")
        if data_links:
            print("Top data links:")
            for i, link in enumerate(data_links[:5], 1):
                print(f"  {i}. '{link['text']}' -> {link['full_url']}")
        
        # Search for specific education-related content
        education_content = []
        
        # Look for text patterns that might indicate tertiary education statistics
        education_patterns = [
            r'tertiary education.*?(?:male|female|men|women|gender)',
            r'higher education.*?(?:male|female|men|women|gender)',
            r'university.*?(?:male|female|men|women|gender)',
            r'(?:male|female|men|women).*?tertiary',
            r'(?:male|female|men|women).*?higher education',
            r'education.*?(?:by gender|gender breakdown)',
            r'2011.*?census.*?education',
            r'educational attainment.*?(?:male|female)'
        ]
        
        for pattern in education_patterns:
            matches = re.findall(pattern, content_text, re.IGNORECASE | re.DOTALL)
            if matches:
                for match in matches[:3]:  # Limit to first 3 matches per pattern
                    # Clean up the match
                    clean_match = re.sub(r'\s+', ' ', match.strip())[:200]
                    education_content.append(clean_match)
        
        if education_content:
            print(f"\nEducation-related content found:")
            for i, content in enumerate(education_content[:5], 1):
                print(f"  {i}. {content}...")
        
        # Look for tables that might contain education data
        relevant_tables = []
        for i, table in enumerate(tables):
            table_text = table.get_text().lower()
            
            # Check if table contains education and gender related terms
            has_education = any(term in table_text for term in ['education', 'tertiary', 'higher', 'university'])
            has_gender = any(term in table_text for term in ['male', 'female', 'men', 'women', 'gender'])
            has_2011 = '2011' in table_text
            
            if has_education and (has_gender or has_2011):
                relevant_tables.append({
                    'table_index': i,
                    'has_education': has_education,
                    'has_gender': has_gender,
                    'has_2011': has_2011,
                    'table_text_sample': table_text[:300]
                })
        
        if relevant_tables:
            print(f"\nRelevant tables found: {len(relevant_tables)}")
            for table_info in relevant_tables:
                print(f"  Table {table_info['table_index']}: Education={table_info['has_education']}, Gender={table_info['has_gender']}, 2011={table_info['has_2011']}")
                print(f"    Sample: {table_info['table_text_sample'][:150]}...")
        
        # Store analysis results
        file_analysis_results.append({
            'filename': filename,
            'file_size': len(html_content),
            'tables_count': len(tables),
            'data_links_count': len(data_links),
            'data_links': data_links,
            'education_content_count': len(education_content),
            'education_content': education_content,
            'relevant_tables_count': len(relevant_tables),
            'relevant_tables': relevant_tables,
            'potential_score': len(relevant_tables) * 10 + len(education_content) * 5 + len(data_links) * 3
        })
        
        print(f"Potential score for this file: {file_analysis_results[-1]['potential_score']}")
        print("\n" + "="*60 + "\n")
    else:
        print(f"File not found: {filename}\n")

# Sort files by potential score
file_analysis_results.sort(key=lambda x: x['potential_score'], reverse=True)

print(f"=== PHASE 1 RESULTS SUMMARY ===\n")
print(f"Files analyzed: {len(file_analysis_results)}")

if file_analysis_results:
    print("\nFiles ranked by potential to contain tertiary education data by gender:")
    for i, result in enumerate(file_analysis_results, 1):
        print(f"\n{i}. {result['filename']} (Score: {result['potential_score']})")
        print(f"   Tables: {result['tables_count']}, Data links: {result['data_links_count']}, Education content: {result['education_content_count']}")
        print(f"   Relevant tables: {result['relevant_tables_count']}")
        
        if result['potential_score'] > 20:
            print(f"   *** HIGH PRIORITY FOR DETAILED ANALYSIS ***")
        elif result['potential_score'] > 10:
            print(f"   ** MODERATE PRIORITY **")

# Save detailed analysis results
detailed_analysis = {
    'objective': '2011 Bulgarian Census - Tertiary Education by Gender - File Analysis',
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'files_analyzed': len(file_analysis_results),
    'file_analysis_results': file_analysis_results,
    'next_steps': [
        'Extract data from highest-scoring files',
        'Parse relevant tables for tertiary education statistics',
        'Download and analyze data file links',
        'Look for specific male/female tertiary education completion numbers'
    ]
}

analysis_output_file = os.path.join(workspace_dir, 'nsi_census_files_detailed_analysis.json')
with open(analysis_output_file, 'w', encoding='utf-8') as f:
    json.dump(detailed_analysis, f, indent=2, ensure_ascii=False)

print(f"\n=== DETAILED ANALYSIS COMPLETE ===\n")
print(f"Detailed analysis saved to: {analysis_output_file}")
print(f"\nNext step: Focus on highest-scoring files for data extraction")

if file_analysis_results:
    top_file = file_analysis_results[0]
    print(f"\nTop priority file: {top_file['filename']} (Score: {top_file['potential_score']})")
    print(f"This file has {top_file['relevant_tables_count']} relevant tables and {top_file['data_links_count']} data links")
    print(f"Ready for detailed table parsing and data extraction")
```