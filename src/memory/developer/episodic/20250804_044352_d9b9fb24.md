### Development Step 29: Extract Survivor Seasons 1–44 Winners from Wikipedia into workspace/survivor_winners_list.json

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Reality fan site backend automation for updating the Survivor season-winner archive in a CMS after each new season
- Academic media studies research on gender, age, and regional trends among Survivor winners over 44 seasons
- TV marketing newsletter automation that schedules and sends “On this day” winner anniversary highlights to subscribers
- Mobile trivia game database population using accurate season-winner pairs for dynamic question generation
- Data journalism interactive dashboard built with D3.js to visualize Survivor winner statistics and streaks over time
- Chatbot knowledge-base integration enabling real-time Q&A about Survivor winners in messaging apps
- Streaming platform analytics correlating viewer engagement spikes with winner announcement dates by merging JSON data with play metrics
- E-commerce catalog automation generating product listings (e.g., DVD or merchandise) for each Survivor season based on winner metadata

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable Wikipedia page for Survivor
title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={title}&printable=yes"
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching Survivor printable page...\nURL: {url}\n")
resp = requests.get(url, headers=headers)
resp.raise_for_status()
print(f"Page fetched successfully (status {resp.status_code})\n")

# 3) Parse HTML
doc = BeautifulSoup(resp.text, 'html.parser')

# 4) Find the simple winners-only table (headers exactly ['Season','Winner']), then fallback
tables = doc.find_all('table', class_='wikitable')
target = None
print(f"Found {len(tables)} wikitable(s). Scanning for simple winners-only table...\n")
for idx, tbl in enumerate(tables, 1):
    first = tbl.find('tr')
    if not first:
        continue
    headers = [th.get_text(strip=True).lower() for th in first.find_all(['th','td'], recursive=False)]
    if headers == ['season', 'winner']:
        target = tbl
        print(f"→ Selected simple winners-only table #{idx} with headers {headers}\n")
        break

# fallback to any table containing both
if not target:
    print("Simple table not found – falling back to any table containing 'Season' & 'Winner'...\n")
    for idx, tbl in enumerate(tables, 1):
        first = tbl.find('tr')
        if not first:
            continue
        headers = [th.get_text(strip=True).lower() for th in first.find_all(['th','td'], recursive=False)]
        if 'season' in headers and 'winner' in headers:
            target = tbl
            print(f"→ Selected fallback table #{idx} with headers {headers}\n")
            break

if not target:
    print("❌ Could not find any suitable table with Season & Winner. Exiting.")
    sys.exit(1)

# 5) Determine the column indices for Season and Winner
first = target.find('tr')
cols = [c.get_text(strip=True).lower() for c in first.find_all(['th','td'], recursive=False)]
season_idx = cols.index('season')
winner_idx = cols.index('winner')
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 6) Extract winners
winners = []
for row in target.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    s_text = cells[season_idx].get_text(strip=True)
    if not s_text.isdigit():
        continue
    season_num = int(s_text)
    if not (1 <= season_num <= 44):
        continue
    # winner extraction: prefer link text
    winner_cell = cells[winner_idx]
    a_tag = winner_cell.find('a')
    if a_tag and re.search(r'[A-Za-z]', a_tag.get_text()):
        name = a_tag.get_text(strip=True)
    else:
        name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season_num} → Winner: '{name}'")
    winners.append({'season': season_num, 'winner': name})

# 7) Sort and verify
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Please verify the logic and page structure.")
else:
    missing = [w['season'] for w in winners_sorted if not w['winner']]
    if missing:
        print(f"⚠️ Seasons with empty winners: {missing}")

# 8) Write JSON output
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {out_path}")
```