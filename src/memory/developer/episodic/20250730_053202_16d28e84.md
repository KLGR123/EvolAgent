### Development Step 6: Download “A Dark Trace” from Project MUSE and Extract Chapter 2 to Identify Freud’s Myth Influencer

**Description**: Access and download the full text of 'A Dark Trace: Sigmund Freud on the Sense of Guilt' by H. Westerink from Project MUSE using DOI 10.1353/book.24372. Since the book was confirmed to be open access, retrieve the complete text and save it to workspace/dark_trace_freud_book.pdf or appropriate format. Focus on locating and extracting Chapter 2 content to identify the author who influenced Freud's belief in 'endopsychic myths'. If the full book is not directly downloadable, extract Chapter 2 specifically or access the book's table of contents to determine the exact chapter title and content structure.

**Use Cases**:
- Digital humanities research workflow automating the retrieval and parsing of Chapter 2 from an open-access monograph on Project MUSE to perform thematic text analysis on Freud’s concept of guilt
- Legal research pipeline extracting specific chapters from open-access law textbooks to compile case law summaries and regulatory commentary for corporate compliance teams
- Healthcare policy analysis system downloading targeted guideline chapters from open-access medical manuals for integration into a machine-readable hospital compliance database
- University library archiving service scanning publisher and repository sites for open-access academic books, downloading PDF files, and cataloging them for institutional e-collections
- NLP dataset preparation process crawling open-access academic books to retrieve and segment individual chapters, building a domain-specific corpus for training language models
- Market intelligence automation fetching and isolating relevant chapters from open-access industry reports on academic platforms to feed into competitive analysis dashboards
- e-Learning content assembly tool downloading publisher-provided PDF chapters and restructuring them into modular units for seamless integration into learning management systems (LMS)

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin
import time

print('=== ANALYZING RAW HTML AND EXPLORING ALTERNATIVE ACCESS METHODS ===')
print('Objective: Examine the saved HTML structure and find alternative ways to access Chapter 2 content\n')

# First, let's inspect the raw HTML file to understand the Project MUSE page structure
html_file = 'workspace/chapter_2_raw_html.html'

if os.path.exists(html_file):
    print('=== ANALYZING RAW HTML STRUCTURE ===')
    
    with open(html_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    print(f'HTML file size: {len(html_content):,} characters')
    
    # Parse the HTML to understand the structure
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Look for any indicators of content loading or authentication requirements
    print('\n=== SEARCHING FOR CONTENT LOADING INDICATORS ===')
    
    # Check for JavaScript that might load content dynamically
    script_tags = soup.find_all('script')
    print(f'Found {len(script_tags)} script tags')
    
    # Look for specific keywords in script content
    script_keywords = ['chapter', 'content', 'load', 'ajax', 'api', 'auth']
    relevant_scripts = []
    
    for i, script in enumerate(script_tags):
        script_text = script.get_text().lower()
        if any(keyword in script_text for keyword in script_keywords):
            relevant_scripts.append((i, script_text[:200]))
    
    if relevant_scripts:
        print(f'Found {len(relevant_scripts)} potentially relevant scripts:')
        for i, (script_idx, preview) in enumerate(relevant_scripts[:3]):
            print(f'{i+1}. Script {script_idx}: {preview}...')
    
    # Look for authentication or access control elements
    print('\n=== CHECKING FOR ACCESS CONTROL ELEMENTS ===')
    
    auth_indicators = ['login', 'authentication', 'institutional', 'subscription', 'access']
    auth_elements = []
    
    for indicator in auth_indicators:
        elements = soup.find_all(text=lambda text: text and indicator.lower() in text.lower())
        if elements:
            auth_elements.extend([(indicator, elem.strip()[:100]) for elem in elements[:2]])
    
    if auth_elements:
        print('Authentication/access indicators found:')
        for indicator, text in auth_elements:
            print(f'- {indicator}: "{text}..."')
    else:
        print('No clear authentication indicators found')
    
    # Look for any hidden content or data attributes
    print('\n=== SEARCHING FOR HIDDEN OR DATA CONTENT ===')
    
    # Check for elements with data attributes that might contain content
    data_elements = soup.find_all(attrs={'data-content': True})
    if data_elements:
        print(f'Found {len(data_elements)} elements with data-content attributes')
        for elem in data_elements[:3]:
            print(f'- {elem.name}: {elem.get("data-content", "")[:100]}')
    
    # Look for any text that mentions the actual chapter content
    chapter_references = ['dark trace', 'chapter 2', 'freud', 'endopsychic', 'myth']
    found_references = []
    
    for ref in chapter_references:
        if ref.lower() in html_content.lower():
            found_references.append(ref)
    
    if found_references:
        print(f'Chapter content references found: {found_references}')
    else:
        print('No direct chapter content references found in HTML')

else:
    print(f'Raw HTML file not found: {html_file}')

print('\n=== TRYING ALTERNATIVE PROJECT MUSE ACCESS PATTERNS ===')

# Let's try some alternative approaches based on common academic platform patterns
book_id = '24372'
base_urls = [
    f'https://muse.jhu.edu/book/{book_id}',
    f'https://www.muse.jhu.edu/book/{book_id}'
]

# Try different content access patterns
access_patterns = [
    '/fulltext',
    '/pdf',
    '/read',
    '/view',
    '/content',
    '/text',
    '/download'
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': f'https://muse.jhu.edu/book/{book_id}'
}

successful_access_attempts = []

for base_url in base_urls:
    for pattern in access_patterns:
        test_url = base_url + pattern
        print(f'\nTrying: {test_url}')
        
        try:
            response = requests.get(test_url, headers=headers, timeout=15)
            print(f'Status: {response.status_code}')
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                content_length = len(response.content)
                
                if 'pdf' in content_type:
                    print(f'*** PDF FOUND - Content-Type: {content_type}, Size: {content_length:,} bytes ***')
                    
                    # Try to save the PDF
                    if content_length > 10000:  # Reasonable PDF size
                        pdf_path = 'workspace/dark_trace_freud_book_full.pdf'
                        with open(pdf_path, 'wb') as pdf_file:
                            pdf_file.write(response.content)
                        
                        file_size = os.path.getsize(pdf_path)
                        print(f'PDF saved to: {pdf_path}')
                        print(f'File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                        
                        successful_access_attempts.append({
                            'url': test_url,
                            'type': 'PDF',
                            'file_path': pdf_path,
                            'size': file_size
                        })
                    
                elif 'html' in content_type and content_length > 50000:
                    print(f'*** SUBSTANTIAL HTML CONTENT FOUND - Size: {content_length:,} bytes ***')
                    
                    # Parse and check for actual book content
                    test_soup = BeautifulSoup(response.content, 'html.parser')
                    test_text = test_soup.get_text().lower()
                    
                    # Check for chapter content indicators
                    content_indicators = ['dark trace', 'freud', 'endopsychic', 'chapter 2', 'sense of guilt']
                    found_indicators = [ind for ind in content_indicators if ind in test_text]
                    
                    if found_indicators:
                        print(f'Content indicators found: {found_indicators}')
                        
                        # Save this promising content
                        content_path = f'workspace/alternative_access_{pattern.replace("/", "_")}_content.html'
                        with open(content_path, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        
                        successful_access_attempts.append({
                            'url': test_url,
                            'type': 'HTML_WITH_CONTENT',
                            'file_path': content_path,
                            'size': content_length,
                            'indicators_found': found_indicators
                        })
                    else:
                        print('No chapter content indicators found')
                
                else:
                    print(f'Content-Type: {content_type}, Size: {content_length:,} bytes')
            
            elif response.status_code in [301, 302]:
                redirect_location = response.headers.get('Location', 'Unknown')
                print(f'Redirect to: {redirect_location}')
            
        except Exception as e:
            print(f'Error: {str(e)}')
        
        # Small delay to be respectful
        time.sleep(0.5)

print('\n=== EXPLORING DIRECT BOOK PUBLISHER ACCESS ===')

# From the CrossRef chooser, we saw there was also a lup.be (Leuven University Press) link
# Let's try accessing the publisher directly
lup_url = 'https://lup.be/book/a-dark-trace/'
print(f'\nTrying direct publisher access: {lup_url}')

try:
    lup_response = requests.get(lup_url, headers=headers, timeout=30)
    print(f'LUP response status: {lup_response.status_code}')
    
    if lup_response.status_code == 200:
        print(f'Content length: {len(lup_response.content):,} bytes')
        
        lup_soup = BeautifulSoup(lup_response.content, 'html.parser')
        
        # Look for download or access options
        print('Searching for download options on publisher site...')
        
        download_selectors = [
            'a[href*=".pdf"]',
            'a[href*="download"]',
            'a:contains("PDF")',
            'a:contains("Download")',
            '.download-link',
            '.pdf-link'
        ]
        
        publisher_downloads = []
        for selector in download_selectors:
            try:
                if ':contains' in selector:
                    continue  # Skip deprecated selectors
                    
                links = lup_soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href:
                        if href.startswith('/'):
                            href = urljoin(lup_url, href)
                        publisher_downloads.append({
                            'url': href,
                            'text': link.get_text().strip(),
                            'selector': selector
                        })
            except:
                pass
        
        if publisher_downloads:
            print(f'Found {len(publisher_downloads)} potential download links:')
            for i, link in enumerate(publisher_downloads, 1):
                print(f'{i}. {link["text"]} -> {link["url"]}')
            
            # Try the first promising download link
            if publisher_downloads:
                target_download = publisher_downloads[0]
                print(f'\nTrying download: {target_download["url"]}')
                
                try:
                    download_response = requests.get(target_download['url'], headers=headers, timeout=60)
                    if download_response.status_code == 200:
                        content_type = download_response.headers.get('content-type', '').lower()
                        
                        if 'pdf' in content_type and len(download_response.content) > 50000:
                            pdf_path = 'workspace/dark_trace_freud_publisher_download.pdf'
                            with open(pdf_path, 'wb') as pdf_file:
                                pdf_file.write(download_response.content)
                            
                            file_size = os.path.getsize(pdf_path)
                            print(f'*** SUCCESS - PDF downloaded from publisher ***')
                            print(f'Saved to: {pdf_path}')
                            print(f'File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                            
                            successful_access_attempts.append({
                                'url': target_download['url'],
                                'type': 'PUBLISHER_PDF',
                                'file_path': pdf_path,
                                'size': file_size
                            })
                        else:
                            print(f'Download failed - Content-Type: {content_type}, Size: {len(download_response.content)}')
                    else:
                        print(f'Download failed - Status: {download_response.status_code}')
                except Exception as dl_error:
                    print(f'Download error: {str(dl_error)}')
        
        else:
            print('No download links found on publisher site')
            
            # Check if there's any open access indication
            page_text = lup_soup.get_text().lower()
            if 'open access' in page_text:
                print('Publisher page mentions open access - content might be available elsewhere')
    
    else:
        print(f'Failed to access publisher site: {lup_response.status_code}')

except Exception as lup_error:
    print(f'Error accessing publisher: {str(lup_error)}')

print('\n=== ACCESS ATTEMPT SUMMARY ===')
print(f'Successful access attempts: {len(successful_access_attempts)}')

if successful_access_attempts:
    print('\nSuccessful downloads/content found:')
    for i, attempt in enumerate(successful_access_attempts, 1):
        print(f'{i}. {attempt["type"]} from {attempt["url"]}')
        print(f'   File: {attempt["file_path"]}')
        print(f'   Size: {attempt["size"]:,} bytes')
        if 'indicators_found' in attempt:
            print(f'   Content indicators: {attempt["indicators_found"]}')
        print()
    
    # Save successful attempts info
    success_info = {
        'total_attempts': len(successful_access_attempts),
        'successful_downloads': successful_access_attempts,
        'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('workspace/successful_book_access_attempts.json', 'w', encoding='utf-8') as f:
        json.dump(success_info, f, indent=2, ensure_ascii=False)
    
    print('Success info saved to: workspace/successful_book_access_attempts.json')
    
else:
    print('\nNo successful access attempts found.')
    print('The book content may require:')
    print('1. Institutional authentication')
    print('2. Dynamic JavaScript loading')
    print('3. Alternative access methods not yet tried')
    print('4. The content may not be freely accessible despite open access claims')

print('\n=== NEXT STEPS RECOMMENDATION ===')
if successful_access_attempts:
    pdf_attempts = [a for a in successful_access_attempts if 'PDF' in a['type']]
    if pdf_attempts:
        print('✓ PDF found - Next: Parse PDF to extract Chapter 2 content')
    else:
        print('✓ HTML content found - Next: Analyze alternative HTML content for chapter text')
else:
    print('⚠ No direct access achieved - Consider:')
    print('  1. Searching for alternative sources of the book')
    print('  2. Looking for academic repositories that might have the chapter')
    print('  3. Checking if preview/sample pages are available elsewhere')

print('\nObjective: Find the author who influenced Freud\'s belief in "endopsychic myths" from Chapter 2')
print('Status: Exploring all available access methods to reach the target content')
```