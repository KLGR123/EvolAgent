### Development Step 2: Counting Twitter/X Citations in Archived August Wikipedia Day Pages from June 2023 Versions

**Description**: Search for and access the archived Wikipedia pages for each day of August (August 1st through August 31st) as they appeared in the last versions from June 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific versions of these 31 daily pages from late June 2023. Extract all reference citations from each page and identify those that cite Twitter/X posts (looking for twitter.com, x.com, or @username patterns in the reference URLs). Count the total number of Twitter/X citations for each individual August day page and compile the results.

**Use Cases**:
- Media integrity research to quantify the prevalence of Twitter/X citations in Wikipedia‚Äôs daily event pages for August, supporting studies on the reliability and sourcing of online encyclopedic content
- Academic analysis of social media‚Äôs influence on historical documentation by tracking how often Twitter/X posts are referenced in Wikipedia‚Äôs daily records for August 2023
- Automated compliance audits for organizations concerned about Wikipedia‚Äôs reliance on ephemeral social media sources, enabling risk assessment for information decay
- Journalism retrospectives investigating how breaking news from Twitter/X was incorporated into Wikipedia‚Äôs August event coverage, informing best practices for digital news archiving
- Digital preservation projects aiming to identify and safeguard at-risk citations (such as Twitter/X links) from Wikipedia before potential link rot or content removal
- Library and information science curriculum development, using real-world data to teach students about citation patterns, digital archiving, and the challenges of referencing social media
- Policy analysis for governmental or non-profit organizations monitoring the spread of information from social media into public knowledge repositories like Wikipedia
- Automated reporting for fact-checking organizations to flag Wikipedia entries that rely heavily on Twitter/X sources, supporting targeted verification efforts

```
import os
import requests
import json
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
import re

print("=== ADJUSTING WAYBACK MACHINE SEARCH STRATEGY FOR JUNE 2023 ===\n")
print("Issue: Previous search found July/August 2023 versions, not June 2023")
print("Solution: Search for earlier dates and use date range approach\n")

# First, let's inspect the existing results file to understand what we have
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using existing workspace: {workspace_dir}\n")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created new workspace: {workspace_dir}\n")

# Check if we have previous results to inspect
results_file = os.path.join(workspace_dir, 'august_pages_initial_analysis.json')
if os.path.exists(results_file):
    print("=== INSPECTING PREVIOUS RESULTS ===\n")
    
    # First check file structure
    with open(results_file, 'r', encoding='utf-8') as f:
        content = f.read()
        print(f"Previous results file size: {len(content):,} characters\n")
    
    # Now load and inspect structure
    with open(results_file, 'r', encoding='utf-8') as f:
        previous_results = json.load(f)
    
    print("Previous results structure:")
    for key in previous_results.keys():
        print(f"  {key}: {type(previous_results[key]).__name__}")
    
    # Extract the August pages list
    august_pages = previous_results.get('august_pages_list', [])
    print(f"\nFound {len(august_pages)} August pages from previous analysis")
    
    # Show what dates were found in the test
    if 'availability_test_results' in previous_results:
        print("\nPrevious availability test results:")
        for page, result in previous_results['availability_test_results'].items():
            if result.get('available', False):
                print(f"  {page}: {result.get('formatted_date', 'Unknown date')}")
            else:
                print(f"  {page}: Not available")
else:
    # Generate August pages list if no previous results
    august_pages = [f"August {day}" for day in range(1, 32)]
    print(f"Generated {len(august_pages)} August pages for analysis\n")

print("\n=== STEP 1: IMPROVED WAYBACK MACHINE SEARCH STRATEGY ===\n")
print("New approach: Search for snapshots from different June 2023 dates")
print("If June not available, find the closest earlier date\n")

# Define multiple target dates to try (working backwards from June 2023)
target_dates = [
    "20230630",  # June 30, 2023
    "20230625",  # June 25, 2023
    "20230620",  # June 20, 2023
    "20230615",  # June 15, 2023
    "20230610",  # June 10, 2023
    "20230605",  # June 5, 2023
    "20230601",  # June 1, 2023
    "20230531",  # May 31, 2023 (fallback)
    "20230530",  # May 30, 2023 (fallback)
]

print("Target dates to try (in order of preference):")
for i, date in enumerate(target_dates, 1):
    formatted = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
    print(f"  {i}. {formatted} ({date})")

def find_best_archive(url, target_dates):
    """Find the best available archive for a URL from a list of target dates"""
    print(f"  Searching for: {url}")
    
    for date in target_dates:
        try:
            api_url = f"https://archive.org/wayback/available?url={url}&timestamp={date}"
            response = requests.get(api_url, timeout=20)
            response.raise_for_status()
            
            data = response.json()
            
            if "archived_snapshots" in data and "closest" in data["archived_snapshots"]:
                closest = data["archived_snapshots"]["closest"]
                if closest["available"]:
                    archive_url = closest["url"]
                    archive_date = closest["timestamp"]
                    formatted_date = f"{archive_date[:4]}-{archive_date[4:6]}-{archive_date[6:8]}"
                    
                    # Check if this is from June 2023 or earlier (what we want)
                    if archive_date[:6] <= "202306":  # June 2023 or earlier
                        print(f"    ‚úì Found good match: {formatted_date} - {archive_url}")
                        return {
                            'available': True,
                            'archive_url': archive_url,
                            'archive_date': archive_date,
                            'formatted_date': formatted_date,
                            'target_date_used': date
                        }
                    else:
                        print(f"    ~ Found later date: {formatted_date} (continuing search...)")
            
            # Small delay between API calls
            time.sleep(0.5)
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error with date {date}: {str(e)}")
            continue
    
    print(f"    ‚ùå No suitable archive found")
    return {'available': False}

print("\n=== STEP 2: COMPREHENSIVE AVAILABILITY CHECK ===\n")
print("Testing improved search strategy on first 5 August pages...\n")

# Test the improved strategy on first 5 pages
test_pages = august_pages[:5]
improved_results = {}

for page_title in test_pages:
    print(f"\n--- Checking: {page_title} ---")
    
    # Construct Wikipedia URL
    wiki_url = f"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}"
    
    # Use improved search strategy
    result = find_best_archive(wiki_url, target_dates)
    improved_results[page_title] = result
    
    # Add delay to be respectful to Archive.org
    time.sleep(2)

print(f"\n=== IMPROVED SEARCH RESULTS ===\n")

available_count = 0
june_count = 0

for page_title, result in improved_results.items():
    if result.get('available', False):
        available_count += 1
        archive_date = result['archive_date']
        formatted_date = result['formatted_date']
        
        # Check if it's from June 2023
        if archive_date.startswith('202306'):
            june_count += 1
            print(f"‚úì {page_title}: June 2023 version found! ({formatted_date})")
        else:
            print(f"‚úì {page_title}: Earlier version found ({formatted_date})")
        
        print(f"  Archive URL: {result['archive_url']}")
        print(f"  Target date used: {result['target_date_used']}")
    else:
        print(f"‚ùå {page_title}: No suitable archive found")

print(f"\n=== SEARCH STRATEGY RESULTS ===\n")
print(f"Pages with archives found: {available_count}/{len(test_pages)}")
print(f"Pages with June 2023 versions: {june_count}/{len(test_pages)}")
print(f"Success rate: {(available_count/len(test_pages)*100):.1f}%")

if june_count > 0:
    print(f"\nüéØ SUCCESS: Found {june_count} pages with June 2023 versions!")
    print("Strategy is working - ready to scale to all 31 pages")
elif available_count > 0:
    print(f"\n‚ö†Ô∏è PARTIAL SUCCESS: Found {available_count} archived pages")
    print("No June 2023 versions, but earlier versions available")
    print("May need to adjust date range or accept closest available dates")
else:
    print(f"\n‚ùå STRATEGY NEEDS ADJUSTMENT: No archives found")
    print("May need to try different date ranges or search approach")

# Save improved results
improved_analysis = {
    'analysis_metadata': {
        'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'strategy': 'improved_multi_date_search',
        'target_dates_tried': target_dates,
        'test_pages_count': len(test_pages),
        'total_august_pages': len(august_pages)
    },
    'target_dates_list': target_dates,
    'test_results': improved_results,
    'summary': {
        'pages_with_archives': available_count,
        'pages_with_june_2023': june_count,
        'success_rate_percent': round((available_count/len(test_pages)*100), 1)
    }
}

improved_file = os.path.join(workspace_dir, 'august_pages_improved_search.json')
with open(improved_file, 'w', encoding='utf-8') as f:
    json.dump(improved_analysis, f, indent=2, ensure_ascii=False)

print(f"\n‚úÖ Improved search results saved to: {os.path.basename(improved_file)}")

print(f"\n=== NEXT STEPS ===\n")
if available_count >= 3:  # If we found at least 3 pages
    print("1. ‚úì Improved search strategy validated")
    print("2. üîÑ Next: Apply strategy to all 31 August pages")
    print("3. üîÑ Next: Extract content from archived pages")
    print("4. üîÑ Next: Analyze Twitter/X citations")
    print("5. üîÑ Next: Compile final results")
    print("\nüöÄ Ready to proceed with full-scale analysis!")
else:
    print("1. ‚ö†Ô∏è Need to further adjust search strategy")
    print("2. üîÑ Consider expanding date range")
    print("3. üîÑ May need alternative approach (direct Wikipedia API)")
    print("\nüîß Strategy refinement needed before scaling up")
```