### Development Step 52: Generate JSON List of Survivor US Season 1–44 Winners

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment app development and Trivia game integration for dynamically loading Survivor winners by season into quiz modules
- Data journalism dashboard creation and interactive visualization of Survivor winner demographics (age, gender, occupation) over seasons 1–44
- Academic media studies research and statistical trend analysis on reality‐TV competition outcomes using historical winner data
- Marketing analytics for streaming platforms and ad-targeting strategies by correlating Survivor winner profiles with viewer engagement metrics
- E-learning curriculum design and hands-on web scraping exercises to teach students how to extract structured tables from live Wikipedia pages
- Podcast production workflow automation and auto-injection of the latest Survivor winners into episode show notes via JSON feeds
- Predictive modeling in data science and machine learning experiments to forecast future Survivor champions based on past winner attributes
- Broadcast operations and internal database synchronization of Survivor season winners for sponsor reporting and rights-management systems

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# === DEBUGGING SCRIPT: Verify Survivor Wikipedia page and list headlines ===
# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
wiki_url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {wiki_url}\n")
resp = requests.get(wiki_url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
})

# 3) Check status and final URL after redirects
try:
    resp.raise_for_status()
    print(f"→ HTTP Status: {resp.status_code} OK")
    print(f"→ Final URL: {resp.url}\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 4) Save a snippet of the HTML for manual inspection
snippet_path = os.path.join(workspace_dir, 'survivor_page_snippet.html')
with open(snippet_path, 'w', encoding='utf-8') as f:
    # Save first 2000 characters to inspect structure
    f.write(resp.text[:2000])
print(f"Saved HTML snippet (first 2000 chars) to: {snippet_path}\n")

# 5) Parse HTML and extract all <span class="mw-headline"> elements
soup = BeautifulSoup(resp.text, 'html.parser')
headlines = soup.find_all('span', class_='mw-headline')
print(f"Found {len(headlines)} <span class=\"mw-headline\"> elements on the page.\n")

# 6) Write the first 30 id/text pairs to a file
headlines_path = os.path.join(workspace_dir, 'survivor_headlines_list.txt')
with open(headlines_path, 'w', encoding='utf-8') as f:
    f.write("# First 30 <span class='mw-headline'> id/text pairs\n")
    for i, span in enumerate(headlines[:30], start=1):
        hid = span.get('id', '')
        text = span.get_text(strip=True)
        line = f"{i:2d}. id='{hid}'  text='{text}'"
        f.write(line + "\n")
print(f"Wrote first {min(30, len(headlines))} headlines to: {headlines_path}\n")

# 7) Print sample headlines to console
print("=== Sample headlines ===")
for i, span in enumerate(headlines[:30], start=1):
    hid = span.get('id', '')
    text = span.get_text(strip=True)
    print(f"{i:2d}. id='{hid}'  text='{text}'")
print("=== End of headlines ===")

# 8) Warn if no headlines found
if not headlines:
    print("⚠️ Warning: No .mw-headline spans found. Check the page structure or URL.")
```