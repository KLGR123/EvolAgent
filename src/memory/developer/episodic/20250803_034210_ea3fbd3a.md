### Development Step 5: Compile Antidisestablishmentarianism Wikipedia Revision History: Total Edits, Timestamps, Metadata Until June 2023

**Description**: Search for and access the Wikipedia page on 'Antidisestablishmentarianism' to locate its edit history or revision log. Extract comprehensive information about all edits made to this page from its creation until June 2023, including the total number of revisions, edit timestamps, and any available metadata about the page's editing activity over time.

**Use Cases**:
- Journalistic investigation tracking edit patterns on politically sensitive Wikipedia pages to identify potential lobbying influences and provide evidence in news articles
- Legal compliance auditing by law firms extracting complete revision history of high-profile article pages to prepare defamation or IP infringement cases with documented edit timelines
- Academic research in political science analyzing longitudinal changes in the definition of ideological terms like Antidisestablishmentarianism to chart conceptual evolution over decades
- Machine learning dataset creation for automated vandalism detection by extracting metadata and user activity patterns from Wikipedia revision logs to train classification models
- Digital humanities timeline visualization of semantic shifts by mapping article size, comment, and edit frequency data onto interactive graphs illustrating the cultural impact of specific terms
- Corporate reputation management monitoring brand-related Wikipedia pages for unauthorized content or defamation by collecting real-time revision metadata and alerting PR teams to critical edits
- Search engine optimization strategy development analyzing historical keyword usage and page size evolution on Wikipedia entries to inform on-page SEO best practices and content structure
- Government transparency reporting archiving pre-policy-change Wikipedia revision histories to comply with open data mandates and allow public auditing of edits before legislative updates

```
import os
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone
import time
from urllib.parse import urljoin, quote
from collections import Counter

print("=== COMPREHENSIVE ANTIDISESTABLISHMENTARIANISM REVISION EXTRACTION ===\n")
print("Objective: Extract ALL revisions from creation until June 2023 using Wikipedia API\n")

# Create workspace directory
workspace_dir = f"workspace_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
os.makedirs(workspace_dir, exist_ok=True)
print(f"Created workspace directory: {workspace_dir}\n")

# DEFINE ALL GLOBAL VARIABLES FIRST TO AVOID SCOPE ISSUES
page_title = "Antidisestablishmentarianism"
cutoff_date = "2023-06-30T23:59:59Z"  # End of June 2023
api_url = "https://en.wikipedia.org/w/api.php"  # DEFINED EARLY TO AVOID SCOPE ISSUES

print(f"Target page: {page_title}")
print(f"Cutoff date: {cutoff_date} (end of June 2023)")
print(f"API endpoint: {api_url}\n")

# Set up headers for requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Function to make API request with rate limiting
def make_api_request(params, delay=1.0):
    """Make API request with rate limiting and error handling"""
    try:
        print(f"  Making API request to: {api_url}")
        print(f"  Parameters: {list(params.keys())}")
        
        time.sleep(delay)  # Respectful rate limiting
        response = requests.get(api_url, params=params, headers=headers, timeout=30)
        
        print(f"  API response status: {response.status_code}")
        
        if response.status_code == 200:
            try:
                data = response.json()
                print(f"  API response received and parsed successfully")
                return data
            except json.JSONDecodeError as e:
                print(f"‚ùå JSON parsing error: {str(e)}")
                print(f"Raw response: {response.text[:500]}")
                return None
        else:
            print(f"‚ùå API request failed: HTTP {response.status_code}")
            print(f"Response text: {response.text[:500]}")
            return None
    except Exception as e:
        print(f"‚ùå API request error: {str(e)}")
        return None

# Function to parse timestamp and check if it's before cutoff
def is_before_cutoff(timestamp_str, cutoff_str):
    """Check if timestamp is before the cutoff date"""
    try:
        timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        cutoff = datetime.fromisoformat(cutoff_str.replace('Z', '+00:00'))
        return timestamp <= cutoff
    except Exception as e:
        print(f"  Warning: timestamp parsing error for {timestamp_str}: {e}")
        return True  # If parsing fails, include the revision

# Start comprehensive revision extraction
print("=== STARTING COMPREHENSIVE REVISION EXTRACTION ===\n")

all_revisions = []
continue_token = None
total_requests = 0
max_requests = 100  # Reasonable limit to avoid overwhelming the API
revisions_after_cutoff = 0

print(f"Starting extraction with max {max_requests} API requests...\n")

while total_requests < max_requests:
    total_requests += 1
    
    # Build API parameters
    api_params = {
        'action': 'query',
        'format': 'json',
        'titles': page_title,
        'prop': 'revisions',
        'rvlimit': '500',  # Maximum allowed per request
        'rvprop': 'timestamp|user|comment|size|ids|flags',
        'rvdir': 'older'  # Start from newest and go backwards
    }
    
    # Add continuation token if we have one
    if continue_token:
        api_params.update(continue_token)
        print(f"  Using continuation: {continue_token}")
    
    print(f"Request {total_requests}: Fetching up to 500 revisions...")
    
    # Make the API request
    api_data = make_api_request(api_params, delay=1.5)
    
    if not api_data:
        print(f"‚ùå Failed to get API response, stopping extraction")
        break
    
    print(f"  Processing API response...")
    
    # Process the response
    if 'query' not in api_data or 'pages' not in api_data['query']:
        print(f"‚ùå Unexpected API response structure")
        print(f"API response keys: {list(api_data.keys())}")
        if 'query' in api_data:
            print(f"Query keys: {list(api_data['query'].keys())}")
        break
    
    pages = api_data['query']['pages']
    page_found = False
    
    print(f"  Found {len(pages)} pages in response")
    
    for page_id, page_data in pages.items():
        print(f"  Processing page ID: {page_id}")
        
        if 'missing' in page_data:
            print(f"‚ùå Page '{page_title}' not found")
            break
        
        if 'revisions' not in page_data:
            print(f"‚ùå No revisions found in response")
            print(f"Page data keys: {list(page_data.keys())}")
            break
        
        page_found = True
        revisions = page_data['revisions']
        print(f"  Retrieved {len(revisions)} revisions")
        
        # Process each revision
        revisions_before_cutoff_batch = 0
        revisions_after_cutoff_batch = 0
        oldest_timestamp = None
        newest_timestamp = None
        
        for revision in revisions:
            timestamp = revision.get('timestamp', '')
            
            # Track date range
            if not oldest_timestamp or timestamp < oldest_timestamp:
                oldest_timestamp = timestamp
            if not newest_timestamp or timestamp > newest_timestamp:
                newest_timestamp = timestamp
            
            # Check if revision is before cutoff date
            if is_before_cutoff(timestamp, cutoff_date):
                all_revisions.append(revision)
                revisions_before_cutoff_batch += 1
            else:
                revisions_after_cutoff += 1
                revisions_after_cutoff_batch += 1
        
        print(f"  Date range: {oldest_timestamp} to {newest_timestamp}")
        print(f"  Revisions before June 2023 (this batch): {revisions_before_cutoff_batch}")
        print(f"  Revisions after June 2023 (this batch): {revisions_after_cutoff_batch}")
        print(f"  Total collected so far: {len(all_revisions)}")
        
        # Check if we should continue
        if 'continue' in api_data:
            continue_token = api_data['continue']
            print(f"  More data available, continuing...")
        else:
            print(f"  ‚úÖ Reached end of revision history")
            break
    
    if not page_found:
        print(f"‚ùå No valid page data found")
        break
    
    # If no continuation token, we're done
    if 'continue' not in api_data:
        print(f"\n‚úÖ Complete revision history extracted!")
        break
    
    print()  # Empty line for readability

print(f"\n=== EXTRACTION COMPLETE ===\n")
print(f"Total API requests made: {total_requests}")
print(f"Total revisions collected: {len(all_revisions)}")
print(f"Revisions after June 2023 (excluded): {revisions_after_cutoff}")

if len(all_revisions) == 0:
    print("‚ùå No revisions were collected")
    print("This could indicate:")
    print("  - API access issues")
    print("  - Page doesn't exist")
    print("  - All revisions are after June 2023")
    print("  - Network connectivity problems")
    
    # Save empty result for debugging
    debug_data = {
        'extraction_metadata': {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'target_page': page_title,
            'cutoff_date': cutoff_date,
            'api_requests_made': total_requests,
            'total_revisions_collected': 0,
            'status': 'failed - no revisions collected'
        }
    }
    
    debug_file = os.path.join(workspace_dir, 'extraction_debug.json')
    with open(debug_file, 'w', encoding='utf-8') as f:
        json.dump(debug_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìÅ Debug data saved to: {os.path.basename(debug_file)}")
    
else:
    # Sort revisions by timestamp (oldest first)
    all_revisions.sort(key=lambda x: x.get('timestamp', ''))
    
    print(f"\n=== REVISION ANALYSIS ===\n")
    
    # Extract key statistics
    timestamps = [rev.get('timestamp', '') for rev in all_revisions if rev.get('timestamp')]
    users = [rev.get('user', 'Unknown') for rev in all_revisions]
    sizes = [rev.get('size', 0) for rev in all_revisions if isinstance(rev.get('size'), int)]
    comments = [rev.get('comment', '') for rev in all_revisions]
    revision_ids = [rev.get('revid', 0) for rev in all_revisions if rev.get('revid')]
    
    # Basic statistics
    print(f"üìä Basic Statistics:")
    print(f"  Total revisions: {len(all_revisions)}")
    if timestamps:
        print(f"  Date range: {min(timestamps)} to {max(timestamps)}")
        print(f"  Page creation date: {min(timestamps)}")
        print(f"  Last edit before June 2023: {max(timestamps)}")
    print(f"  Unique users: {len(set(users))}")
    if sizes:
        print(f"  Average page size: {sum(sizes) // len(sizes)} bytes")
        print(f"  Size range: {min(sizes)} to {max(sizes)} bytes")
    if revision_ids:
        print(f"  Revision ID range: {min(revision_ids)} to {max(revision_ids)}")
    
    # User activity analysis
    user_counts = Counter(users)
    top_users = user_counts.most_common(10)
    
    print(f"\nüë• Top 10 Most Active Users:")
    for i, (user, count) in enumerate(top_users, 1):
        print(f"  {i:2d}. {user}: {count} edits")
    
    # Temporal analysis
    years = {}
    months = {}
    for timestamp in timestamps:
        if timestamp:
            year = timestamp[:4]
            month = timestamp[:7]  # YYYY-MM
            years[year] = years.get(year, 0) + 1
            months[month] = months.get(month, 0) + 1
    
    print(f"\nüìÖ Edits by Year:")
    for year in sorted(years.keys()):
        print(f"  {year}: {years[year]} edits")
    
    # Show most active months
    top_months = sorted(months.items(), key=lambda x: x[1], reverse=True)[:5]
    print(f"\nüìÖ Top 5 Most Active Months:")
    for month, count in top_months:
        print(f"  {month}: {count} edits")
    
    # Sample revisions
    print(f"\nüìù Sample Revisions:")
    print(f"\nFirst revision (page creation):")
    first_rev = all_revisions[0]
    for key, value in first_rev.items():
        print(f"  {key}: {value}")
    
    if len(all_revisions) > 1:
        print(f"\nMost recent revision (before June 2023):")
        last_rev = all_revisions[-1]
        for key, value in last_rev.items():
            print(f"  {key}: {value}")
    
    # Save comprehensive data
    print(f"\n=== SAVING COMPREHENSIVE DATA ===\n")
    
    # Create comprehensive dataset
    comprehensive_data = {
        'extraction_metadata': {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'target_page': page_title,
            'cutoff_date': cutoff_date,
            'api_requests_made': total_requests,
            'total_revisions_collected': len(all_revisions),
            'revisions_after_cutoff_excluded': revisions_after_cutoff,
            'date_range': {
                'earliest': min(timestamps) if timestamps else None,
                'latest': max(timestamps) if timestamps else None
            },
            'extraction_method': 'Wikipedia API with pagination'
        },
        'statistics': {
            'total_revisions': len(all_revisions),
            'unique_users': len(set(users)),
            'average_size': sum(sizes) // len(sizes) if sizes else 0,
            'size_range': {
                'min': min(sizes) if sizes else 0,
                'max': max(sizes) if sizes else 0
            },
            'revision_id_range': {
                'min': min(revision_ids) if revision_ids else 0,
                'max': max(revision_ids) if revision_ids else 0
            },
            'edits_by_year': years,
            'edits_by_month': dict(top_months),
            'top_users': dict(top_users)
        },
        'all_revisions': all_revisions
    }
    
    # Save main data file
    data_file = os.path.join(workspace_dir, 'antidisestablishmentarianism_complete_history.json')
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Complete revision data saved to: {os.path.basename(data_file)}")
    print(f"   File size: {os.path.getsize(data_file):,} bytes")
    
    # Create summary report
    summary_file = os.path.join(workspace_dir, 'revision_summary.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"ANTIDISESTABLISHMENTARIANISM - COMPLETE REVISION HISTORY\n")
        f.write(f"={'='*60}\n\n")
        f.write(f"Extraction Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Target Page: {page_title}\n")
        f.write(f"Cutoff Date: {cutoff_date}\n\n")
        
        f.write(f"EXTRACTION RESULTS:\n")
        f.write(f"- Total API requests: {total_requests}\n")
        f.write(f"- Total revisions collected: {len(all_revisions)}\n")
        f.write(f"- Revisions excluded (after June 2023): {revisions_after_cutoff}\n")
        if timestamps:
            f.write(f"- Date range: {min(timestamps)} to {max(timestamps)}\n")
        f.write(f"- Unique contributors: {len(set(users))}\n\n")
        
        f.write(f"TEMPORAL DISTRIBUTION:\n")
        for year in sorted(years.keys()):
            f.write(f"- {year}: {years[year]} edits\n")
        
        f.write(f"\nTOP CONTRIBUTORS:\n")
        for i, (user, count) in enumerate(top_users[:5], 1):
            f.write(f"- {i}. {user}: {count} edits\n")
        
        f.write(f"\nPAGE EVOLUTION:\n")
        if timestamps:
            f.write(f"- Created: {min(timestamps)}\n")
            f.write(f"- Last edit before June 2023: {max(timestamps)}\n")
        if sizes:
            f.write(f"- Size evolution: {min(sizes)} to {max(sizes)} bytes\n")
    
    print(f"‚úÖ Summary report saved to: {os.path.basename(summary_file)}")
    
    # Create CSV export for easy analysis
    csv_file = os.path.join(workspace_dir, 'revisions_export.csv')
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write('revision_id,parent_id,timestamp,user,size,comment\n')
        for rev in all_revisions:
            # Escape commas and quotes in comments
            comment = rev.get('comment', '').replace('"', '""')
            f.write(f"{rev.get('revid', '')},{rev.get('parentid', '')},{rev.get('timestamp', '')},{rev.get('user', '')},{rev.get('size', '')},\"{comment}\"\n")
    
    print(f"‚úÖ CSV export saved to: {os.path.basename(csv_file)}")
    
    print(f"\n=== EXTRACTION SUCCESS ===\n")
    print(f"üéâ Successfully extracted complete revision history!")
    print(f"üìä {len(all_revisions)} revisions from {min(timestamps) if timestamps else 'unknown'} to {max(timestamps) if timestamps else 'unknown'}")
    print(f"üë• {len(set(users))} unique contributors")
    print(f"üìÅ All data saved to workspace: {workspace_dir}")
    print(f"‚úÖ Ready for detailed analysis and reporting")
    
    # Final summary for the PLAN
    print(f"\n=== PLAN COMPLETION SUMMARY ===\n")
    print(f"‚úÖ OBJECTIVE ACHIEVED: Comprehensive edit history extracted")
    print(f"üìã Total revisions from creation until June 2023: {len(all_revisions)}")
    print(f"üìÖ Complete temporal coverage: {min(timestamps) if timestamps else 'N/A'} to {max(timestamps) if timestamps else 'N/A'}")
    print(f"üîç Comprehensive metadata extracted: timestamps, users, comments, sizes, revision IDs")
    print(f"üìä Statistical analysis completed: user activity, temporal distribution, page evolution")
    print(f"üíæ Multiple output formats created: JSON (structured), TXT (summary), CSV (spreadsheet)")
```