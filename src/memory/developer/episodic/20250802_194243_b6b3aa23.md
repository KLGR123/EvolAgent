### Development Step 16: Identify Emily Midkiff’s June 2014 dragon depictions article in Fafnir journal.

**Description**: Search for information about Emily Midkiff's June 2014 article, focusing on identifying the journal named after one of Hreidmar's sons from Norse mythology. Research Hreidmar's sons (Fafnir, Regin, and Otr) to determine which one has a journal named after them, then locate Midkiff's specific article from June 2014 that discusses dragon depictions and contains quotes from two different authors expressing distaste.

**Use Cases**:
- Archived newspaper retrieval for historical research to quickly locate articles from specific dates in large HTML archive dumps
- FDA safety report extraction by scraping offline regulatory HTML documents to find and extract drug review articles from a particular author and date
- Corporate intranet policy audit automation to search saved HTML snapshots for outdated guidelines and generate remediation recommendations
- Local news media monitoring for brand management by scanning archived webpage files for mentions of new product launches with sentiment tagging
- Compliance review of SEC filing archives by parsing HTML annual report snapshots to identify quarter-specific financial disclosures
- SEO site architecture assessment by analyzing exported HTML pages to locate missing navigational links and optimize internal search functionality
- Localization quality assurance by searching through localized HTML build files to ensure date-specific content updates across multiple language versions

```
import os
import json
from bs4 import BeautifulSoup

print('=== ANALYZING SAVED FAFNIR JOURNAL FILES FOR EMILY MIDKIFF ARTICLE ===')
print('Objective: Review saved HTML files to find alternative paths to Emily Midkiff June 2014 article')
print('Strategy: Inspect saved files and explore different navigation approaches\n')

# Define workspace path
workspace = 'workspace'
if not os.path.exists(workspace):
    print(f'ERROR: Workspace directory {workspace} does not exist')
    print('Please ensure the previous Fafnir journal research has been completed')
else:
    print(f'✓ Workspace directory found: {workspace}')

# List all files in workspace to understand what we have
print('\n=== STEP 1: INSPECTING WORKSPACE FILES ===')
print()

try:
    workspace_files = os.listdir(workspace)
    print(f'Found {len(workspace_files)} files in workspace:')
    for i, filename in enumerate(workspace_files, 1):
        filepath = os.path.join(workspace, filename)
        file_size = os.path.getsize(filepath)
        print(f'  {i}. {filename} ({file_size:,} bytes)')
except Exception as e:
    print(f'Error listing workspace files: {str(e)}')

# Look for the key files from previous research
key_files = {
    'homepage': 'fafnir_journal_homepage.html',
    'navigation': 'fafnir_navigation_page.html',
    'results': 'emily_midkiff_research_results.json'
}

available_files = {}
for file_type, filename in key_files.items():
    filepath = os.path.join(workspace, filename)
    if os.path.exists(filepath):
        available_files[file_type] = filepath
        print(f'✓ Found {file_type} file: {filename}')
    else:
        print(f'⚠ Missing {file_type} file: {filename}')

print('\n=== STEP 2: ANALYZING HOMEPAGE FOR ALTERNATIVE NAVIGATION ===')
print()

if 'homepage' in available_files:
    try:
        with open(available_files['homepage'], 'r', encoding='utf-8') as f:
            homepage_content = f.read()
        
        homepage_soup = BeautifulSoup(homepage_content, 'html.parser')
        print(f'Homepage content loaded: {len(homepage_content):,} characters')
        
        # Look for search functionality
        search_elements = homepage_soup.find_all(['input', 'form'], attrs={'type': 'search'})
        search_elements.extend(homepage_soup.find_all(attrs={'class': lambda x: x and 'search' in str(x).lower()}))
        search_elements.extend(homepage_soup.find_all(attrs={'id': lambda x: x and 'search' in str(x).lower()}))
        
        if search_elements:
            print(f'Found {len(search_elements)} search-related elements:')
            for i, element in enumerate(search_elements[:5], 1):
                print(f'  {i}. {element.name} - {element.get("class", "no class")} - {element.get("id", "no id")}')
        else:
            print('No obvious search functionality found on homepage')
        
        # Look for direct article links or author mentions
        all_links = homepage_soup.find_all('a', href=True)
        print(f'\nAnalyzing all {len(all_links)} links for Emily Midkiff or 2014 references:')
        
        midkiff_links = []
        year_2014_links = []
        article_links = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                full_url = f'https://fafnir.journal.fi{href}'
            elif not href.startswith('http'):
                full_url = f'https://fafnir.journal.fi/{href}'
            else:
                full_url = href
            
            text_lower = text.lower()
            href_lower = href.lower()
            
            # Check for Midkiff references
            if 'midkiff' in text_lower or 'midkiff' in href_lower:
                midkiff_links.append({
                    'url': full_url,
                    'text': text[:100],
                    'type': 'midkiff_reference'
                })
            
            # Check for 2014 references
            if '2014' in text_lower or '2014' in href_lower:
                year_2014_links.append({
                    'url': full_url,
                    'text': text[:100],
                    'type': '2014_reference'
                })
            
            # Check for article-like links
            if '/article/' in href_lower or 'view' in href_lower:
                article_links.append({
                    'url': full_url,
                    'text': text[:100],
                    'type': 'article_link'
                })
        
        print(f'  Midkiff references: {len(midkiff_links)}')
        print(f'  2014 references: {len(year_2014_links)}')
        print(f'  Article links: {len(article_links)}')
        
        if midkiff_links:
            print('\n*** MIDKIFF REFERENCES FOUND ON HOMEPAGE! ***')
            for i, link in enumerate(midkiff_links, 1):
                print(f'  {i}. "{link["text"]}"')
                print(f'     URL: {link["url"]}')
        
        if year_2014_links:
            print('\n*** 2014 REFERENCES FOUND ON HOMEPAGE! ***')
            for i, link in enumerate(year_2014_links[:3], 1):
                print(f'  {i}. "{link["text"]}"')
                print(f'     URL: {link["url"]}')
        
        # Look for volume/issue structure
        volume_issue_patterns = ['vol', 'volume', 'issue', 'no.', 'number']
        volume_links = []
        
        for link in all_links:
            text = link.get_text().strip().lower()
            href = link.get('href', '').lower()
            
            if any(pattern in text or pattern in href for pattern in volume_issue_patterns):
                if href.startswith('/'):
                    full_url = f'https://fafnir.journal.fi{href}'
                elif not href.startswith('http'):
                    full_url = f'https://fafnir.journal.fi/{href}'
                else:
                    full_url = href
                
                volume_links.append({
                    'url': full_url,
                    'text': link.get_text().strip()[:100]
                })
        
        if volume_links:
            print(f'\nFound {len(volume_links)} volume/issue links:')
            for i, link in enumerate(volume_links[:5], 1):
                print(f'  {i}. "{link["text"]}"')
                print(f'     URL: {link["url"]}')
    
    except Exception as e:
        print(f'Error analyzing homepage: {str(e)}')
else:
    print('Homepage file not available for analysis')

print('\n=== STEP 3: ANALYZING NAVIGATION/ARCHIVE PAGE ===')
print()

if 'navigation' in available_files:
    try:
        with open(available_files['navigation'], 'r', encoding='utf-8') as f:
            nav_content = f.read()
        
        nav_soup = BeautifulSoup(nav_content, 'html.parser')
        print(f'Navigation page content loaded: {len(nav_content):,} characters')
        
        # Extract all text to search for years and issues
        nav_text = nav_soup.get_text()
        print(f'Navigation page text length: {len(nav_text):,} characters')
        
        # Look for year patterns
        import re
        year_pattern = r'\b(20\d{2})\b'
        years_found = re.findall(year_pattern, nav_text)
        unique_years = sorted(set(years_found))
        
        print(f'Years found on navigation page: {unique_years}')
        
        if '2014' in unique_years:
            print('\n*** 2014 FOUND ON NAVIGATION PAGE! ***')
            
            # Find context around 2014 mentions
            year_2014_positions = []
            search_text = nav_text.lower()
            start_pos = 0
            
            while True:
                pos = search_text.find('2014', start_pos)
                if pos == -1:
                    break
                year_2014_positions.append(pos)
                start_pos = pos + 1
            
            print(f'Found {len(year_2014_positions)} mentions of 2014:')
            for i, pos in enumerate(year_2014_positions[:3], 1):
                context_start = max(0, pos - 100)
                context_end = min(len(nav_text), pos + 150)
                context = nav_text[context_start:context_end].strip()
                print(f'  {i}. Context: ...{context}...')
        
        # Look for all links on navigation page
        nav_links = nav_soup.find_all('a', href=True)
        print(f'\nFound {len(nav_links)} links on navigation page')
        
        # Categorize links by year and content
        year_2014_nav_links = []
        issue_links = []
        
        for link in nav_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            if href.startswith('/'):
                full_url = f'https://fafnir.journal.fi{href}'
            elif not href.startswith('http'):
                full_url = f'https://fafnir.journal.fi/{href}'
            else:
                full_url = href
            
            text_lower = text.lower()
            href_lower = href.lower()
            
            # Check for 2014 in links
            if '2014' in text_lower or '2014' in href_lower:
                year_2014_nav_links.append({
                    'url': full_url,
                    'text': text[:150],
                    'href': href
                })
            
            # Check for issue/volume patterns
            issue_patterns = ['vol', 'issue', 'number', 'no.']
            if any(pattern in text_lower for pattern in issue_patterns) and len(text) > 5:
                issue_links.append({
                    'url': full_url,
                    'text': text[:150],
                    'href': href
                })
        
        if year_2014_nav_links:
            print(f'\n*** FOUND {len(year_2014_nav_links)} LINKS WITH 2014 REFERENCES! ***')
            for i, link in enumerate(year_2014_nav_links, 1):
                print(f'  {i}. "{link["text"]}"')
                print(f'     URL: {link["url"]}')
                print(f'     Original href: {link["href"]}')
                print()
        
        if issue_links:
            print(f'\nFound {len(issue_links)} issue/volume links:')
            for i, link in enumerate(issue_links[:8], 1):
                print(f'  {i}. "{link["text"]}"')
                print(f'     URL: {link["url"]}')
    
    except Exception as e:
        print(f'Error analyzing navigation page: {str(e)}')
else:
    print('Navigation page file not available for analysis')

print('\n=== STEP 4: CHECKING FOR EXISTING RESULTS FILE ===')
print()

if 'results' in available_files:
    try:
        # First inspect the structure of the results file
        with open(available_files['results'], 'r', encoding='utf-8') as f:
            results_content = f.read()
        
        print(f'Results file found with {len(results_content)} characters')
        print('\nInspecting results file structure...')
        
        # Parse JSON and inspect keys
        results_data = json.loads(results_content)
        
        print('Top-level keys in results file:')
        for key in results_data.keys():
            print(f'  - {key}')
        
        # Display key information
        if 'research_objective' in results_data:
            print(f'\nResearch objective: {results_data["research_objective"]}')
        
        if 'search_results' in results_data:
            search_results = results_data['search_results']
            print('\nSearch results summary:')
            for key, value in search_results.items():
                print(f'  {key}: {value}')
        
        if 'completion_status' in results_data:
            print(f'\nCompletion status: {results_data["completion_status"]}')
    
    except Exception as e:
        print(f'Error reading results file: {str(e)}')
else:
    print('No existing results file found')

print('\n=== STEP 5: ALTERNATIVE SEARCH STRATEGY RECOMMENDATIONS ===')
print()

print('Based on file analysis, recommended next steps:')
print()

if 'midkiff_links' in locals() and midkiff_links:
    print('🎯 PRIORITY: Direct Midkiff links found on homepage!')
    print('   → Access these links immediately to find the article')
elif 'year_2014_nav_links' in locals() and year_2014_nav_links:
    print('🎯 HIGH PRIORITY: 2014 links found on navigation page!')
    print('   → Access these 2014-specific links to find June 2014 issue')
elif 'issue_links' in locals() and issue_links:
    print('📋 MEDIUM PRIORITY: Issue/volume links found')
    print('   → Systematically check issue links for 2014 publications')
else:
    print('🔍 ALTERNATIVE APPROACHES NEEDED:')
    print('   → Try direct URL construction for 2014 archives')
    print('   → Look for site search functionality')
    print('   → Check journal\'s table of contents pages')

print('\n📁 FILES SAVED FOR MANUAL REVIEW:')
for file_type, filepath in available_files.items():
    print(f'   - {file_type}: {os.path.basename(filepath)}')

print('\n*** ANALYSIS COMPLETE ***')
print('Next: Use findings above to access specific 2014 or Midkiff-related links')
```