### Development Step 8: Extract Survivor US Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment website content automation and dynamic updating of Survivor winner listings for fan engagement
- Data journalism project building interactive timelines and trend analyses of Survivor champions across seasons
- Academic research on demographic diversity by extracting and analyzing winners from seasons 1–44
- Trivia and quiz app development importing historical Survivor winner data for question generation
- Broadcast graphics automation for TV networks generating on-screen stats and winner highlights
- Media analytics team correlating Survivor winner profiles with viewership and social media metrics
- Data science training workshops demonstrating real-world web scraping and JSON export workflows
- Automated newsletter service compiling “On this day in Survivor history” features with past winners

```
import os
import json
import requests
from bs4 import BeautifulSoup

# 1) Ensure workspace exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2) Fetch Wikipedia page
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
print(f"Fetching page: {url}")
resp = requests.get(url)
resp.raise_for_status()
print("Page fetched successfully.")

# 3) Parse HTML
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Locate the “Winners by season” section span
print("Locating 'Winners by season' section...")
section_span = soup.find('span', id='Winners_by_season')
if not section_span:
    print("Could not find span#Winners_by_season. Available section headlines:")
    for sp in soup.find_all('span', class_='mw-headline'):
        print(f"  id='{sp.get('id')}' text='{sp.get_text(strip=True)}'")
    raise RuntimeError("Cannot locate 'Winners by season' section")
print("Found section span, extracting next table sibling...")

# 5) From the heading, find the next <table>
heading_tag = section_span.find_parent(['h2','h3','h4'])
winners_table = None
for sib in heading_tag.next_siblings:
    if getattr(sib, 'name', None) == 'table':
        winners_table = sib
        break
if not winners_table:
    raise RuntimeError("No table found immediately after the 'Winners by season' heading")
print("Winners table located. Parsing header row...")

# 6) Parse header row to find column indices
header_row = winners_table.find('tr')
header_cells = header_row.find_all(['th','td'], recursive=False)
headers = []
for cell in header_cells:
    # remove footnote markers
    for sup in cell.find_all('sup'):
        sup.decompose()
    headers.append(cell.get_text(strip=True).lower())
print(f"Header columns: {headers}")

if 'season' not in headers or 'winner' not in headers:
    raise RuntimeError(f"Unexpected headers in winners table. Expected 'Season' and 'Winner'. Got: {headers}")
season_idx = headers.index('season')
winner_idx = headers.index('winner')
print(f"Indices → season: {season_idx}, winner: {winner_idx}")

# 7) Iterate data rows
rows = winners_table.find_all('tr')[1:]  # skip header
winners = []
print(f"Total data rows (excluding header): {len(rows)}")
for row in rows:
    cells = row.find_all(['th','td'], recursive=False)
    # need at least two cols
    if len(cells) <= max(season_idx, winner_idx):
        print(f" Skipping row: only {len(cells)} cells")
        continue
    # parse season number
    season_cell = cells[season_idx]
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    season_text = season_cell.get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f" Skipping row: invalid season '{season_text}'")
        continue
    if not (1 <= season_num <= 44):
        print(f" Skipping season {season_num}: out of target range 1–44")
        continue
    # parse winner name
    winner_cell = cells[winner_idx]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    winner_name = winner_cell.get_text(strip=True)
    print(f" Parsed season {season_num} → winner: {winner_name}")
    winners.append({'season': season_num, 'winner': winner_name})

# 8) Sort and save
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"Total winners extracted for seasons 1–44: {len(winners_sorted)}")
if len(winners_sorted) != 44:
    print("⚠️ Warning: expected 44 entries but found {len(winners_sorted)}. Check parsing logic.")

out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Winners list saved to: {out_path}")
```