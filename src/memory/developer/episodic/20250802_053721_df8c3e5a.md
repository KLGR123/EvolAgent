### Development Step 14: Download Leicester Paper PDF: 'Can Hiccup Supply Enough Fish for Dragon Diet'

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragonâ€™s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Legal research automation: bulk download of recent court rulings in PDF from official judiciary portals for case analysis workflow
- Pharmaceutical development monitoring: automated retrieval of clinical trial protocols and study reports in PDF from government and academic sites
- Corporate competitive intelligence: scheduled scraping and downloading of competitor whitepapers and market analysis PDFs from company websites
- Academic library digitization: systematic harvesting of theses and dissertations in PDF format from university repositories for digital archives
- Financial compliance monitoring: extraction and archiving of regulatory filings (e.g., SEC 10-K and 10-Q reports) in PDF for audit and risk assessment
- Educational test preparation: automated collection of standardized exam papers (SAT, ACT, GRE) in PDF from testing authority websites for question-bank generation
- Environmental policy research: automated retrieval of environmental impact assessment reports and regulatory guidelines in PDF from government portals for sustainability studies

```
import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote, urljoin

# 1. Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)
print(f"[INFO] Using workspace: {workspace}\n")

# 2. Allow override via environment variable
override_pdf = os.getenv('DRAGON_PDF_URL')
if override_pdf:
    print(f"[INFO] DRAGON_PDF_URL override detected: {override_pdf}\n")
    landing_pdf = override_pdf
else:
    # 3. Define saved search HTML files
    ddg_html = os.path.join(workspace, 'site_le_duckduckgo_search.html')
    bing_html = os.path.join(workspace, 'site_le_bing_search.html')
    for path in (ddg_html, bing_html):
        if not os.path.exists(path):
            print(f"ERROR: Missing required file: {path}")
            sys.exit(1)
    print(f"[INFO] Found search HTML:\n - {ddg_html}\n - {bing_html}\n")

    # 4. Extract landing page URLs on le.ac.uk
    def parse_search_for_leicester(html_path, label):
        urls = []
        print(f"[PARSE] Looking for Leicester landing links in {label}...")
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')
        for a in soup.find_all('a', href=True):
            href = a['href']
            # handle DuckDuckGo uddg redirects
            if '/l/?uddg=' in href:
                parsed = urlparse(href)
                q = parse_qs(parsed.query).get('uddg', [])
                if q:
                    href = unquote(q[0])
            # absolute or relative
            full = href
            # ensure we have absolute URL
            if not urlparse(full).scheme:
                continue
            if 'le.ac.uk' in urlparse(full).netloc.lower():
                print(f"  [FOUND] {full}")
                urls.append(full)
        return urls

    ddg_links = parse_search_for_leicester(ddg_html, 'DuckDuckGo')
    bing_links = parse_search_for_leicester(bing_html, 'Bing')
    landing_links = []
    for u in ddg_links + bing_links:
        if u not in landing_links:
            landing_links.append(u)
    print(f"\n[INFO] Unique Leicester landing URLs: {len(landing_links)}\n")

    if not landing_links:
        print("ERROR: No le.ac.uk landing pages found.\n       Please inspect the HTML files or set DRAGON_PDF_URL to the direct PDF URL.")
        sys.exit(1)

    # 5. For each landing page, fetch and search for .pdf link
    landing_pdf = None
    for idx, page_url in enumerate(landing_links, start=1):
        print(f"[FETCH] Landing page {idx}/{len(landing_links)}: {page_url}")
        try:
            resp = requests.get(page_url, headers={'User-Agent':'Mozilla/5.0'}, timeout=20)
            resp.raise_for_status()
        except Exception as e:
            print(f"  [WARN] Could not fetch landing page: {e}\n")
            continue
        soup2 = BeautifulSoup(resp.text, 'html.parser')
        pdf_candidates = []
        for a in soup2.find_all('a', href=True):
            href2 = a['href']
            if href2.lower().endswith('.pdf') or '.pdf?' in href2.lower():
                full_pdf = urljoin(page_url, href2)
                pdf_candidates.append(full_pdf)
        if pdf_candidates:
            landing_pdf = pdf_candidates[0]
            print(f"  [FOUND] PDF link on page: {landing_pdf}\n")
            break
        else:
            print("  [INFO] No PDF links on this landing page.\n")
        time.sleep(1)

    if not landing_pdf:
        print("ERROR: No PDF link found on any landing pages.\n       Please set DRAGON_PDF_URL or refine the parser.")
        sys.exit(1)

# 6. Download the identified PDF
pdf_path = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')
print(f"[DOWNLOAD] Downloading PDF from: {landing_pdf}\n         to: {pdf_path}\n")
size = 0
try:
    with requests.get(landing_pdf, headers={'User-Agent':'Mozilla/5.0','Accept':'application/pdf'}, stream=True, timeout=60) as r:
        r.raise_for_status()
        with open(pdf_path, 'wb') as f:
            for chunk in r.iter_content(8192):
                if chunk:
                    f.write(chunk)
                    size += len(chunk)
except Exception as e:
    print(f"ERROR: Failed to download PDF: {e}")
    sys.exit(1)

# 7. Final report
print(f"[DONE] Download complete: {size} bytes ({size/1024/1024:.2f} MB)")
if size < 10_000:
    print("WARNING: File size <10 KB; may be incomplete or incorrect.")
else:
    print(f"SUCCESS: PDF saved at {pdf_path}")
```