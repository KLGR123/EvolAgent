### Development Step 1: Search and Compile Comprehensive List of Washington State County Seats and Their Counties

**Description**: Search for and compile a comprehensive list of all county seats in Washington state along with their respective counties. Use authoritative government sources such as the Washington State government website, U.S. Census Bureau resources, or reliable reference materials to ensure accuracy and completeness of the county seat information.

**Use Cases**:
- A regional logistics company automating delivery route optimization by using county seat locations as central hubs for multi-stop daily shipments.
- A political campaign team scheduling candidate visits and public events in each county’s seat to ensure comprehensive voter outreach across Washington.
- A GIS software developer integrating county seat coordinates into an emergency response mapping tool for faster dispatch and resource allocation during natural disasters.
- A state tourism board building a mobile app that highlights historical sites, museums, and annual festivals in every Washington county seat.
- A market research analyst segmenting consumer data by county seat to design hyper-local advertising campaigns for retail chains.
- A public health department mapping vaccination clinic coverage relative to each county seat’s population center to identify underserved areas.
- A news organization automating the creation of county-by-county fact sheets for an election guide, including demographics and administrative contacts for each county seat.
- A real estate platform enhancing property search filters by distance to county seat government offices and public amenities for prospective homebuyers.

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
import re

# Ensure workspace directory exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Initialize results list
results = []

# First, try to get county information from Washington State government sources
print("Step 1: Fetching Washington State county information...")

# Try the Washington State Association of Counties website
wsac_url = 'https://www.wsac.org/counties'
print(f"Fetching: {wsac_url}")

try:
    response = requests.get(wsac_url, timeout=15)
    response.raise_for_status()
    wsac_html = response.text
    
    # Save the HTML for inspection
    wsac_html_path = os.path.join(workspace, 'wsac_counties.html')
    with open(wsac_html_path, 'w', encoding='utf-8') as f:
        f.write(wsac_html)
    print(f"Saved WSAC HTML to {wsac_html_path}")
    
    # Parse the HTML
    soup = BeautifulSoup(wsac_html, 'html.parser')
    
    # Look for county information - this will vary based on the site structure
    # Let's examine the page structure first
    print("\nExamining WSAC page structure...")
    
    # Look for common patterns that might contain county information
    county_links = soup.find_all('a', href=True)
    county_mentions = []
    
    for link in county_links:
        link_text = link.get_text(strip=True)
        if 'county' in link_text.lower() and len(link_text) < 50:
            county_mentions.append({
                'text': link_text,
                'href': link.get('href')
            })
    
    print(f"Found {len(county_mentions)} potential county mentions:")
    for mention in county_mentions[:10]:  # Show first 10
        print(f"  - {mention['text']} -> {mention['href']}")
    
except Exception as e:
    print(f"Error fetching WSAC data: {e}")
    wsac_html = None

time.sleep(2)

# Try Wikipedia as a reliable backup source
print("\nStep 2: Fetching from Wikipedia...")
wiki_url = 'https://en.wikipedia.org/wiki/List_of_counties_in_Washington'
print(f"Fetching: {wiki_url}")

try:
    response = requests.get(wiki_url, timeout=15)
    response.raise_for_status()
    wiki_html = response.text
    
    # Save the HTML
    wiki_html_path = os.path.join(workspace, 'wiki_wa_counties.html')
    with open(wiki_html_path, 'w', encoding='utf-8') as f:
        f.write(wiki_html)
    print(f"Saved Wikipedia HTML to {wiki_html_path}")
    
    # Parse Wikipedia page
    wiki_soup = BeautifulSoup(wiki_html, 'html.parser')
    
    # Look for the main table with county information
    tables = wiki_soup.find_all('table', class_='wikitable')
    print(f"Found {len(tables)} wikitable(s)")
    
    if tables:
        main_table = tables[0]  # Usually the first table contains the county list
        rows = main_table.find_all('tr')
        print(f"Found {len(rows)} rows in the main table")
        
        # Examine the header to understand the structure
        if rows:
            header_row = rows[0]
            headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
            print(f"Table headers: {headers}")
            
            # Process data rows
            for i, row in enumerate(rows[1:], 1):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:  # Need at least county name and county seat
                    county_name = cells[0].get_text(strip=True)
                    county_seat = cells[1].get_text(strip=True) if len(cells) > 1 else ''
                    
                    # Clean up the data
                    county_name = re.sub(r'\[.*?\]', '', county_name).strip()
                    county_seat = re.sub(r'\[.*?\]', '', county_seat).strip()
                    
                    if county_name and county_seat:
                        results.append({
                            'county': county_name,
                            'county_seat': county_seat,
                            'source': 'Wikipedia'
                        })
                        print(f"  Row {i}: {county_name} -> {county_seat}")
                        
except Exception as e:
    print(f"Error fetching Wikipedia data: {e}")

time.sleep(2)

# Try U.S. Census Bureau as another authoritative source
print("\nStep 3: Attempting U.S. Census Bureau source...")
census_url = 'https://www.census.gov/geographies/reference-files/2020/demo/popest/2020-fips.html'
print(f"Fetching: {census_url}")

try:
    response = requests.get(census_url, timeout=15)
    response.raise_for_status()
    census_html = response.text
    
    # Save for inspection
    census_html_path = os.path.join(workspace, 'census_fips.html')
    with open(census_html_path, 'w', encoding='utf-8') as f:
        f.write(census_html)
    print(f"Saved Census HTML to {census_html_path}")
    
except Exception as e:
    print(f"Error fetching Census data: {e}")

# Display results summary
print(f"\nStep 4: Results Summary")
print(f"Total county seats found: {len(results)}")

if results:
    print("\nFirst 10 entries:")
    for i, entry in enumerate(results[:10], 1):
        print(f"  {i}. {entry['county']} County -> {entry['county_seat']}")
    
    # Save results to JSON
    output_path = os.path.join(workspace, 'wa_county_seats.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    print(f"\nSaved {len(results)} county seat records to {output_path}")
    
    # Also create a simple CSV format
    csv_path = os.path.join(workspace, 'wa_county_seats.csv')
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write('County,County Seat,Source\n')
        for entry in results:
            f.write(f"{entry['county']},{entry['county_seat']},{entry['source']}\n")
    print(f"Also saved as CSV to {csv_path}")
else:
    print("No county seat data was successfully extracted. Will need to try alternative approaches.")

print("\nStep 5: Verification - Washington should have 39 counties")
print(f"We found {len(results)} counties. Expected: 39")
if len(results) != 39:
    print("Count mismatch detected - may need to refine extraction or try additional sources")
```