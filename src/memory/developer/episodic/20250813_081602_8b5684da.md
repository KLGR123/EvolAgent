### Development Step 2: Research Abel Hugo’s Napoleonic Military Service, Madrid Publications, and Hospital de Saint Louis Connection

**Description**: Research Abel Hugo, Victor Hugo's brother, focusing on his military service during the Napoleonic Wars and his time in Madrid. Search for information about his authorship of works containing maps of Napoleon's camps in Spain, his connection to the Hospital de Saint Louis in Madrid (converted to military hospital in 1809), and identify during which ruler's reign these events occurred. Look for biographical details about Abel Hugo's military career, his publications, and the specific timeframe when he was in Madrid with Victor Hugo.

**Use Cases**:
- Academic historians conducting digital archival research on Abel Hugo’s military service to cross-reference Napoleonic War campaigns and validate historical narratives
- Genealogists verifying Victor Hugo’s family lineage by extracting biographical and military details about his brother Abel Hugo for detailed family trees
- Cultural heritage curators automating the extraction of information on Madrid’s Hospital de Saint Louis conversion to a military hospital for museum exhibition catalogs
- Educational content developers generating accurate timelines, maps, and contextual annotations for 19th-century European history textbooks
- Biographical authors compiling authoritative profiles of lesser-known literary figures by mining publications, map references, and ruler contexts from primary and secondary sources
- Digital librarians enriching metadata for library collections focused on Napoleonic-era cartographic works and military publications to enhance discoverability
- Historical data analysts integrating extracted camp locations and troop movement data into GIS platforms to visually reconstruct the Peninsular War in Spain
- Travel journalists creating historically informed walking tours in Madrid by mapping former military hospital sites and Napoleonic camp locations mentioned in period works

```
import json
import os
from datetime import datetime

print("Analyzing Abel Hugo research results and fixing Wikipedia parsing issues...")

# First, let's inspect what files we have in workspace
print("\n=== INSPECTING WORKSPACE FILES ===")
if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    print(f"Found {len(workspace_files)} files in workspace:")
    for file in workspace_files:
        print(f"  - {file}")
else:
    print("No workspace directory found")

# Check if we have the research results file from previous search
results_file = 'workspace/abel_hugo_research_results.json'
if os.path.exists(results_file):
    print(f"\n=== INSPECTING SEARCH RESULTS FILE STRUCTURE ===")
    with open(results_file, 'r') as f:
        search_data = json.load(f)
    
    print("Search results file structure:")
    for key, value in search_data.items():
        if isinstance(value, dict):
            print(f"  - {key}: dict with keys: {list(value.keys())}")
        elif isinstance(value, list):
            print(f"  - {key}: list with {len(value)} items")
        else:
            print(f"  - {key}: {type(value).__name__} - {str(value)[:100]}...")
    
    # Analyze the existing results
    print(f"\n=== ANALYZING EXISTING ABEL HUGO RESEARCH DATA ===")
    
    all_results = search_data.get('all_results', [])
    specific_analysis = search_data.get('specific_analysis', {})
    search_summary = search_data.get('search_summary', {})
    
    print(f"Total results from previous search: {len(all_results)}")
    print(f"Wikipedia pages attempted: {search_summary.get('wikipedia_pages_searched', 0)}")
    print(f"Google Books queries: {search_summary.get('google_books_queries', 0)}")
    print(f"Search errors: {search_summary.get('total_errors', 0)}")
    
    # Display the Google Books results we did get
    print(f"\n=== GOOGLE BOOKS RESULTS ANALYSIS ===")
    google_books_results = [r for r in all_results if r.get('source') == 'Google Books']
    print(f"Found {len(google_books_results)} Google Books results")
    
    for i, book in enumerate(google_books_results[:8], 1):
        print(f"\nBook {i}:")
        print(f"  Title: {book['title']}")
        print(f"  Authors: {', '.join(book['authors'])}")
        print(f"  Keywords: {', '.join(book['keywords_found'])}")
        print(f"  Relevance Score: {book['relevance_score']}")
        if book.get('description'):
            print(f"  Description: {book['description'][:200]}...")
    
    # Analyze specific findings
    print(f"\n=== SPECIFIC FINDINGS FROM PREVIOUS SEARCH ===")
    for finding_type, findings in specific_analysis.items():
        if findings and finding_type != 'potential_works':
            print(f"\n{finding_type.replace('_', ' ').title()}: {len(findings)} mentions")
            for finding in findings[:1]:  # Show first mention
                print(f"  Source: {finding['source']}")
                print(f"  Text: {finding['text_sample'][:150]}...")
    
    potential_works = specific_analysis.get('potential_works', [])
    if potential_works:
        print(f"\nPotential Works: {len(potential_works)}")
        for work in potential_works[:5]:
            print(f"  - {work}")

else:
    print(f"Previous search results file not found: {results_file}")
    search_data = {}

# Now let's fix the Wikipedia parsing and get the missing information
print(f"\n{'='*80}")
print("FIXING WIKIPEDIA PARSING AND GATHERING ABEL HUGO INFORMATION")
print(f"{'='*80}")

import requests
from bs4 import BeautifulSoup
import time

# Initialize new results storage
new_results = []
search_errors = []

# Wikipedia URLs to search (corrected Hospital name)
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Abel_Hugo",
    "https://fr.wikipedia.org/wiki/Abel_Hugo",  # French Wikipedia often has more details
    "https://en.wikipedia.org/wiki/Victor_Hugo",
    "https://en.wikipedia.org/wiki/Napoleonic_Wars",
    "https://en.wikipedia.org/wiki/Peninsular_War",
    "https://en.wikipedia.org/wiki/Madrid"
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print("\n=== CORRECTED WIKIPEDIA SEARCH ===")

for url in wikipedia_urls:
    try:
        print(f"\nFetching: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        print(f"Page title: {title}")
        
        # Extract main content - FIXED VERSION
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove unwanted elements - CORRECTED SCOPE
            unwanted_elements = content_div.find_all(['script', 'style'])
            for element in unwanted_elements:
                element.decompose()
            
            # Remove navigation and info boxes
            nav_elements = content_div.find_all('div')
            for element in nav_elements:
                if element.get('class'):
                    classes = element.get('class')
                    if any(cls in str(classes) for cls in ['navbox', 'infobox', 'sidebar']):
                        element.decompose()
            
            content = content_div.get_text(separator=' ', strip=True)
            print(f"Content length: {len(content)} characters")
            
            # Target keywords for Abel Hugo research
            target_keywords = [
                'abel hugo', 'victor hugo', 'napoleonic wars', 'madrid', 'spain', 
                'military service', 'military hospital', 'hospital',
                'napoleon', 'camps', 'maps', 'publications', 'author', 'brother',
                '1809', 'peninsular war', 'french army', 'officer', 'career',
                'joseph bonaparte', 'ferdinand vii'
            ]
            
            # Find matching keywords
            content_lower = content.lower()
            found_keywords = []
            for keyword in target_keywords:
                if keyword in content_lower:
                    found_keywords.append(keyword)
            
            print(f"Keywords found: {', '.join(found_keywords)}")
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content[:4000],  # First 4000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                new_results.append(result)
                print(f"Added relevant result with {len(found_keywords)} keyword matches")
                
                # Look for specific Abel Hugo information
                if 'abel hugo' in content_lower:
                    print("\n*** ABEL HUGO SPECIFIC CONTENT FOUND ***")
                    # Extract Abel Hugo specific passages
                    abel_passages = []
                    sentences = content.split('.')
                    for sentence in sentences:
                        if 'abel hugo' in sentence.lower():
                            abel_passages.append(sentence.strip())
                    
                    if abel_passages:
                        print(f"Found {len(abel_passages)} sentences mentioning Abel Hugo:")
                        for i, passage in enumerate(abel_passages[:3], 1):
                            print(f"  {i}. {passage[:200]}...")
            
        time.sleep(2)  # Be respectful
        
    except Exception as e:
        error_msg = f"Error fetching {url}: {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

# Analyze the new Wikipedia results for Abel Hugo specific information
print(f"\n=== ANALYZING NEW WIKIPEDIA RESULTS FOR ABEL HUGO ===")

abel_hugo_info = {
    'military_service': [],
    'madrid_time': [],
    'napoleon_maps': [],
    'hospital_connections': [],
    'publications': [],
    'timeframe': [],
    'ruler_context': [],
    'family_relations': []
}

for result in new_results:
    content = result.get('content', '').lower()
    
    # Extract Abel Hugo specific information
    if 'abel hugo' in content:
        print(f"\nAnalyzing {result['title']} for Abel Hugo information...")
        
        # Look for military service mentions
        if any(term in content for term in ['military', 'army', 'officer', 'service', 'soldier']):
            abel_hugo_info['military_service'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found military service information")
        
        # Look for Madrid connections
        if 'madrid' in content:
            abel_hugo_info['madrid_time'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found Madrid connection")
        
        # Look for Napoleon/maps references
        if 'napoleon' in content and ('maps' in content or 'camps' in content):
            abel_hugo_info['napoleon_maps'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found Napoleon maps/camps reference")
        
        # Look for hospital mentions
        if 'hospital' in content:
            abel_hugo_info['hospital_connections'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found hospital connection")
        
        # Look for publications
        if any(term in content for term in ['author', 'wrote', 'published', 'work', 'book']):
            abel_hugo_info['publications'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found publication information")
        
        # Look for 1809 timeframe
        if '1809' in content:
            abel_hugo_info['timeframe'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found 1809 timeframe reference")
        
        # Look for ruler context
        if any(ruler in content for ruler in ['joseph bonaparte', 'napoleon', 'ferdinand']):
            abel_hugo_info['ruler_context'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found ruler context")
        
        # Look for family relations
        if 'victor hugo' in content and 'brother' in content:
            abel_hugo_info['family_relations'].append({
                'source': result['title'],
                'url': result['url'],
                'content_sample': result['content'][:1000]
            })
            print("  - Found family relation information")

# Combine with previous results if available
if search_data:
    combined_results = search_data.get('all_results', []) + new_results
else:
    combined_results = new_results

# Save comprehensive Abel Hugo research
final_abel_hugo_data = {
    'research_date': datetime.now().isoformat(),
    'research_focus': 'Abel Hugo - Victor Hugo\'s brother, military service during Napoleonic Wars, Madrid connections, Napoleon camp maps, Hospital de Saint Louis, publications, ruler context',
    'search_summary': {
        'new_wikipedia_results': len(new_results),
        'previous_google_books_results': len(search_data.get('all_results', [])) if search_data else 0,
        'total_combined_results': len(combined_results),
        'search_errors': len(search_errors)
    },
    'all_results': combined_results,
    'abel_hugo_specific_analysis': abel_hugo_info,
    'search_errors': search_errors
}

output_file = 'workspace/abel_hugo_comprehensive_research.json'
with open(output_file, 'w') as f:
    json.dump(final_abel_hugo_data, f, indent=2)

print(f"\n=== COMPREHENSIVE ABEL HUGO RESEARCH SUMMARY ===")
print(f"Results saved to: {output_file}")
print(f"New Wikipedia results: {len(new_results)}")
print(f"Total combined results: {len(combined_results)}")

# Display key findings
print(f"\n=== KEY ABEL HUGO FINDINGS ===")
for category, findings in abel_hugo_info.items():
    if findings:
        print(f"\n{category.replace('_', ' ').title()}: {len(findings)} sources")
        for finding in findings[:1]:  # Show first source
            print(f"  Source: {finding['source']}")
            print(f"  Content sample: {finding['content_sample'][:300]}...")

print(f"\n=== WIKIPEDIA SEARCH ERRORS ===")
if search_errors:
    for error in search_errors:
        print(f"  - {error}")
else:
    print("No errors in Wikipedia search!")

print(f"\nAbel Hugo research data collection completed successfully!")
```