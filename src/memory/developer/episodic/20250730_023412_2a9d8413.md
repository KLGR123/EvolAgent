### Development Step 11: Trace LOTR to Song of Ice and Fire Link Paths via 2023-07-03 Wiki Archive

**Description**: Access the archived Wikipedia page for 'A Song of Ice and Fire' (book series) as it appeared at the end of July 3, 2023. Use the same Wayback Machine approach to retrieve the specific version from that date. Extract all inbound links and cross-reference them with the previously extracted outbound links from The Lord of the Rings page to identify direct connections. If no direct connection exists, systematically trace potential multi-step paths by examining intermediate pages that were linked from LOTR, particularly focusing on the high-priority fantasy/literature connections like 'International Fantasy Award', 'High fantasy', and other genre-related pages that might contain links to George R.R. Martin's work.

**Use Cases**:
- Competitive product analysis in the automotive industry: retrieve archived Wikipedia pages for two rival car models, extract inbound/outbound links, and identify direct connections or shared supplier references to map supply-chain overlaps.
- Academic collaboration mapping in climate research: access archived versions of two leading journal articles, cross-reference citations (inbound/outbound links), and trace multi-step paths through high-priority topics like “IPCC reports” to reveal hidden co-authorship networks.
- Regulatory compliance auditing in pharmaceuticals: pull archived FDA guidance and drug monograph pages, extract links to legislation and clinical trials, and identify direct or multi-step connections to ensure all referenced safety standards are up to date.
- Digital humanities intertextual network analysis: use archived Wikipedia entries for two classical authors, compare their host of referenced works and literary awards pages, and unveil shared influences via “Epic poetry” or “Mythology” intermediary links.
- Brand reputation monitoring in consumer electronics: retrieve archived corporate press release pages and product FAQ entries, extract all backlinks, and cross-reference with major tech review pages to spot direct or indirect mentions impacting public perception.
- SEO backlink auditing for an e-commerce site: fetch previous snapshots of your site’s product listings and top competitor catalogs, extract outbound and inbound link data, and identify overlapping affiliate or review sites that drive search rankings.
- Policy impact research in public health: access archived health policy proposals and related WHO guideline pages, cross-link citations, and trace pathways through “Global health security” or “Pandemic response” pages to map influence chains among international agencies.

```
import os
import json
from datetime import datetime
from collections import defaultdict
import re

print('=== CROSS-REFERENCING LOTR AND ASOIAF WIKIPEDIA LINKS ===')
print('Objective: Find direct connections between LOTR outbound links and ASOIAF inbound links')
print('Strategy: Compare the 379 ASOIAF links with 878 LOTR links to identify matches\n')

# First, let's understand the workspace structure and file contents
workspace_files = os.listdir('workspace')
print(f'Available workspace files: {workspace_files}\n')

# Load and inspect the ASOIAF data structure
asoiaf_file = 'workspace/asoiaf_wikipedia_archived_july_2023.json'
print('=== INSPECTING ASOIAF DATA STRUCTURE ===')

with open(asoiaf_file, 'r', encoding='utf-8') as f:
    asoiaf_data = json.load(f)

print(f'ASOIAF data top-level keys: {list(asoiaf_data.keys())}')
for key, value in asoiaf_data.items():
    if isinstance(value, list):
        print(f'  {key}: List with {len(value)} items')
    elif isinstance(value, dict):
        print(f'  {key}: Dictionary with keys: {list(value.keys())}')
    else:
        print(f'  {key}: {type(value).__name__} - {str(value)[:100]}...')

# Inspect the inbound links structure
if 'inbound_links' in asoiaf_data:
    sample_links = asoiaf_data['inbound_links'][:3] if len(asoiaf_data['inbound_links']) > 0 else []
    print(f'\nSample ASOIAF inbound links structure:')
    for i, link in enumerate(sample_links, 1):
        print(f'  Link {i}: {link}')

# Load and inspect the LOTR data structure  
lotr_file = 'workspace/lotr_wikipedia_links_july_2023.json'
print('\n=== INSPECTING LOTR DATA STRUCTURE ===')

with open(lotr_file, 'r', encoding='utf-8') as f:
    lotr_data = json.load(f)

print(f'LOTR data top-level keys: {list(lotr_data.keys())}')
for key, value in lotr_data.items():
    if isinstance(value, list):
        print(f'  {key}: List with {len(value)} items')
    elif isinstance(value, dict):
        print(f'  {key}: Dictionary with keys: {list(value.keys())}')
        if key == 'categorized_links':
            for cat_key, cat_value in value.items():
                if isinstance(cat_value, list):
                    print(f'    {cat_key}: {len(cat_value)} items')
    else:
        print(f'  {key}: {type(value).__name__}')

# Inspect the wikipedia links structure
if 'wikipedia_links' in lotr_data:
    sample_lotr_links = lotr_data['wikipedia_links'][:3] if len(lotr_data['wikipedia_links']) > 0 else []
    print(f'\nSample LOTR wikipedia links structure:')
    for i, link in enumerate(sample_lotr_links, 1):
        print(f'  Link {i}: {link}')

print('\n=== EXTRACTING CLEAN URLS FOR COMPARISON ===')

# Extract clean URLs from ASOIAF inbound links
asoiaf_links = asoiaf_data['inbound_links']
print(f'Total ASOIAF inbound links: {len(asoiaf_links)}')

# Clean ASOIAF URLs by removing Wayback Machine prefixes and extracting article titles
asoiaf_articles = set()
asoiaf_clean_urls = {}

for link in asoiaf_links:
    url = link['url']
    article_title = link['article_title']
    
    # Extract the actual Wikipedia URL from Wayback Machine URL
    if 'web.archive.org' in url:
        # Extract the original URL after the timestamp
        parts = url.split('https://en.wikipedia.org/wiki/')
        if len(parts) > 1:
            clean_article = parts[-1]
        else:
            clean_article = article_title.replace(' ', '_')
    else:
        clean_article = url.split('/wiki/')[-1]
    
    # Normalize the article title
    normalized_title = clean_article.replace('_', ' ').strip()
    asoiaf_articles.add(normalized_title.lower())
    asoiaf_clean_urls[normalized_title.lower()] = {
        'original_url': url,
        'article_title': article_title,
        'normalized_title': normalized_title
    }

print(f'Unique ASOIAF articles (normalized): {len(asoiaf_articles)}')
print('\nFirst 10 ASOIAF articles:')
for i, article in enumerate(sorted(list(asoiaf_articles))[:10], 1):
    print(f'  {i:2d}. {article}')

# Extract clean URLs from LOTR outbound links
lotr_links = lotr_data['wikipedia_links']
print(f'\nTotal LOTR outbound links: {len(lotr_links)}')

# Clean LOTR URLs
lotr_articles = set()
lotr_clean_urls = {}

for link in lotr_links:
    url = link['url']
    article_title = link['article_title']
    
    # Extract clean article name
    if 'web.archive.org' in url:
        parts = url.split('https://en.wikipedia.org/wiki/')
        if len(parts) > 1:
            clean_article = parts[-1]
        else:
            clean_article = article_title.replace(' ', '_')
    else:
        clean_article = url.split('/wiki/')[-1]
    
    # Normalize the article title
    normalized_title = clean_article.replace('_', ' ').strip()
    lotr_articles.add(normalized_title.lower())
    lotr_clean_urls[normalized_title.lower()] = {
        'original_url': url,
        'article_title': article_title,
        'normalized_title': normalized_title
    }

print(f'Unique LOTR articles (normalized): {len(lotr_articles)}')
print('\nFirst 10 LOTR articles:')
for i, article in enumerate(sorted(list(lotr_articles))[:10], 1):
    print(f'  {i:2d}. {article}')

print('\n=== FINDING DIRECT CONNECTIONS ===')

# Find direct matches between LOTR outbound links and ASOIAF inbound links
direct_connections = lotr_articles.intersection(asoiaf_articles)

print(f'\nDirect connections found: {len(direct_connections)}')

if direct_connections:
    print('\n*** DIRECT CONNECTIONS DISCOVERED ***')
    for i, connection in enumerate(sorted(direct_connections), 1):
        print(f'{i:2d}. {connection}')
        
        # Show details from both sides
        lotr_info = lotr_clean_urls[connection]
        asoiaf_info = asoiaf_clean_urls[connection]
        
        print(f'    LOTR -> {lotr_info["article_title"]} ({lotr_info["original_url"]})')
        print(f'    ASOIAF -> {asoiaf_info["article_title"]} ({asoiaf_info["original_url"]})')
        print()
else:
    print('\n❌ NO DIRECT CONNECTIONS FOUND')
    print('Need to explore multi-step paths through intermediate pages')

# Let's also check high-priority fantasy/literature connections for potential stepping stones
print('\n=== ANALYZING HIGH-PRIORITY FANTASY/LITERATURE CONNECTIONS ===')

# Load the LOTR path-finding analysis
lotr_analysis_file = 'workspace/lotr_path_finding_analysis.json'
with open(lotr_analysis_file, 'r', encoding='utf-8') as f:
    lotr_analysis = json.load(f)

print(f'LOTR analysis structure: {list(lotr_analysis.keys())}')

if 'high_priority_links' in lotr_analysis:
    high_priority_links = lotr_analysis['high_priority_links']
    print(f'High-priority LOTR links: {len(high_priority_links)}')
    
    # Check if any high-priority links are also in ASOIAF inbound links
    high_priority_matches = []
    
    for hp_link in high_priority_links:
        if isinstance(hp_link, dict) and 'article_title' in hp_link:
            hp_title = hp_link['article_title'].lower()
            if hp_title in asoiaf_articles:
                high_priority_matches.append((hp_link, asoiaf_clean_urls[hp_title]))
    
    print(f'\nHigh-priority matches with ASOIAF: {len(high_priority_matches)}')
    
    if high_priority_matches:
        print('\n*** HIGH-PRIORITY CONNECTIONS FOUND ***')
        for i, (lotr_link, asoiaf_link) in enumerate(high_priority_matches, 1):
            print(f'{i}. {lotr_link["article_title"]} (Priority: {lotr_link.get("priority", "unknown")})')
            print(f'   Category: {lotr_link.get("category", "unknown")}')
            print(f'   LOTR URL: {lotr_link["url"]}')
            print(f'   ASOIAF match: {asoiaf_link["normalized_title"]}')
            print()

# Identify potential stepping stones for multi-step paths
print('\n=== IDENTIFYING POTENTIAL STEPPING STONES ===')

# Look for fantasy/literature-related terms that might serve as intermediate connections
fantasy_keywords = [
    'fantasy', 'epic fantasy', 'high fantasy', 'dark fantasy', 'sword and sorcery',
    'literature', 'fiction', 'novel', 'author', 'writer', 'publishing',
    'award', 'hugo award', 'nebula award', 'world fantasy award', 'international fantasy award',
    'tolkien', 'martin', 'genre', 'medieval', 'magic', 'dragon', 'mythology'
]

stepping_stone_candidates = []

# Check LOTR links for fantasy-related terms
for link in lotr_links:
    article_title = link['article_title'].lower()
    if any(keyword in article_title for keyword in fantasy_keywords):
        stepping_stone_candidates.append({
            'source': 'lotr',
            'article_title': link['article_title'],
            'url': link['url'],
            'matching_keywords': [kw for kw in fantasy_keywords if kw in article_title]
        })

print(f'Potential stepping stones from LOTR: {len(stepping_stone_candidates)}')

# Show top stepping stone candidates
print('\nTop 15 stepping stone candidates:')
for i, candidate in enumerate(stepping_stone_candidates[:15], 1):
    print(f'{i:2d}. {candidate["article_title"]} (Keywords: {", ".join(candidate["matching_keywords"])})')

# Save comprehensive analysis results
connection_analysis = {
    'analysis_metadata': {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'lotr_links_analyzed': len(lotr_articles),
        'asoiaf_links_analyzed': len(asoiaf_articles),
        'direct_connections_found': len(direct_connections),
        'high_priority_matches_found': len(high_priority_matches) if 'high_priority_matches' in locals() else 0,
        'stepping_stone_candidates': len(stepping_stone_candidates)
    },
    'direct_connections': list(direct_connections),
    'high_priority_matches': high_priority_matches if 'high_priority_matches' in locals() else [],
    'stepping_stone_candidates': stepping_stone_candidates,
    'lotr_articles': sorted(list(lotr_articles)),
    'asoiaf_articles': sorted(list(asoiaf_articles))
}

output_file = 'workspace/lotr_asoiaf_connection_analysis.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(connection_analysis, f, indent=2, ensure_ascii=False)

print(f'\n=== ANALYSIS COMPLETE ===')
print(f'✓ Analyzed {len(lotr_articles)} LOTR outbound links vs {len(asoiaf_articles)} ASOIAF inbound links')
print(f'✓ Found {len(direct_connections)} direct connections')
print(f'✓ Identified {len(high_priority_matches) if "high_priority_matches" in locals() else 0} high-priority fantasy/literature matches')
print(f'✓ Located {len(stepping_stone_candidates)} potential stepping stones for multi-step paths')
print(f'✓ Analysis results saved to: {output_file}')

if direct_connections:
    print(f'\n🎉 SUCCESS: Direct Wikipedia connections found between LOTR and ASOIAF pages!')
    print(f'The connection exists through {len(direct_connections)} shared articles')
else:
    print(f'\n➡️ NEXT STEP: No direct connections found. Need to explore multi-step paths through the {len(stepping_stone_candidates)} identified stepping stones')
    print(f'Most promising candidates appear to be fantasy/literature-related pages that could link to both series')
```