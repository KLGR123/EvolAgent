### Development Step 28: Extract the repeated critical word from two authors in Emily Midkiff’s June 2014 Fafnir article.

**Description**: Access Emily Midkiff's June 2014 article in the Fafnir journal to extract the full text and identify the specific word that was quoted from two different authors expressing distaste for the nature of dragon depictions. Parse the article content systematically to locate quotes from multiple authors that contain the same critical word about dragon portrayals.

**Use Cases**:
- Digital humanities scholar conducting a comparative study of medieval and modern dragon narratives by automatically extracting and analyzing critical quotations across multiple journal articles
- Fantasy game development team auditing player feedback and literary critiques to identify recurring negative descriptors of dragon designs for in-game model improvements
- Academic librarian building a themed repository of dragon criticism by programmatically harvesting quotes from specialized journals and tagging shared evaluative terms
- Publishing editor performing batch content quality checks on mythological creature portrayals to ensure consistency in critical language across forthcoming anthology chapters
- Cultural studies researcher mapping the evolution of “dragon” imagery in 21st-century fantasy literature by mining and cross-referencing direct author quotes for key negative adjectives
- Marketing analyst for a tabletop RPG publisher leveraging text extraction to compile common consumer complaints about dragon lore for targeted ad copy revisions
- AI-driven content curation platform ingesting dragon-related articles to surface the most cited critical terms and support recommendation engines for fantasy media reviews
- NLP engineer creating a specialized sentiment dataset on mythical creatures by extracting and aligning shared critical words from academic critique articles

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin
import time
import re

print('=== ACCESSING FAFNIR 2/2014 ISSUE PAGE FOR EMILY MIDKIFF ARTICLE ===\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Target URL for Fafnir 2/2014 issue
target_issue_url = 'https://journal.finfar.org/journal/archive/fafnir-22014/'
print(f'Target issue: Fafnir 2/2014')
print(f'URL: {target_issue_url}\n')

# Headers to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

print('=== STEP 1: ACCESSING FAFNIR 2/2014 ISSUE PAGE ===\n')

try:
    print(f'Requesting: {target_issue_url}')
    issue_response = requests.get(target_issue_url, headers=headers, timeout=30)
    print(f'Response status: {issue_response.status_code}')
    print(f'Content length: {len(issue_response.content):,} bytes')
    print(f'Content type: {issue_response.headers.get("Content-Type", "unknown")}\n')
    
    if issue_response.status_code == 200:
        # Parse the HTML content
        soup = BeautifulSoup(issue_response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'Page title: {page_title.get_text().strip()}')
        
        # Save the raw HTML for analysis
        html_file = os.path.join(workspace, 'fafnir_2_2014_issue.html')
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(issue_response.text)
        print(f'Raw HTML saved to: {html_file}\n')
        
        # Extract all text content for analysis
        page_text = soup.get_text()
        print(f'Total page text length: {len(page_text):,} characters\n')
        
        # Confirm this page contains Emily Midkiff
        if 'midkiff' in page_text.lower():
            print('✓ Confirmed: Page contains "Midkiff"')
            
            # Find the exact context where Midkiff appears
            midkiff_indices = []
            text_lower = page_text.lower()
            start = 0
            while True:
                index = text_lower.find('midkiff', start)
                if index == -1:
                    break
                midkiff_indices.append(index)
                start = index + 1
            
            print(f'Found "Midkiff" at {len(midkiff_indices)} positions in the text')
            
            # Show context around each occurrence
            for i, index in enumerate(midkiff_indices, 1):
                context_start = max(0, index - 100)
                context_end = min(len(page_text), index + 100)
                context = page_text[context_start:context_end].replace('\n', ' ').strip()
                print(f'\nOccurrence {i} context:')
                print(f'...{context}...')
        else:
            print('⚠ Warning: "Midkiff" not found in page text')
        
        print('\n=== STEP 2: EXTRACTING ALL ARTICLE LINKS FROM THE ISSUE PAGE ===\n')
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Total links found on page: {len(all_links)}')
        
        # Filter links that might be articles
        potential_article_links = []
        
        for link in all_links:
            href = link.get('href')
            link_text = link.get_text().strip()
            
            # Skip empty links or navigation links
            if not href or not link_text:
                continue
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                href = urljoin('https://journal.finfar.org', href)
            elif not href.startswith('http'):
                href = urljoin(target_issue_url, href)
            
            # Look for links that might be articles (contain meaningful text)
            if len(link_text) > 10 and not any(nav_word in link_text.lower() for nav_word in ['home', 'archive', 'about', 'contact', 'menu', 'navigation']):
                potential_article_links.append({
                    'text': link_text,
                    'url': href,
                    'has_midkiff': 'midkiff' in link_text.lower()
                })
        
        print(f'Potential article links found: {len(potential_article_links)}')
        
        # Show all potential article links
        print('\n--- All Potential Article Links ---')
        for i, link in enumerate(potential_article_links, 1):
            marker = '*** MIDKIFF ***' if link['has_midkiff'] else ''
            print(f'{i:2d}. {marker}')
            print(f'    Text: {link["text"][:100]}...' if len(link['text']) > 100 else f'    Text: {link["text"]}')
            print(f'    URL:  {link["url"]}')
            print()
        
        # Find Emily Midkiff's specific article
        midkiff_links = [link for link in potential_article_links if link['has_midkiff']]
        
        if midkiff_links:
            print(f'=== FOUND {len(midkiff_links)} MIDKIFF ARTICLE LINK(S) ===\n')
            
            # Use the first Midkiff link (should be the main one)
            target_article = midkiff_links[0]
            print(f'Selected article:')
            print(f'Title: {target_article["text"]}')
            print(f'URL: {target_article["url"]}\n')
            
            print('=== STEP 3: ACCESSING EMILY MIDKIFF\'S ARTICLE ===\n')
            
            try:
                print(f'Accessing article: {target_article["url"]}')
                article_response = requests.get(target_article['url'], headers=headers, timeout=30)
                print(f'Article response status: {article_response.status_code}')
                print(f'Article content length: {len(article_response.content):,} bytes\n')
                
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.content, 'html.parser')
                    
                    # Get article title from the page
                    article_title_elem = article_soup.find('title')
                    if article_title_elem:
                        article_title = article_title_elem.get_text().strip()
                        print(f'Article page title: {article_title}')
                    
                    # Remove scripts, styles, and navigation elements
                    for element in article_soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
                        element.decompose()
                    
                    # Try multiple selectors to find the main article content
                    content_selectors = [
                        '.article-content',
                        '.article-body', 
                        '.entry-content',
                        '.post-content',
                        '.content',
                        'main',
                        '#content',
                        '.text',
                        'article'
                    ]
                    
                    article_content = None
                    used_selector = None
                    
                    for selector in content_selectors:
                        content_elem = article_soup.select_one(selector)
                        if content_elem:
                            article_content = content_elem.get_text()
                            used_selector = selector
                            print(f'✓ Article content extracted using selector: {selector}')
                            break
                    
                    if not article_content:
                        # Fallback to full page text
                        article_content = article_soup.get_text()
                        used_selector = 'full_page_fallback'
                        print('Using full page text as fallback')
                    
                    # Clean up the extracted text
                    lines = (line.strip() for line in article_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))
                    clean_content = ' '.join(chunk for chunk in chunks if chunk)
                    
                    print(f'✓ Cleaned article text: {len(clean_content):,} characters\n')
                    
                    # Save the full article text
                    article_text_file = os.path.join(workspace, 'midkiff_fafnir_article_full_text.txt')
                    with open(article_text_file, 'w', encoding='utf-8') as f:
                        f.write(f'Title: {target_article["text"]}\n')
                        f.write(f'URL: {target_article["url"]}\n')
                        f.write(f'Extraction method: {used_selector}\n')
                        f.write(f'Extracted: {time.strftime("%Y-%m-%d %H:%M:%S")}\n')
                        f.write('=' * 80 + '\n\n')
                        f.write(clean_content)
                    
                    print(f'✓ Full article text saved to: {article_text_file}')
                    
                    # Save raw HTML for backup
                    article_html_file = os.path.join(workspace, 'midkiff_fafnir_article_raw.html')
                    with open(article_html_file, 'w', encoding='utf-8') as f:
                        f.write(article_response.text)
                    
                    print(f'✓ Raw article HTML saved to: {article_html_file}\n')
                    
                    print('=== STEP 4: ANALYZING ARTICLE FOR DRAGON CRITICISM QUOTES ===\n')
                    
                    # Look for quoted text using multiple quote patterns
                    quote_patterns = [
                        r'"([^"]{10,300})"',  # Standard double quotes
                        r'"([^"]{10,300})"',  # Smart opening quote
                        r'"([^"]{10,300})"',  # Smart closing quote  
                        r'"([^"]{10,300})"',  # Mixed smart quotes
                        r"'([^']{10,300})'",  # Single quotes for longer passages
                    ]
                    
                    all_quotes = []
                    for pattern in quote_patterns:
                        matches = re.findall(pattern, clean_content, re.DOTALL)
                        for match in matches:
                            # Clean up the quote text
                            cleaned_quote = ' '.join(match.split())
                            if cleaned_quote not in [q['text'] for q in all_quotes]:  # Avoid duplicates
                                all_quotes.append({
                                    'text': cleaned_quote,
                                    'pattern': pattern
                                })
                    
                    print(f'Total unique quotes found: {len(all_quotes)}')
                    
                    # Filter quotes that might be about dragons, criticism, or literary analysis
                    relevant_keywords = [
                        'dragon', 'dragons', 'wyrm', 'wyvern', 'serpent', 'beast', 'creature', 'monster',
                        'fantasy', 'mythology', 'mythical', 'legendary',
                        'depict', 'depiction', 'portrayal', 'representation', 'image', 'imagery',
                        'character', 'characterization', 'nature', 'vision', 'interpretation',
                        'literature', 'literary', 'narrative', 'story', 'tale', 'text',
                        'author', 'writer', 'critic', 'criticism', 'critique', 'analysis',
                        'distaste', 'dislike', 'negative', 'poor', 'bad', 'terrible', 'awful',
                        'disappointing', 'problematic', 'troubling', 'concerning', 'objectionable'
                    ]
                    
                    relevant_quotes = []
                    for quote_obj in all_quotes:
                        quote_text = quote_obj['text']
                        quote_lower = quote_text.lower()
                        
                        # Check if quote contains relevant keywords
                        relevance_score = sum(1 for keyword in relevant_keywords if keyword in quote_lower)
                        
                        if relevance_score >= 1:  # At least one relevant keyword
                            relevant_quotes.append({
                                'text': quote_text,
                                'relevance_score': relevance_score,
                                'pattern': quote_obj['pattern']
                            })
                    
                    # Sort by relevance score
                    relevant_quotes.sort(key=lambda x: x['relevance_score'], reverse=True)
                    
                    print(f'Relevant quotes found: {len(relevant_quotes)}\n')
                    
                    # Display the most relevant quotes with context
                    quotes_analysis = []
                    for i, quote_obj in enumerate(relevant_quotes[:10], 1):  # Show top 10
                        quote_text = quote_obj['text']
                        print(f'--- Quote {i} (Relevance: {quote_obj["relevance_score"]}) ---')
                        print(f'"{quote_text}"\n')
                        
                        # Find context around this quote in the original text
                        quote_index = clean_content.find(quote_text)
                        if quote_index != -1:
                            context_start = max(0, quote_index - 300)
                            context_end = min(len(clean_content), quote_index + len(quote_text) + 300)
                            context = clean_content[context_start:context_end]
                            
                            print(f'Context:')
                            print(f'...{context}...\n')
                            
                            quotes_analysis.append({
                                'quote': quote_text,
                                'context': context,
                                'relevance_score': quote_obj['relevance_score'],
                                'index_in_text': quote_index
                            })
                        else:
                            print('Context: [Not found in cleaned text]\n')
                    
                    print('=== STEP 5: SEARCHING FOR SHARED CRITICAL WORDS ===\n')
                    
                    if len(relevant_quotes) >= 2:
                        # Extract individual words from each quote
                        quote_word_sets = []
                        for quote_obj in relevant_quotes:
                            # Extract meaningful words (excluding common stop words)
                            words = re.findall(r'\b\w+\b', quote_obj['text'].lower())
                            # Filter out very common words
                            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}
                            meaningful_words = set(word for word in words if word not in stop_words and len(word) > 2)
                            quote_word_sets.append(meaningful_words)
                        
                        # Find words that appear in multiple quotes
                        if len(quote_word_sets) >= 2:
                            # Start with first quote's words
                            common_words = quote_word_sets[0].copy()
                            
                            # Find intersection with each subsequent quote
                            for word_set in quote_word_sets[1:]:
                                common_words = common_words.intersection(word_set)
                            
                            # Also find words that appear in at least 2 quotes (not necessarily all)
                            word_frequency = {}
                            for word_set in quote_word_sets:
                                for word in word_set:
                                    word_frequency[word] = word_frequency.get(word, 0) + 1
                            
                            frequent_words = [word for word, freq in word_frequency.items() if freq >= 2]
                            
                            print(f'Words appearing in ALL quotes: {sorted(list(common_words))}')
                            print(f'Words appearing in 2+ quotes: {sorted(frequent_words)}')
                            
                            # Focus on critical/evaluative words
                            critical_words = []
                            evaluative_terms = ['bad', 'poor', 'terrible', 'awful', 'disappointing', 'weak', 'lacking', 'inadequate', 'insufficient', 'problematic', 'troubling', 'concerning', 'objectionable', 'distasteful', 'unacceptable', 'unsatisfactory', 'deficient', 'flawed', 'wrong', 'incorrect', 'false', 'misleading', 'inaccurate']
                            
                            for word in frequent_words:
                                if word in evaluative_terms or any(eval_term in word for eval_term in evaluative_terms):
                                    critical_words.append(word)
                            
                            if critical_words:
                                print(f'\n*** POTENTIAL SHARED CRITICAL WORDS: {critical_words} ***\n')
                            
                            # Show which quotes contain each frequent word
                            print('\n--- Word Distribution Across Quotes ---')
                            for word in sorted(frequent_words):
                                quote_indices = []
                                for i, word_set in enumerate(quote_word_sets):
                                    if word in word_set:
                                        quote_indices.append(i + 1)
                                print(f'\'{word}\': appears in quotes {quote_indices}')
                    
                    else:
                        print('Need at least 2 relevant quotes to find shared words')
                    
                    # Save comprehensive analysis
                    analysis_data = {
                        'article_info': {
                            'title': target_article['text'],
                            'url': target_article['url'],
                            'extraction_method': used_selector,
                            'content_length': len(clean_content)
                        },
                        'analysis_results': {
                            'total_quotes_found': len(all_quotes),
                            'relevant_quotes_count': len(relevant_quotes),
                            'quotes_analysis': quotes_analysis,
                            'shared_words_all_quotes': sorted(list(common_words)) if 'common_words' in locals() else [],
                            'shared_words_2plus_quotes': sorted(frequent_words) if 'frequent_words' in locals() else [],
                            'potential_critical_words': critical_words if 'critical_words' in locals() else []
                        },
                        'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    analysis_file = os.path.join(workspace, 'midkiff_dragon_quotes_analysis.json')
                    with open(analysis_file, 'w', encoding='utf-8') as f:
                        json.dump(analysis_data, f, indent=2, ensure_ascii=False)
                    
                    print(f'\n✓ Comprehensive analysis saved to: {analysis_file}')
                    
                    # Summary
                    print('\n=== ANALYSIS SUMMARY ===\n')
                    print(f'✓ Successfully accessed Emily Midkiff\'s article in Fafnir 2/2014')
                    print(f'✓ Extracted {len(clean_content):,} characters of article text')
                    print(f'✓ Found {len(all_quotes)} total quotes, {len(relevant_quotes)} relevant to dragons/criticism')
                    
                    if 'frequent_words' in locals() and frequent_words:
                        print(f'✓ Identified {len(frequent_words)} words appearing in multiple quotes')
                        if 'critical_words' in locals() and critical_words:
                            print(f'✓ Found potential shared critical words: {critical_words}')
                    
                    print(f'✓ All results saved to workspace files for detailed review')
                    
                else:
                    print(f'✗ Failed to access article: HTTP {article_response.status_code}')
                    
            except Exception as e:
                print(f'Error accessing article: {str(e)}')
        
        else:
            print('⚠ No links containing "Midkiff" found on the issue page')
            print('\nLet me search more broadly in the page content...')
            
            # Alternative approach: search for Emily Midkiff in the page text more systematically
            text_lines = page_text.split('\n')
            midkiff_lines = [line.strip() for line in text_lines if 'midkiff' in line.lower() and line.strip()]
            
            print(f'\nLines containing "Midkiff": {len(midkiff_lines)}')
            for i, line in enumerate(midkiff_lines, 1):
                print(f'{i}. {line[:200]}...' if len(line) > 200 else f'{i}. {line}')
    
    else:
        print(f'✗ Failed to access issue page: HTTP {issue_response.status_code}')
        print(f'Response content preview: {issue_response.text[:500]}...')

except Exception as e:
    print(f'Error during issue page access: {str(e)}')

print('\n=== EMILY MIDKIFF ARTICLE EXTRACTION COMPLETE ===')
print('Objective: Extract full text and identify shared critical word about dragon depictions')
print('Status: Article access and quote analysis completed')
```