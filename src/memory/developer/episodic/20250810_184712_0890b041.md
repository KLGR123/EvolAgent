### Development Step 2: Identify ‚ÄòSacred Desire‚Äôs‚Äô Soviet paratrooper hero, organization he co-founded, and 1992 crackdown escape

**Description**: Conduct a comprehensive web search to identify the book 'Sacred Desire' and its protagonist, focusing on finding information about a former Soviet paratrooper who co-founded an organization and fled due to a 1992 crackdown. Search for keywords including 'Sacred Desire book protagonist Soviet paratrooper', 'Sacred Desire novel former Soviet paratrooper co-founder', '1992 crackdown organization Soviet paratrooper fled', and 'Sacred Desire character co-founded organization 1992'. Extract details about the protagonist's identity, the organization they co-founded, and the circumstances of the 1992 crackdown.

**Use Cases**:
- Investigative journalism fact‚Äêchecking and timeline reconstruction for political expos√©s using automated web searches and relevance scoring
- Patent prior‚Äêart discovery and technical term matching in engineering domains to streamline IP filing workflows
- Market intelligence gathering and competitor feature analysis for consumer electronics product development
- Legal e‚Äêdiscovery and case law reference identification for corporate litigation support teams
- Academic literature pre‚Äêscreening and keyword‚Äêbased relevance ranking for systematic reviews in social sciences
- Brand reputation monitoring and sentiment filtering across news outlets and blogs for public relations management
- Cyber threat intelligence collection and hacker forum surveillance for security operations centers

```
import json
import os
from ddgs import DDGS
import time
from datetime import datetime

# First, let's inspect the existing search results file to understand its structure
results_file = 'workspace/sacred_desire_comprehensive_search_results.json'

print('=== INSPECTING PREVIOUS SEARCH RESULTS AND FIXING ANALYSIS ===') 
print('=' * 70)

if os.path.exists(results_file):
    print(f'‚úÖ Found previous search results file: {results_file}')
    
    # Load and inspect the JSON structure
    with open(results_file, 'r', encoding='utf-8') as f:
        previous_results = json.load(f)
    
    print(f'\nüìã INSPECTING FILE STRUCTURE:')
    for key in previous_results.keys():
        if isinstance(previous_results[key], list):
            print(f'  ‚Ä¢ {key}: list with {len(previous_results[key])} items')
        elif isinstance(previous_results[key], dict):
            print(f'  ‚Ä¢ {key}: dict with keys: {list(previous_results[key].keys())}')
        else:
            print(f'  ‚Ä¢ {key}: {type(previous_results[key])} - {str(previous_results[key])[:100]}')
    
    # Check if we have all_results data to re-analyze
    if 'all_results' in previous_results and len(previous_results['all_results']) > 0:
        print(f'\nüîç RE-ANALYZING {len(previous_results["all_results"])} SEARCH RESULTS WITH FIXED LOGIC')
        print('-' * 60)
        
        # Initialize corrected analysis containers
        corrected_analysis = {
            'high_relevance_results': [],
            'book_candidates': [],
            'protagonist_details': [],
            'soviet_paratrooper_matches': [],
            'organization_matches': [],
            'crackdown_1992_matches': []
        }
        
        # Re-analyze each result with corrected logic
        for result in previous_results['all_results']:
            title = result.get('title', 'No title')
            description = result.get('description', 'No description')
            url = result.get('url', 'No URL')
            
            # Fix the bug: properly define combined_text
            combined_text = f'{title.lower()} {description.lower()}'
            
            # Recalculate relevance score with corrected logic
            relevance_score = 0
            matched_terms = []
            
            # Enhanced key terms and weights
            key_terms = {
                'sacred desire': 5,
                'soviet': 3,
                'paratrooper': 4,
                'co-founded': 3,
                'organization': 2,
                '1992': 3,
                'crackdown': 3,
                'fled': 2,
                'protagonist': 2,
                'character': 1,
                'book': 2,
                'novel': 2,
                'military': 1,
                'veteran': 2,
                'founder': 2
            }
            
            for term, weight in key_terms.items():
                if term in combined_text:
                    relevance_score += weight
                    matched_terms.append(term)
            
            # Update result with corrected analysis
            result['corrected_relevance_score'] = relevance_score
            result['corrected_matched_terms'] = matched_terms
            
            print(f'\nAnalyzing: {title[:60]}...')
            print(f'  URL: {url}')
            print(f'  Corrected Score: {relevance_score}')
            print(f'  Matched Terms: {matched_terms}')
            
            # Categorize results with corrected logic
            if relevance_score >= 8:
                corrected_analysis['high_relevance_results'].append(result)
                print('  ‚≠ê HIGH RELEVANCE')
            
            if 'sacred desire' in combined_text and relevance_score >= 5:
                corrected_analysis['book_candidates'].append(result)
                print('  üìö BOOK CANDIDATE')
            
            if any(term in combined_text for term in ['soviet', 'paratrooper']) and relevance_score >= 4:
                corrected_analysis['protagonist_details'].append(result)
                print('  üë§ PROTAGONIST DETAIL')
            
            # Specific category matches
            if 'soviet' in combined_text and 'paratrooper' in combined_text:
                corrected_analysis['soviet_paratrooper_matches'].append(result)
                print('  ü™ñ SOVIET PARATROOPER MATCH')
            
            if any(term in combined_text for term in ['organization', 'co-founded', 'founder']):
                corrected_analysis['organization_matches'].append(result)
                print('  üè¢ ORGANIZATION MATCH')
            
            if '1992' in combined_text and any(term in combined_text for term in ['crackdown', 'fled']):
                corrected_analysis['crackdown_1992_matches'].append(result)
                print('  üìÖ 1992 CRACKDOWN MATCH')
        
        # Display corrected analysis results
        print('\n' + '=' * 70)
        print('üìä CORRECTED ANALYSIS RESULTS')
        print('=' * 70)
        
        print(f'\nüìà CATEGORY BREAKDOWN:')
        print(f'  ‚Ä¢ High relevance results: {len(corrected_analysis["high_relevance_results"])}')
        print(f'  ‚Ä¢ Book candidates: {len(corrected_analysis["book_candidates"])}')
        print(f'  ‚Ä¢ Protagonist details: {len(corrected_analysis["protagonist_details"])}')
        print(f'  ‚Ä¢ Soviet paratrooper matches: {len(corrected_analysis["soviet_paratrooper_matches"])}')
        print(f'  ‚Ä¢ Organization matches: {len(corrected_analysis["organization_matches"])}')
        print(f'  ‚Ä¢ 1992 crackdown matches: {len(corrected_analysis["crackdown_1992_matches"])}')
        
        # Show top results by category
        if corrected_analysis['high_relevance_results']:
            print(f'\nüéØ TOP HIGH-RELEVANCE RESULTS:')
            sorted_high = sorted(corrected_analysis['high_relevance_results'], 
                               key=lambda x: x['corrected_relevance_score'], reverse=True)
            for i, result in enumerate(sorted_high[:3], 1):
                print(f'\n{i}. SCORE: {result["corrected_relevance_score"]}')
                print(f'   Title: {result["title"]}')
                print(f'   URL: {result["url"]}')
                print(f'   Terms: {result["corrected_matched_terms"]}')
                print(f'   Description: {result["description"][:200]}...')
        
        if corrected_analysis['book_candidates']:
            print(f'\nüìö BOOK CANDIDATES:')
            for i, result in enumerate(corrected_analysis['book_candidates'], 1):
                print(f'\n{i}. {result["title"]}')
                print(f'   Score: {result["corrected_relevance_score"]}')
                print(f'   URL: {result["url"]}')
                print(f'   Description: {result["description"][:150]}...')
        
        # Save corrected analysis
        corrected_results = previous_results.copy()
        corrected_results['corrected_analysis'] = corrected_analysis
        corrected_results['analysis_timestamp'] = datetime.now().isoformat()
        
        corrected_file = 'workspace/sacred_desire_corrected_analysis.json'
        with open(corrected_file, 'w', encoding='utf-8') as f:
            json.dump(corrected_results, f, indent=2, ensure_ascii=False)
        
        print(f'\nüíæ Corrected analysis saved to: {corrected_file}')
        
    else:
        print('\n‚ùå No search results data found in previous file')
else:
    print(f'‚ùå Previous search results file not found: {results_file}')

# Since the previous search didn't find the specific book, let's try additional targeted searches
print('\n' + '=' * 70)
print('üîç CONDUCTING ADDITIONAL TARGETED SEARCHES')
print('=' * 70)

# New search queries focusing on different aspects
additional_queries = [
    'Soviet paratrooper co-founded organization 1992 crackdown fled',
    '1992 Russia military veteran organization crackdown',
    'Soviet airborne forces veteran organization 1992',
    'Russian paratrooper political organization 1992 suppression',
    'post-Soviet military veterans organization crackdown 1992'
]

# Initialize new search results
new_search_results = {
    'search_timestamp': datetime.now().isoformat(),
    'additional_queries': additional_queries,
    'new_results': [],
    'relevant_findings': []
}

print(f'Executing {len(additional_queries)} additional targeted searches...')

for query_num, query in enumerate(additional_queries, 1):
    print(f'\nüîç ADDITIONAL SEARCH {query_num}/{len(additional_queries)}: {query}')
    print('-' * 50)
    
    try:
        searcher = DDGS(timeout=15)
        results = searcher.text(
            query,
            max_results=10,
            page=1,
            backend=["google", "duckduckgo", "bing"],
            safesearch="off",
            region="en-us"
        )
        
        if results == []:
            print(f'‚ùå No results for: "{query}"')
        else:
            print(f'‚úÖ Found {len(results)} results')
            
            for result_num, result in enumerate(results, 1):
                title = result.get('title', 'No title')
                body = result.get('body', 'No description')
                href = result.get('href', 'No URL')
                
                # Analyze for relevance
                combined_text = f'{title.lower()} {body.lower()}'
                
                relevance_indicators = []
                if 'soviet' in combined_text or 'russian' in combined_text:
                    relevance_indicators.append('soviet/russian')
                if 'paratrooper' in combined_text or 'airborne' in combined_text:
                    relevance_indicators.append('paratrooper/airborne')
                if 'organization' in combined_text or 'co-founded' in combined_text:
                    relevance_indicators.append('organization')
                if '1992' in combined_text:
                    relevance_indicators.append('1992')
                if 'crackdown' in combined_text or 'suppression' in combined_text:
                    relevance_indicators.append('crackdown')
                
                result_data = {
                    'query': query,
                    'title': title,
                    'url': href,
                    'description': body,
                    'relevance_indicators': relevance_indicators,
                    'relevance_count': len(relevance_indicators)
                }
                
                new_search_results['new_results'].append(result_data)
                
                if len(relevance_indicators) >= 2:
                    new_search_results['relevant_findings'].append(result_data)
                    print(f'\n  ‚≠ê RELEVANT Result {result_num}:')
                    print(f'     Title: {title}')
                    print(f'     URL: {href}')
                    print(f'     Indicators: {relevance_indicators}')
                    print(f'     Description: {body[:150]}...')
                
    except Exception as e:
        print(f'‚ùå Error in search {query_num}: {str(e)}')
    
    if query_num < len(additional_queries):
        time.sleep(2)

# Final analysis
print('\n' + '=' * 70)
print('üìã FINAL COMPREHENSIVE ANALYSIS')
print('=' * 70)

print(f'\nüìä ADDITIONAL SEARCH SUMMARY:')
print(f'  ‚Ä¢ New queries executed: {len(additional_queries)}')
print(f'  ‚Ä¢ New results collected: {len(new_search_results["new_results"])}')
print(f'  ‚Ä¢ Relevant findings: {len(new_search_results["relevant_findings"])}')

if new_search_results['relevant_findings']:
    print(f'\nüéØ MOST RELEVANT NEW FINDINGS:')
    sorted_new = sorted(new_search_results['relevant_findings'], 
                       key=lambda x: x['relevance_count'], reverse=True)
    
    for i, finding in enumerate(sorted_new[:5], 1):
        print(f'\n{i}. Relevance Score: {finding["relevance_count"]}')  
        print(f'   Title: {finding["title"]}')
        print(f'   URL: {finding["url"]}')
        print(f'   Indicators: {finding["relevance_indicators"]}')
        print(f'   Description: {finding["description"][:200]}...')

# Save additional search results
additional_file = 'workspace/sacred_desire_additional_searches.json'
with open(additional_file, 'w', encoding='utf-8') as f:
    json.dump(new_search_results, f, indent=2, ensure_ascii=False)

print(f'\nüíæ Additional search results saved to: {additional_file}')

print('\nüéØ CONCLUSION:')
if new_search_results['relevant_findings']:
    print('‚úÖ Found relevant information about Soviet paratroopers and 1992 events')
    print('üìã Next steps: Investigate specific findings for "Sacred Desire" connections')
else:
    print('‚ö†Ô∏è  "Sacred Desire" with the specified protagonist may be:')
    print('   ‚Ä¢ A fictional or very obscure work')
    print('   ‚Ä¢ Known by a different title')
    print('   ‚Ä¢ Part of a larger work or anthology')
    print('üìã Recommend: Search literary databases or contact specialized libraries')

print('\n=== COMPREHENSIVE SEARCH AND ANALYSIS COMPLETE ===')
```