### Development Step 48: Extract Survivor US Seasons 1–44 Winners into workspace/survivor_winners_list.json

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Television analytics and diversity reporting for entertainment news outlets tracking winner demographics season by season
- Academic research on reality TV trends and winner profiles for sociology and media studies publications
- Automated content for a Survivor trivia game backend generating dynamic quiz questions by season
- Fan community platform development populating a searchable winners directory for Survivor enthusiasts
- Voice assistant integration (e.g., Alexa/Google Assistant) providing on-demand answers to “Who won season X?” queries
- Data-driven social media campaigns creating “Did you know?” posts highlighting past winners for the show’s official accounts
- Coding bootcamp tutorial on web scraping and data extraction using a real-world example with Wikipedia tables
- QA pipeline automation verifying Wikipedia table structure changes and alerting developers to scraper failures

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
wiki_url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {wiki_url}\n")
resp = requests.get(wiki_url, headers={
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)',
    'Accept-Language': 'en-US'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP {resp.status_code} OK")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML
txt = resp.text
soup = BeautifulSoup(txt, 'html.parser')

# 4) Collect debug info for each section heading (h2/h3/h4 .mw-headline)
debug_lines = []
spans = soup.find_all('span', class_='mw-headline')
print(f"Found {len(spans)} section headlines (.mw-headline spans)\n")
for span in spans:
    hid = span.get('id', '')
    text = span.get_text(strip=True)
    parent_heading = span.find_parent(['h2', 'h3', 'h4'])
    parent_tag = parent_heading.name if parent_heading else 'UNKNOWN'
    # count tables until the next heading of same or higher level
    tbl_count = 0
    if parent_heading:
        for sib in parent_heading.find_next_siblings():
            if sib.name in ['h2', 'h3', 'h4']:
                break
            if sib.name == 'table':
                tbl_count += 1
    debug_line = f"{parent_tag} id='{hid}' text='{text}' → {tbl_count} table(s)"
    debug_lines.append(debug_line)

# 5) Write debug info to survivor_debug_headings.txt
dbg_file = os.path.join(workspace_dir, 'survivor_debug_headings.txt')
with open(dbg_file, 'w', encoding='utf-8') as f:
    f.write("# Debug: section headings (h2/h3/h4) with .mw-headline and count of tables until next heading\n")
    for line in debug_lines:
        f.write(line + "\n")

# 6) Print debug info
print(f"✅ Debug info written to: {dbg_file}\n")
print("=== Contents of survivor_debug_headings.txt ===")
for line in debug_lines:
    print(line)
print("=== End of debug info ===")

# 7) Sanity check
if not debug_lines:
    print("⚠️ Warning: No headings captured. Check if page structure or selectors have changed.")
```