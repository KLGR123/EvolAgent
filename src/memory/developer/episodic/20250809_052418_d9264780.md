### Development Step 3: Charles 'Pete' Conrad: NASA Astronaut Group Membership and Selection Year Identification

**Description**: Research Charles 'Pete' Conrad's NASA Astronaut Group membership to determine which astronaut group he belonged to. Look for official NASA records, astronaut biographies, or space agency databases that specify Conrad's astronaut group number and selection year. This information is typically found in NASA's official astronaut biographical data or space history resources.

**Use Cases**:
- Space history documentary production requiring automated verification of astronaut group memberships for accurate narration and on-screen graphics
- Academic research compiling biographical datasets of NASA astronauts for statistical studies on astronaut selection and mission assignments
- Museum exhibit curation needing authoritative sourcing of astronaut group affiliations for interactive displays and educational materials
- Aerospace industry HR or PR teams preparing commemorative materials or press releases about notable astronauts and their NASA selection cohorts
- Automated fact-checking tools for journalists writing articles about Apollo missions, ensuring correct astronaut group attribution
- Genealogy or ancestry platforms integrating verified astronaut biographical data for users tracing family connections to space history
- Educational software developers building interactive timelines or quizzes about the U.S. space program, requiring precise astronaut group data
- Library or archive digitalization projects cataloging and cross-referencing astronaut biographies with official NASA group records for public access

```
import os
import json
from bs4 import BeautifulSoup
import re
from datetime import datetime

print("=== ANALYZING DOWNLOADED SOURCES FOR CHARLES 'PETE' CONRAD'S ASTRONAUT GROUP ===\n")
print("Objective: Parse the successfully downloaded Wikipedia and NASA sources to extract definitive astronaut group information\n")

# First, let's inspect the workspace to understand what files we have
workspace_files = []
if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    print(f"Found {len(workspace_files)} files in workspace:")
    for file in sorted(workspace_files):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")
else:
    print("No workspace directory found")
    exit()

# Let's first inspect the research summary JSON to understand what was found
research_json_files = [f for f in workspace_files if 'research.json' in f]

if research_json_files:
    research_file = research_json_files[0]
    print(f"\n=== INSPECTING RESEARCH SUMMARY: {research_file} ===\n")
    
    research_path = os.path.join('workspace', research_file)
    
    # First, let's peek at the JSON structure
    with open(research_path, 'r', encoding='utf-8') as f:
        try:
            research_data = json.load(f)
            print("Research JSON structure:")
            for key in research_data.keys():
                value = research_data[key]
                if isinstance(value, list):
                    print(f"  - {key}: list with {len(value)} items")
                elif isinstance(value, dict):
                    print(f"  - {key}: dictionary with keys: {list(value.keys())[:5]}{'...' if len(value) > 5 else ''}")
                else:
                    print(f"  - {key}: {type(value).__name__} - {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}")
        except json.JSONDecodeError as e:
            print(f"Error reading JSON: {e}")
            exit()
    
    # Check the conclusion
    if 'conclusion' in research_data:
        print(f"\nPrevious conclusion: {research_data['conclusion']}")
    
    # Look at the group sources that were successfully accessed
    successful_group_sources = []
    if 'group_sources' in research_data:
        for source in research_data['group_sources']:
            if source.get('access_successful', False):
                successful_group_sources.append(source)
        
        print(f"\nSuccessful group sources: {len(successful_group_sources)}")
        for i, source in enumerate(successful_group_sources, 1):
            print(f"  {i}. {source['url']} - HTML file: {source.get('html_filename', 'N/A')}")
            if source.get('conrad_found'):
                print(f"     ‚òÖ Conrad mentioned in this source")
            if source.get('group2_info'):
                print(f"     ‚òÖ Group 2 information found: {len(source['group2_info'])} items")
else:
    print("\nNo research summary JSON found")

# Now let's focus on the Wikipedia source which should have the most comprehensive information
wikipedia_files = [f for f in workspace_files if 'astronaut_groups_4.html' in f]  # This was the Wikipedia source

if wikipedia_files:
    wikipedia_file = wikipedia_files[0]
    print(f"\n=== ANALYZING WIKIPEDIA SOURCE: {wikipedia_file} ===\n")
    
    wikipedia_path = os.path.join('workspace', wikipedia_file)
    
    with open(wikipedia_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    print(f"Wikipedia HTML file size: {len(html_content):,} characters")
    
    # Parse the HTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Extract the page title
    page_title = soup.find('title')
    if page_title:
        print(f"Page title: {page_title.get_text().strip()}")
    
    # Get clean text content
    page_text = soup.get_text()
    print(f"Clean text length: {len(page_text):,} characters")
    
    # Search for Conrad specifically
    print(f"\n=== SEARCHING FOR CONRAD IN WIKIPEDIA CONTENT ===\n")
    
    conrad_patterns = [
        'Conrad',
        'Pete Conrad', 
        'Charles Conrad',
        'Charles "Pete" Conrad',
        'Charles P. Conrad'
    ]
    
    conrad_mentions = []
    
    for pattern in conrad_patterns:
        if pattern in page_text:
            print(f"Found '{pattern}' in Wikipedia page")
            
            # Find all occurrences and extract context
            import re
            matches = list(re.finditer(re.escape(pattern), page_text, re.IGNORECASE))
            
            for i, match in enumerate(matches, 1):
                start = max(0, match.start() - 300)
                end = min(len(page_text), match.end() + 300)
                context = page_text[start:end].strip()
                
                conrad_mentions.append({
                    'pattern': pattern,
                    'match_number': i,
                    'context': context
                })
                
                print(f"  Match {i} context:")
                print(f"    ...{context[:250]}...")
                print()
            
            break  # Found Conrad, no need to check other patterns
    
    # Search for specific Group 2 information
    print(f"=== SEARCHING FOR GROUP 2 DETAILS ===\n")
    
    # Look for the key information about Group 2
    group_keywords = [
        'NASA Astronaut Group 2',
        'Group 2',
        'New Nine',
        'Next Nine',
        '1962',
        'September 1962',
        'second group'
    ]
    
    group_info_found = []
    
    for keyword in group_keywords:
        if keyword in page_text:
            print(f"Found '{keyword}' in Wikipedia page")
            
            # Get context around this keyword
            matches = list(re.finditer(re.escape(keyword), page_text, re.IGNORECASE))
            
            for match in matches[:2]:  # Show first 2 matches
                start = max(0, match.start() - 200)
                end = min(len(page_text), match.end() + 400)
                context = page_text[start:end].strip()
                
                group_info_found.append({
                    'keyword': keyword,
                    'context': context
                })
                
                print(f"  Context: ...{context[:300]}...")
                print()
    
    # Look for the astronaut roster/list
    print(f"=== SEARCHING FOR ASTRONAUT ROSTER ===\n")
    
    # Look for patterns that indicate a list of astronauts
    roster_patterns = [
        r'Front row[^.]*Conrad[^.]*',
        r'Back row[^.]*',
        r'astronauts[^.]*Conrad[^.]*',
        r'Conrad[^.]*Borman[^.]*Armstrong[^.]*Young',
        r'nine astronauts[^.]*selected'
    ]
    
    roster_info = []
    
    for pattern in roster_patterns:
        matches = re.findall(pattern, page_text, re.IGNORECASE | re.DOTALL)
        if matches:
            print(f"Roster pattern '{pattern}' found:")
            for match in matches[:2]:  # Show first 2 matches
                clean_match = ' '.join(match.split())  # Clean up whitespace
                roster_info.append({
                    'pattern': pattern,
                    'match': clean_match
                })
                print(f"  Match: {clean_match[:200]}...")
            print()
    
    # Look for selection year and details
    print(f"=== SEARCHING FOR SELECTION DETAILS ===\n")
    
    selection_patterns = [
        r'selected[^.]*1962[^.]*',
        r'1962[^.]*selected[^.]*',
        r'Year selected[^.]*1962',
        r'September[^.]*1962[^.]*',
        r'announced[^.]*1962[^.]*'
    ]
    
    selection_info = []
    
    for pattern in selection_patterns:
        matches = re.findall(pattern, page_text, re.IGNORECASE | re.DOTALL)
        if matches:
            print(f"Selection pattern '{pattern}' found:")
            for match in matches[:2]:  # Show first 2 matches
                clean_match = ' '.join(match.split())  # Clean up whitespace
                selection_info.append({
                    'pattern': pattern,
                    'match': clean_match
                })
                print(f"  Match: {clean_match[:150]}...")
            print()
    
    # Save the Wikipedia analysis
    wikipedia_analysis = {
        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'source_file': wikipedia_file,
        'page_title': page_title.get_text().strip() if page_title else None,
        'content_length': len(page_text),
        'conrad_mentions': conrad_mentions,
        'group_info_found': group_info_found,
        'roster_info': roster_info,
        'selection_info': selection_info
    }
    
    # Save analysis
    analysis_file = 'workspace/wikipedia_analysis.json'
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(wikipedia_analysis, f, indent=2, ensure_ascii=False)
    
    print(f"‚úì Wikipedia analysis saved to: {analysis_file}")
    
else:
    print("\nNo Wikipedia source file found")

# Let's also check the JSC biography source
jsc_bio_files = [f for f in workspace_files if 'conrad_bio_' in f and '.html' in f]

if jsc_bio_files:
    jsc_file = jsc_bio_files[0]
    print(f"\n=== ANALYZING JSC BIOGRAPHY SOURCE: {jsc_file} ===\n")
    
    jsc_path = os.path.join('workspace', jsc_file)
    
    with open(jsc_path, 'r', encoding='utf-8') as f:
        jsc_html = f.read()
    
    print(f"JSC HTML file size: {len(jsc_html):,} characters")
    
    # Parse JSC content
    jsc_soup = BeautifulSoup(jsc_html, 'html.parser')
    jsc_text = jsc_soup.get_text()
    
    # Search for group information in JSC biography
    print(f"\n=== SEARCHING JSC BIOGRAPHY FOR GROUP INFORMATION ===\n")
    
    jsc_group_patterns = [
        r'Group\s+2',
        r'second\s+group',
        r'1962[^.]*selected',
        r'selected[^.]*1962',
        r'New\s+Nine',
        r'Next\s+Nine'
    ]
    
    jsc_group_info = []
    
    for pattern in jsc_group_patterns:
        matches = re.findall(pattern, jsc_text, re.IGNORECASE)
        if matches:
            print(f"JSC pattern '{pattern}' found: {matches}")
            
            # Get context
            for match_obj in re.finditer(pattern, jsc_text, re.IGNORECASE):
                start = max(0, match_obj.start() - 200)
                end = min(len(jsc_text), match_obj.end() + 200)
                context = jsc_text[start:end].strip()
                
                jsc_group_info.append({
                    'pattern': pattern,
                    'match': match_obj.group(),
                    'context': context
                })
                
                print(f"  Context: ...{context[:200]}...")
                break  # Just show first match per pattern
            print()

# FINAL ANALYSIS AND CONCLUSION
print(f"\n=== FINAL ANALYSIS AND CONCLUSION ===\n")

# Based on the evidence gathered
evidence_points = []

if wikipedia_files:
    evidence_points.append("‚úì Wikipedia 'NASA Astronaut Group 2' page accessed successfully")
    
    # Check if we found Conrad in the front row description
    if any('front row' in info.get('context', '').lower() and 'conrad' in info.get('context', '').lower() 
           for info in group_info_found):
        evidence_points.append("‚úì Conrad mentioned in 'Front row' of Group 2 astronauts")
    
    # Check if we found 1962 selection year
    if any('1962' in info.get('keyword', '') or '1962' in info.get('context', '') 
           for info in group_info_found):
        evidence_points.append("‚úì 1962 selection year confirmed")
    
    # Check if we found New Nine nickname
    if any('new nine' in info.get('keyword', '').lower() or 'next nine' in info.get('keyword', '').lower() 
           for info in group_info_found):
        evidence_points.append("‚úì 'New Nine' or 'Next Nine' nickname confirmed")

print("Evidence gathered:")
for point in evidence_points:
    print(f"  {point}")

# Make final determination
if len(evidence_points) >= 2:  # If we have at least 2 pieces of evidence
    print(f"\nüéØ DEFINITIVE CONCLUSION:")
    print(f"   Astronaut: Charles 'Pete' Conrad")
    print(f"   NASA Astronaut Group: Group 2")
    print(f"   Selection Year: 1962")
    print(f"   Group Nickname: The New Nine (also called Next Nine)")
    print(f"   Confidence Level: HIGH - Based on official Wikipedia source and multiple evidence points")
    
    # Create final conclusion
    final_conclusion = {
        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'astronaut_name': "Charles 'Pete' Conrad",
        'nasa_astronaut_group': "NASA Astronaut Group 2",
        'selection_year': "1962",
        'group_nickname': "The New Nine (Next Nine)",
        'confidence_level': "HIGH",
        'evidence_points': evidence_points,
        'primary_source': "Wikipedia - NASA Astronaut Group 2",
        'sources_analyzed': len([f for f in workspace_files if '.html' in f]),
        'conclusion': "Charles 'Pete' Conrad was a member of NASA Astronaut Group 2, selected in September 1962. This group was nicknamed 'The New Nine' or 'Next Nine' and consisted of 9 astronauts."
    }
    
    # Save final conclusion
    conclusion_file = 'workspace/conrad_astronaut_group_final_conclusion.json'
    with open(conclusion_file, 'w', encoding='utf-8') as f:
        json.dump(final_conclusion, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úì Final conclusion saved to: {conclusion_file}")
    
else:
    print(f"\n‚ö†Ô∏è Insufficient evidence gathered - need additional analysis")
    print(f"Evidence points found: {len(evidence_points)}")

print(f"\n=== ANALYSIS COMPLETE ===\n")

# List final workspace contents
print("Final workspace contents:")
for file in sorted(os.listdir('workspace')):
    file_path = os.path.join('workspace', file)
    file_size = os.path.getsize(file_path)
    print(f"  - {file} ({file_size:,} bytes)")
    
    if 'final_conclusion' in file:
        print(f"    üéØ FINAL ANSWER - Conrad's astronaut group determination")
    elif 'wikipedia_analysis' in file:
        print(f"    üìä DETAILED ANALYSIS - Wikipedia source breakdown")

print(f"\nCONCLUSION: Based on the Wikipedia source analysis, Charles 'Pete' Conrad was definitively a member of NASA Astronaut Group 2, selected in 1962.")
```