### Development Step 35: Download PDF of Leicester’s study “Can Hiccup Supply Enough Fish to Maintain Dragon’s Diet?”

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic librarian automating bulk retrieval of faculty publication PDFs from university repository domains to update the institutional knowledge base
- R&D team scraping technical whitepapers in PDF format from competitor websites to perform comparative feature analysis and market research
- Legal support staff automatically downloading recent court ruling PDFs from official judiciary portals for case preparation and precedent research
- Financial analysts collecting annual report PDFs from publicly listed company investor relations sites to feed into spreadsheet models and ratio analyses
- Environmental policy researchers retrieving government PDF reports on air quality and emissions from official .gov domains to input into climate impact assessments
- Healthcare data scientists extracting clinical trial result PDFs from major medical journal sites to build a structured dataset for systematic reviews
- Journalists gathering corporate governance and SEC filing PDFs from regulatory domains to fact-check executive statements during investigative reporting

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
bing_html = os.path.join(workspace, 'bing_search.html')
candidates_out = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure workspace exists ---
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# --- Step 1: Perform a fresh Bing search for the specific paper ---
query = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet? filetype:pdf site:le.ac.uk"
bing_url = 'https://www.bing.com/search'
params = {'q': query}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"[INFO] Executing Bing search with query:\n      {query}\n")
response = requests.get(bing_url, params=params, headers=headers, timeout=30)
print(f"[INFO] Bing response status: {response.status_code}")
if response.status_code != 200:
    print(f"ERROR: Bing search failed with status {response.status_code}. Exiting.")
    sys.exit(1)

# Save the raw HTML for inspection
with open(bing_html, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"[SAVE] Search results saved to: {bing_html}\n")

# --- Step 2: Parse the HTML and extract PDF URLs ---
print("[INFO] Parsing saved HTML to extract .pdf URLs...")
with open(bing_html, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()

# 2a) Direct <a> tags ending with .pdf or containing .pdf?
print("[STEP] Scanning <a> tags for literal .pdf links...")
for a in soup.find_all('a', href=True):
    href = a['href']
    low = href.lower()
    if low.endswith('.pdf') or '.pdf?' in low:
        pdf_urls.add(href)
print(f"    Found {len(pdf_urls)} direct .pdf href(s)")

# 2b) Parse Bing redirect links (/url?q=...)
print("[STEP] Parsing Bing redirect links for .pdf URLs...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q','u','url'):
            if key in qs:
                real = unquote(qs[key][0])
                lowr = real.lower()
                if lowr.endswith('.pdf') or '.pdf?' in lowr:
                    pdf_urls.add(real)
                break
print(f"    Total after redirect parsing: {len(pdf_urls)} candidate(s)")

# 2c) Regex fallback on raw HTML
print("[STEP] Running regex fallback for http(s)://...pdf patterns...")
# Corrected regex: double-quoted raw string, escaping internal double-quote
pattern = r"https?://[^\s\"']+?\.pdf(?:\?[^\"']*)?"
matches = re.findall(pattern, html, flags=re.IGNORECASE)
for m in matches:
    pdf_urls.add(m)
print(f"    Total after regex fallback: {len(pdf_urls)} candidate(s)")

# 2d) Filter for Leicester-related domains
print("[STEP] Filtering for Leicester domains (le.ac.uk, lra.le.ac.uk, core.ac.uk, hdl.handle.net)...")
domains = ('le.ac.uk','lra.le.ac.uk','core.ac.uk','hdl.handle.net')
le_urls = [u for u in pdf_urls if any(d in u.lower() for d in domains)]
if le_urls:
    print(f"    Found {len(le_urls)} Leicester-specific PDF URL(s):")
    for u in le_urls:
        print(f"      - {u}")
else:
    print("    No Leicester-specific PDFs; listing all candidates:")
    for u in sorted(pdf_urls):
        print(f"      - {u}")

# Save all found URLs for manual review
with open(candidates_out, 'w', encoding='utf-8') as f:
    for u in sorted(pdf_urls):
        f.write(u + '\n')
print(f"\n[SAVE] All PDF URL candidates written to: {candidates_out}")
print("[COMPLETE] Extraction phase finished. Please inspect `found_pdf_urls.txt` to choose the correct PDF link.")

```