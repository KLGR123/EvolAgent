### Development Step 31: Download University of Leicester ‘Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?’ PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated academic literature gathering and context snippet extraction for systematic reviews in ecology
- Competitive market intelligence collection by scanning industry whitepaper PDFs for emerging technology trends
- Legal case law retrieval and key excerpt harvesting from court decision PDFs for paralegal research automation
- Healthcare guideline acquisition and targeted snippet extraction from clinical protocol PDFs for hospital policy updates
- Patent document discovery and contextual snippet generation from patent office PDF archives for R&D patent landscaping
- Financial compliance report scraping and critical section extraction from regulatory PDF filings for audit preparedness
- Engineering standards compilation and relevant passage extraction from ISO specification PDFs for product design validation
- Environmental impact assessment report retrieval and summary snippet extraction from government PDF publications for sustainability analysis

```
import os
import sys
import re

# --- Configuration ---
workspace = 'workspace'
html_file = os.path.join(workspace, 'bing_search.html')
snippets_file = os.path.join(workspace, 'pdf_snippets_v2.txt')

# --- Preconditions ---
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' not found. Please run the search script first.")
    sys.exit(1)
if not os.path.isfile(html_file):
    print(f"ERROR: Bing search HTML not found at {html_file}. Please generate it first.")
    sys.exit(1)

print(f"[INFO] Reading Bing HTML from: {html_file}\n")
with open(html_file, 'r', encoding='utf-8') as f:
    html = f.read()

# --- Find all occurrences of '.pdf' (case-insensitive) ---
pdf_indices = [m.start() for m in re.finditer(r"\.pdf", html, flags=re.IGNORECASE)]
count = len(pdf_indices)
print(f"[SCAN] Found {count} occurrences of '.pdf' in the HTML\n")

# --- Extract context snippets around each occurrence ---
context_radius = 200  # characters before and after
max_snippets = 20      # limit number of snippets to save
snippets = []
for idx in pdf_indices[:max_snippets]:
    start = max(idx - context_radius, 0)
    end = min(idx + context_radius, len(html))
    snippet = html[start:end]
    # normalize whitespace for readability
    snippet = snippet.replace('\n', ' ').replace('\r', '')
    snippets.append(snippet)

# --- Save snippets to file ---
with open(snippets_file, 'w', encoding='utf-8') as out:
    for snip in snippets:
        out.write(snip + '\n' + '-'*60 + '\n')

print(f"[SAVE] Wrote {len(snippets)} context snippets to: {snippets_file}\n")

# --- Print first few snippets for quick inspection ---
print("[PREVIEW] First 5 PDF-context snippets:\n")
for i, snip in enumerate(snippets[:5], 1):
    print(f"--- Snippet {i} ---\n{snip}\n")
```