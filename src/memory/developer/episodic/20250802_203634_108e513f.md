### Development Step 36: Retrieve 2011 Bulgarian census tertiary education completion counts by gender from NSI

**Description**: Search for the official 2011 Bulgarian census data from the Bulgarian National Statistical Institute (NSI) or other authoritative government sources that specifically contain the tertiary education completion numbers by gender. Focus on finding the exact counts or percentages of men and women who completed tertiary education according to the 2011 census results.

**Use Cases**:
- National education policy planning: Ministry of Education analysts automate retrieval of 2011 Bulgarian census tables on tertiary education by gender to inform funding allocations and future program design.
- Academic demographic research: University scholars scrape and parse official census data to study historical male–female gaps in higher education across Bulgarian regions.
- Market entry demographic profiling: Consulting firms extract regional tertiary education completion rates by gender from the 2011 census to assess skilled labor availability for new investments.
- NGO gender-equality reporting: Non-profits automatically gather authoritative male and female higher-education statistics from the NSI to support advocacy and policy recommendations.
- Longitudinal trend analysis: Think-tank data teams use the link discovery and scraping script to compile and compare gender-disaggregated tertiary education data across multiple census years.
- GIS-based educational mapping: Urban planners integrate scraped 2011 census figures into geographic information systems to visualize male and female tertiary attainment at municipal level.
- Accreditation and compliance audits: Educational quality assurance bodies auto-extract gender balance metrics in higher education from the official census to verify institutional compliance.
- Public-facing dashboard feeds: Government IT departments automate collection of 2011 NSI census data on tertiary education by gender to populate interactive citizen dashboards.

```
import os
import json
from bs4 import BeautifulSoup
import re
import requests
import time

workspace_dir = 'workspace'

print("=== ANALYZING DOWNLOADED NSI CONTENT FOR CENSUS DATA NAVIGATION ===")
print("\nObjective: Find links to actual 2011 census tables with tertiary education by gender\n")

# First, let's examine the structure of the downloaded NSI files
nsi_files = [
    'www_nsi_bg_en_content_6704_population-census-2011.html',
    'www_nsi_bg_en_content_6710_population-education.html', 
    'www_nsi_bg_en.html'
]

print("=== PHASE 1: EXAMINING NSI FILE STRUCTURES ===\n")

all_relevant_links = []

for filename in nsi_files:
    filepath = os.path.join(workspace_dir, filename)
    if os.path.exists(filepath):
        print(f"--- Analyzing: {filename} ---")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f"File size: {len(html_content):,} characters")
        
        # Parse with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Get page title
        title = soup.find('title')
        if title:
            print(f"Page title: {title.get_text().strip()}")
        
        # Look for navigation menu or content sections
        nav_elements = soup.find_all(['nav', 'menu', 'ul', 'div'], class_=re.compile(r'(menu|nav|content|main)', re.I))
        print(f"Navigation elements found: {len(nav_elements)}")
        
        # Look for links that might lead to census data
        all_links = soup.find_all('a', href=True)
        print(f"Total links found: {len(all_links)}")
        
        # Filter for census/education related links - PROPERLY DEFINE VARIABLES
        relevant_links = []
        for link in all_links:
            # Define variables BEFORE using them in any expression
            href_value = link.get('href', '')
            text_value = link.get_text().strip()
            text_lower = text_value.lower()
            href_lower = href_value.lower()
            
            # Check for census, education, or statistical data related terms
            relevant_terms = ['census', '2011', 'education', 'population', 'statistics', 
                            'demographic', 'data', 'table', 'result', 'publication']
            
            # Check if any relevant terms are in the link text or href
            is_relevant = False
            for term in relevant_terms:
                if term in href_lower or term in text_lower:
                    is_relevant = True
                    break
            
            if is_relevant:
                # Construct full URL
                if href_value.startswith('http'):
                    full_url = href_value
                elif href_value.startswith('/'):
                    full_url = f"https://www.nsi.bg{href_value}"
                else:
                    full_url = f"https://www.nsi.bg/en/{href_value}"
                
                relevant_links.append({
                    'text': text_value,
                    'href': href_value,
                    'full_url': full_url,
                    'source_file': filename
                })
        
        print(f"Relevant links found: {len(relevant_links)}")
        
        if relevant_links:
            print("\nTop relevant links:")
            for i, link in enumerate(relevant_links[:10], 1):
                print(f"  {i}. Text: '{link['text']}'")
                print(f"     URL: {link['full_url']}")
        
        # Add to master list
        all_relevant_links.extend(relevant_links)
        
        # Look for specific content about census or education
        text_content = soup.get_text()
        
        # Search for specific phrases that might indicate census data availability
        census_phrases = [
            'population census 2011',
            'census results',
            'educational attainment',
            'tertiary education',
            'higher education',
            'education level',
            'by gender',
            'male female',
            'statistical data',
            'census tables'
        ]
        
        found_phrases = []
        for phrase in census_phrases:
            if phrase in text_content.lower():
                # Get context around the phrase
                pattern = re.compile(f'.{{0,100}}{re.escape(phrase)}.{{0,100}}', re.IGNORECASE | re.DOTALL)
                matches = pattern.findall(text_content)
                if matches:
                    found_phrases.append((phrase, matches[0].strip()))
        
        if found_phrases:
            print(f"\nRelevant content phrases found:")
            for phrase, context in found_phrases[:5]:
                print(f"  - '{phrase}': {context[:150]}...")
        
        print("\n" + "="*60 + "\n")
    else:
        print(f"File not found: {filename}\n")

print(f"=== CONSOLIDATED RELEVANT LINKS ANALYSIS ===\n")
print(f"Total relevant links found across all NSI files: {len(all_relevant_links)}")

# Remove duplicates and prioritize by relevance
unique_links = {}
for link in all_relevant_links:
    url = link['full_url']
    if url not in unique_links:
        unique_links[url] = link

print(f"Unique relevant links: {len(unique_links)}")

# Prioritize links that are most likely to contain census education data
priority_links = []
for url, link in unique_links.items():
    score = 0
    text_lower = link['text'].lower()
    url_lower = url.lower()
    
    # Scoring system for link relevance
    if '2011' in text_lower or '2011' in url_lower:
        score += 10
    if 'census' in text_lower or 'census' in url_lower:
        score += 8
    if 'education' in text_lower or 'education' in url_lower:
        score += 6
    if 'population' in text_lower or 'population' in url_lower:
        score += 4
    if 'result' in text_lower or 'result' in url_lower:
        score += 3
    if 'table' in text_lower or 'table' in url_lower:
        score += 5
    if 'data' in text_lower or 'data' in url_lower:
        score += 2
    
    if score > 0:
        priority_links.append((score, link))

# Sort by score
priority_links.sort(key=lambda x: x[0], reverse=True)

print(f"\nTop 15 priority links for census education data:")
for i, (score, link) in enumerate(priority_links[:15], 1):
    print(f"\n{i}. Score: {score}")
    print(f"   Text: '{link['text']}'")
    print(f"   URL: {link['full_url']}")
    print(f"   Source: {link['source_file']}")

print(f"\n=== PHASE 2: ATTEMPTING TO ACCESS TOP PRIORITY CENSUS LINKS ===\n")

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

successful_census_sources = []

# Try accessing the top 8 priority links
for i, (score, link) in enumerate(priority_links[:8], 1):
    url = link['full_url']
    print(f"{i}. Trying priority link (Score: {score}): {url}")
    print(f"   Link text: '{link['text']}'")
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        print(f"   Status: {response.status_code}")
        
        if response.status_code == 200:
            print(f"   ✓ Success! Content length: {len(response.content):,} bytes")
            
            # Save the content
            filename = f"priority_census_link_{i}.html"
            filepath = os.path.join(workspace_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"   HTML file saved: {filename}")
            
            # Quick analysis for HTML content
            soup = BeautifulSoup(response.content, 'html.parser')
            tables = soup.find_all('table')
            print(f"   Tables found: {len(tables)}")
            
            # Check for education/gender content
            content_text = soup.get_text().lower()
            education_indicators = {
                'tertiary': content_text.count('tertiary'),
                'education': content_text.count('education'),
                'gender': content_text.count('gender'),
                'male': content_text.count('male'),
                'female': content_text.count('female'),
                '2011': content_text.count('2011'),
                'higher': content_text.count('higher'),
                'university': content_text.count('university')
            }
            
            relevant_indicators = {k: v for k, v in education_indicators.items() if v > 0}
            if relevant_indicators:
                print(f"   Education indicators: {relevant_indicators}")
                
                # If this looks promising, mark it
                total_score = sum(relevant_indicators.values())
                if total_score > 15:
                    print(f"   *** HIGH POTENTIAL - Total indicator score: {total_score} ***")
                elif total_score > 5:
                    print(f"   ** MODERATE POTENTIAL - Total indicator score: {total_score} **")
            
            successful_census_sources.append({
                'url': url,
                'filename': filename,
                'original_text': link['text'],
                'priority_score': score,
                'content_size': len(response.content),
                'tables_found': len(tables),
                'education_indicators': relevant_indicators,
                'total_indicator_score': sum(relevant_indicators.values())
            })
            
        else:
            print(f"   ✗ Failed - Status: {response.status_code}")
            
    except Exception as e:
        print(f"   ✗ Error: {str(e)}")
    
    time.sleep(2)  # Be respectful to the server

print(f"\n=== PHASE 2 RESULTS ===\n")
print(f"Successfully accessed: {len(successful_census_sources)} priority census sources")

if successful_census_sources:
    print("\nSuccessful downloads (ranked by education content potential):")
    # Sort by total education indicator score
    successful_census_sources.sort(key=lambda x: x.get('total_indicator_score', 0), reverse=True)
    
    for i, source in enumerate(successful_census_sources, 1):
        print(f"\n{i}. {source['filename']}")
        print(f"   Original link text: '{source['original_text']}'")
        print(f"   Priority score: {source['priority_score']}")
        print(f"   Content size: {source['content_size']:,} bytes")
        print(f"   Tables found: {source['tables_found']}")
        print(f"   Education indicators: {source['education_indicators']}")
        print(f"   Total indicator score: {source['total_indicator_score']}")
        print(f"   URL: {source['url']}")
        
        if source['total_indicator_score'] > 15:
            print(f"   *** RECOMMENDED FOR DETAILED ANALYSIS ***")
else:
    print("No priority census sources were successfully accessed.")

# Save comprehensive analysis results
analysis_results = {
    'objective': '2011 Bulgarian Census - Tertiary Education by Gender',
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'nsi_files_analyzed': len([f for f in nsi_files if os.path.exists(os.path.join(workspace_dir, f))]),
    'total_relevant_links_found': len(all_relevant_links),
    'unique_relevant_links': len(unique_links),
    'priority_links_identified': len(priority_links),
    'successful_census_sources': len(successful_census_sources),
    'census_sources_details': successful_census_sources,
    'top_priority_links': [{'score': score, 'text': link['text'], 'url': link['full_url']} for score, link in priority_links[:15]],
    'recommendation': 'Analyze downloaded priority census files for tertiary education data tables',
    'next_steps': [
        'Parse successfully downloaded census HTML files for education tables',
        'Look for gender-disaggregated tertiary education statistics', 
        'Extract specific numbers/percentages for male and female tertiary education completion',
        'Verify data is from 2011 Bulgarian census'
    ]
}

results_file = os.path.join(workspace_dir, 'nsi_priority_census_analysis_fixed.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)

print(f"\n=== ANALYSIS COMPLETE ===\n")
print(f"Comprehensive analysis saved to: {results_file}")
print(f"\nNext step: Parse any successfully downloaded census files for tertiary education data by gender")
print(f"Focus on files with high education indicator scores and multiple tables")

# Show final summary
if successful_census_sources:
    high_potential = [s for s in successful_census_sources if s['total_indicator_score'] > 15]
    moderate_potential = [s for s in successful_census_sources if 5 < s['total_indicator_score'] <= 15]
    
    print(f"\n=== FINAL SUMMARY ===\n")
    print(f"High potential sources (score > 15): {len(high_potential)}")
    print(f"Moderate potential sources (score 6-15): {len(moderate_potential)}")
    print(f"Low potential sources (score 1-5): {len(successful_census_sources) - len(high_potential) - len(moderate_potential)}")
    
    if high_potential:
        print(f"\n*** PRIORITY FILES FOR NEXT ANALYSIS ***")
        for source in high_potential:
            print(f"  - {source['filename']} (score: {source['total_indicator_score']})")
else:
    print(f"\nNo census sources were successfully downloaded. May need alternative search strategy.")
```