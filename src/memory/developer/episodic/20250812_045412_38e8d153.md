### Development Step 4: Identify 1970’s 29-Base Stealer Nicknamed for His Gait and His 1971 Tommy McCraw Trade

**Description**: Search for information about a baseball player who had an unusual walk style nickname, stole 29 bases in the 1970 season, and was involved in a trade between the Chicago White Sox and another team in 1971 where Tommy McCraw was exchanged. Focus on identifying this player's distinctive nickname related to his walking style, his 1970 season statistics showing exactly 29 stolen bases, and the specific 1971 trade details involving McCraw going to the team that had previously traded this player to the White Sox.

**Use Cases**:
- Sports journalism content enrichment by automatically extracting historical trade details, 1970 stolen-base stats, and unique walking-style nicknames for feature articles on early 1970s baseball players
- Baseball analytics platforms integrating automated retrieval of stolen-base leaders and quirky player monikers to enhance sabermetric models and player comparisons
- Interactive museum exhibit systems sourcing player nicknames, career stats, and trade timelines for touchscreen displays highlighting memorable baseball stories
- Fantasy baseball trivia engines generating quiz questions based on real 1970 season stolen-base records, trade events, and eccentric player walk-style nicknames
- Academic sports sociology research compiling a structured dataset of nickname usage, stolen-base performance, and trade movements to study cultural trends in baseball history
- E-commerce sports memorabilia catalogs auto-populating item descriptions with accurate 1970s player statistics, trade dates, and nickname origins for collector marketplaces
- Premium sports data subscription services automating extraction of niche historical statistics and 1971 trade anecdotes for in-depth baseball intelligence reports
- Podcast production workflows curating engaging storylines on underappreciated players’ unique walk styles, their 29-steal season, and pivotal trades involving Tommy McCraw

```
import os
from bs4 import BeautifulSoup
import re
import requests
import json

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== COMPLETELY RESTRUCTURING CODE TO AVOID GENERATOR EXPRESSION ERRORS ===")
print("Using simple loops instead of generator expressions to fix variable scope issues")
print()

# First, let's check what files we have in workspace
print("Files currently in workspace:")
if os.path.exists('workspace'):
    for file in os.listdir('workspace'):
        print(f"  - {file}")
else:
    print("  - workspace directory not found")

# Parse the saved Tommy McCraw HTML file with simple loop structure
tommy_file = 'workspace/tommy_mccraw_baseball_reference.html'

if os.path.exists(tommy_file):
    print(f"\nStep 1: Analyzing {tommy_file}...")
    
    with open(tommy_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    print("\n=== Searching for 1971 trade information - USING SIMPLE LOOPS ===")
    
    # Get all text and split into lines
    page_text = soup.get_text()
    text_lines = page_text.split('\n')
    
    print(f"Total lines of text: {len(text_lines)}")
    
    # Search for 1971 trade information using simple loop structure
    trade_lines = []
    trade_terms = ['trade', 'traded', 'acquired', 'sent', 'white sox', 'chicago']
    
    for line_text in text_lines:
        line_text = line_text.strip()
        if line_text and '1971' in line_text:
            # Check each trade term individually
            contains_trade_term = False
            for term in trade_terms:
                if term in line_text.lower():
                    contains_trade_term = True
                    break
            
            if contains_trade_term:
                trade_lines.append(line_text)
                print(f"Found 1971 trade line: {line_text}")
    
    print(f"\nTotal 1971 trade-related lines found: {len(trade_lines)}")
    
    # Look for career statistics tables
    print("\n=== Analyzing tables for career statistics ===")
    
    tables = soup.find_all('table')
    print(f"Found {len(tables)} tables on page")
    
    career_data = []
    
    for table_index, table in enumerate(tables):
        # Get table headers
        headers = table.find_all('th')
        header_texts = [th.get_text().strip() for th in headers]
        
        # Check if this table has year and team information
        has_year = False
        has_team = False
        
        for header in header_texts:
            if 'year' in header.lower():
                has_year = True
            if 'tm' in header.lower() or 'team' in header.lower():
                has_team = True
        
        if has_year and has_team:
            print(f"\n*** Table {table_index + 1} has year/team data ***")
            print(f"Headers: {header_texts[:10]}")  # Show first 10 headers
            
            # Extract rows containing years around 1971
            rows = table.find_all('tr')
            for row_index, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                cell_data = [cell.get_text().strip() for cell in cells]
                
                if len(cell_data) > 1:
                    # Check if row contains relevant years
                    row_contains_relevant_year = False
                    relevant_years = ['1970', '1971', '1972']
                    
                    for cell_text in cell_data:
                        for year in relevant_years:
                            if year in cell_text:
                                row_contains_relevant_year = True
                                break
                        if row_contains_relevant_year:
                            break
                    
                    if row_contains_relevant_year:
                        print(f"  Row {row_index + 1}: {cell_data[:8]}")  # Show first 8 cells
                        career_data.append({
                            'table_index': table_index + 1,
                            'row_index': row_index + 1,
                            'data': cell_data[:10]  # Limit to first 10 cells
                        })
    
    # Save McCraw analysis results
    mccraw_results = {
        'file_analyzed': tommy_file,
        'trade_lines_1971': trade_lines,
        'career_data_relevant_years': career_data,
        'total_tables': len(tables),
        'analysis_status': 'completed'
    }
    
    with open('workspace/mccraw_analysis.json', 'w') as f:
        json.dump(mccraw_results, f, indent=2)
    
    print(f"\nMcCraw analysis saved to workspace/mccraw_analysis.json")
    print(f"Found {len(trade_lines)} trade-related lines and {len(career_data)} relevant career data rows")
    
else:
    print(f"Tommy McCraw HTML file not found: {tommy_file}")

print("\n" + "="*60)
print("Step 2: Searching for 1970 stolen base data with alternative URLs...")

# Try alternative URLs for 1970 stolen base statistics
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

alternative_urls = [
    "https://www.baseball-reference.com/years/1970-batting.shtml",
    "https://www.baseball-reference.com/leagues/AL/1970-batting.shtml",
    "https://www.baseball-reference.com/leagues/NL/1970-batting.shtml"
]

successful_files = []

for url_num, url in enumerate(alternative_urls, 1):
    try:
        print(f"\nAttempting URL {url_num}: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        print(f"Response status: {response.status_code}")
        
        if response.status_code == 200:
            print(f"SUCCESS! Downloaded data from {url}")
            
            # Save the HTML file
            filename = f"1970_batting_data_{url_num}.html"
            filepath = f"workspace/{filename}"
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            successful_files.append(filepath)
            print(f"Saved to {filepath}")
            
            # Parse for stolen base data
            soup = BeautifulSoup(response.content, 'html.parser')
            tables = soup.find_all('table')
            print(f"Found {len(tables)} tables in this page")
            
            # Look for tables with stolen base data and "29"
            players_with_29 = []
            
            for table_idx, table in enumerate(tables):
                table_text = table.get_text()
                
                # Check if table has stolen base data
                has_sb_data = False
                sb_indicators = ['sb', 'stolen', 'steal']
                for indicator in sb_indicators:
                    if indicator in table_text.lower():
                        has_sb_data = True
                        break
                
                if has_sb_data and '29' in table_text:
                    print(f"\n*** Table {table_idx + 1} has SB data and contains '29' ***")
                    
                    # Extract rows with "29"
                    rows = table.find_all('tr')
                    for row_idx, row in enumerate(rows):
                        cells = row.find_all(['td', 'th'])
                        cell_data = [cell.get_text().strip() for cell in cells]
                        
                        if '29' in cell_data:
                            print(f"  Row {row_idx + 1} contains '29': {cell_data[:8]}")
                            
                            # Find player name in this row
                            player_name = None
                            for cell_idx, cell_text in enumerate(cell_data):
                                if cell_text == '29':
                                    # Look for player name (usually first non-numeric cell)
                                    for search_idx in range(len(cell_data)):
                                        candidate = cell_data[search_idx]
                                        if candidate and not candidate.isdigit() and len(candidate) > 2:
                                            player_name = candidate
                                            break
                                    break
                            
                            if player_name:
                                print(f"    *** POTENTIAL PLAYER: {player_name} ***")
                                players_with_29.append({
                                    'player_name': player_name,
                                    'row_data': cell_data,
                                    'table_index': table_idx + 1,
                                    'source_url': url,
                                    'source_file': filename
                                })
            
            # Save findings for this URL
            if players_with_29:
                results_file = f"workspace/players_29_steals_source_{url_num}.json"
                with open(results_file, 'w') as f:
                    json.dump(players_with_29, f, indent=2)
                print(f"\nSaved {len(players_with_29)} potential players to {results_file}")
        
        else:
            print(f"Failed to access URL: HTTP {response.status_code}")
    
    except Exception as e:
        print(f"Error accessing {url}: {str(e)}")

print("\n" + "="*60)
print("=== SEARCH PROGRESS SUMMARY ===")
print("✓ Fixed all variable scope errors by using simple loops")
print("✓ Successfully analyzed Tommy McCraw HTML file")
print(f"✓ Attempted {len(alternative_urls)} URLs for 1970 stolen base data")
print(f"✓ Successfully downloaded {len(successful_files)} files")

print("\nFiles now in workspace:")
if os.path.exists('workspace'):
    for file in os.listdir('workspace'):
        print(f"  - {file}")

print("\nNext step: Cross-reference the saved data to identify the mystery player")
```