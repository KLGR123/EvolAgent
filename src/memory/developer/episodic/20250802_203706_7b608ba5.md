### Development Step 38: Official 2011 Bulgarian Census: Tertiary Education Completion Statistics by Gender

**Description**: Search for the official 2011 Bulgarian census data from the Bulgarian National Statistical Institute (NSI) or other authoritative government sources that specifically contain the tertiary education completion numbers by gender. Focus on finding the exact counts or percentages of men and women who completed tertiary education according to the 2011 census results.

**Use Cases**:
- Government education ministry policy planning to assess regional gender disparities in tertiary graduation rates and allocate funding for targeted scholarship programs
- Academic research study on the impact of socioeconomic factors on male and female university completion across Bulgarian municipalities
- NGO evaluation of womenâ€™s empowerment initiatives by comparing pre- and post-2011 census tertiary education attainment among female beneficiaries
- University benchmarking dashboard automating extraction of gender-segmented degree completion data to inform diversity and inclusion strategies
- HR consultancy workforce analysis integrating census education statistics by gender to advise clients on talent recruitment and training investments
- Data journalism project scraping and visualizing male vs. female higher education trends from official census tables for an interactive news feature
- Real estate market intelligence tool incorporating local tertiary education completion rates by gender into demographic profiles for housing development planning

```
import os
import json
from bs4 import BeautifulSoup
import re
import time

workspace_dir = 'workspace'

print("=== PARSING HIGH-POTENTIAL NSI CENSUS FILES FOR TERTIARY EDUCATION DATA BY GENDER ===")
print("\nObjective: Extract 2011 Bulgarian census tertiary education completion statistics by gender\n")

# First, let's inspect what files we have in the workspace
print("--- Inspecting Workspace Files ---")
if os.path.exists(workspace_dir):
    workspace_files = os.listdir(workspace_dir)
    print(f"Files in workspace: {len(workspace_files)}")
    
    # Show relevant files
    priority_files = [f for f in workspace_files if f.startswith('priority_census_link') and f.endswith('.html')]
    analysis_files = [f for f in workspace_files if 'analysis' in f and f.endswith('.json')]
    
    print(f"\nPriority census HTML files: {len(priority_files)}")
    for f in priority_files:
        print(f"  - {f}")
    
    print(f"\nAnalysis JSON files: {len(analysis_files)}")
    for f in analysis_files:
        print(f"  - {f}")
else:
    print("Workspace directory not found!")
    exit(1)

# Load previous analysis if available
analysis_file = os.path.join(workspace_dir, 'nsi_priority_census_analysis_fixed.json')
if os.path.exists(analysis_file):
    print("\n--- Loading Previous Analysis Results ---")
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    print(f"Analysis timestamp: {analysis_data.get('analysis_timestamp', 'Unknown')}")
    print(f"Successful census sources: {analysis_data.get('successful_census_sources', 0)}")
    
    # Get the high-potential sources
    census_sources = analysis_data.get('census_sources_details', [])
    high_potential_sources = [s for s in census_sources if s.get('total_indicator_score', 0) > 15]
    
    print(f"\nHigh-potential sources identified: {len(high_potential_sources)}")
    for i, source in enumerate(high_potential_sources, 1):
        print(f"  {i}. {source['filename']} - Score: {source['total_indicator_score']}")
        print(f"     Original text: '{source['original_text']}'")
        print(f"     Tables found: {source['tables_found']}")
else:
    print("Previous analysis file not found. Will examine all available priority census files.")
    high_potential_sources = []

# Define the priority files to analyze
priority_files_to_analyze = [
    'priority_census_link_5.html',  # Education and Lifelong Learning (score: 91)
    'priority_census_link_2.html',  # Census (score: 80)
    'priority_census_link_7.html',  # Population (Demography, Migration and Projections) (score: 79)
    'priority_census_link_6.html',  # Supply Use Tables (score: 79)
    'priority_census_link_8.html'   # Population Grid (score: 79)
]

print(f"\n=== PHASE 1: EXAMINING PRIORITY FILE STRUCTURES ===\n")

file_analysis_results = []

for filename in priority_files_to_analyze:
    filepath = os.path.join(workspace_dir, filename)
    if os.path.exists(filepath):
        print(f"--- Analyzing: {filename} ---")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f"File size: {len(html_content):,} characters")
        
        # Parse with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Get page title
        title = soup.find('title')
        if title:
            print(f"Page title: {title.get_text().strip()}")
        
        # Look for main content areas
        main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('body')
        if main_content:
            content_text = main_content.get_text()
        else:
            content_text = soup.get_text()
        
        # Find all tables
        tables = soup.find_all('table')
        print(f"Tables found: {len(tables)}")
        
        # Look for links to data files or detailed census results - FIX THE VARIABLE SCOPING
        data_links = []
        for link in soup.find_all('a', href=True):
            # Define variables BEFORE using them
            link_href = link.get('href', '')
            link_text = link.get_text().strip()
            
            # Look for links to Excel files, PDFs, or census data
            has_data_file = any(ext in link_href.lower() for ext in ['.xls', '.xlsx', '.pdf', '.csv'])
            has_relevant_text = any(term in link_text.lower() for term in ['download', 'data', 'table', 'census', '2011', 'education'])
            
            if has_data_file or has_relevant_text:
                # Construct full URL
                if link_href.startswith('http'):
                    full_url = link_href
                elif link_href.startswith('/'):
                    full_url = f"https://www.nsi.bg{link_href}"
                else:
                    full_url = f"https://www.nsi.bg/en/{link_href}"
                
                data_links.append({
                    'text': link_text,
                    'href': link_href,
                    'full_url': full_url,
                    'has_data_file': has_data_file,
                    'has_relevant_text': has_relevant_text
                })
        
        print(f"Data file links found: {len(data_links)}")
        if data_links:
            print("Top data links:")
            for i, link in enumerate(data_links[:8], 1):
                print(f"  {i}. '{link['text']}'")
                print(f"     URL: {link['full_url']}")
                print(f"     Data file: {link['has_data_file']}, Relevant text: {link['has_relevant_text']}")
        
        # Search for specific education-related content
        education_content = []
        
        # Look for text patterns that might indicate tertiary education statistics
        education_patterns = [
            r'tertiary education.*?(?:male|female|men|women|gender)',
            r'higher education.*?(?:male|female|men|women|gender)',
            r'university.*?(?:male|female|men|women|gender)',
            r'(?:male|female|men|women).*?tertiary',
            r'(?:male|female|men|women).*?higher education',
            r'education.*?(?:by gender|gender breakdown)',
            r'2011.*?census.*?education',
            r'educational attainment.*?(?:male|female)',
            r'completed.*?tertiary.*?education',
            r'bachelor.*?degree.*?(?:male|female)',
            r'university.*?graduate.*?(?:male|female)'
        ]
        
        for pattern in education_patterns:
            matches = re.findall(pattern, content_text, re.IGNORECASE | re.DOTALL)
            if matches:
                for match in matches[:3]:  # Limit to first 3 matches per pattern
                    # Clean up the match
                    clean_match = re.sub(r'\s+', ' ', match.strip())[:200]
                    education_content.append(clean_match)
        
        if education_content:
            print(f"\nEducation-related content found: {len(education_content)}")
            for i, content in enumerate(education_content[:5], 1):
                print(f"  {i}. {content}...")
        
        # Look for tables that might contain education data
        relevant_tables = []
        for i, table in enumerate(tables):
            table_text = table.get_text().lower()
            
            # Check if table contains education and gender related terms
            has_education = any(term in table_text for term in ['education', 'tertiary', 'higher', 'university', 'degree'])
            has_gender = any(term in table_text for term in ['male', 'female', 'men', 'women', 'gender'])
            has_2011 = '2011' in table_text
            has_numbers = bool(re.search(r'\d+[,.]?\d*\s*%?', table_text))  # Look for numbers/percentages
            
            if has_education and (has_gender or has_2011):
                # Extract table headers for better analysis
                headers = []
                for th in table.find_all(['th', 'td'])[:10]:  # First 10 cells as potential headers
                    header_text = th.get_text().strip()
                    if header_text:
                        headers.append(header_text)
                
                relevant_tables.append({
                    'table_index': i,
                    'has_education': has_education,
                    'has_gender': has_gender,
                    'has_2011': has_2011,
                    'has_numbers': has_numbers,
                    'headers': headers[:5],  # First 5 headers
                    'table_text_sample': table_text[:300]
                })
        
        if relevant_tables:
            print(f"\nRelevant tables found: {len(relevant_tables)}")
            for table_info in relevant_tables:
                print(f"  Table {table_info['table_index']}: Education={table_info['has_education']}, Gender={table_info['has_gender']}, 2011={table_info['has_2011']}, Numbers={table_info['has_numbers']}")
                print(f"    Headers: {table_info['headers']}")
                print(f"    Sample: {table_info['table_text_sample'][:150]}...")
        
        # Look for specific census data indicators
        census_indicators = {
            'tertiary_mentions': content_text.lower().count('tertiary'),
            'higher_education_mentions': content_text.lower().count('higher education'),
            'university_mentions': content_text.lower().count('university'),
            'male_mentions': content_text.lower().count('male'),
            'female_mentions': content_text.lower().count('female'),
            'gender_mentions': content_text.lower().count('gender'),
            'census_2011_mentions': len(re.findall(r'2011.*?census|census.*?2011', content_text, re.IGNORECASE)),
            'percentage_numbers': len(re.findall(r'\d+[,.]?\d*\s*%', content_text))
        }
        
        print(f"\nCensus data indicators:")
        for indicator, count in census_indicators.items():
            if count > 0:
                print(f"  {indicator}: {count}")
        
        # Store analysis results
        potential_score = (
            len(relevant_tables) * 15 +  # Relevant tables are most important
            len(education_content) * 8 +  # Education content is very important
            len(data_links) * 5 +         # Data links are important
            census_indicators['census_2011_mentions'] * 10 +  # 2011 census mentions
            census_indicators['percentage_numbers'] * 3       # Statistical data
        )
        
        file_analysis_results.append({
            'filename': filename,
            'file_size': len(html_content),
            'tables_count': len(tables),
            'data_links_count': len(data_links),
            'data_links': data_links,
            'education_content_count': len(education_content),
            'education_content': education_content,
            'relevant_tables_count': len(relevant_tables),
            'relevant_tables': relevant_tables,
            'census_indicators': census_indicators,
            'potential_score': potential_score
        })
        
        print(f"\nPotential score for this file: {potential_score}")
        if potential_score > 50:
            print("*** VERY HIGH POTENTIAL FOR CENSUS DATA ***")
        elif potential_score > 25:
            print("** HIGH POTENTIAL FOR CENSUS DATA **")
        elif potential_score > 10:
            print("* MODERATE POTENTIAL FOR CENSUS DATA *")
        
        print("\n" + "="*60 + "\n")
    else:
        print(f"File not found: {filename}\n")

# Sort files by potential score
file_analysis_results.sort(key=lambda x: x['potential_score'], reverse=True)

print(f"=== PHASE 1 RESULTS SUMMARY ===\n")
print(f"Files analyzed: {len(file_analysis_results)}")

if file_analysis_results:
    print("\nFiles ranked by potential to contain tertiary education data by gender:")
    for i, result in enumerate(file_analysis_results, 1):
        print(f"\n{i}. {result['filename']} (Score: {result['potential_score']})")
        print(f"   File size: {result['file_size']:,} characters")
        print(f"   Tables: {result['tables_count']}, Relevant tables: {result['relevant_tables_count']}")
        print(f"   Data links: {result['data_links_count']}, Education content: {result['education_content_count']}")
        
        # Show key indicators
        indicators = result['census_indicators']
        key_indicators = {k: v for k, v in indicators.items() if v > 0}
        if key_indicators:
            print(f"   Key indicators: {key_indicators}")
        
        if result['potential_score'] > 50:
            print(f"   *** VERY HIGH PRIORITY FOR DETAILED ANALYSIS ***")
        elif result['potential_score'] > 25:
            print(f"   ** HIGH PRIORITY FOR DETAILED ANALYSIS **")
        elif result['potential_score'] > 10:
            print(f"   * MODERATE PRIORITY *")

# Save detailed analysis results
detailed_analysis = {
    'objective': '2011 Bulgarian Census - Tertiary Education by Gender - File Analysis',
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'files_analyzed': len(file_analysis_results),
    'file_analysis_results': file_analysis_results,
    'summary': {
        'very_high_potential': len([r for r in file_analysis_results if r['potential_score'] > 50]),
        'high_potential': len([r for r in file_analysis_results if 25 < r['potential_score'] <= 50]),
        'moderate_potential': len([r for r in file_analysis_results if 10 < r['potential_score'] <= 25]),
        'low_potential': len([r for r in file_analysis_results if r['potential_score'] <= 10])
    },
    'next_steps': [
        'Extract data from highest-scoring files',
        'Parse relevant tables for tertiary education statistics',
        'Download and analyze data file links',
        'Look for specific male/female tertiary education completion numbers'
    ]
}

analysis_output_file = os.path.join(workspace_dir, 'nsi_census_files_detailed_analysis.json')
with open(analysis_output_file, 'w', encoding='utf-8') as f:
    json.dump(detailed_analysis, f, indent=2, ensure_ascii=False)

print(f"\n=== DETAILED ANALYSIS COMPLETE ===\n")
print(f"Detailed analysis saved to: {analysis_output_file}")

if file_analysis_results:
    top_file = file_analysis_results[0]
    print(f"\nTop priority file: {top_file['filename']} (Score: {top_file['potential_score']})")
    print(f"This file has:")
    print(f"  - {top_file['relevant_tables_count']} relevant tables")
    print(f"  - {top_file['data_links_count']} data links")
    print(f"  - {top_file['education_content_count']} education content matches")
    
    # Show the most promising data links from the top file
    if top_file['data_links']:
        print(f"\nMost promising data links from {top_file['filename']}:")
        for i, link in enumerate(top_file['data_links'][:5], 1):
            print(f"  {i}. '{link['text']}'")
            print(f"     {link['full_url']}")
    
    print(f"\nReady for detailed table parsing and data extraction from the top-scoring files.")
else:
    print("\nNo files were successfully analyzed. Check file availability.")
```