### Development Step 2: Pre-1999 Fiona Apple and Paula Cole Discography: Studio Albums, EPs, and Release Dates

**Description**: Research and compile a comprehensive list of all albums released by Fiona Apple and Paula Cole before 1999, including their exact release dates and album titles. Focus on their studio albums, EPs, and any other significant releases during this timeframe. Use multiple music databases, discographies, and reliable sources to ensure completeness.

**Use Cases**:
- Music streaming metadata synchronization and quality control for platform curators ensuring complete and accurate pre-1999 discographies of Fiona Apple and Paula Cole
- Academic musicology research on the evolution of 1990s female singer-songwriters by compiling exact release dates and album titles for scholarly analysis
- Automated content generation for artist fan sites, dynamically updating discography pages with verified pre-1999 studio albums, EPs, and significant releases
- Licensing and royalty audit workflows at a record label, cross-referencing scraped data to validate payments for all Fiona Apple and Paula Cole releases before 1999
- Machine learning dataset preparation for a music recommendation engine, using timestamped album data from the 1990s to improve chronological suggestion accuracy
- Digital archiving and preservation for a public library’s music collection management system, integrating complete pre-1999 discographies into its catalog
- Marketing campaign planning for a nostalgia-themed reissue series, using exact anniversary dates of Fiona Apple and Paula Cole albums to schedule promotions

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
import re

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== FIONA APPLE & PAULA COLE DISCOGRAPHY RESEARCH (PRE-1999) - FIXED ===\n")
print("Target: All albums released before 1999 by both artists")
print("Focus: Studio albums, EPs, and significant releases with exact dates\n")

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Target URLs for both artists
target_urls = {
    'fiona_apple': [
        "https://en.wikipedia.org/wiki/Fiona_Apple",
        "https://en.wikipedia.org/wiki/Fiona_Apple_discography"
    ],
    'paula_cole': [
        "https://en.wikipedia.org/wiki/Paula_Cole",
        "https://en.wikipedia.org/wiki/Paula_Cole_discography"
    ]
}

successful_sources = []
failed_sources = []

print("=== PHASE 1: ACCESSING ARTIST WIKIPEDIA PAGES ===\n")

# Process each artist
for artist_name, urls in target_urls.items():
    print(f"--- Researching {artist_name.replace('_', ' ').title()} ---")
    
    for url in urls:
        print(f"\nTrying: {url}")
        try:
            response = requests.get(url, headers=headers, timeout=20)
            print(f"Response status: {response.status_code}")
            
            if response.status_code == 200:
                print(f"✓ Successfully accessed {url}")
                
                # Save the content for analysis
                filename = url.replace('https://', '').replace('http://', '').replace('/', '_').replace('.', '_') + '.html'
                filepath = f'workspace/{filename}'
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                # Quick content analysis - FIX: Move content_text definition before if/else block
                soup = BeautifulSoup(response.content, 'html.parser')
                title = soup.find('title')
                title_text = title.get_text().strip() if title else 'No title found'
                
                # FIX: Define content_text here, before using it
                content_text = soup.get_text().lower()
                
                # Look for discography/album related content
                discography_indicators = [
                    'discography', 'album', 'studio album', 'ep', 'single',
                    '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998',
                    'debut', 'release', 'recorded'
                ]
                has_discography_info = any(indicator in content_text for indicator in discography_indicators)
                
                # Look for specific album titles known from the era
                known_albums = {
                    'fiona_apple': ['tidal', 'criminal', 'shadowboxer'],
                    'paula_cole': ['harbinger', 'this fire', 'where have all the cowboys gone', 'i don\'t want to wait']
                }
                
                album_mentions = []
                if artist_name in known_albums:
                    for album_hint in known_albums[artist_name]:
                        if album_hint in content_text:
                            album_mentions.append(album_hint)
                
                successful_sources.append({
                    'artist': artist_name,
                    'url': url,
                    'title': title_text,
                    'filename': filepath,
                    'has_discography_info': has_discography_info,
                    'content_length': len(response.text),
                    'album_mentions': album_mentions
                })
                
                print(f"  Title: {title_text}")
                print(f"  Content length: {len(response.text)} characters")
                print(f"  Contains discography info: {has_discography_info}")
                print(f"  Album mentions found: {album_mentions}")
                
            else:
                failed_sources.append({'artist': artist_name, 'url': url, 'status': response.status_code})
                print(f"✗ Failed to access {url} - Status: {response.status_code}")
                
        except Exception as e:
            failed_sources.append({'artist': artist_name, 'url': url, 'error': str(e)})
            print(f"✗ Error accessing {url}: {str(e)}")
        
        time.sleep(2)  # Be respectful to servers

print(f"\n=== INITIAL ACCESS RESULTS ===")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources")

# Analyze successful sources for discography content
if successful_sources:
    print("\n=== ANALYZING SUCCESSFUL SOURCES ===\n")
    
    for i, source in enumerate(successful_sources, 1):
        print(f"{i}. {source['artist'].replace('_', ' ').title()}")
        print(f"   URL: {source['url']}")
        print(f"   Title: {source['title']}")
        print(f"   File saved: {source['filename']}")
        print(f"   Has discography info: {source['has_discography_info']}")
        print(f"   Album mentions: {source['album_mentions']}")
        
        if source['has_discography_info']:
            print(f"   *** PRIORITY SOURCE - Contains discography information ***")
        print()
    
    # Detailed analysis of priority sources
    priority_sources = [s for s in successful_sources if s['has_discography_info']]
    
    if priority_sources:
        print(f"=== DETAILED DISCOGRAPHY ANALYSIS ===")
        print(f"Found {len(priority_sources)} priority sources with discography information\n")
        
        artist_analysis = {}
        
        for source in priority_sources:
            artist = source['artist']
            print(f"--- Analyzing {artist.replace('_', ' ').title()} ---")
            
            with open(source['filename'], 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Look for discography section
            discography_sections = []
            headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            for heading in headings:
                heading_text = heading.get_text().lower()
                if any(word in heading_text for word in ['discography', 'albums', 'studio albums', 'career']):
                    discography_sections.append({
                        'heading': heading.get_text().strip(),
                        'level': heading.name,
                        'element': heading
                    })
            
            print(f"Found {len(discography_sections)} discography-related sections:")
            for section in discography_sections:
                print(f"  - {section['level'].upper()}: {section['heading']}")
            
            # Look for tables that might contain album information
            tables = soup.find_all('table')
            print(f"Found {len(tables)} tables in the page")
            
            # Analyze tables for album data
            album_tables = []
            for i, table in enumerate(tables):
                table_text = table.get_text().lower()
                
                # Check if table contains album/year information for pre-1999 period
                has_pre_1999_years = bool(re.search(r'199[0-8]|1990|1991|1992|1993|1994|1995|1996|1997|1998', table.get_text()))
                has_album_indicators = any(word in table_text for word in ['album', 'title', 'year', 'studio', 'release', 'ep'])
                
                if has_pre_1999_years and has_album_indicators:
                    album_tables.append({
                        'index': i,
                        'element': table,
                        'has_target_years': has_pre_1999_years
                    })
                    print(f"  Table {i+1}: Contains pre-1999 album/year data - *** POTENTIAL DISCOGRAPHY TABLE ***")
                else:
                    print(f"  Table {i+1}: General content")
            
            # Look for years in target range (pre-1999)
            target_year_pattern = r'199[0-8]|1990|1991|1992|1993|1994|1995|1996|1997|1998'
            years_found = re.findall(target_year_pattern, soup.get_text())
            unique_target_years = sorted(set(years_found))
            
            print(f"Years in target range (pre-1999) found: {unique_target_years}")
            
            # Look for album titles and release indicators
            album_keywords = ['debut album', 'studio album', 'ep', 'single', 'released', 'recorded']
            album_mentions = []
            
            for keyword in album_keywords:
                if keyword in soup.get_text().lower():
                    album_mentions.append(keyword)
            
            print(f"Album-related keywords found: {album_mentions}")
            
            # Store analysis for this artist
            artist_analysis[artist] = {
                'source_url': source['url'],
                'discography_sections_found': len(discography_sections),
                'album_tables_identified': len(album_tables),
                'target_years_found': unique_target_years,
                'album_keywords_present': album_mentions,
                'content_indicators': {
                    'has_discography_section': len(discography_sections) > 0,
                    'has_album_tables': len(album_tables) > 0,
                    'has_pre_1999_years': len(unique_target_years) > 0
                }
            }
            
            print()
        
        # Save comprehensive analysis
        comprehensive_analysis = {
            'research_target': 'Fiona Apple & Paula Cole albums pre-1999',
            'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'artists_analyzed': list(artist_analysis.keys()),
            'total_priority_sources': len(priority_sources),
            'artist_analysis': artist_analysis
        }
        
        with open('workspace/fiona_apple_paula_cole_preliminary_analysis.json', 'w') as f:
            json.dump(comprehensive_analysis, f, indent=2)
        
        print(f"Comprehensive analysis saved to: workspace/fiona_apple_paula_cole_preliminary_analysis.json")
        
    else:
        print("\nNo sources with discography information found.")
else:
    print("\nNo sources successfully accessed. Need to try alternative approaches.")

# Save overall research summary
research_summary = {
    'research_target': 'Fiona Apple & Paula Cole albums pre-1999',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'successful_sources': len(successful_sources),
    'failed_sources': len(failed_sources),
    'priority_sources': len([s for s in successful_sources if s.get('has_discography_info', False)]),
    'sources_data': successful_sources,
    'failed_attempts': failed_sources,
    'next_steps': [
        'Parse discography tables for album data',
        'Extract albums from pre-1999 period for both artists', 
        'Distinguish studio albums, EPs, and other releases',
        'Compile systematic list with exact release dates',
        'Verify completeness across multiple sources'
    ]
}

with open('workspace/fiona_apple_paula_cole_research_summary.json', 'w') as f:
    json.dump(research_summary, f, indent=2)

print(f"\n=== RESEARCH PHASE 1 COMPLETE ===")
print(f"Research summary saved to: workspace/fiona_apple_paula_cole_research_summary.json")
print(f"Next steps: Parse Wikipedia content for detailed album information")
print(f"Focus: Extract all albums released before 1999 with exact release dates")
```