### Development Step 11: Download PDF: Leicester’s “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?”

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- University marine biology lab automating retrieval of “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF via Crossref/Unpaywall for diet simulation model inputs
- Veterinary science research group integrating DOI-driven PDF downloads into their pipeline to feed fish‐based nutrition data into animal health neural networks
- Environmental policy analysts fetching full-text fisheries management studies overnight to update regulatory impact reports with the latest quantitative evidence
- Academic librarian scheduling a nightly job to pull newly indexed marine biology and ecology papers into the institutional repository for open access compliance
- Corporate R&D team in an aquaculture startup automatically archiving relevant journal articles to support feasibility studies on large‐scale dragon fish farming
- Data science consultancy extracting PDF metadata and figures from fish diet research to populate a knowledge graph used in predictive supply chain analytics
- E-learning platform dynamically downloading and caching dragon diet research articles as PDFs to provide students with up-to-date reading material in advanced zoology courses

```
import os
import sys
import requests
import json
from urllib.parse import urljoin

# 1. Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)
print(f"[INFO] Using workspace: {workspace}\n")

# 2. Define the exact paper title
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?"
print(f"[INFO] Target title: '{title}'\n")

# 3. Query Crossref for the title
cr_url = 'https://api.crossref.org/works'
params = {'query.title': title, 'rows': 10}
print(f"[INFO] Querying Crossref API for works matching title...")
try:
    cr_resp = requests.get(cr_url, params=params, timeout=30)
    cr_resp.raise_for_status()
    items = cr_resp.json().get('message', {}).get('items', [])
    print(f"[INFO] Crossref returned {len(items)} items\n")
except Exception as e:
    print(f"ERROR: Failed to query Crossref: {e}")
    sys.exit(1)

if not items:
    print("ERROR: No items found in Crossref for the given title.")
    sys.exit(1)

# 4. Inspect returned items and find the best match
selected = None
print("[INFO] Inspecting Crossref candidates:")
for idx, item in enumerate(items, start=1):
    cand_title = item.get('title', [''])[0]
    doi = item.get('DOI')
    print(f"  {idx}. Title: {cand_title}\n     DOI: {doi}\n")
    # Exact case-insensitive match
    if cand_title.strip().lower() == title.strip().lower():
        selected = item
        print(f"[INFO] Exact title match found at index {idx}\n")
        break

# 5. Fallback to first item if no exact match
if not selected:
    selected = items[0]
    print(f"[WARN] No exact title match; defaulting to first Crossref item:\n     {selected.get('title', [''])[0]} (DOI: {selected.get('DOI')})\n")

doi = selected.get('DOI')
if not doi:
    print("ERROR: Selected Crossref item has no DOI; cannot proceed.")
    sys.exit(1)
print(f"[INFO] Selected DOI: {doi}\n")

# 6. Look for PDF link in Crossref 'link' field
pdf_link = None
links = selected.get('link', [])
print(f"[INFO] Inspecting Crossref 'link' entries ({len(links)} total):")
for linkobj in links:
    url = linkobj.get('URL')
    content_type = linkobj.get('content-type', '')
    print(f"  - URL: {url} (type: {content_type})")
    if 'application/pdf' in content_type.lower():
        pdf_link = url
        print(f"[INFO] Found PDF via Crossref 'link' field: {pdf_link}\n")
        break

# 7. If no PDF link from Crossref, try Unpaywall
if not pdf_link:
    email = os.getenv('UNPAY_EMAIL')
    if not email:
        print("[ERROR] No PDF link in Crossref metadata and UNPAY_EMAIL not set.")
        print("Set the UNPAY_EMAIL environment variable to your email for Unpaywall API access.")
        sys.exit(1)
    up_url = f'https://api.unpaywall.org/v2/{doi}'
    print(f"[INFO] Querying Unpaywall for DOI {doi}...")
    try:
        up_resp = requests.get(up_url, params={'email': email}, timeout=30)
        up_resp.raise_for_status()
        up_data = up_resp.json()
        best = up_data.get('best_oa_location', {})
        pdf_link = best.get('url_for_pdf')
        if pdf_link:
            print(f"[INFO] Found PDF via Unpaywall: {pdf_link}\n")
        else:
            print("[ERROR] Unpaywall did not return a PDF URL.")
            sys.exit(1)
    except Exception as e:
        print(f"ERROR: Failed to query Unpaywall: {e}")
        sys.exit(1)

# 8. Final check of pdf_link
if not pdf_link:
    print("ERROR: Could not determine any PDF URL for the selected DOI.")
    sys.exit(1)

# 9. Download the PDF
pdf_path = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')
print(f"[DOWNLOAD] Fetching PDF: {pdf_link}\n        Saving to: {pdf_path}\n")
headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/pdf'}
total = 0
try:
    with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as r:
        r.raise_for_status()
        with open(pdf_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    total += len(chunk)
except Exception as e:
    print(f"ERROR: Failed to download PDF: {e}")
    sys.exit(1)

# 10. Report results
print(f"[DONE] Download complete: {total} bytes ({total/1024/1024:.2f} MB)")
if total < 10_000:
    print("[WARN] File size <10 KB; may be incomplete.")
else:
    print(f"[SUCCESS] PDF saved at {pdf_path}")
```