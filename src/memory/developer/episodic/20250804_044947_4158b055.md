### Development Step 53: Extract Survivor Winners (Seasons 1–44) into workspace JSON file

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Television content management: Automate updating a streaming platform’s “Survivor Hall of Fame” page with verified season winners and bio links for seamless editorial workflows
- TV fan community analytics: Integrate historical winner data into a community forum to generate trending polls, discussion threads, and fan-led retrospectives for seasons 1–44
- Podcast episode scripting: Auto-populate retrospective podcast segments with past winners’ names and season numbers to streamline scripting and production of anniversary episodes
- Academic research on media diversity: Compile winner demographics across 44 seasons for statistical analysis of gender, age, and geographic representation in a media studies thesis
- Social media marketing automation: Schedule Instagram and Twitter anniversary posts celebrating individual Survivor winners, driving engagement with exact winner names and season milestones
- Trivia and gaming app development: Populate quiz databases with accurate answer lists for mobile trivia games focused on reality TV history, ensuring up-to-date questions on Survivor champions
- Recommendation engine enrichment: Feed winner metadata into a streaming service’s algorithm to suggest related reality-TV content based on viewers’ favorite seasons’ victors
- Data journalism reference tool: Provide entertainment reporters with a ready-to-use JSON file of Survivor winners for rapid fact-checking and year-over-year comparison articles

```
import os
import sys
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
data_url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {data_url}\n")
resp = requests.get(data_url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP Status: {resp.status_code} OK\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Find the "Series overview" table by caption
wikitables = soup.find_all('table', class_='wikitable')
print(f"Found {len(wikitables)} wikitables on the page. Scanning for 'Series overview' caption...\n")
series_table = None
for idx, tbl in enumerate(wikitables, start=1):
    cap = tbl.find('caption')
    cap_text = cap.get_text(strip=True).lower() if cap else ''
    print(f"Table {idx}: caption='{cap_text}'")
    if 'series overview' in cap_text:
        series_table = tbl
        print(f"→ Matched 'Series overview' table at index {idx}\n")
        break

# 5) Fallback: match header row containing both 'Season' and 'Winner'
if not series_table:
    print("No caption match found, trying header‐based fallback...\n")
    for idx, tbl in enumerate(wikitables, start=1):
        first_row = tbl.find('tr')
        if not first_row:
            continue
        headers = [th.get_text(strip=True).lower() for th in first_row.find_all(['th','td'])]
        print(f"Fallback check Table {idx}: headers={headers[:5]}")
        if 'season' in headers and 'winner' in headers:
            series_table = tbl
            print(f"→ Fallback matched table at index {idx}\n")
            break

if not series_table:
    print("❌ Could not locate the Series overview table. Exiting.")
    sys.exit(1)

# 6) Identify column indices for Season and Winner
header_cells = series_table.find('tr').find_all(['th','td'])
col_texts = [c.get_text(strip=True) for c in header_cells]
print(f"Series overview columns: {col_texts}\n")
col_lower = [t.lower() for t in col_texts]
try:
    season_idx = col_lower.index('season')
    winner_idx = col_lower.index('winner')
except ValueError:
    # try partial matches
    season_idx = next((i for i,t in enumerate(col_lower) if 'season' in t), None)
    winner_idx = next((i for i,t in enumerate(col_lower) if 'winner' in t), None)

if season_idx is None or winner_idx is None:
    print("❌ Could not find 'Season' or 'Winner' columns. Exiting.")
    sys.exit(1)
print(f"Detected season column at index {season_idx}, winner column at index {winner_idx}\n")

# 7) Extract rows for seasons 1–44
data = []
for row in series_table.find_all('tr')[1:]:
    cells = row.find_all(['td','th'])
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # parse season number
    season_text = cells[season_idx].get_text(strip=True).split()[0]
    try:
        season_num = int(season_text)
    except:
        continue
    if season_num < 1 or season_num > 44:
        continue
    # parse winner name
    winner_cell = cells[winner_idx]
    winner_link = winner_cell.find('a')
    winner_name = winner_link.get_text(strip=True) if winner_link else winner_cell.get_text(strip=True)
    data.append({'season': season_num, 'winner': winner_name})
    print(f"Parsed Season {season_num}: {winner_name}")

if not data:
    print("⚠️ No winners parsed. Check parsing logic.")
    sys.exit(1)

# 8) Sort and save JSON
data_sorted = sorted(data, key=lambda x: x['season'])
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(data_sorted, f, indent=2, ensure_ascii=False)
print(f"\n✅ Wrote {len(data_sorted)} entries to {out_file}")
```