### Development Step 10: Extract Survivor US Seasons 1â€“44 Winners from Wikipedia into JSON file

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Interactive fan mobile app for Survivor enthusiasts that automatically lists all season winners with bios, eliminating manual updates and ensuring up-to-date content.
- Pop culture research report on gender and age trends among reality-show champions, using scraped Survivor winner data to generate charts and publish academic findings.
- Marketing campaign planning for streaming platforms that tailors ads based on popular Survivor winners per region, leveraging the JSON list to segment audiences.
- Trivia game backend service that auto-populates questions and answers about Survivor winners across seasons, streamlining quiz content generation.
- Data science capstone project predicting future Survivor champions by analyzing historical winner attributes, using the extracted table as training data.
- Reality TV production dashboard for network executives to visualize winner demographics over 44 seasons, guiding casting and format decisions.
- Media analytics pipeline that correlates season premiere dates and winner profiles with social media engagement spikes, feeding automated reporting.
- Educational case study in web scraping techniques for journalism students, demonstrating how to programmatically extract and store public domain TV series data.

```
import os
import requests
from bs4 import BeautifulSoup

# 1) Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2) Fetch the Survivor Wikipedia page
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
print(f"Fetching page: {url}")
response = requests.get(url)
response.raise_for_status()
print("Page fetched successfully. Parsing HTML...")

# 3) Parse HTML and extract all section headlines
soup = BeautifulSoup(response.text, 'html.parser')
headlines = soup.find_all('span', class_='mw-headline')
print(f"Found {len(headlines)} section headlines on the page.")

# 4) Write section headlines (id and text) to a file for inspection
out_file = os.path.join(workspace_dir, 'section_headlines.txt')
with open(out_file, 'w', encoding='utf-8') as f:
    f.write("# List of <span class='mw-headline'> elements on Survivor page\n")
    f.write("# Format: id=<span id>  text=<headline text>\n\n")
    for span in headlines:
        span_id = span.get('id', '')
        text = span.get_text(strip=True)
        f.write(f"id={span_id}\ttext={text}\n")

# 5) Print summary and a preview of the first few lines
print(f"Section headlines written to: {out_file}")
print("Preview of first 10 headlines:")
with open(out_file, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        print(line.strip())
        if i >= 9:
            break

```