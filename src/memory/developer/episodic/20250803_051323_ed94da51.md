### Development Step 22: Christgau’s Letter Grades for Pre-1999 Fiona Apple and Paula Cole Albums

**Description**: Research Robert Christgau's reviews and letter grades for all the pre-1999 albums by Fiona Apple and Paula Cole that were identified in the previous research. Search Christgau's Consumer Guide database, his official website, and archived reviews to determine which albums received letter grades (A+, A, A-, B+, B, B-, C+, C, C-, D+, D, etc.) and which albums were not reviewed or did not receive letter grades. Focus on finding his specific reviews for albums like Fiona Apple's 'Tidal' (1996) and Paula Cole's 'Harbinger' (1994) and 'This Fire' (1996), among others from the compiled list.

**Use Cases**:
- Music journalist compiling Robert Christgau’s letter grades for Fiona Apple’s “Tidal” and Paula Cole’s “This Fire” to enrich a retrospective feature on 90s singer-songwriters
- Academic musicologist extracting pre-1999 album review data to analyze critical reception trends in female-led alternative pop for a peer-reviewed paper
- Online music database automating the population of critic ratings and review snippets for Fiona Apple and Paula Cole discography pages
- Radio programming team scraping expert grades to curate a “Best of 90s Female Singer-Songwriters” specialty show lineup
- Podcast producer gathering historical review excerpts and scores to provide context when discussing seminal 90s albums in an episode
- University course instructor in Popular Music Studies assembling real-world critic feedback for a class module on album reception analysis
- Music recommendation startup integrating Christgau’s letter grades into its algorithm to factor expert ratings into 90s album suggestions
- Niche music blog automating daily updates of archived critic reviews and grades for Fiona Apple and Paula Cole to engage dedicated fan communities

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

print('=== ROBERT CHRISTGAU REVIEWS SEARCH: PHASE 4 ===')
print('Objective: Search Christgau\'s Consumer Guide for reviews and letter grades')
print('Strategy: Access Christgau\'s website and search for each album systematically\n')

# Step 1: Load the comprehensive research targets from Phase 3
workspace_dir = 'workspace'
targets_file = 'christgau_research_comprehensive_targets.json'
targets_path = os.path.join(workspace_dir, targets_file)

print('=== LOADING RESEARCH TARGETS FROM PHASE 3 ===')
print()

if os.path.exists(targets_path):
    print(f'✓ Found research targets file: {targets_file}')
    
    with open(targets_path, 'r', encoding='utf-8') as f:
        research_data = json.load(f)
    
    print(f'Research data loaded successfully')
    print(f'Total albums to research: {research_data.get("total_albums_to_research", 0)}')
    
    target_albums = research_data.get('target_albums', [])
    print(f'Albums extracted: {len(target_albums)}')
    
    # Display the albums we'll be searching for
    print('\nAlbums to search for Christgau reviews:')
    for i, album in enumerate(target_albums, 1):
        print(f'  {i}. {album["artist"]}: "{album["title"]}" ({album["year"]})')
    
else:
    print(f'✗ Research targets file not found: {targets_file}')
    print('Cannot proceed without album list. Please run Phase 3 first.')
    exit()

print('\n=== CHRISTGAU WEBSITE ACCESS STRATEGY ===')
print()

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# Christgau website URLs to try
christgau_urls = {
    'main_site': 'https://www.robertchristgau.com',
    'consumer_guide_search': 'https://www.robertchristgau.com/xg/cg/cg-search.php',
    'artist_search': 'https://www.robertchristgau.com/get_artist.php',
    'consumer_guide_main': 'https://www.robertchristgau.com/get_chap.php?k=C&bk=70'
}

print('Target URLs:')
for url_name, url in christgau_urls.items():
    print(f'  - {url_name}: {url}')

print('\n=== STEP 1: ACCESS CHRISTGAU MAIN SITE ===')
print()

try:
    main_response = requests.get(christgau_urls['main_site'], headers=headers, timeout=20)
    print(f'Main site response: {main_response.status_code}')
    
    if main_response.status_code == 200:
        print('✓ Successfully accessed Christgau main site')
        
        # Save main page for analysis
        main_page_file = 'christgau_main_page.html'
        main_page_path = os.path.join(workspace_dir, main_page_file)
        
        with open(main_page_path, 'w', encoding='utf-8') as f:
            f.write(main_response.text)
        
        # Parse main page to understand site structure
        main_soup = BeautifulSoup(main_response.content, 'html.parser')
        title = main_soup.find('title')
        title_text = title.get_text().strip() if title else 'No title'
        
        print(f'Page title: {title_text}')
        print(f'Page content length: {len(main_response.text):,} characters')
        print(f'Main page saved: {main_page_file}')
        
        # Look for search functionality or navigation links
        search_forms = main_soup.find_all('form')
        search_inputs = main_soup.find_all('input', {'type': 'search'})
        
        print(f'\nSite navigation analysis:')
        print(f'  Forms found: {len(search_forms)}')
        print(f'  Search inputs found: {len(search_inputs)}')
        
        # Find links that might lead to Consumer Guide or artist searches
        all_links = main_soup.find_all('a', href=True)
        relevant_links = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip().lower()
            
            if any(keyword in text for keyword in ['consumer guide', 'search', 'artist', 'album', 'review']):
                relevant_links.append({
                    'text': link.get_text().strip(),
                    'href': href,
                    'full_url': href if href.startswith('http') else f"https://www.robertchristgau.com{href}"
                })
        
        print(f'\nRelevant navigation links found: {len(relevant_links)}')
        for i, link in enumerate(relevant_links[:10], 1):  # Show first 10
            print(f'  {i}. "{link["text"]}" -> {link["full_url"]}')
        
    else:
        print(f'✗ Failed to access main site: {main_response.status_code}')
        
except Exception as e:
    print(f'✗ Error accessing main site: {str(e)}')

time.sleep(2)  # Be respectful to the server

print('\n=== STEP 2: ACCESS CONSUMER GUIDE SEARCH ===')
print()

try:
    cg_search_response = requests.get(christgau_urls['consumer_guide_search'], headers=headers, timeout=20)
    print(f'Consumer Guide search response: {cg_search_response.status_code}')
    
    if cg_search_response.status_code == 200:
        print('✓ Successfully accessed Consumer Guide search page')
        
        # Save search page
        cg_search_file = 'christgau_consumer_guide_search.html'
        cg_search_path = os.path.join(workspace_dir, cg_search_file)
        
        with open(cg_search_path, 'w', encoding='utf-8') as f:
            f.write(cg_search_response.text)
        
        # Parse search page to understand search functionality
        cg_soup = BeautifulSoup(cg_search_response.content, 'html.parser')
        
        # Look for search forms and input fields
        search_forms = cg_soup.find_all('form')
        print(f'\nConsumer Guide search page analysis:')
        print(f'  Search forms found: {len(search_forms)}')
        
        for i, form in enumerate(search_forms, 1):
            print(f'\n  Form {i}:')
            action = form.get('action', 'No action')
            method = form.get('method', 'No method')
            print(f'    Action: {action}')
            print(f'    Method: {method}')
            
            # Find input fields in the form
            inputs = form.find_all('input')
            selects = form.find_all('select')
            
            print(f'    Input fields: {len(inputs)}')
            for input_field in inputs:
                input_type = input_field.get('type', 'text')
                input_name = input_field.get('name', 'no name')
                input_value = input_field.get('value', 'no value')
                print(f'      - {input_type}: {input_name} = "{input_value}"')
            
            print(f'    Select fields: {len(selects)}')
            for select_field in selects:
                select_name = select_field.get('name', 'no name')
                options = select_field.find_all('option')
                print(f'      - Select: {select_name} ({len(options)} options)')
        
        print(f'\nConsumer Guide search page saved: {cg_search_file}')
        
    else:
        print(f'✗ Failed to access Consumer Guide search: {cg_search_response.status_code}')
        
except Exception as e:
    print(f'✗ Error accessing Consumer Guide search: {str(e)}')

time.sleep(2)

print('\n=== STEP 3: SEARCH FOR FIONA APPLE REVIEWS ===')
print()

# Try to search for Fiona Apple specifically
fiona_albums = [a for a in target_albums if 'fiona apple' in a['artist'].lower()]
print(f'Fiona Apple albums to search: {len(fiona_albums)}')

for album in fiona_albums[:3]:  # Start with first 3 albums
    print(f'\nSearching for: {album["artist"]} - "{album["title"]}" ({album["year"]})')
    
    # Try different search approaches
    search_queries = [
        f'fiona apple {album["title"]}',
        f'fiona apple',
        album['title']
    ]
    
    for query in search_queries:
        print(f'  Trying query: "{query}"')
        
        try:
            # Try artist search URL with query parameter
            search_url = f"{christgau_urls['artist_search']}?artist={query.replace(' ', '+')}"
            
            search_response = requests.get(search_url, headers=headers, timeout=15)
            print(f'    Response: {search_response.status_code}')
            
            if search_response.status_code == 200:
                # Save the search result
                result_filename = f'christgau_search_{query.replace(" ", "_").replace('"', '')}.html'
                result_path = os.path.join(workspace_dir, result_filename)
                
                with open(result_path, 'w', encoding='utf-8') as f:
                    f.write(search_response.text)
                
                # Quick analysis of search results
                result_soup = BeautifulSoup(search_response.content, 'html.parser')
                result_text = result_soup.get_text().lower()
                
                # Look for letter grades and album mentions
                grade_pattern = r'\b[A-E][+-]?\b'
                grades_found = re.findall(grade_pattern, search_response.text)
                
                album_mentioned = album['title'].lower() in result_text
                artist_mentioned = 'fiona apple' in result_text
                
                print(f'    Result saved: {result_filename}')
                print(f'    Content length: {len(search_response.text):,} characters')
                print(f'    Artist mentioned: {artist_mentioned}')
                print(f'    Album mentioned: {album_mentioned}')
                print(f'    Potential grades found: {grades_found[:5]}')
                
                if artist_mentioned or album_mentioned:
                    print(f'    *** PROMISING RESULT - Contains relevant content ***')
                    break  # Found relevant content, move to next album
            
        except Exception as e:
            print(f'    Error with query "{query}": {str(e)}')
        
        time.sleep(1)  # Brief pause between requests

print('\n=== STEP 4: SEARCH FOR PAULA COLE REVIEWS ===')
print()

paula_albums = [a for a in target_albums if 'paula cole' in a['artist'].lower()]
print(f'Paula Cole albums to search: {len(paula_albums)}')

for album in paula_albums[:3]:  # Start with first 3 albums
    print(f'\nSearching for: {album["artist"]} - "{album["title"]}" ({album["year"]})')
    
    search_queries = [
        f'paula cole {album["title"]}',
        f'paula cole',
        album['title']
    ]
    
    for query in search_queries:
        print(f'  Trying query: "{query}"')
        
        try:
            search_url = f"{christgau_urls['artist_search']}?artist={query.replace(' ', '+')}"
            
            search_response = requests.get(search_url, headers=headers, timeout=15)
            print(f'    Response: {search_response.status_code}')
            
            if search_response.status_code == 200:
                result_filename = f'christgau_search_{query.replace(" ", "_").replace('"', '')}.html'
                result_path = os.path.join(workspace_dir, result_filename)
                
                with open(result_path, 'w', encoding='utf-8') as f:
                    f.write(search_response.text)
                
                result_soup = BeautifulSoup(search_response.content, 'html.parser')
                result_text = result_soup.get_text().lower()
                
                grade_pattern = r'\b[A-E][+-]?\b'
                grades_found = re.findall(grade_pattern, search_response.text)
                
                album_mentioned = album['title'].lower() in result_text
                artist_mentioned = 'paula cole' in result_text
                
                print(f'    Result saved: {result_filename}')
                print(f'    Content length: {len(search_response.text):,} characters')
                print(f'    Artist mentioned: {artist_mentioned}')
                print(f'    Album mentioned: {album_mentioned}')
                print(f'    Potential grades found: {grades_found[:5]}')
                
                if artist_mentioned or album_mentioned:
                    print(f'    *** PROMISING RESULT - Contains relevant content ***')
                    break
            
        except Exception as e:
            print(f'    Error with query "{query}": {str(e)}')
        
        time.sleep(1)

print('\n=== INITIAL SEARCH RESULTS SUMMARY ===')
print()

# Count saved search result files
search_result_files = [f for f in os.listdir(workspace_dir) if f.startswith('christgau_search_')]
print(f'Search result files saved: {len(search_result_files)}')

for result_file in search_result_files:
    file_path = os.path.join(workspace_dir, result_file)
    file_size = os.path.getsize(file_path)
    print(f'  - {result_file} ({file_size:,} bytes)')

# Save initial search summary
search_summary = {
    'search_phase': 'Initial Christgau website access and artist searches',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'albums_researched': len(target_albums),
    'search_attempts': len(search_result_files),
    'christgau_urls_tested': list(christgau_urls.keys()),
    'search_result_files': search_result_files,
    'next_steps': [
        'Analyze saved search result files for letter grades',
        'Parse HTML content for specific album reviews',
        'Extract letter grades and review text',
        'Identify albums with no reviews found',
        'Compile comprehensive grade summary'
    ]
}

summary_file = 'christgau_search_initial_summary.json'
summary_path = os.path.join(workspace_dir, summary_file)

with open(summary_path, 'w', encoding='utf-8') as f:
    json.dump(search_summary, f, indent=2)

print(f'\n=== PHASE 4 COMPLETE ===')
print(f'Initial search summary saved: {summary_file}')
print(f'Search result files ready for analysis: {len(search_result_files)}')
print('Next phase: Parse search results to extract letter grades and reviews')
```