### Development Step 7: USGS Reports on Nonnative Clownfish (Finding Nemo) Occurrences with Zip Codes Before 2020

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting for coastal development projects: integrate zip-code-level USGS occurrence records of nonnative clownfish to ensure compliance with state and federal invasive species regulations during site assessments
- State wildlife agency field operations planning: use pre-2020 zip-coded occurrence data to prioritize targeted removal of invasive Amphiprion ocellaris populations in specific counties
- Marine ecologist research modeling: correlate zip-code-based nonnative clownfish records with water temperature and salinity datasets to publish spread predictions in a peer-reviewed journal
- GIS-driven urban planning risk assessment: overlay invasive clownfish hotspot zip codes with critical infrastructure maps to guide permit decisions for wastewater and stormwater projects
- Policy development for fisheries management: leverage detailed location data by zip code to allocate budget toward public outreach and rapid response teams in the most impacted municipalities
- Aquarium trade compliance auditing: cross-reference shipment destinations against known nonnative occurrence zip codes to prevent accidental introductions when importing live Amphiprion ocellaris specimens
- Public health and seafood safety monitoring: assess potential ecosystem disruptions in zip-coded coastal zones to issue localized advisories on shellfish and reef fish harvesting
- Citizen science and community engagement platforms: auto-populate local invasive species incident reports using USGS point map zip-coded data to empower neighborhood monitoring and rapid reporting

```
print("=== ACCESSING HIGH PRIORITY POINT MAP FOR ZIP CODE LOCATION DATA ===\n")

# Access the highest priority target: USGS Point Map for clownfish occurrences
import requests
from bs4 import BeautifulSoup
import re
import json
import os
from urllib.parse import urljoin, urlparse

print("Target: USGS Point Map for Amphiprion ocellaris (Clownfish)")
print("Objective: Extract specific location data with zip codes for nonnative occurrences before 2020\n")

# First, let's inspect the priority targets file to understand what we have
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using workspace directory: {workspace_dir}")
else:
    print("No workspace directory found, creating new one")
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)

# Inspect the priority targets file structure first
priority_file = os.path.join(workspace_dir, 'priority_location_extraction_plan.json')
if os.path.exists(priority_file):
    print(f"\n=== INSPECTING PRIORITY TARGETS FILE ===\n")
    print(f"File: {priority_file}")
    
    with open(priority_file, 'r') as f:
        priority_data = json.load(f)
    
    print("Top-level keys in priority data:")
    for key, value in priority_data.items():
        if isinstance(value, dict):
            print(f"  {key}: Dictionary with {len(value)} keys")
        elif isinstance(value, list):
            print(f"  {key}: List with {len(value)} items")
        else:
            print(f"  {key}: {type(value).__name__} - {str(value)[:100]}...")
    
    # Extract the high priority point map target
    if 'priority_targets' in priority_data:
        priority_targets = priority_data['priority_targets']
        print(f"\nPriority targets available: {len(priority_targets)}")
        
        # Find the point map target
        point_map_target = None
        for target in priority_targets:
            if 'point map' in target.get('keywords_found', []):
                point_map_target = target
                break
        
        if point_map_target:
            print(f"\nPoint Map Target Identified:")
            print(f"  Text: {point_map_target['text']}")
            print(f"  URL: {point_map_target['url']}")
            print(f"  Keywords: {point_map_target['keywords_found']}")
            
            # Access the point map URL
            point_map_url = point_map_target['url']
            
            print(f"\n=== ACCESSING USGS POINT MAP ===\n")
            print(f"Target URL: {point_map_url}")
            
            try:
                # Set headers to mimic browser request
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Connection': 'keep-alive'
                }
                
                # Access the point map page
                response = requests.get(point_map_url, headers=headers, timeout=30)
                response.raise_for_status()
                
                print(f"Successfully accessed point map page (Status: {response.status_code})")
                print(f"Content length: {len(response.content):,} bytes\n")
                
                # Parse the HTML content
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract page title
                title_element = soup.find('title')
                page_title = title_element.get_text().strip() if title_element else 'Unknown'
                print(f"Point Map Page Title: {page_title}")
                
                # Look for occurrence data, coordinates, or location information
                print(f"\n=== SEARCHING FOR LOCATION DATA ON POINT MAP PAGE ===\n")
                
                # Get page text for analysis
                page_text = soup.get_text()
                
                # Look for coordinate patterns (latitude/longitude)
                coord_patterns = [
                    r'(-?\d+\.\d+),\s*(-?\d+\.\d+)',  # lat,lon pairs
                    r'Lat(?:itude)?[:\s]+(-?\d+\.\d+)',  # Latitude: value
                    r'Lon(?:gitude)?[:\s]+(-?\d+\.\d+)',  # Longitude: value
                    r'(\d{2}\.\d+°?[NS])\s+(\d{2,3}\.\d+°?[EW])'  # Degree format
                ]
                
                coordinates_found = []
                for pattern in coord_patterns:
                    matches = re.findall(pattern, page_text, re.IGNORECASE)
                    if matches:
                        coordinates_found.extend(matches)
                        print(f"Coordinate pattern found: {pattern} -> {matches[:3]}...")  # Show first 3
                
                # Look for zip code patterns
                zip_patterns = [
                    r'\b(\d{5})\b',  # 5-digit zip
                    r'\b(\d{5}-\d{4})\b',  # ZIP+4
                    r'[Zz]ip[:\s]+(\d{5}(?:-\d{4})?)',  # Zip: value
                ]
                
                zip_codes_found = []
                for pattern in zip_patterns:
                    matches = re.findall(pattern, page_text)
                    if matches:
                        zip_codes_found.extend(matches)
                        print(f"ZIP code pattern found: {pattern} -> {matches[:5]}...")  # Show first 5
                
                # Look for state and location information
                state_patterns = [
                    r'([A-Z]{2})\s+(?:State|County)',  # State abbreviations
                    r'State[:\s]+([A-Z]{2}|[A-Za-z\s]+)',  # State: value
                    r'County[:\s]+([A-Za-z\s]+)',  # County: value
                ]
                
                locations_found = []
                for pattern in state_patterns:
                    matches = re.findall(pattern, page_text, re.IGNORECASE)
                    if matches:
                        locations_found.extend(matches)
                        print(f"Location pattern found: {pattern} -> {matches[:5]}...")  # Show first 5
                
                # Look for year information (to filter for pre-2020)
                year_patterns = [
                    r'\b(19\d{2}|20[01]\d)\b',  # Years 1900-2019
                    r'Year[:\s]+(19\d{2}|20[01]\d)',  # Year: value
                    r'Date[:\s]+\d{1,2}/\d{1,2}/(19\d{2}|20[01]\d)'  # Date: mm/dd/yyyy
                ]
                
                years_found = []
                for pattern in year_patterns:
                    matches = re.findall(pattern, page_text)
                    if matches:
                        # Filter for years before 2020
                        pre_2020_years = [year for year in matches if int(year) < 2020]
                        if pre_2020_years:
                            years_found.extend(pre_2020_years)
                            print(f"Pre-2020 year pattern found: {pattern} -> {pre_2020_years[:5]}...")  # Show first 5
                
                # Look for specific data tables or structured information
                print(f"\n=== SEARCHING FOR DATA TABLES AND STRUCTURED CONTENT ===\n")
                
                # Find tables that might contain occurrence data
                tables = soup.find_all('table')
                print(f"Found {len(tables)} tables on the point map page")
                
                table_data = []
                for i, table in enumerate(tables, 1):
                    # Extract table headers
                    headers = []
                    header_row = table.find('tr')
                    if header_row:
                        header_cells = header_row.find_all(['th', 'td'])
                        headers = [cell.get_text().strip() for cell in header_cells]
                    
                    # Extract first few rows of data
                    rows = table.find_all('tr')[1:6]  # Skip header, get first 5 data rows
                    row_data = []
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        row_data.append([cell.get_text().strip() for cell in cells])
                    
                    table_info = {
                        'table_number': i,
                        'headers': headers,
                        'sample_rows': row_data,
                        'total_rows': len(table.find_all('tr'))
                    }
                    table_data.append(table_info)
                    
                    print(f"Table {i}:")
                    print(f"  Headers: {headers[:5]}...")  # Show first 5 headers
                    print(f"  Total rows: {table_info['total_rows']}")
                    if row_data:
                        print(f"  Sample row: {row_data[0][:3]}...")  # Show first 3 cells of first row
                    print(f"  {'-'*60}")
                
                # Look for JavaScript or AJAX data sources
                print(f"\n=== SEARCHING FOR DYNAMIC DATA SOURCES ===\n")
                
                # Find script tags that might contain data
                scripts = soup.find_all('script')
                print(f"Found {len(scripts)} script tags")
                
                data_sources = []
                for script in scripts:
                    script_content = script.get_text() if script.get_text() else script.get('src', '')
                    
                    # Look for data-related keywords in scripts
                    data_keywords = ['coordinates', 'latitude', 'longitude', 'zipcode', 'location', 'occurrence', 'json', 'ajax']
                    
                    script_has_data = False
                    for keyword in data_keywords:
                        if keyword.lower() in script_content.lower():
                            script_has_data = True
                            break
                    
                    if script_has_data:
                        data_sources.append({
                            'type': 'inline' if script.get_text() else 'external',
                            'src': script.get('src', 'inline'),
                            'content_preview': script_content[:200] + '...' if len(script_content) > 200 else script_content
                        })
                
                print(f"Found {len(data_sources)} scripts with potential data sources:")
                for i, source in enumerate(data_sources, 1):
                    print(f"  Script {i}: {source['type']} - {source['src']}")
                    print(f"    Preview: {source['content_preview'][:100]}...")
                
                # Save all extracted point map data
                point_map_data = {
                    'point_map_access': {
                        'url': point_map_url,
                        'page_title': page_title,
                        'access_date': '2024',
                        'content_length': len(response.content),
                        'status': 'success'
                    },
                    'location_data_extracted': {
                        'coordinates_found': coordinates_found,
                        'zip_codes_found': list(set(zip_codes_found)),  # Remove duplicates
                        'locations_found': list(set(locations_found)),  # Remove duplicates
                        'pre_2020_years_found': list(set(years_found))  # Remove duplicates
                    },
                    'structured_data': {
                        'tables_found': len(tables),
                        'table_data': table_data,
                        'data_scripts_found': len(data_sources),
                        'script_sources': data_sources
                    },
                    'analysis_summary': {
                        'coordinates_available': len(coordinates_found) > 0,
                        'zip_codes_available': len(zip_codes_found) > 0,
                        'pre_2020_data_available': len(years_found) > 0,
                        'structured_tables_available': len(tables) > 0
                    },
                    'next_steps': [
                        'Extract specific occurrence records from tables',
                        'Access dynamic data sources if available',
                        'Convert coordinates to zip codes if needed',
                        'Filter all records for pre-2020 timeframe',
                        'Compile final location data with zip codes'
                    ]
                }
                
                # Save the point map analysis
                point_map_file = os.path.join(workspace_dir, 'usgs_point_map_location_data.json')
                with open(point_map_file, 'w') as f:
                    json.dump(point_map_data, f, indent=2)
                
                print(f"\n=== POINT MAP ANALYSIS COMPLETE ===\n")
                print(f"Point Map URL: {point_map_url}")
                print(f"Coordinates Found: {len(coordinates_found)}")
                print(f"ZIP Codes Found: {len(set(zip_codes_found))}")
                print(f"Locations Found: {len(set(locations_found))}")
                print(f"Pre-2020 Years Found: {len(set(years_found))}")
                print(f"Data Tables Found: {len(tables)}")
                print(f"Data Scripts Found: {len(data_sources)}")
                print(f"\nPoint map analysis saved to: {point_map_file}")
                
                if coordinates_found or zip_codes_found or tables:
                    print(f"\n*** LOCATION DATA SUCCESSFULLY IDENTIFIED ***")
                    print(f"Ready to extract specific occurrence records with zip codes for pre-2020 nonnative clownfish populations.")
                else:
                    print(f"\nNo direct location data found on point map page.")
                    print(f"Will need to access alternative data sources or use different extraction methods.")
                
            except requests.exceptions.RequestException as e:
                print(f"Error accessing point map: {e}")
                print("Will try alternative data sources.")
                
            except Exception as e:
                print(f"Unexpected error during point map analysis: {e}")
                print("Error details for debugging:")
                import traceback
                traceback.print_exc()
        
        else:
            print(f"\nNo point map target found in priority targets.")
            print(f"Available targets: {[t.get('text', 'Unknown') for t in priority_targets]}")
    
    else:
        print(f"\nNo priority_targets key found in priority data.")
        print(f"Available keys: {list(priority_data.keys())}")

else:
    print(f"Priority targets file not found: {priority_file}")
    print(f"Available files in workspace:")
    if os.path.exists(workspace_dir):
        for file in os.listdir(workspace_dir):
            print(f"  - {file}")
    else:
        print(f"  No workspace directory found.")
```