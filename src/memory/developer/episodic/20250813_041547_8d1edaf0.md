### Development Step 3: Search for Author of Third Volume ‘Francia’s Reign of Terror’ Paraguay Letters

**Description**: Search for information about a collection of letters titled 'Francia's Reign of Terror' that documents a four-year stay under Dictator Francia's rule in Paraguay. Focus on identifying the author of this work, which is described as the third volume that criticizes local laziness and government policies and was written while relying on local hospitality. Search using keywords including 'Francia's Reign of Terror letters Paraguay dictator', 'four year stay Francia Paraguay author', and 'third volume Francia Paraguay criticism government policies'.

**Use Cases**:
- Historical research and author identification for a university historian compiling primary sources on Paraguay under Dictator Francia
- Academic bibliography compilation for a Latin American studies professor preparing a syllabus on early 19th-century political correspondence
- Museum exhibit curation and provenance verification for a cultural institution showcasing letters from Francia’s tenure
- Journalistic fact-checking and source validation for a documentary producer covering Paraguay’s “Reign of Terror” period
- Library cataloging and metadata enrichment for a special collections librarian managing rare South American manuscripts
- Publishing house rights clearance and author attribution for a reprint edition of the third volume of Francia’s correspondence
- Digital humanities text mining and keyword analysis for a computational linguist building a corpus of colonial-era government critiques

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Searching for information about 'Francia's Reign of Terror' letters collection...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Define all functions first before any execution
def search_with_requests(query, num_results=10):
    """Search using requests and DuckDuckGo HTML search"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # Use DuckDuckGo HTML search as alternative
    search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"
    
    try:
        print(f"Searching: {query}")
        response = requests.get(search_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # Parse DuckDuckGo results
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:num_results]:
            title_elem = div.find('a', class_='result__a')
            snippet_elem = div.find('a', class_='result__snippet')
            
            if title_elem:
                title = title_elem.get_text(strip=True)
                href = title_elem.get('href', '')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                
                results.append({
                    'title': title,
                    'href': href,
                    'body': snippet,
                    'search_query': query
                })
        
        print(f"Found {len(results)} results for: {query}")
        return results
        
    except Exception as e:
        print(f"Error searching for '{query}': {str(e)}")
        return []

def search_wikipedia_directly():
    """Search Wikipedia directly for Francia-related articles"""
    print("\n=== SEARCHING WIKIPEDIA DIRECTLY ===")
    
    wikipedia_searches = [
        "https://en.wikipedia.org/wiki/Jos%C3%A9_Gaspar_Rodr%C3%ADguez_de_Francia",
        "https://en.wikipedia.org/wiki/Paraguay",
        "https://en.wikipedia.org/wiki/History_of_Paraguay"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    wikipedia_results = []
    
    for url in wikipedia_searches:
        try:
            print(f"Fetching Wikipedia page: {url}")
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract title
            title_elem = soup.find('h1', class_='firstHeading')
            title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
            
            # Extract main content
            content_div = soup.find('div', {'id': 'mw-content-text'})
            if content_div:
                # Remove unwanted elements
                for elem in content_div.find_all(['script', 'style', 'table', 'div']):
                    if elem.get('class') and any(cls in str(elem.get('class')) for cls in ['navbox', 'infobox', 'reference']):
                        elem.decompose()
                
                content = content_div.get_text(separator=' ', strip=True)
                
                # Look for Francia-related keywords
                content_lower = content.lower()
                francia_keywords = ['francia', 'dictator', 'paraguay', 'reign', 'terror', 'letters', 'correspondence', 'memoir', 'account']
                
                found_keywords = [kw for kw in francia_keywords if kw in content_lower]
                
                if found_keywords:
                    wikipedia_results.append({
                        'title': title,
                        'href': url,
                        'body': content[:2000],  # First 2000 characters
                        'keywords_found': found_keywords,
                        'source': 'Wikipedia'
                    })
                    print(f"Found relevant content in {title} - Keywords: {', '.join(found_keywords)}")
            
            time.sleep(1)  # Be respectful to Wikipedia
            
        except Exception as e:
            print(f"Error fetching Wikipedia page {url}: {str(e)}")
    
    return wikipedia_results

def search_google_books_api(query):
    """Search Google Books API for Francia-related books"""
    print(f"\n=== SEARCHING GOOGLE BOOKS API FOR: {query} ===")
    
    try:
        # Google Books API endpoint
        api_url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': query,
            'maxResults': 10,
            'printType': 'books',
            'langRestrict': 'en'
        }
        
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        books_results = []
        
        if 'items' in data:
            for item in data['items']:
                volume_info = item.get('volumeInfo', {})
                title = volume_info.get('title', 'Unknown Title')
                authors = volume_info.get('authors', ['Unknown Author'])
                description = volume_info.get('description', '')
                published_date = volume_info.get('publishedDate', 'Unknown Date')
                
                books_results.append({
                    'title': title,
                    'authors': authors,
                    'body': description,
                    'published_date': published_date,
                    'source': 'Google Books',
                    'search_query': query
                })
            
            print(f"Found {len(books_results)} books for query: {query}")
        else:
            print(f"No books found for query: {query}")
        
        return books_results
        
    except Exception as e:
        print(f"Error searching Google Books for '{query}': {str(e)}")
        return []

def search_francia_letters():
    """Main search function for Francia's Reign of Terror letters"""
    print("\n=== SEARCHING FOR FRANCIA'S REIGN OF TERROR LETTERS ===")
    
    # Define search queries
    search_queries = [
        "Francia's Reign of Terror letters Paraguay dictator",
        "four year stay Francia Paraguay author",
        "third volume Francia Paraguay criticism government policies",
        "Francia Paraguay dictator letters collection",
        "Paraguay Francia reign terror author four years",
        "Francia dictator Paraguay third volume letters",
        "Paraguay Francia government criticism letters",
        "Francia Paraguay local hospitality author letters",
        "Jose Gaspar Rodriguez de Francia letters memoir",
        "Paraguay dictator Francia correspondence"
    ]
    
    all_search_results = []
    
    # Search using requests-based method
    for i, query in enumerate(search_queries, 1):
        print(f"\nSearch {i}/{len(search_queries)}: {query}")
        results = search_with_requests(query, num_results=8)
        all_search_results.extend(results)
        time.sleep(2)  # Be respectful to search engines
    
    # Also search Wikipedia directly
    wikipedia_results = search_wikipedia_directly()
    all_search_results.extend(wikipedia_results)
    
    # Search Google Books for relevant books
    book_queries = [
        "Francia Paraguay dictator letters",
        "Paraguay Francia reign of terror",
        "Francia Paraguay memoir correspondence"
    ]
    
    for book_query in book_queries:
        book_results = search_google_books_api(book_query)
        all_search_results.extend(book_results)
        time.sleep(1)
    
    return all_search_results

def analyze_search_results(results):
    """Analyze search results for relevant information about Francia's letters"""
    print(f"\n=== ANALYZING {len(results)} TOTAL SEARCH RESULTS ===")
    
    # Keywords to look for in results
    relevant_keywords = [
        'francia',
        'paraguay',
        'dictator',
        'letters',
        'reign of terror',
        'four year',
        'four-year',
        'third volume',
        'author',
        'criticism',
        'government policies',
        'local hospitality',
        'laziness',
        'stay',
        'collection',
        'memoir',
        'correspondence',
        'account',
        'documented'
    ]
    
    relevant_results = []
    
    for result in results:
        title = result.get('title', '').lower()
        body = result.get('body', '').lower()
        combined_text = title + ' ' + body
        
        # Count keyword matches
        keyword_matches = []
        for keyword in relevant_keywords:
            if keyword in combined_text:
                keyword_matches.append(keyword)
        
        # Consider result relevant if it has multiple keyword matches
        if len(keyword_matches) >= 2:
            result['keyword_matches'] = keyword_matches
            result['relevance_score'] = len(keyword_matches)
            relevant_results.append(result)
    
    # Sort by relevance score
    relevant_results.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    print(f"Found {len(relevant_results)} highly relevant results")
    
    return relevant_results

def extract_author_information(results):
    """Extract potential author information from search results"""
    print("\n=== EXTRACTING AUTHOR INFORMATION ===")
    
    author_candidates = []
    
    # Common author name patterns in historical texts about Paraguay/Francia
    potential_authors = [
        'robertson',
        'washburn',
        'masterman',
        'rengger',
        'longchamp',
        'wisner',
        'parish',
        'henderson',
        'thompson',
        'carlyle',
        'warren',
        'williams',
        'graham',
        'hopkins'
    ]
    
    for result in results:
        title = result.get('title', '')
        body = result.get('body', '')
        authors = result.get('authors', [])  # For Google Books results
        combined_text = title + ' ' + body + ' ' + ' '.join(authors) if authors else title + ' ' + body
        combined_lower = combined_text.lower()
        
        # Look for author-related patterns
        author_indicators = [
            'written by',
            'author',
            'by ',
            'memoir',
            'account',
            'letters',
            'correspondence',
            'documented by',
            'recorded by',
            'volume',
            'published'
        ]
        
        # Check for potential author names
        found_authors = []
        for author in potential_authors:
            if author in combined_lower:
                found_authors.append(author)
        
        for indicator in author_indicators:
            if indicator in combined_lower:
                # Extract surrounding text that might contain author name
                pos = combined_lower.find(indicator)
                if pos != -1:
                    # Get text around the indicator
                    start = max(0, pos - 150)
                    end = min(len(combined_text), pos + 300)
                    context = combined_text[start:end]
                    
                    author_candidates.append({
                        'source_title': title,
                        'source_url': result.get('href', ''),
                        'indicator': indicator,
                        'context': context,
                        'relevance_score': result.get('relevance_score', 0),
                        'potential_authors': found_authors,
                        'google_books_authors': authors if authors else []
                    })
    
    return author_candidates

def analyze_for_specific_details(results):
    """Look for specific details mentioned in the plan"""
    print("\n=== ANALYZING FOR SPECIFIC DETAILS ===")
    
    specific_details = {
        'four_year_stay': [],
        'third_volume': [],
        'criticism_laziness': [],
        'government_policies': [],
        'local_hospitality': [],
        'reign_of_terror': []
    }
    
    for result in results:
        title = result.get('title', '')
        body = result.get('body', '')
        combined_text = (title + ' ' + body).lower()
        
        # Look for four-year stay mentions
        if 'four year' in combined_text or 'four-year' in combined_text:
            specific_details['four_year_stay'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for third volume mentions
        if 'third volume' in combined_text or 'volume 3' in combined_text or 'vol. 3' in combined_text:
            specific_details['third_volume'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for criticism of laziness
        if 'laziness' in combined_text or 'lazy' in combined_text:
            specific_details['criticism_laziness'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for government policies criticism
        if 'government policies' in combined_text or 'policy' in combined_text:
            specific_details['government_policies'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for local hospitality mentions
        if 'hospitality' in combined_text or 'local hospitality' in combined_text:
            specific_details['local_hospitality'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for reign of terror mentions
        if 'reign of terror' in combined_text:
            specific_details['reign_of_terror'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
    
    return specific_details

# Execute the search - all functions are now defined above
print("Starting comprehensive search for Francia's Reign of Terror letters...")
search_results = search_francia_letters()

print(f"\nTotal search results collected: {len(search_results)}")

# Analyze results for relevance
relevant_results = analyze_search_results(search_results)

# Extract author information
author_candidates = extract_author_information(relevant_results)

# Analyze for specific details
specific_details = analyze_for_specific_details(relevant_results)

# Save all results to workspace
all_data = {
    'search_date': datetime.now().isoformat(),
    'search_summary': {
        'total_results': len(search_results),
        'relevant_results': len(relevant_results),
        'author_candidates': len(author_candidates)
    },
    'all_search_results': search_results,
    'relevant_results': relevant_results,
    'author_candidates': author_candidates,
    'specific_details': specific_details
}

output_file = 'workspace/francia_letters_search_results.json'
with open(output_file, 'w') as f:
    json.dump(all_data, f, indent=2)

print(f"\nAll search data saved to: {output_file}")

# Display top relevant results
print("\n=== TOP RELEVANT RESULTS ===")
for i, result in enumerate(relevant_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result.get('title', 'No title')}")
    print(f"URL: {result.get('href', 'No URL')}")
    print(f"Keywords found: {', '.join(result['keyword_matches'])}")
    print(f"Description: {result.get('body', 'No description')[:400]}...")

# Display author candidates
print("\n=== POTENTIAL AUTHOR INFORMATION ===")
for i, candidate in enumerate(author_candidates[:5], 1):
    print(f"\nAuthor Candidate {i}:")
    print(f"Source: {candidate['source_title']}")
    print(f"URL: {candidate['source_url']}")
    print(f"Indicator: {candidate['indicator']}")
    print(f"Potential Authors: {', '.join(candidate['potential_authors']) if candidate['potential_authors'] else 'None identified'}")
    if candidate['google_books_authors']:
        print(f"Google Books Authors: {', '.join(candidate['google_books_authors'])}")
    print(f"Context: {candidate['context'][:300]}...")

# Display specific details found
print("\n=== SPECIFIC DETAILS ANALYSIS ===")
for detail_type, details in specific_details.items():
    if details:
        print(f"\n{detail_type.replace('_', ' ').title()}: {len(details)} mentions found")
        for detail in details[:2]:  # Show first 2 mentions
            print(f"  - Source: {detail['source']}")
            print(f"    Context: {detail['context'][:200]}...")

print(f"\n=== SEARCH SUMMARY ===")
print(f"Total queries executed: 13 (10 web searches + 3 Wikipedia + 3 Google Books)")
print(f"Total results found: {len(search_results)}")
print(f"Relevant results: {len(relevant_results)}")
print(f"Author candidates identified: {len(author_candidates)}")
print(f"\nSearch completed successfully!")
```