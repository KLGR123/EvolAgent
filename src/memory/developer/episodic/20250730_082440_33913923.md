### Development Step 1: Extract 2018 Chinstrap Penguin Population Data and Source Context from Wikipedia

**Description**: Search for and access the Wikipedia page about penguin species populations to locate the specific chinstrap penguin population data as of the end of 2018. Extract the population figure, noting whether it's given in breeding pairs or individual penguins, and document the exact source and context of this information.

**Use Cases**:
- Wildlife conservation NGO compiling chinstrap penguin population figures for grant proposals to track species decline
- Academic ecologist automating extraction of penguin breeding pair counts to feed into population viability models in R
- Journalistic data analyst gathering 2018 population stats for a feature article on climate change impacts in Antarctica
- Environmental consultancy sourcing exact Wikipedia-based penguin metrics for inclusion in an ecological impact assessment report
- University data-science lecturer demonstrating web scraping and JSON processing workflows to students using real-world wildlife data
- Software engineer building a wildlife fact API that retrieves up-to-date penguin population numbers for a mobile education app
- Government research office integrating automated Wikipedia searches of species data to maintain a national biodiversity policy dashboard
- NGO communications team pulling authoritative population citations and context from Wikipedia to update their fundraising campaign webpage

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time

print("=== CHINSTRAP PENGUIN POPULATION RESEARCH - STEP 1 ===\n")
print("Objective: Search Wikipedia for chinstrap penguin population data as of end of 2018")
print("Focus: Find specific population figures with breeding pairs or individual counts\n")

# Get SerpAPI key from environment variables
api_key = os.getenv("SERPAPI_API_KEY")

if api_key is None:
    print("Error: Missing API key. Make sure you have SERPAPI_API_KEY in your environment variables.")
    exit()

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Search for chinstrap penguin population information on Wikipedia
queries_to_try = [
    "chinstrap penguin population 2018 site:en.wikipedia.org",
    "chinstrap penguin breeding pairs population site:en.wikipedia.org", 
    "penguin species population chinstrap site:en.wikipedia.org",
    "chinstrap penguin conservation status population site:en.wikipedia.org"
]

all_search_results = []

print("Searching for chinstrap penguin population data...\n")

for query_idx, query in enumerate(queries_to_try, 1):
    print(f"Search {query_idx}: {query}")
    
    # Prepare API request parameters
    params = {
        "q": query,
        "api_key": api_key,
        "engine": "google",
        "google_domain": "google.com",
        "safe": "off",
        "num": 10,
        "type": "search"
    }
    
    try:
        # Make API request to SerpAPI
        response = requests.get("https://serpapi.com/search.json", params=params)
        
        if response.status_code == 200:
            results = response.json()
            
            if results.get("organic_results"):
                search_results = results["organic_results"]
                print(f"  Found {len(search_results)} results")
                
                # Store results with query context
                query_results = {
                    'query': query,
                    'results_count': len(search_results),
                    'results': search_results
                }
                all_search_results.append(query_results)
                
                # Show top results
                for i, result in enumerate(search_results[:3], 1):
                    title = result.get('title', 'No title')
                    link = result.get('link', 'No link')
                    snippet = result.get('snippet', 'No snippet')[:100] + "..."
                    print(f"    {i}. {title}")
                    print(f"       URL: {link}")
                    print(f"       Snippet: {snippet}\n")
            else:
                print(f"  No results found for this query\n")
        else:
            print(f"  Error: API request failed with status {response.status_code}\n")
            
    except Exception as e:
        print(f"  Error during search: {str(e)}\n")
    
    time.sleep(1)  # Be respectful to the API

# Save all search results
search_data = {
    'research_objective': 'Find chinstrap penguin population data as of end 2018',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'queries_executed': len(queries_to_try),
    'total_results_found': sum(len(q.get('results', [])) for q in all_search_results),
    'search_results_by_query': all_search_results
}

with open('workspace/chinstrap_penguin_search_results.json', 'w') as f:
    json.dump(search_data, f, indent=2)

print(f"\n=== SEARCH RESULTS SUMMARY ===\n")
print(f"Total queries executed: {search_data['queries_executed']}")
print(f"Total results found: {search_data['total_results_found']}")
print(f"Search results saved to: workspace/chinstrap_penguin_search_results.json")

# Identify the most promising Wikipedia pages to fetch
wikipedia_pages = []
for query_result in all_search_results:
    for result in query_result['results']:
        link = result.get('link', '')
        if 'en.wikipedia.org' in link and 'chinstrap' in link.lower():
            wikipedia_pages.append({
                'title': result.get('title'),
                'url': link,
                'snippet': result.get('snippet'),
                'from_query': query_result['query']
            })

# Remove duplicates based on URL
unique_pages = []
seen_urls = set()
for page in wikipedia_pages:
    if page['url'] not in seen_urls:
        unique_pages.append(page)
        seen_urls.add(page['url'])

print(f"\nPromising Wikipedia pages identified: {len(unique_pages)}")
for i, page in enumerate(unique_pages, 1):
    print(f"  {i}. {page['title']}")
    print(f"     URL: {page['url']}")
    print(f"     From query: {page['from_query']}\n")

if unique_pages:
    print("\nNext step: Fetch and parse the most relevant Wikipedia pages")
    
    # Save the identified pages for the next step
    pages_to_fetch = {
        'target_pages': unique_pages,
        'research_focus': 'chinstrap penguin population data end of 2018',
        'extraction_targets': ['population figures', 'breeding pairs', '2018 data', 'conservation status'],
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('workspace/wikipedia_pages_to_fetch.json', 'w') as f:
        json.dump(pages_to_fetch, f, indent=2)
    
    print(f"Wikipedia pages to fetch saved to: workspace/wikipedia_pages_to_fetch.json")
else:
    print("\nNo specific chinstrap penguin Wikipedia pages found.")
    print("Will try broader penguin species or population searches.")
```