### Development Step 2: Identifying Common Two-Word Customer Retention Model in 2018-2019 Studies by Kashyap and Fader

**Description**: Conduct a focused academic literature search for research papers on customer retention studies published during 2018-2019 by authors Manash Pratim Kashyap and PS Fader. Use academic search engines like Google Scholar, ResearchGate, or academic databases. Focus the search on terms like 'Manash Pratim Kashyap customer retention 2018-2019' and 'PS Fader customer retention 2018-2019'. Extract information about the models used in their respective studies, looking specifically for any common two-word model type mentioned in both authors' works.

**Use Cases**:
- Telecom marketing analytics team conducting a competitive churn-rate study by extracting Pareto/NBD model details from 2018–2019 retention papers by Kashyap and Fader
- SaaS pricing manager automating retrieval of customer lifetime value models to fine-tune subscription tiers based on Fader’s and Kashyap’s two-word model references
- PhD candidate in marketing science gathering focused literature on customer retention frameworks for a dissertation chapter on predictive analytics
- Data consulting firm streamlining proposal development by programmatically summarizing retention model methodologies from leading academics’ 2018–2019 publications
- Business intelligence platform integrating scheduled academic searches to update clients on the latest two-word retention models used across industries
- University professor curating a graduate-level syllabus by extracting and comparing “Pareto NBD” and “Beta-Geometric” model use cases from Fader’s and Kashyap’s studies
- Fintech competitive intelligence team analyzing retail banking churn-prevention techniques by mapping relevant retention models published in 2018–2019
- E-commerce growth squad adapting telecom churn-model insights from academic papers to forecast repeat-purchase behavior in online retail stores

```
import requests
import os
import re
import json
import time
from bs4 import BeautifulSoup
from urllib.parse import quote

# Define the workspace directory
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

print("Starting academic literature search for customer retention studies...")

# Function to perform searches with exponential backoff
def search_with_backoff(url, params=None, headers=None, max_retries=5):
    headers = headers or {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(max_retries):
        try:
            print(f"Attempt {attempt + 1} for URL: {url}")
            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            wait_time = 2 ** attempt
            if attempt < max_retries - 1:
                print(f"Error: {e}. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Failed after {max_retries} attempts: {e}")
                return None

# Function to search Google Scholar (simulated - we'll use a hybrid approach)
def search_google_scholar(query):
    print(f"\nSimulating Google Scholar search for: {query}")
    
    # In a real implementation
```