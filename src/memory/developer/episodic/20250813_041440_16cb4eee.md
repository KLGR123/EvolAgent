### Development Step 1: Identify author of third volume 'Francia’s Reign of Terror' letters under Paraguay’s Francia

**Description**: Search for information about a collection of letters titled 'Francia's Reign of Terror' that documents a four-year stay under Dictator Francia's rule in Paraguay. Focus on identifying the author of this work, which is described as the third volume that criticizes local laziness and government policies and was written while relying on local hospitality. Search using keywords including 'Francia's Reign of Terror letters Paraguay dictator', 'four year stay Francia Paraguay author', and 'third volume Francia Paraguay criticism government policies'.

**Use Cases**:
- Academic historian conducting primary source mining and author attribution for a study on Paraguay’s early 19th-century dictatorship
- University librarian automating metadata extraction and cataloging for a rare correspondence collection on Dictator Francia
- Publishing house editorial team verifying the author identity and publication lineage before reissuing the third volume of Francia’s letters
- Genealogical researcher tracing family ties through personal letters written during a four-year stay under Francia’s regime
- Investigative journalist fact-checking the provenance and authorship of leaked correspondence about Paraguayan political history
- Digital humanities project automating extraction of author metadata from fragmented web search results to build a correspondence network
- Legal historian gathering eyewitness accounts and author backgrounds for human rights cases under Francia’s reign of terror
- Museum curator sourcing and verifying original letters and author information for a Latin American dictatorship exhibit

```
from ddgs import DDGS
import json
import os
from datetime import datetime

print("Searching for information about 'Francia's Reign of Terror' letters collection...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

def search_francia_letters():
    """Search for information about Francia's Reign of Terror letters"""
    print("\n=== SEARCHING FOR FRANCIA'S REIGN OF TERROR LETTERS ===")
    
    # Define search queries with different keyword combinations
    search_queries = [
        "Francia's Reign of Terror letters Paraguay dictator",
        "four year stay Francia Paraguay author",
        "third volume Francia Paraguay criticism government policies",
        "Francia Paraguay dictator letters collection",
        "Paraguay Francia reign terror author four years",
        "Francia dictator Paraguay third volume letters",
        "Paraguay Francia government criticism letters",
        "Francia Paraguay local hospitality author letters"
    ]
    
    # Search parameters
    max_results = 15
    backend = ["google", "duckduckgo", "yandex", "brave", "bing", "yahoo", "mojeek"]
    
    all_search_results = []
    
    searcher = DDGS(timeout=10)
    
    for i, query in enumerate(search_queries, 1):
        print(f"\nSearch {i}/{len(search_queries)}: {query}")
        
        try:
            results = searcher.text(
                query, 
                max_results=max_results, 
                page=1, 
                backend=backend, 
                safesearch="off", 
                region="en-us"
            )
            
            if results == []:
                print(f"No results found for query: {query}")
            else:
                print(f"Found {len(results)} results for query: {query}")
                
                # Add query info to each result
                for result in results:
                    result['search_query'] = query
                    result['search_index'] = i
                
                all_search_results.extend(results)
                
                # Display first few results for this query
                for j, result in enumerate(results[:3], 1):
                    print(f"  Result {j}: {result.get('title', 'No title')}")
                    print(f"    URL: {result.get('href', 'No URL')}")
                    print(f"    Snippet: {result.get('body', 'No description')[:200]}...")
                    print()
        
        except Exception as e:
            print(f"Error searching for query '{query}': {str(e)}")
    
    return all_search_results

def analyze_search_results(results):
    """Analyze search results for relevant information about Francia's letters"""
    print(f"\n=== ANALYZING {len(results)} TOTAL SEARCH RESULTS ===")
    
    # Keywords to look for in results
    relevant_keywords = [
        'francia',
        'paraguay',
        'dictator',
        'letters',
        'reign of terror',
        'four year',
        'four-year',
        'third volume',
        'author',
        'criticism',
        'government policies',
        'local hospitality',
        'laziness',
        'stay',
        'collection'
    ]
    
    relevant_results = []
    
    for result in results:
        title = result.get('title', '').lower()
        body = result.get('body', '').lower()
        combined_text = title + ' ' + body
        
        # Count keyword matches
        keyword_matches = []
        for keyword in relevant_keywords:
            if keyword in combined_text:
                keyword_matches.append(keyword)
        
        # Consider result relevant if it has multiple keyword matches
        if len(keyword_matches) >= 2:
            result['keyword_matches'] = keyword_matches
            result['relevance_score'] = len(keyword_matches)
            relevant_results.append(result)
    
    # Sort by relevance score
    relevant_results.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    print(f"Found {len(relevant_results)} highly relevant results")
    
    return relevant_results

def extract_author_information(results):
    """Extract potential author information from search results"""
    print("\n=== EXTRACTING AUTHOR INFORMATION ===")
    
    author_candidates = []
    
    for result in results:
        title = result.get('title', '')
        body = result.get('body', '')
        combined_text = title + ' ' + body
        
        # Look for author-related patterns
        author_indicators = [
            'written by',
            'author',
            'by ',
            'memoir',
            'account',
            'letters',
            'correspondence',
            'documented by',
            'recorded by'
        ]
        
        for indicator in author_indicators:
            if indicator in combined_text.lower():
                # Extract surrounding text that might contain author name
                pos = combined_text.lower().find(indicator)
                if pos != -1:
                    # Get text around the indicator
                    start = max(0, pos - 100)
                    end = min(len(combined_text), pos + 200)
                    context = combined_text[start:end]
                    
                    author_candidates.append({
                        'source_title': title,
                        'source_url': result.get('href', ''),
                        'indicator': indicator,
                        'context': context,
                        'relevance_score': result.get('relevance_score', 0)
                    })
    
    return author_candidates

# Execute the search
print("Starting comprehensive search for Francia's Reign of Terror letters...")
search_results = search_francia_letters()

print(f"\nTotal search results collected: {len(search_results)}")

# Analyze results for relevance
relevant_results = analyze_search_results(search_results)

# Extract author information
author_candidates = extract_author_information(relevant_results)

# Save all results to workspace
all_data = {
    'search_date': datetime.now().isoformat(),
    'search_summary': {
        'total_results': len(search_results),
        'relevant_results': len(relevant_results),
        'author_candidates': len(author_candidates)
    },
    'all_search_results': search_results,
    'relevant_results': relevant_results,
    'author_candidates': author_candidates
}

output_file = 'workspace/francia_letters_search_results.json'
with open(output_file, 'w') as f:
    json.dump(all_data, f, indent=2)

print(f"\nAll search data saved to: {output_file}")

# Display top relevant results
print("\n=== TOP RELEVANT RESULTS ===")
for i, result in enumerate(relevant_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result.get('title', 'No title')}")
    print(f"URL: {result.get('href', 'No URL')}")
    print(f"Keywords found: {', '.join(result['keyword_matches'])}")
    print(f"Description: {result.get('body', 'No description')[:300]}...")

# Display author candidates
print("\n=== POTENTIAL AUTHOR INFORMATION ===")
for i, candidate in enumerate(author_candidates[:5], 1):
    print(f"\nAuthor Candidate {i}:")
    print(f"Source: {candidate['source_title']}")
    print(f"URL: {candidate['source_url']}")
    print(f"Indicator: {candidate['indicator']}")
    print(f"Context: {candidate['context'][:200]}...")

print(f"\n=== SEARCH SUMMARY ===")
print(f"Total queries executed: 8")
print(f"Total results found: {len(search_results)}")
print(f"Relevant results: {len(relevant_results)}")
print(f"Author candidates identified: {len(author_candidates)}")
print(f"\nSearch completed successfully!")
```