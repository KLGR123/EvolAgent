### Development Step 49: 2011 Bulgarian NSI Census: Tertiary Education Completion by Gender

**Description**: Search for the official 2011 Bulgarian census data from the Bulgarian National Statistical Institute (NSI) or other authoritative government sources that specifically contain the tertiary education completion numbers by gender. Focus on finding the exact counts or percentages of men and women who completed tertiary education according to the 2011 census results.

**Use Cases**:
- Government education policy planning and funding allocation based on regional male vs. female tertiary completion rates from the 2011 Bulgarian census
- Academic sociological research into historical gender disparities in higher education participation across Bulgarian provinces
- NGO grant writing and program design for gender equality initiatives using exact male/female university graduation figures
- Data journalism and interactive web stories visualizing the 2011 census tertiary education gap between men and women
- HR consultancy market analysis benchmarking workforce skill levels by gender to advise multinational firms entering Bulgaria
- Public data portal automation that ingests NSI PDF/HTML census files to power real-time dashboards on educational attainment
- University ranking and accreditation reporting comparing institutional demographics against national male/female degree completion statistics
- Mobile or web app development integrating historical gendered education data to provide learners context on local tertiary education trends

```
import os
import json
import requests
import time
from bs4 import BeautifulSoup

workspace_dir = 'workspace'

print("=== ACCESSING 2011 BULGARIAN CENSUS TERTIARY EDUCATION DATA FILES ===")
print("\nObjective: Download and extract official 2011 Bulgarian census tertiary education completion statistics by gender\n")

# First, let's inspect the findings file to understand what was discovered
findings_file = os.path.join(workspace_dir, 'tertiary_education_gender_findings.json')

if os.path.exists(findings_file):
    print("--- Loading Previous Findings ---")
    with open(findings_file, 'r', encoding='utf-8') as f:
        findings_content = f.read()
    
    print(f"Findings file size: {len(findings_content):,} characters")
    
    # Parse JSON safely
    try:
        findings_data = json.loads(findings_content)
        print("\nFindings structure:")
        print(f"Top-level keys: {list(findings_data.keys())}")
        
        if 'findings' in findings_data:
            findings_summary = findings_data['findings']
            print(f"\nFindings summary: {findings_summary}")
        
        if 'key_phrases' in findings_data:
            key_phrases = findings_data['key_phrases']
            print(f"\nKey phrases found: {key_phrases}")
        
        if 'data_files' in findings_data:
            data_files = findings_data['data_files']
            print(f"\nData files identified: {len(data_files)}")
            for i, file_info in enumerate(data_files, 1):
                print(f"  {i}. {file_info.get('text', 'Unknown')} ({file_info.get('file_type', 'unknown')})")
                print(f"     URL: {file_info.get('full_url', 'No URL')}")
        
    except json.JSONDecodeError as e:
        print(f"Error parsing findings file: {e}")
        findings_data = None
else:
    print("Findings file not found. Will proceed with direct search.")
    findings_data = None

# The previous analysis found Census 2021 files, but we need Census 2011 data
# Let's search for specific 2011 census education data URLs
print("\n=== SEARCHING FOR 2011 CENSUS EDUCATION DATA ===")

# Known NSI URLs for 2011 census education data
census_2011_urls = [
    'https://www.nsi.bg/census2011/PDOCS2/Census2011final_en.pdf',
    'https://www.nsi.bg/census2011/PDOCS2/Census2011_Education_en.pdf',
    'https://www.nsi.bg/census2011/PDOCS2/Census2011_Population_Education.pdf',
    'https://www.nsi.bg/en/content/6710/population-education',
    'https://www.nsi.bg/en/content/6705/education',
    'https://www.nsi.bg/census2011/PDOCS2/Census2011_Demographic_en.pdf',
    'https://www.nsi.bg/census2011/PDOCS2/Census2011_Tables_en.pdf'
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

successful_downloads = []
failed_downloads = []

for url in census_2011_urls:
    print(f"\nTrying to access: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=30)
        print(f"Response status: {response.status_code}")
        
        if response.status_code == 200:
            print(f"✓ Successfully accessed {url}")
            print(f"Content length: {len(response.content):,} bytes")
            
            # Determine file type and save appropriately
            if url.endswith('.pdf'):
                filename = f"census_2011_{url.split('/')[-1]}"
                filepath = os.path.join(workspace_dir, filename)
                
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                print(f"PDF saved to: {filepath}")
                
                successful_downloads.append({
                    'url': url,
                    'filename': filename,
                    'filepath': filepath,
                    'size': len(response.content),
                    'type': 'pdf'
                })
                
            else:
                # HTML content
                filename = f"census_2011_{url.split('/')[-1]}.html"
                filepath = os.path.join(workspace_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                print(f"HTML saved to: {filepath}")
                
                # Quick analysis of HTML content
                soup = BeautifulSoup(response.text, 'html.parser')
                content_text = soup.get_text().lower()
                
                # Check for relevant content
                tertiary_count = content_text.count('tertiary')
                education_count = content_text.count('education')
                male_count = content_text.count('male')
                female_count = content_text.count('female')
                census_2011_count = content_text.count('2011')
                
                print(f"Content analysis:")
                print(f"  Tertiary: {tertiary_count}, Education: {education_count}")
                print(f"  Male: {male_count}, Female: {female_count}, 2011: {census_2011_count}")
                
                # Look for download links to data files
                links = soup.find_all('a', href=True)
                data_file_links = []
                
                for link in links:
                    link_href = link.get('href', '')
                    link_text = link.get_text().strip()
                    
                    # Check for data files
                    if any(ext in link_href.lower() for ext in ['.pdf', '.xls', '.xlsx', '.csv']):
                        if any(term in link_text.lower() or term in link_href.lower() for term in ['education', '2011', 'census', 'tertiary']):
                            full_url = link_href if link_href.startswith('http') else f"https://www.nsi.bg{link_href}"
                            data_file_links.append({
                                'text': link_text,
                                'url': full_url
                            })
                
                if data_file_links:
                    print(f"  Found {len(data_file_links)} data file links:")
                    for i, link_info in enumerate(data_file_links[:3], 1):
                        print(f"    {i}. '{link_info['text']}'")
                        print(f"       {link_info['url']}")
                
                successful_downloads.append({
                    'url': url,
                    'filename': filename,
                    'filepath': filepath,
                    'size': len(response.text),
                    'type': 'html',
                    'content_analysis': {
                        'tertiary_count': tertiary_count,
                        'education_count': education_count,
                        'male_count': male_count,
                        'female_count': female_count,
                        'census_2011_count': census_2011_count
                    },
                    'data_file_links': data_file_links
                })
                
        else:
            print(f"✗ Failed to access {url} - Status: {response.status_code}")
            failed_downloads.append({
                'url': url,
                'status_code': response.status_code
            })
            
    except Exception as e:
        print(f"✗ Error accessing {url}: {str(e)}")
        failed_downloads.append({
            'url': url,
            'error': str(e)
        })
    
    time.sleep(2)  # Be respectful to the server

print(f"\n=== DOWNLOAD SUMMARY ===")
print(f"Successful downloads: {len(successful_downloads)}")
print(f"Failed downloads: {len(failed_downloads)}")

if successful_downloads:
    print("\n--- Successfully Downloaded Files ---")
    for i, download in enumerate(successful_downloads, 1):
        print(f"\n{i}. {download['filename']} ({download['type'].upper()})")
        print(f"   Source: {download['url']}")
        print(f"   Size: {download['size']:,} bytes")
        print(f"   Saved to: {download['filepath']}")
        
        if download['type'] == 'html' and 'content_analysis' in download:
            analysis = download['content_analysis']
            print(f"   Content: Tertiary({analysis['tertiary_count']}), Education({analysis['education_count']}), Male({analysis['male_count']}), Female({analysis['female_count']}), 2011({analysis['census_2011_count']})")
            
            if download.get('data_file_links'):
                print(f"   Contains {len(download['data_file_links'])} data file links")

# Now let's examine any PDF files we downloaded for tertiary education data
print("\n=== EXAMINING DOWNLOADED FILES FOR TERTIARY EDUCATION DATA ===")

pdf_files = [d for d in successful_downloads if d['type'] == 'pdf']
html_files = [d for d in successful_downloads if d['type'] == 'html']

print(f"PDF files downloaded: {len(pdf_files)}")
print(f"HTML files downloaded: {len(html_files)}")

# For HTML files, look for specific tertiary education content
for html_file in html_files:
    print(f"\n--- Analyzing {html_file['filename']} ---")
    
    with open(html_file['filepath'], 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    content_text = soup.get_text()
    
    # Search for specific patterns related to tertiary education by gender
    tertiary_patterns = [
        r'tertiary education.*?(?:male|female|men|women).*?\d+[,.]?\d*',
        r'(?:male|female|men|women).*?tertiary education.*?\d+[,.]?\d*',
        r'higher education.*?(?:male|female|men|women).*?\d+[,.]?\d*',
        r'university.*?graduates.*?(?:male|female|men|women).*?\d+[,.]?\d*',
        r'bachelor.*?degree.*?(?:male|female|men|women).*?\d+[,.]?\d*',
        r'completed.*?tertiary.*?(?:male|female|men|women).*?\d+[,.]?\d*'
    ]
    
    found_patterns = []
    for pattern in tertiary_patterns:
        matches = re.findall(pattern, content_text, re.IGNORECASE | re.DOTALL)
        for match in matches[:3]:  # Limit to 3 per pattern
            clean_match = re.sub(r'\s+', ' ', match.strip())
            if len(clean_match) > 20 and clean_match not in found_patterns:
                found_patterns.append(clean_match[:200])  # Limit length
    
    if found_patterns:
        print(f"Found {len(found_patterns)} tertiary education patterns:")
        for i, pattern in enumerate(found_patterns, 1):
            print(f"  {i}. {pattern}")
    else:
        print("No specific tertiary education patterns found.")
    
    # Look for tables with education data
    tables = soup.find_all('table')
    education_tables = []
    
    for i, table in enumerate(tables):
        table_text = table.get_text().lower()
        
        has_education = any(term in table_text for term in ['education', 'tertiary', 'university', 'degree'])
        has_gender = any(term in table_text for term in ['male', 'female', 'men', 'women', 'sex'])
        has_numbers = bool(re.search(r'\d+[,.]?\d*', table_text))
        
        if has_education and (has_gender or has_numbers):
            education_tables.append({
                'index': i,
                'has_education': has_education,
                'has_gender': has_gender,
                'has_numbers': has_numbers,
                'text_sample': table_text[:300]
            })
    
    if education_tables:
        print(f"Found {len(education_tables)} education-related tables:")
        for table_info in education_tables:
            print(f"  Table {table_info['index']}: Education={table_info['has_education']}, Gender={table_info['has_gender']}, Numbers={table_info['has_numbers']}")
            print(f"    Sample: {table_info['text_sample'][:100]}...")

# Save comprehensive results
results_summary = {
    'objective': '2011 Bulgarian Census - Tertiary Education by Gender Data Access',
    'access_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'successful_downloads': len(successful_downloads),
    'failed_downloads': len(failed_downloads),
    'downloaded_files': successful_downloads,
    'failed_attempts': failed_downloads,
    'summary': {
        'pdf_files': len(pdf_files),
        'html_files': len(html_files),
        'total_size': sum(d['size'] for d in successful_downloads)
    },
    'next_steps': [
        'Parse PDF files for detailed census tables',
        'Access data file links found in HTML pages',
        'Search for specific NSI statistical yearbooks with 2011 census education data',
        'Contact NSI directly for detailed tertiary education statistics by gender'
    ]
}

results_file = os.path.join(workspace_dir, 'census_2011_data_access_results.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(results_summary, f, indent=2, ensure_ascii=False)

print(f"\n=== FINAL SUMMARY ===")
print(f"Results saved to: {results_file}")
print(f"\nAccess Results:")
print(f"- Successfully downloaded: {len(successful_downloads)} files")
print(f"- Total size: {sum(d['size'] for d in successful_downloads):,} bytes")
print(f"- PDF files: {len(pdf_files)}")
print(f"- HTML files: {len(html_files)}")

if successful_downloads:
    print(f"\n*** PROGRESS UPDATE ***")
    print(f"Successfully accessed official NSI sources for 2011 Bulgarian census data.")
    
    # Check if we found any files with promising content
    promising_files = []
    for download in successful_downloads:
        if download['type'] == 'html':
            analysis = download.get('content_analysis', {})
            if (analysis.get('tertiary_count', 0) > 0 or 
                analysis.get('education_count', 0) > 5 or 
                len(download.get('data_file_links', [])) > 0):
                promising_files.append(download)
        elif download['type'] == 'pdf' and download['size'] > 100000:  # Large PDFs likely contain detailed data
            promising_files.append(download)
    
    if promising_files:
        print(f"\nMost promising files for tertiary education data:")
        for i, file_info in enumerate(promising_files, 1):
            print(f"  {i}. {file_info['filename']} ({file_info['size']:,} bytes)")
            if file_info['type'] == 'html' and file_info.get('data_file_links'):
                print(f"     Contains {len(file_info['data_file_links'])} data file links")
    
    print(f"\nThe analysis has successfully accessed official NSI census sources and identified")
    print(f"potential files containing 2011 Bulgarian census tertiary education data by gender.")
    print(f"Ready for detailed parsing of the downloaded files to extract specific statistics.")
else:
    print(f"\nNo files were successfully downloaded. May need to try alternative approaches.")

print(f"\n=== DATA ACCESS COMPLETE ===")
```