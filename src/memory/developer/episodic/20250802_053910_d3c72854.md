### Development Step 20: Download PDF: 'Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?' University of Leicester

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated retrieval of marine biology research papers (PDFs) on fish-based diets for ecological meta-analysis
- Scheduled download of quarterly SEC-filed financial statements (PDFs) for algorithmic investment backtesting
- Bulk acquisition of clinical trial protocol PDFs from NIH archives for pharmaceutical data aggregation
- Automated scraping of environmental impact assessment PDFs from government portals for sustainability consulting
- Systematic collection of renewable energy patent filings (PDF format) for R&D patent landscaping and freedom-to-operate analyses
- Periodic download of ISO compliance and safety standard PDFs for manufacturing quality-assurance workflows
- Automated harvesting of educational curriculum guideline PDFs from university sites for e-learning platform content updates

```
import os
import sys
import re
import requests

# Step 1: Ensure workspace directory exists
workspace = 'workspace'
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    sys.exit(1)
print(f"[INFO] Using workspace: {workspace}\n")

# Step 2: Perform a live Bing search for the target paper PDF on le.ac.uk
search_url = 'https://www.bing.com/search'
query = 'site:le.ac.uk "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" filetype:pdf'
params = {'q': query}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept': 'text/html'
}
print(f"[SEARCH] Querying Bing: {query}")
try:
    resp = requests.get(search_url, params=params, headers=headers, timeout=30)
    resp.raise_for_status()
    html = resp.text
    print(f"[SEARCH] Received {len(html)} characters of HTML from Bing.")
except Exception as e:
    print(f"ERROR: Failed to fetch Bing search results: {e}")
    sys.exit(1)

# Step 3: Extract all .pdf URLs from the HTML using a properly quoted regex
print("[PARSE] Extracting .pdf links from search results...")
pattern = r"https?://[^\s\"'>]+\.pdf"  # match http(s) URLs ending in .pdf, stopping at space, ' or \" or >
all_pdfs = re.findall(pattern, html)
print(f"[PARSE] Found {len(all_pdfs)} total .pdf URLs in the HTML.")

# Step 4: Filter for le.ac.uk domain
leicester_pdfs = [link for link in set(all_pdfs) if 'le.ac.uk' in link.lower()]
print(f"[FILTER] {len(leicester_pdfs)} PDF links on le.ac.uk domain found.")
for link in leicester_pdfs:
    print(f"  [MATCH] {link}")

if not leicester_pdfs:
    print("ERROR: No .pdf links on le.ac.uk found in Bing results.")
    print("Please verify that the paper is hosted on le.ac.uk or adjust the query.")
    sys.exit(1)

# Step 5: Download the first matching PDF
pdf_url = leicester_pdfs[0]
pdf_path = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')
print(f"[DOWNLOAD] Downloading PDF from: {pdf_url}")
try:
    rpdf = requests.get(pdf_url, headers=headers, stream=True, timeout=60)
    rpdf.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in rpdf.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
    size = os.path.getsize(pdf_path)
    print(f"[SUCCESS] Saved PDF to: {pdf_path}")
    print(f"[INFO] File size: {size} bytes ({size/1024:.2f} KB)")
    if size < 10 * 1024:
        print("WARNING: PDF file is smaller than 10 KB. It may not be the correct paper.")
        sys.exit(1)
    print("[COMPLETE] PDF download appears successful and exceeds size threshold.")
except Exception as e:
    print(f"ERROR: Failed to download PDF: {e}")
    if os.path.exists(pdf_path):
        os.remove(pdf_path)
    sys.exit(1)
```