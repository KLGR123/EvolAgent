### Development Step 23: Download University of Leicester’s “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated meta-analysis in marine biology: automatically retrieve research PDFs on specific fish-nutrition studies by exact title for consolidating data into a central database
- Legal research team workflow: fetch landmark case documents in PDF by case name for building a litigation prep library
- Financial due diligence automation: download corporate annual and quarterly report PDFs by report title directly from company websites for real-time investment modeling
- Engineering maintenance planning: retrieve equipment operation and maintenance manuals in PDF by model number for integration into a computerized maintenance management system
- Academic library acquisition: batch-download requested journal articles in PDF by precise title and archive them in the institutional repository for course reserves
- Regulatory compliance monitoring: automatically pull newly released environmental regulation PDFs by regulation title for compliance audit checklists
- Product marketing competitive analysis: fetch competitor product catalog PDFs by product line name for side-by-side feature and pricing comparisons
- Investigative journalism research: retrieve public records and leaked report PDFs by document title via search engines for deep-dive reporting projects

```
import os
import sys
import re
import requests

def ensure_workspace():
    workspace = 'workspace'
    if not os.path.isdir(workspace):
        print(f"ERROR: Workspace directory '{workspace}' does not exist.")
        sys.exit(1)
    return workspace

# Utility: download a PDF from a URL into workspace
def download_pdf(pdf_url, dest_path):
    print(f"[DOWNLOAD] Attempting download from: {pdf_url}")
    try:
        r = requests.get(pdf_url, headers=HEADERS, stream=True, timeout=60)
        r.raise_for_status()
        with open(dest_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        size = os.path.getsize(dest_path)
        print(f"[SUCCESS] Saved PDF to: {dest_path} ({size} bytes, {size/1024:.2f} KB)")
        if size < 10 * 1024:
            print("WARNING: File smaller than 10 KB. It may not be the correct PDF.")
            sys.exit(1)
        print("[COMPLETE] PDF download successful and exceeds size threshold.")
        return True
    except Exception as e:
        print(f"ERROR: Failed to download PDF: {e}")
        if os.path.exists(dest_path):
            os.remove(dest_path)
        return False

# Search via Bing for filetype:pdf
def search_bing(title):
    print(f"[PHASE] Bing search for: \"{title}\" filetype:pdf")
    resp = requests.get(
        'https://www.bing.com/search',
        params={'q': f'"{title}" filetype:pdf'},
        headers=HEADERS,
        timeout=30
    )
    resp.raise_for_status()
    html = resp.text
    print(f"[SEARCH] Retrieved {len(html)} chars of HTML from Bing.")
    return html

# Search via DuckDuckGo HTML for filetype:pdf
def search_ddg(title):
    print(f"[PHASE] DuckDuckGo search for: \"{title}\" filetype:pdf")
    resp = requests.get(
        'https://duckduckgo.com/html/',
        data={'q': f'"{title}" filetype:pdf'},
        headers=HEADERS,
        timeout=30
    )
    resp.raise_for_status()
    html = resp.text
    print(f"[SEARCH] Retrieved {len(html)} chars of HTML from DuckDuckGo.")
    return html

# Extract PDF URLs from HTML using robust regex
def extract_pdfs(html):
    pattern = r"https?://[^\s\"'<>()]+\.pdf(?:\?[^\s\"'<>()]*)?"
    matches = re.findall(pattern, html, flags=re.IGNORECASE)
    unique = list(dict.fromkeys(matches))  # dedupe
    print(f"[PARSE] Extracted {len(unique)} unique PDF URLs.")
    for i, url in enumerate(unique[:20], 1):
        print(f"  {i}. {url}")
    return unique

# Main logic
if __name__ == '__main__':
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept': 'text/html'
    }

    workspace = ensure_workspace()
    title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
    dest_pdf = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

    # 1) Check for manual override via environment variable
    manual_url = os.getenv('DRAGON_PDF_URL')
    if manual_url:
        print(f"[OVERRIDE] Using DRAGON_PDF_URL: {manual_url}")
        if download_pdf(manual_url, dest_pdf):
            sys.exit(0)
        else:
            sys.exit(1)

    # 2) Phase 1: Bing search
    html_bing = search_bing(title)
    pdfs = extract_pdfs(html_bing)

    # 3) If none, Phase 2: DuckDuckGo search
    if not pdfs:
        html_ddg = search_ddg(title)
        pdfs = extract_pdfs(html_ddg)

    if not pdfs:
        print("ERROR: No PDF URLs found from both Bing and DuckDuckGo searches.")
        print("Please manually locate the PDF URL and set DRAGON_PDF_URL environment variable.")
        sys.exit(1)

    # 4) Filter candidates by keywords
    kws = ['hiccup', 'dragon', 'fish', 'le.ac.uk']
    candidates = [u for u in pdfs if any(k in u.lower() for k in kws)]
    if candidates:
        print(f"[FILTER] {len(candidates)} candidate(s) matched keywords {kws}")
        pdf_url = candidates[0]
    else:
        print("[FILTER] No keyword match; defaulting to first extracted PDF URL.")
        pdf_url = pdfs[0]

    print(f"[SELECT] PDF URL chosen: {pdf_url}\n")

    # 5) Download
    success = download_pdf(pdf_url, dest_pdf)
    sys.exit(0 if success else 1)

```