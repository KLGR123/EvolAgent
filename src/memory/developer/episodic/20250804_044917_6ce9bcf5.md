### Development Step 51: Extract Survivor (US) Seasons 1–44 Winners from Wikipedia and Save to JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Pop culture content platform automating the import of Survivor winners to generate “Where Are They Now?” profile pages for fans
- Market research team extracting historical Survivor champion data to analyze demographic trends and inform new reality TV show development
- Trivia mobile app pipeline auto-updating its question bank each season with Survivor winner names and seasons to keep quizzes fresh
- Streaming service recommendation engine correlating Survivor winner characteristics with viewer watch patterns to suggest personalized reality series
- Social media marketing tool tracking Survivor finale winners for real-time campaign triggers, hashtag performance, and audience engagement analysis
- Academic researcher compiling Survivor champion data for a gender representation study in reality television, complete with statistical charts and publication support
- Theme park digital signage system auto-refreshing “Survivor Hall of Fame” displays with the latest season winners and highlights
- E-commerce platform automating the creation of limited-edition Survivor champion merchandise listings (e.g., mugs, shirts) immediately after finale results are published

```
import os
import sys
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
wiki_url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {wiki_url}\n")
resp = requests.get(wiki_url, headers={
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)',
    'Accept-Language': 'en-US'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP {resp.status_code} OK\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML and find the correct winners table
soup = BeautifulSoup(resp.text, 'html.parser')
tables = soup.find_all('table')
print(f"Found {len(tables)} tables on the page. Searching for the winners table...\n")

winner_table = None
for idx, tbl in enumerate(tables, start=1):
    # Get header cells
    header = tbl.find('tr')
    if not header:
        continue
    cols = [th.get_text(strip=True).lower() for th in header.find_all(['th', 'td'])]
    # Look for 'season' and 'winner' in first two columns
    if len(cols) >= 2 and 'season' in cols[0] and 'winner' in cols[1]:
        winner_table = tbl
        print(f"→ Matched winners table at index {idx} (header: {cols[:3]})\n")
        break

if not winner_table:
    print("❌ Could not locate the winners table. The page structure may have changed.")
    sys.exit(1)

# 4) Parse rows: extract seasons 1–44 and winners
data = []
rows = winner_table.find_all('tr')[1:]  # skip header
for row in rows:
    cells = row.find_all(['td', 'th'])
    if len(cells) < 2:
        continue
    # Season number
    season_text = cells[0].get_text(strip=True).split()[0]  # take first token
    try:
        season_num = int(season_text)
    except:
        continue
    if season_num > 44:
        continue
    # Winner name
    winner_cell = cells[1]
    # Use link text if available, else full text
    link = winner_cell.find('a')
    winner_name = link.get_text(strip=True) if link else winner_cell.get_text(strip=True)
    data.append({
        'season': season_num,
        'winner': winner_name
    })
    print(f"Parsed Season {season_num}: {winner_name}")

if not data:
    print("⚠️ No winners data parsed. Check table parsing logic.")
    sys.exit(1)

# 5) Save to JSON
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print(f"\n✅ Successfully wrote {len(data)} entries to: {out_path}")
```