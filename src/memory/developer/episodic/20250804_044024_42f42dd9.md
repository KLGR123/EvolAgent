### Development Step 16: Extract Survivor US Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated content updates for an entertainment news website that populates its “Survivor Winners” section with the latest season champions without manual entry
- Data journalism trend analysis where reporters visualize winner demographics and victory patterns across seasons 1–44 for a feature on reality TV evolution
- Academic sociology research compiling Survivor winner data to study correlations between game format changes and contestant success rates
- Predictive modeling in sports betting platforms that integrate historical Survivor winner information to refine odds and forecast future champions
- Mobile TV fan app development that uses the JSON list to drive trivia quizzes and interactive timelines of Survivor winners
- Marketing automation for an entertainment newsletter service sending anniversary highlights of past Survivor champions to segmented subscriber lists
- Coding bootcamp or data science course demonstration teaching students real-world web scraping techniques and JSON data handling using the Survivor winners example
- Chatbot and voice-assistant integration enabling real-time answers to user queries like “Who won Survivor season 20?” by querying the extracted winners JSON file

```
import os
import sys
import requests
from bs4 import BeautifulSoup
import json

# 1) Locate the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable desktop version of the Survivor page
page_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={page_title}&printable=yes"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/114.0.5735.199 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching printable page:\n  URL: {url}\n  Headers: {headers}\n")
resp = requests.get(url, headers=headers)
resp.raise_for_status()
print(f"Page fetched successfully (status code: {resp.status_code})\n")

# 3) Save the printable HTML for inspection
html_file = os.path.join(workspace_dir, 'survivor_page_printable.html')
with open(html_file, 'w', encoding='utf-8') as f:
    f.write(resp.text)
print(f"Saved printable HTML to: {html_file}\n")

# 4) Parse the HTML and locate the correct table
soup = BeautifulSoup(resp.text, 'html.parser')
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} tables with class 'wikitable'.\n")

target_table = None
for idx, table in enumerate(tables, start=1):
    # Extract header texts from first row
    header_row = table.find('tr')
    headers_txt = [th.get_text(strip=True).lower() for th in header_row.find_all(['th','td'], recursive=False)]
    print(f"Table {idx} headers: {headers_txt}")
    # Check for both 'season' and 'winner'
    if 'season' in headers_txt and 'winner' in headers_txt:
        target_table = table
        print(f"→ Selected table {idx} for parsing (has both 'season' and 'winner').\n")
        break

if not target_table:
    print("❌ Could not find a table containing both 'Season' and 'Winner' columns. Exiting.")
    sys.exit(1)

# 5) Determine column indices
first_row = target_table.find('tr')
cols = [th.get_text(strip=True).lower() for th in first_row.find_all(['th','td'], recursive=False)]
season_idx = cols.index('season')
winner_idx = cols.index('winner')
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 6) Extract data rows (skip header)
winners = []
for row in target_table.find_all('tr')[1:]:
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # Clean footnotes
    for cell in (cells[season_idx], cells[winner_idx]):
        for sup in cell.find_all('sup'):
            sup.decompose()
    # Parse season number
    season_text = cells[season_idx].get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        continue
    if not (1 <= season_num <= 44):
        continue
    # Parse winner name
    winner_name = cells[winner_idx].get_text(strip=True)
    print(f"Parsed Season {season_num} -> Winner: {winner_name}")
    winners.append({ 'season': season_num, 'winner': winner_name })

# 7) Validate count and sort
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted for seasons 1–44: {len(winners_sorted)}")
if len(winners_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Please verify the table structure.\n")

# 8) Write output JSON
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as out:
    json.dump(winners_sorted, out, indent=2)
print(f"Saved winners list to: {out_path}")
```