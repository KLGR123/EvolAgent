### Development Step 2: Identify Transformer encoder layer count in “Attention Is All You Need” paper

**Description**: Search for the original 'Attention is All You Need' paper to identify the number of encoder layers in the Transformer architecture. Use these search approaches: (1) Search for the paper title 'Attention is All You Need' on Google Scholar or arXiv, (2) Look for the official paper PDF which should be available on arXiv, (3) Extract the architecture details specifically focusing on the encoder structure and number of layers/blocks mentioned in the model description.

**Use Cases**:
- Automating NLP research experiment setup by extracting the six encoder layers count from the original ‘Attention is All You Need’ paper and generating accurate Transformer configuration files
- Verifying model compliance in AI audits by programmatically retrieving published encoder block details to ensure production implementations match the six-layer architecture
- Populating an MLOps model registry with official Transformer specs (e.g., number of encoder layers) directly from the arXiv PDF to maintain up-to-date metadata
- Building an educational platform that auto-generates lecture slides and visualizations of Transformer architectures by pulling encoder/decoder layer counts from the source paper
- Configuring AutoML pipelines to instantiate the baseline Transformer model using the exact six encoder layers from the original publication for fair hyperparameter search
- Enriching a machine learning knowledge graph with academic architecture details—such as encoder depth—by scraping the ‘Attention is All You Need’ PDF for precise model metadata
- Driving benchmark suite creation for NLP frameworks by extracting and applying the standard six-layer encoder design from the seminal paper to ensure consistency across comparisons
- Conducting patent landscape analysis by programmatically obtaining the encoder structure specifics of the Transformer model to compare against new patent claims in sequence modeling

```
import json
import os
import requests
from urllib.parse import urlparse

# First, let's inspect the search results file to understand its structure
search_results_file = 'workspace/attention_paper_search_results.json'

if os.path.exists(search_results_file):
    print("Inspecting search results file structure:")
    with open(search_results_file, 'r') as f:
        results = json.load(f)
    
    print(f"Main keys in search results: {list(results.keys())}")
    
    if 'organic_results' in results:
        print(f"\nNumber of organic results: {len(results['organic_results'])}")
        
        # Look for arXiv links in the results
        arxiv_links = []
        for i, result in enumerate(results['organic_results']):
            title = result.get('title', 'No title')
            link = result.get('link', 'No link')
            
            print(f"\nResult {i+1}:")
            print(f"Title: {title}")
            print(f"Link: {link}")
            
            # Check for arXiv links
            if 'arxiv.org' in link.lower():
                arxiv_links.append(link)
                print(f"*** ARXIV LINK FOUND: {link} ***")
        
        print(f"\nTotal arXiv links found: {len(arxiv_links)}")
        
        # Now let's try to download the PDF from arXiv
        if arxiv_links:
            arxiv_url = arxiv_links[0]  # Use the first arXiv link
            print(f"\nUsing arXiv URL: {arxiv_url}")
            
            # Convert arXiv abstract URL to PDF URL
            if '/abs/' in arxiv_url:
                pdf_url = arxiv_url.replace('/abs/', '/pdf/') + '.pdf'
            else:
                pdf_url = arxiv_url
            
            print(f"PDF URL: {pdf_url}")
            
            # Download the PDF
            print("\nDownloading the 'Attention is All You Need' paper...")
            
            try:
                response = requests.get(pdf_url, stream=True)
                response.raise_for_status()
                
                pdf_path = 'workspace/attention_is_all_you_need.pdf'
                with open(pdf_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                print(f"PDF successfully downloaded to: {pdf_path}")
                print(f"File size: {os.path.getsize(pdf_path)} bytes")
                
                # Verify the file was downloaded properly
                if os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 0:
                    print("PDF download successful! Ready for text extraction.")
                else:
                    print("PDF download failed - file is empty or doesn't exist.")
                    
            except requests.exceptions.RequestException as e:
                print(f"Error downloading PDF: {e}")
                
        else:
            print("No arXiv links found in search results. Cannot proceed with PDF download.")
    
    else:
        print("No organic_results found in search results file.")
else:
    print(f"Search results file not found: {search_results_file}")
```