### Development Step 4: Axis Labels from a June 2022 arXiv AI Regulation Paper Figure with Three Labeled Axes

**Description**: Search arXiv.org for AI regulation papers originally submitted in June 2022. Focus on identifying papers that discuss artificial intelligence regulation, policy, governance, or related topics. Look for papers with submission dates in June 2022 and examine their content to find one containing a figure with three axes where each axis has label words at both ends. Extract the specific words used as axis labels in this figure.

**Use Cases**:
- Academic literature review automation for policy researchers seeking visual frameworks in AI governance papers
- Regulatory intelligence gathering for government agencies evaluating global approaches to AI oversight
- Automated extraction of conceptual models from scientific papers for curriculum development in university AI ethics courses
- Competitive landscape mapping for think tanks analyzing trends in AI regulation discourse
- Visual taxonomy identification for legal tech firms building AI policy compliance tools
- Research support for journalists investigating the evolution of AI policy debates and frameworks
- Dataset creation for machine learning teams training models to recognize multi-dimensional regulatory frameworks in academic literature
- Knowledge base enrichment for consulting firms advising clients on international AI governance strategies

```
import os
import json
import requests
from langchain_community.document_loaders import PyPDFLoader
import re
from datetime import datetime

print('=== DOWNLOADING AND ANALYZING AI REGULATION PAPERS FOR THREE-AXIS FIGURES ===')
print('Objective: Find figure with three axes containing label words at both ends of each axis')
print('Strategy: Download highest-ranked papers and analyze PDF content for figure descriptions\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Load the download plan to get paper priorities
plan_file = os.path.join(workspace, 'pdf_download_plan.json')

if os.path.exists(plan_file):
    print('Loading download plan...')
    with open(plan_file, 'r', encoding='utf-8') as f:
        download_plan = json.load(f)
    
    papers_to_analyze = download_plan.get('download_priority', [])
    print(f'Found {len(papers_to_analyze)} papers to analyze\n')
    
    # Start with the top 3 highest-ranked papers
    top_papers = papers_to_analyze[:3]
    
    analysis_results = []
    
    for i, paper in enumerate(top_papers, 1):
        title = paper.get('title', 'No title')
        arxiv_id = paper.get('arxiv_id', 'No ID')
        pdf_url = paper.get('pdf_url', 'No URL')
        relevance_score = paper.get('relevance_score', 0)
        
        print(f'=== ANALYZING PAPER {i}/3 ===') 
        print(f'Title: {title}')
        print(f'arXiv ID: {arxiv_id}')
        print(f'Relevance Score: {relevance_score}')
        print(f'PDF URL: {pdf_url}\n')
        
        # Download the PDF
        pdf_filename = f'{arxiv_id.replace(".", "_")}.pdf'
        pdf_path = os.path.join(workspace, pdf_filename)
        
        try:
            print(f'Downloading PDF: {pdf_filename}')
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(pdf_url, headers=headers, stream=True, timeout=60)
            response.raise_for_status()
            
            with open(pdf_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(pdf_path)
            print(f'✓ PDF downloaded successfully: {file_size:,} bytes')
            
            # Load and analyze PDF content
            print('Loading PDF content with LangChain...')
            loader = PyPDFLoader(pdf_path)
            pages = loader.load_and_split()
            
            print(f'✓ PDF loaded: {len(pages)} pages')
            
            # Combine all pages for comprehensive search
            full_content = '\n'.join([page.page_content for page in pages])
            
            print(f'Total content length: {len(full_content):,} characters')
            
            # Search for figure-related content
            print('\n--- SEARCHING FOR FIGURE REFERENCES ---')
            
            # Patterns to find figure references
            figure_patterns = [
                r'Figure\s+(\d+)[^.]*?([^.]*?axis[^.]*?)',
                r'Fig\.?\s+(\d+)[^.]*?([^.]*?axis[^.]*?)',
                r'figure\s+(\d+)[^.]*?([^.]*?axis[^.]*?)',
                r'(Figure\s+\d+[^.]*?three[^.]*?ax[ie]s[^.]*?)',
                r'(Fig\.?\s+\d+[^.]*?three[^.]*?ax[ie]s[^.]*?)',
                r'([^.]*?three[^.]*?ax[ie]s[^.]*?Figure\s+\d+[^.]*?)',
                r'([^.]*?three[^.]*?ax[ie]s[^.]*?Fig\.?\s+\d+[^.]*?)'
            ]
            
            figure_matches = []
            
            for pattern in figure_patterns:
                matches = re.findall(pattern, full_content, re.IGNORECASE | re.DOTALL)
                if matches:
                    print(f'Pattern "{pattern[:30]}..." found {len(matches)} matches')
                    figure_matches.extend(matches)
            
            # Also search for general axis-related content
            print('\n--- SEARCHING FOR AXIS-RELATED CONTENT ---')
            
            axis_patterns = [
                r'([^.]*?three[^.]*?ax[ie]s[^.]*?)',
                r'([^.]*?ax[ie]s[^.]*?label[^.]*?)',
                r'([^.]*?label[^.]*?ax[ie]s[^.]*?)',
                r'([^.]*?x-axis[^.]*?y-axis[^.]*?z-axis[^.]*?)',
                r'([^.]*?horizontal[^.]*?vertical[^.]*?depth[^.]*?)',
                r'([^.]*?dimension[^.]*?ax[ie]s[^.]*?)'
            ]
            
            axis_matches = []
            
            for pattern in axis_patterns:
                matches = re.findall(pattern, full_content, re.IGNORECASE | re.DOTALL)
                if matches:
                    print(f'Axis pattern found {len(matches)} matches')
                    axis_matches.extend(matches[:3])  # Limit to first 3 matches per pattern
            
            # Search for specific terms that might indicate axis labels
            print('\n--- SEARCHING FOR POTENTIAL AXIS LABEL TERMS ---')
            
            label_terms = [
                'low', 'high', 'left', 'right', 'top', 'bottom', 'start', 'end',
                'minimum', 'maximum', 'weak', 'strong', 'less', 'more', 'few', 'many',
                'small', 'large', 'narrow', 'wide', 'short', 'long', 'near', 'far',
                'early', 'late', 'before', 'after', 'first', 'last', 'initial', 'final'
            ]
            
            potential_labels = []
            for term in label_terms:
                # Look for the term in context with other axis-related words
                context_pattern = f'([^.]*?{term}[^.]*?(?:axis|dimension|scale|range)[^.]*?)'
                matches = re.findall(context_pattern, full_content, re.IGNORECASE)
                if matches:
                    potential_labels.extend(matches[:2])  # Limit to 2 per term
            
            print(f'Found {len(potential_labels)} potential axis label contexts')
            
            # Save analysis results
            paper_analysis = {
                'paper_info': {
                    'title': title,
                    'arxiv_id': arxiv_id,
                    'pdf_url': pdf_url,
                    'relevance_score': relevance_score,
                    'pdf_path': pdf_path,
                    'file_size': file_size,
                    'page_count': len(pages)
                },
                'figure_matches': figure_matches,
                'axis_matches': axis_matches[:10],  # Limit to first 10
                'potential_labels': potential_labels[:15],  # Limit to first 15
                'analysis_timestamp': datetime.now().isoformat()
            }
            
            analysis_results.append(paper_analysis)
            
            # Display key findings for this paper
            print(f'\n=== ANALYSIS SUMMARY FOR PAPER {i} ===')
            print(f'Figure matches: {len(figure_matches)}')
            print(f'Axis matches: {len(axis_matches)}')
            print(f'Potential label contexts: {len(potential_labels)}')
            
            if figure_matches:
                print('\nKey figure matches:')
                for j, match in enumerate(figure_matches[:3], 1):
                    match_text = str(match) if isinstance(match, str) else ' '.join(str(m) for m in match if m)
                    print(f'  {j}. {match_text[:200]}...')
            
            if axis_matches:
                print('\nKey axis matches:')
                for j, match in enumerate(axis_matches[:3], 1):
                    match_text = str(match) if isinstance(match, str) else str(match)
                    print(f'  {j}. {match_text[:150]}...')
            
            print(f'\n{"="*60}\n')
            
        except Exception as e:
            print(f'❌ Error analyzing paper {i}: {str(e)}')
            error_analysis = {
                'paper_info': {
                    'title': title,
                    'arxiv_id': arxiv_id,
                    'error': str(e)
                },
                'analysis_timestamp': datetime.now().isoformat()
            }
            analysis_results.append(error_analysis)
            continue
    
    # Save comprehensive analysis results
    results_file = os.path.join(workspace, 'pdf_analysis_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'analysis_date': datetime.now().isoformat(),
            'objective': 'Find figure with three axes containing label words at both ends',
            'papers_analyzed': len(analysis_results),
            'analysis_results': analysis_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f'=== COMPREHENSIVE ANALYSIS COMPLETE ===')
    print(f'✓ Analyzed {len(analysis_results)} papers')
    print(f'✓ Results saved to: {results_file}')
    
    # Summary of findings across all papers
    total_figure_matches = sum(len(result.get('figure_matches', [])) for result in analysis_results if 'figure_matches' in result)
    total_axis_matches = sum(len(result.get('axis_matches', [])) for result in analysis_results if 'axis_matches' in result)
    
    print(f'\n=== OVERALL FINDINGS SUMMARY ===')
    print(f'Total figure-related matches: {total_figure_matches}')
    print(f'Total axis-related matches: {total_axis_matches}')
    
    if total_figure_matches > 0 or total_axis_matches > 0:
        print('\n✓ Found potential figure/axis content in the papers')
        print('✓ Ready for detailed analysis of specific matches')
        print('✓ Next step: Examine matches for three-axis figures with label words')
    else:
        print('\n⚠ No obvious figure/axis matches found in initial analysis')
        print('May need to expand search to remaining papers or use different search terms')
    
else:
    print(f'❌ Download plan file not found: {plan_file}')
    print('Need to run the paper identification phase first')

print('\n*** PDF ANALYSIS PHASE COMPLETE ***')
```