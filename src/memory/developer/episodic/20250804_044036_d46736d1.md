### Development Step 17: Extract Survivor US Seasons 1‚Äì44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Reality TV analytics for entertainment agencies tracking ‚ÄúSurvivor‚Äù winners over seasons to identify casting success patterns and viewer engagement trends
- Media production libraries automatically compiling a JSON of ‚ÄúSurvivor‚Äù champions to populate on-air graphics and digital promos
- Academic research in media studies collecting season‚Äêby‚Äêseason winner data to correlate show format changes with audience demographics
- Chatbot and virtual assistant integrations enriching TV trivia modules with up‚Äêto‚Äêdate ‚ÄúSurvivor‚Äù winner information for fan engagement
- Social media scheduling tools that auto‚Äêpost winner anniversaries and highlight past champions to boost network account reach
- QA automation in data engineering pipelines monitoring Wikipedia page structure changes to ensure reliable scraper performance
- Digital humanities archives preserving pop culture metadata by extracting long‚Äêrunning reality show winner lists for historical analysis
- Data science training exercises teaching robust web scraping and HTML parsing techniques using ‚ÄúSurvivor‚Äù winner extraction as a hands‚Äêon example

```
import os
import sys
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory dynamically
candidates = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not candidates:
    print("‚ùå No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(candidates, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Path to the printable HTML saved earlier
html_path = os.path.join(workspace_dir, 'survivor_page_printable.html')
if not os.path.exists(html_path):
    print(f"‚ùå File not found: {html_path}")
    sys.exit(1)
print(f"Inspecting HTML file: {html_path}\n")

# 3) Load and parse the HTML
text = open(html_path, 'r', encoding='utf-8').read()
soup = BeautifulSoup(text, 'html.parser')

# 4) Find all section‚Äêheading spans and collect their ids and texts
spans = soup.find_all('span', class_='mw-headline')
print(f"Found {len(spans)} <span class='mw-headline'> elements.\n")

# 5) Write full list to a debug file for manual review
debug_file = os.path.join(workspace_dir, 'span_headlines_list.txt')
with open(debug_file, 'w', encoding='utf-8') as out:
    out.write('# id\ttext\n')
    for span in spans:
        sid = span.get('id', '')
        txt = span.get_text(strip=True)
        out.write(f"{sid}\t{txt}\n")
print(f"Wrote {len(spans)} headings to: {debug_file}\n")

# 6) Print a preview (first 20) and highlight any containing 'winner'
print("=== Preview of first 20 headings ===")
for i, span in enumerate(spans[:20], start=1):
    sid = span.get('id', '')
    txt = span.get_text(strip=True)
    marker = ' <-- contains "winner"' if 'winner' in txt.lower() or 'winner' in sid.lower() else ''
    print(f"{i:2d}. id='{sid}'  text='{txt}'{marker}")

# 7) If none match 'winner', suggest manual inspection
matches = [s for s in spans if 'winner' in s.get('id','').lower() or 'winner' in s.get_text(strip=True).lower()]
if not matches:
    print("\n‚ö†Ô∏è No headings with 'winner' found in the first pass. Please open 'span_headlines_list.txt' and look for the correct section name or id (e.g. it might be 'winners', 'champions', etc.).")
else:
    print(f"\nüéØ Found {len(matches)} potential 'winner' headings. See above preview for markers.")
```