### Development Step 6: Extract Survivor Season Winners 1–44 from Wikipedia and Save to JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment news website automates extraction of Survivor winners to generate an interactive timeline graphic comparing winner demographics across seasons
- Media analytics consultancy correlates the scraped winners list with Nielsen ratings to study how champion profiles affect viewership trends
- Mobile trivia app for Survivor fans integrates the JSON file to power season-by-season winner quiz questions and track user progress
- Academic sociology research project analyzes gender and ethnic diversity among Survivor winners from seasons 1–44 by ingesting the structured JSON data
- Social media marketing team uses the list of past winners to identify high-profile alumni for partnership outreach and promotional campaigns around new season premieres
- Digital archiving department of a media library imports the extracted winners data into their cataloging system to maintain a complete record of reality TV show outcomes
- Educational web development course teaches students web scraping and data processing techniques by guiding them through the extraction of Survivor winner information from Wikipedia
- AI-powered chat interface for television networks references the JSON winners list to answer real-time viewer queries about past Survivor champions

```
import os
import json
import requests
from bs4 import BeautifulSoup

# 1. Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2. Fetch the Survivor page
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
print(f"Fetching page: {url}")
response = requests.get(url)
response.raise_for_status()
print("Page fetched successfully, parsing HTML...")

# 3. Parse HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 4. Locate the "Winners by season" table by matching its caption
print("Scanning wikitables for caption 'Winners by season'...")
winners_table = None
for idx, table in enumerate(soup.find_all('table', class_='wikitable'), start=1):
    caption = table.find('caption')
    caption_text = caption.get_text(strip=True).lower() if caption else ''
    print(f"  Table {idx} caption: '{caption_text}'")
    if 'winners by season' in caption_text:
        winners_table = table
        print(f"--> Selected table {idx} as 'Winners by season' table")
        break

if not winners_table:
    raise RuntimeError("Could not locate a table with caption 'Winners by season'.")

# 5. Determine column indices for 'Season' and 'Winner'
header_row = winners_table.find('tr')
header_cells = header_row.find_all(['th','td'], recursive=False)
# Clean header texts
headers = []
for cell in header_cells:
    for sup in cell.find_all('sup'):
        sup.decompose()
    headers.append(cell.get_text(strip=True).lower())
print(f"Header columns found: {headers}")

try:
    season_idx = headers.index('season')
    winner_idx = headers.index('winner')
    print(f"Found 'season' at index {season_idx}, 'winner' at index {winner_idx}")
except ValueError as e:
    raise RuntimeError("Could not find 'Season' and 'Winner' columns in the winners table.")

# 6. Parse each data row
rows = winners_table.find_all('tr')[1:]
winners = []
for row in rows:
    cells = row.find_all(['th','td'], recursive=False)
    # Skip if not enough cells
    if len(cells) <= max(season_idx, winner_idx):
        print(f"Skipping row: only {len(cells)} cells, need at least {max(season_idx, winner_idx)+1}")
        continue

    # Extract and parse season
    season_cell = cells[season_idx]
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    season_text = season_cell.get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"Skipping row: cannot parse season '{season_text}'")
        continue
    if not (1 <= season_num <= 44):
        print(f"Skipping season {season_num}: out of target range 1-44")
        continue

    # Extract winner name
    winner_cell = cells[winner_idx]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    winner_name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season_num}: Winner -> {winner_name}")

    winners.append({'season': season_num, 'winner': winner_name})

# 7. Sort and save results
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"Total winners extracted for seasons 1–44: {len(winners_sorted)}")

output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Winners list saved to: {output_path}")

```