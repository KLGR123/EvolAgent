### Development Step 4: Search for Author of “Francia’s Reign of Terror” Letters, Third Volume Criticizing Paraguay’s Dictator

**Description**: Search for information about a collection of letters titled 'Francia's Reign of Terror' that documents a four-year stay under Dictator Francia's rule in Paraguay. Focus on identifying the author of this work, which is described as the third volume that criticizes local laziness and government policies and was written while relying on local hospitality. Search using keywords including 'Francia's Reign of Terror letters Paraguay dictator', 'four year stay Francia Paraguay author', and 'third volume Francia Paraguay criticism government policies'.

**Use Cases**:
- Academic historians conducting in-depth research on 19th-century Paraguayan politics by automatically harvesting and verifying primary correspondence under Dictator Francia
- University library archivists building a curated metadata registry of rare “Francia’s Reign of Terror” letter volumes, including author attribution and thematic indexing
- Digital humanities teams creating an interactive web portal of colonial Latin American memoirs, populated via automated extraction of content and author details from online sources
- Investigative journalists fact-checking claims about José Gaspar Rodríguez de Francia for a documentary, using the solution to rapidly compile letter excerpts and author credentials
- Museum curators assembling a traveling exhibit on authoritarian regimes in South America, leveraging scripted searches to source period-accurate letters and contextual government criticisms
- Political science educators developing course materials on dictatorship and resistance by retrieving third-volume critiques of local laziness and policy failures in Paraguay
- Human rights organizations analyzing historical patterns of government suppression by mining descriptions of “reign of terror” correspondence and lodging authorial evidence for advocacy reports
- Independent academic publishers sourcing out-of-print volumes and confirming author information to produce a critical edition of the four-year stay letters under Francia’s rule

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Searching for information about 'Francia's Reign of Terror' letters collection...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Initialize results storage
all_search_results = []
search_summary = {
    'total_queries': 0,
    'successful_queries': 0,
    'total_results': 0,
    'errors': []
}

print("\n=== SEARCHING WIKIPEDIA FOR FRANCIA INFORMATION ===")

# Search Wikipedia directly for Francia-related information
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Jos%C3%A9_Gaspar_Rodr%C3%ADguez_de_Francia",
    "https://en.wikipedia.org/wiki/Paraguay",
    "https://en.wikipedia.org/wiki/History_of_Paraguay"
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

for url in wikipedia_urls:
    try:
        print(f"\nFetching Wikipedia page: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        
        # Extract main content
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove unwanted elements
            for elem in content_div.find_all(['script', 'style']):
                elem.decompose()
            
            content = content_div.get_text(separator=' ', strip=True)
            
            # Look for Francia-related keywords
            content_lower = content.lower()
            target_keywords = [
                'francia', 'dictator', 'paraguay', 'reign', 'terror', 'letters', 
                'correspondence', 'memoir', 'account', 'four year', 'third volume',
                'criticism', 'government policies', 'local hospitality', 'laziness'
            ]
            
            found_keywords = [kw for kw in target_keywords if kw in content_lower]
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content[:3000],  # First 3000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                all_search_results.append(result)
                print(f"Found relevant content in {title}")
                print(f"Keywords found: {', '.join(found_keywords)}")
                print(f"Content preview: {content[:300]}...")
            else:
                print(f"No relevant keywords found in {title}")
        
        search_summary['successful_queries'] += 1
        time.sleep(2)  # Be respectful to Wikipedia
        
    except Exception as e:
        error_msg = f"Error fetching Wikipedia page {url}: {str(e)}"
        print(error_msg)
        search_summary['errors'].append(error_msg)
    
    search_summary['total_queries'] += 1

print(f"\n=== SEARCHING GOOGLE BOOKS API ===")

# Search Google Books API for Francia-related books
book_queries = [
    "Francia Paraguay dictator letters",
    "Paraguay Francia reign of terror",
    "Francia Paraguay memoir correspondence",
    "Francia's Reign of Terror Paraguay",
    "four year stay Francia Paraguay",
    "third volume Francia Paraguay"
]

for query in book_queries:
    try:
        print(f"\nSearching Google Books for: {query}")
        api_url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': query,
            'maxResults': 10,
            'printType': 'books',
            'langRestrict': 'en'
        }
        
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            for item in data['items']:
                volume_info = item.get('volumeInfo', {})
                title = volume_info.get('title', 'Unknown Title')
                authors = volume_info.get('authors', ['Unknown Author'])
                description = volume_info.get('description', '')
                published_date = volume_info.get('publishedDate', 'Unknown Date')
                
                # Check relevance
                combined_text = (title + ' ' + ' '.join(authors) + ' ' + description).lower()
                target_keywords = [
                    'francia', 'paraguay', 'dictator', 'letters', 'reign of terror',
                    'four year', 'third volume', 'criticism', 'government policies',
                    'local hospitality', 'laziness', 'memoir', 'correspondence'
                ]
                
                found_keywords = [kw for kw in target_keywords if kw in combined_text]
                
                if len(found_keywords) >= 2:  # Must have at least 2 relevant keywords
                    result = {
                        'title': title,
                        'authors': authors,
                        'description': description,
                        'published_date': published_date,
                        'keywords_found': found_keywords,
                        'source': 'Google Books',
                        'search_query': query,
                        'relevance_score': len(found_keywords)
                    }
                    all_search_results.append(result)
                    print(f"Found relevant book: {title}")
                    print(f"Authors: {', '.join(authors)}")
                    print(f"Keywords: {', '.join(found_keywords)}")
                    print(f"Description: {description[:200]}...")
            
            print(f"Processed {len(data['items'])} books for query: {query}")
        else:
            print(f"No books found for query: {query}")
        
        search_summary['successful_queries'] += 1
        time.sleep(1)  # Be respectful to API
        
    except Exception as e:
        error_msg = f"Error searching Google Books for '{query}': {str(e)}"
        print(error_msg)
        search_summary['errors'].append(error_msg)
    
    search_summary['total_queries'] += 1

print(f"\n=== ANALYZING SEARCH RESULTS ===")

# Sort results by relevance score
all_search_results.sort(key=lambda x: x['relevance_score'], reverse=True)
search_summary['total_results'] = len(all_search_results)

print(f"Total search results collected: {len(all_search_results)}")

# Analyze for specific details mentioned in the plan
specific_findings = {
    'four_year_stay': [],
    'third_volume': [],
    'criticism_laziness': [],
    'government_policies': [],
    'local_hospitality': [],
    'reign_of_terror': [],
    'potential_authors': []
}

for result in all_search_results:
    # Get text content for analysis
    if 'content' in result:
        text_content = result['content'].lower()
    elif 'description' in result:
        text_content = result['description'].lower()
    else:
        text_content = result.get('title', '').lower()
    
    # Look for specific details
    if 'four year' in text_content or 'four-year' in text_content:
        specific_findings['four_year_stay'].append({
            'source': result['title'],
            'content': text_content[:500]
        })
    
    if 'third volume' in text_content or 'volume 3' in text_content:
        specific_findings['third_volume'].append({
            'source': result['title'],
            'content': text_content[:500]
        })
    
    if 'laziness' in text_content or 'lazy' in text_content:
        specific_findings['criticism_laziness'].append({
            'source': result['title'],
            'content': text_content[:500]
        })
    
    if 'government policies' in text_content or 'policy' in text_content:
        specific_findings['government_policies'].append({
            'source': result['title'],
            'content': text_content[:500]
        })
    
    if 'hospitality' in text_content:
        specific_findings['local_hospitality'].append({
            'source': result['title'],
            'content': text_content[:500]
        })
    
    if 'reign of terror' in text_content:
        specific_findings['reign_of_terror'].append({
            'source': result['title'],
            'content': text_content[:500]
        })
    
    # Extract potential authors
    if 'authors' in result:
        for author in result['authors']:
            if author not in specific_findings['potential_authors']:
                specific_findings['potential_authors'].append(author)

# Save comprehensive results to workspace
final_data = {
    'search_date': datetime.now().isoformat(),
    'search_summary': search_summary,
    'all_search_results': all_search_results,
    'specific_findings': specific_findings,
    'search_focus': "Francia's Reign of Terror letters collection - four year stay under Dictator Francia's rule in Paraguay, third volume criticizing local laziness and government policies"
}

output_file = 'workspace/francia_letters_comprehensive_search.json'
with open(output_file, 'w') as f:
    json.dump(final_data, f, indent=2)

print(f"\nComprehensive search data saved to: {output_file}")

# Display top results
print("\n=== TOP RELEVANT RESULTS ===")
for i, result in enumerate(all_search_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result['title']}")
    print(f"Source: {result['source']}")
    if 'authors' in result:
        print(f"Authors: {', '.join(result['authors'])}")
    print(f"Keywords found: {', '.join(result['keywords_found'])}")
    
    # Display content preview
    if 'content' in result:
        print(f"Content preview: {result['content'][:300]}...")
    elif 'description' in result:
        print(f"Description: {result['description'][:300]}...")

# Display specific findings
print("\n=== SPECIFIC FINDINGS ANALYSIS ===")
for finding_type, findings in specific_findings.items():
    if findings and finding_type != 'potential_authors':
        print(f"\n{finding_type.replace('_', ' ').title()}: {len(findings)} mentions found")
        for finding in findings[:2]:  # Show first 2 mentions
            print(f"  - Source: {finding['source']}")
            print(f"    Content: {finding['content'][:200]}...")

if specific_findings['potential_authors']:
    print(f"\nPotential Authors Found: {len(specific_findings['potential_authors'])}")
    for author in specific_findings['potential_authors'][:10]:  # Show first 10 authors
        print(f"  - {author}")

print(f"\n=== FINAL SEARCH SUMMARY ===")
print(f"Total queries executed: {search_summary['total_queries']}")
print(f"Successful queries: {search_summary['successful_queries']}")
print(f"Total relevant results found: {search_summary['total_results']}")
print(f"Errors encountered: {len(search_summary['errors'])}")

if search_summary['errors']:
    print("\nErrors:")
    for error in search_summary['errors']:
        print(f"  - {error}")

print("\nSearch for Francia's Reign of Terror letters completed successfully!")
```