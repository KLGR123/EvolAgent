### Development Step 6: Extract 2023-07-03 Internal Wikipedia Links from ‚ÄòThe Lord of the Rings‚Äô for ASOIAF Pathfinding

**Description**: Access the archived Wikipedia page for 'The Lord of the Rings' (book) as it appeared at the end of July 3, 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific version from that date. Extract all outbound links from the page content, focusing on internal Wikipedia links that could potentially lead toward 'A Song of Ice and Fire'. Create a comprehensive list of linked pages including literature, fantasy, author, publisher, and genre-related links that might serve as stepping stones in the path-finding process.

**Use Cases**:
- Fantasy literature scholars constructing a directed graph of narrative connections between major fantasy series by extracting and tracing internal Wikipedia links from archived page revisions
- Academic librarians automating the curation of dynamic bibliographies for genre collections by mining historic Wikipedia links to authors, publishers, and related works
- Digital marketing teams mapping competitive landscapes in the publishing industry by analyzing outbound link networks from high-priority genre pages to spot emerging authors
- Curriculum designers building thematic learning pathways across medieval literature and modern fantasy by following internal Wikipedia links between key topics and revisions
- Data analysts auditing shifts in public interest by comparing fantasy-related link centrality on Wikipedia before and after major book releases or media adaptations
- Recommendation engine developers enriching knowledge graphs with genre relationships by harvesting and connecting fantasy author and series links from historical Wikipedia snapshots
- Diversity audit researchers examining the evolution of author representation in fantasy literature by extracting and analyzing link patterns from archived page versions

```
import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import time

print("=== MANUAL EXPLORATION OF MOST PROMISING FANTASY LITERATURE CONNECTIONS ===")
print("Objective: Manually explore high-priority leads to find path to 'A Song of Ice and Fire'\n")

# Locate workspace and load previous analysis
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("‚ùå No workspace directory found")
    exit()

workspace_dir = workspace_dirs[0]
analysis_file = os.path.join(workspace_dir, 'promising_leads_analysis.json')

if not os.path.exists(analysis_file):
    print("‚ùå Analysis file not found. Let me check what files are available:")
    for file in os.listdir(workspace_dir):
        print(f"  - {file}")
    exit()

print(f"Loading analysis from: {os.path.basename(analysis_file)}\n")

with open(analysis_file, 'r', encoding='utf-8') as f:
    analysis_data = json.load(f)

# Define our target variations
target_variations = [
    "A Song of Ice and Fire",
    "Game of Thrones", 
    "George R. R. Martin",
    "George R.R. Martin",
    "George Martin",
    "A Game of Thrones"
]

# Function to scrape and check a page for target connections
def explore_page_for_targets(page_title, max_links=100):
    """Explore a Wikipedia page for connections to our target"""
    try:
        url_title = page_title.replace(' ', '_')
        url = f"https://en.wikipedia.org/wiki/{requests.utils.quote(url_title)}"
        
        print(f"  üîç Exploring: {page_title}")
        print(f"      URL: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Get page text for target detection
            page_text = soup.get_text().lower()
            
            # Check for direct target mentions in the page content
            target_mentions = []
            for target in target_variations:
                if target.lower() in page_text:
                    target_mentions.append(target)
            
            if target_mentions:
                print(f"      üéØ TARGET FOUND IN CONTENT: {target_mentions}")
                return {
                    'page': page_title,
                    'targets_found': target_mentions,
                    'connection_type': 'content_mention',
                    'url': url
                }
            
            # Extract links and check for target links
            main_content = soup.find('div', {'id': 'mw-content-text'})
            if not main_content:
                main_content = soup
            
            target_links = []
            fantasy_related_links = []
            
            for link in main_content.find_all('a', href=True):
                href = link.get('href', '')
                if href.startswith('/wiki/') and ':' not in href.split('/')[-1]:
                    article_name = href.split('/')[-1].replace('_', ' ')
                    article_name = requests.utils.unquote(article_name)
                    link_text = link.get_text().strip()
                    
                    # Check if this link matches our target
                    for target in target_variations:
                        if (target.lower() in article_name.lower() or 
                            target.lower() in link_text.lower()):
                            target_links.append({
                                'article_name': article_name,
                                'link_text': link_text,
                                'target_matched': target
                            })
                            print(f"      üéØ TARGET LINK FOUND: {article_name} (matched: {target})")
                            return {
                                'page': page_title,
                                'target_link': article_name,
                                'target_matched': target,
                                'connection_type': 'direct_link',
                                'url': url
                            }
                    
                    # Also collect fantasy-related links for potential next steps
                    if any(keyword in article_name.lower() for keyword in 
                           ['fantasy', 'martin', 'epic', 'author', 'literature', 'series', 'saga']):
                        fantasy_related_links.append(article_name)
            
            print(f"      üìä Found {len(fantasy_related_links)} fantasy-related links")
            if fantasy_related_links[:5]:  # Show first 5
                print(f"      üîó Sample links: {fantasy_related_links[:5]}")
            
            return {
                'page': page_title,
                'fantasy_links': fantasy_related_links[:20],  # Keep top 20
                'connection_type': 'no_direct_connection',
                'url': url
            }
            
        else:
            print(f"      ‚ùå HTTP error {response.status_code}")
            return None
            
    except Exception as e:
        print(f"      ‚ùå Error: {str(e)}")
        return None

# Start with the most promising targets from our analysis
print("=== EXPLORING TOP FANTASY LITERATURE CONNECTIONS ===\n")

# Get the high-priority targets from our analysis
high_priority_targets = []
if 'manual_exploration_targets' in analysis_data:
    for target in analysis_data['manual_exploration_targets']:
        high_priority_targets.append(target['target'])

# Add some specific high-value targets
specific_targets = [
    "Fantasy literature",
    "List of fantasy authors", 
    "High fantasy",
    "George R. Stewart",
    "International Fantasy Award"
]

# Combine and deduplicate
all_targets = list(set(high_priority_targets + specific_targets))
print(f"Exploring {len(all_targets)} high-priority targets:\n")

exploration_results = []
max_explorations = 10  # Limit to avoid too many requests

for i, target in enumerate(all_targets[:max_explorations], 1):
    print(f"--- Exploration {i}/{min(len(all_targets), max_explorations)}: {target} ---")
    
    result = explore_page_for_targets(target)
    if result:
        exploration_results.append(result)
        
        # If we found a direct connection, stop and celebrate!
        if result.get('connection_type') in ['content_mention', 'direct_link']:
            print(f"\nüéâ BREAKTHROUGH! Found connection on page: {target}")
            break
    
    # Add delay to be respectful
    time.sleep(2)
    print()

print(f"\n=== EXPLORATION COMPLETE ===")
print(f"Pages explored: {len(exploration_results)}")
print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Analyze results
direct_connections = [r for r in exploration_results if r.get('connection_type') in ['content_mention', 'direct_link']]
fantasy_connections = [r for r in exploration_results if r.get('fantasy_links')]

print("=== RESULTS ANALYSIS ===\n")

if direct_connections:
    print(f"üéØ DIRECT CONNECTIONS FOUND: {len(direct_connections)}")
    for connection in direct_connections:
        print(f"\nPage: {connection['page']}")
        print(f"Type: {connection['connection_type']}")
        if connection.get('targets_found'):
            print(f"Targets found: {connection['targets_found']}")
        if connection.get('target_link'):
            print(f"Target link: {connection['target_link']} (matched: {connection['target_matched']})")
        print(f"URL: {connection['url']}")
else:
    print("‚ùå No direct connections to 'A Song of Ice and Fire' found in this exploration")

if fantasy_connections:
    print(f"\nüîó FANTASY-RELATED CONNECTIONS: {len(fantasy_connections)}")
    
    # Aggregate all fantasy links found
    all_fantasy_links = []
    for connection in fantasy_connections:
        if connection.get('fantasy_links'):
            all_fantasy_links.extend(connection['fantasy_links'])
    
    # Count occurrences and find most promising
    from collections import Counter
    link_counts = Counter(all_fantasy_links)
    
    print(f"Total fantasy-related links found: {len(all_fantasy_links)}")
    print(f"Unique fantasy links: {len(link_counts)}")
    print("\nMost frequently mentioned fantasy links:")
    for link, count in link_counts.most_common(10):
        print(f"  {count}x {link}")
    
    # Check if any contain Martin or similar
    martin_related = [link for link in all_fantasy_links if 'martin' in link.lower()]
    if martin_related:
        print(f"\nüìö Martin-related links found: {martin_related}")
else:
    print("\n‚ùå No fantasy-related connections found")

# Save comprehensive results
final_results = {
    'exploration_metadata': {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'targets_explored': all_targets[:max_explorations],
        'total_explorations': len(exploration_results),
        'method': 'manual_targeted_exploration'
    },
    'direct_connections': direct_connections,
    'fantasy_connections': fantasy_connections,
    'exploration_results': exploration_results,
    'summary': {
        'direct_paths_found': len(direct_connections),
        'fantasy_pages_explored': len(fantasy_connections),
        'total_fantasy_links_discovered': len(all_fantasy_links) if 'all_fantasy_links' in locals() else 0,
        'success': len(direct_connections) > 0
    }
}

results_file = os.path.join(workspace_dir, 'manual_exploration_results.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)

print(f"\nüìÅ Detailed results saved to: {os.path.basename(results_file)}")

print(f"\n=== FINAL SUMMARY ===")
if direct_connections:
    print(f"üéâ SUCCESS: Found {len(direct_connections)} direct path(s) to 'A Song of Ice and Fire'!")
    print(f"‚úÖ Path discovery complete - connections established from LOTR to target")
else:
    print(f"‚ö†Ô∏è No direct connections found in this round")
    print(f"üìà Discovered {len(all_fantasy_links) if 'all_fantasy_links' in locals() else 0} additional fantasy literature connections")
    print(f"üîÑ Ready for extended search or deeper exploration of discovered links")

print(f"\nüéØ PLAN STATUS: Link extraction complete, path-finding {'SUCCESSFUL' if direct_connections else 'IN PROGRESS'}")
```