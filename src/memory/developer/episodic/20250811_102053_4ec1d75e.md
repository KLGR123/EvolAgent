### Development Step 6: Counting Twitter/X Citations in August Wikipedia Pages from June 2023 Archived Versions

**Description**: Search for and access the archived Wikipedia pages for each day of August (August 1st through August 31st) as they appeared in the last versions from June 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific versions of these 31 daily pages from late June 2023. Extract all reference citations from each page and identify those that cite Twitter/X posts (looking for twitter.com, x.com, or @username patterns in the reference URLs). Count the total number of Twitter/X citations for each individual August day page and compile the results.

**Use Cases**:
- Media literacy research to track the prevalence of social media sources (specifically Twitter/X) in Wikipediaâ€™s coverage of current events for August, helping journalists and educators assess source reliability trends over time
- Academic studies analyzing the evolution of citation practices on Wikipedia, quantifying how often Twitter/X posts were referenced in daily event summaries as of June 2023
- Automated auditing for Wikipedia editors or Wikimedia Foundation staff to identify and flag day pages that rely heavily on ephemeral Twitter/X content, supporting efforts to improve citation durability
- Fact-checking organizations reviewing the historical use of Twitter/X as a reference in Wikipediaâ€™s August day pages, to assess the risk of misinformation propagation from social media
- Digital preservation projects evaluating how much Wikipediaâ€™s event documentation depends on potentially disappearing Twitter/X content, informing archiving priorities
- Social media researchers measuring the influence of Twitter/X on mainstream knowledge platforms by quantifying its citation footprint in Wikipediaâ€™s daily event logs
- Policy analysts investigating the impact of Twitter/Xâ€™s policy or ownership changes (e.g., rebranding to X) on its use as a cited source in Wikipediaâ€™s event documentation
- Data journalists building visualizations or reports on the changing landscape of Wikipediaâ€™s references, using the compiled citation counts to illustrate the rise or fall of social media as a primary source

```
import os
import requests
import json
from datetime import datetime
import time
from bs4 import BeautifulSoup
import re

print("=== FINAL PHASE: COMPLETE TWITTER CITATION ANALYSIS ===\n")
print("Processing all 20 August pages with June 2023 revisions")
print("Objective: Count Twitter/X citations for each page and compile final results\n")

# First, inspect workspace to understand current state
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using existing workspace: {workspace_dir}\n")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created new workspace: {workspace_dir}\n")

print("=== INSPECTING WORKSPACE FILES FOR SUCCESSFUL PAGES DATA ===\n")
existing_files = [f for f in os.listdir(workspace_dir) if f.endswith('.json')]
print(f"Found {len(existing_files)} JSON files in workspace:")

for file in existing_files:
    file_path = os.path.join(workspace_dir, file)
    file_size = os.path.getsize(file_path)
    print(f"  - {file} ({file_size:,} bytes)")

# Load the comprehensive results file to get all successful pages
comprehensive_file = os.path.join(workspace_dir, 'august_pages_comprehensive_june_2023.json')

if os.path.exists(comprehensive_file):
    print(f"\n=== LOADING COMPREHENSIVE RESULTS FILE ===\n")
    
    # First inspect the file structure before loading
    with open(comprehensive_file, 'r', encoding='utf-8') as f:
        content = f.read()
        print(f"Comprehensive file size: {len(content):,} characters")
    
    # Now load and inspect structure
    with open(comprehensive_file, 'r', encoding='utf-8') as f:
        comprehensive_data = json.load(f)
    
    print(f"\nFile structure:")
    for key in comprehensive_data.keys():
        value = comprehensive_data[key]
        if isinstance(value, dict):
            print(f"  {key}: dict with {len(value)} keys")
        elif isinstance(value, list):
            print(f"  {key}: list with {len(value)} items")
        else:
            print(f"  {key}: {type(value).__name__}")
    
    # Extract successful pages data
    if 'successful_pages' in comprehensive_data:
        successful_pages = comprehensive_data['successful_pages']
        print(f"\nFound {len(successful_pages)} successful pages with June 2023 revisions")
        
        # Show structure of first successful page entry
        if successful_pages:
            print(f"\nSample successful page structure:")
            sample_page = successful_pages[0]
            for key, value in sample_page.items():
                print(f"  {key}: {type(value).__name__} = {value}")
    else:
        print("âŒ No 'successful_pages' key found in comprehensive data")
        successful_pages = []
else:
    print(f"âŒ Comprehensive results file not found: {comprehensive_file}")
    successful_pages = []

if not successful_pages:
    print("\nâŒ No successful pages data available - cannot proceed with citation analysis")
    exit()

print(f"\n=== PROCESSING ALL {len(successful_pages)} SUCCESSFUL PAGES ===\n")

# Wikipedia API configuration
api_url = "https://en.wikipedia.org/w/api.php"

def get_revision_content(revision_id):
    """Get the full content of a specific Wikipedia revision"""
    params = {
        'action': 'query',
        'format': 'json',
        'prop': 'revisions',
        'revids': revision_id,
        'rvprop': 'content|timestamp|ids'
    }
    
    try:
        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if 'query' in data and 'pages' in data['query']:
            pages = data['query']['pages']
            page_id = list(pages.keys())[0]
            
            if 'revisions' in pages[page_id] and len(pages[page_id]['revisions']) > 0:
                revision = pages[page_id]['revisions'][0]
                if '*' in revision:  # Content is in the '*' field
                    content = revision['*']
                    return {
                        'success': True,
                        'content': content,
                        'content_length': len(content)
                    }
        
        return {'success': False, 'error': 'No content found'}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def extract_twitter_citations(content):
    """Extract Twitter/X citations from Wikipedia content"""
    twitter_citations = []
    
    # Enhanced patterns to match Twitter/X citations in Wikipedia
    patterns = [
        # Direct URLs in references
        r'https?://(?:www\.)?twitter\.com/[^\s\]\}\|\n<>"]+',
        r'https?://(?:www\.)?x\.com/[^\s\]\}\|\n<>"]+',
        
        # URLs in citation templates
        r'\|\s*url\s*=\s*https?://(?:www\.)?twitter\.com/[^\s\]\}\|\n<>"]+',
        r'\|\s*url\s*=\s*https?://(?:www\.)?x\.com/[^\s\]\}\|\n<>"]+',
        
        # Archive URLs that contain Twitter
        r'https?://[^\s]*archive[^\s]*twitter\.com[^\s\]\}\|\n<>"]*',
        r'https?://[^\s]*archive[^\s]*x\.com[^\s\]\}\|\n<>"]*',
        
        # @username patterns in references (more restrictive)
        r'@[A-Za-z0-9_]{1,15}(?=\s|\]|\}|\||\n|<|>|$)',
        
        # Twitter in external links section
        r'\*\s*\[https?://(?:www\.)?twitter\.com/[^\s\]\}\|\n<>"]+[^\]]*\]',
        r'\*\s*\[https?://(?:www\.)?x\.com/[^\s\]\}\|\n<>"]+[^\]]*\]'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            # Clean up the match
            clean_match = match.strip()
            if clean_match and clean_match not in twitter_citations:
                # Additional filtering to avoid false positives
                if len(clean_match) > 3:  # Avoid very short matches
                    twitter_citations.append(clean_match)
    
    return twitter_citations

# Process all successful pages
print("Starting comprehensive citation analysis...\n")

all_citation_results = {}
total_citations_found = 0
processed_count = 0
start_time = datetime.now()

# Process in batches for progress tracking
batch_size = 5
total_batches = (len(successful_pages) + batch_size - 1) // batch_size

for batch_num in range(total_batches):
    start_idx = batch_num * batch_size
    end_idx = min(start_idx + batch_size, len(successful_pages))
    batch_pages = successful_pages[start_idx:end_idx]
    
    print(f"\n--- Batch {batch_num + 1}/{total_batches}: Processing pages {start_idx + 1}-{end_idx} ---")
    
    for page_info in batch_pages:
        page_name = page_info['page']
        revision_id = page_info['revision_id']
        page_date = page_info['date']
        
        print(f"\n  Processing: {page_name} (Revision {revision_id}, {page_date})")
        
        # Get revision content
        print(f"    Fetching content...")
        content_result = get_revision_content(revision_id)
        
        if content_result['success']:
            content = content_result['content']
            content_length = content_result['content_length']
            print(f"      âœ“ Retrieved: {content_length:,} characters")
            
            # Extract Twitter citations
            print(f"    Analyzing Twitter/X citations...")
            twitter_citations = extract_twitter_citations(content)
            citations_count = len(twitter_citations)
            total_citations_found += citations_count
            
            print(f"      Found: {citations_count} Twitter/X citations")
            
            # Show sample citations if found
            if twitter_citations:
                print(f"      Sample citations:")
                for i, citation in enumerate(twitter_citations[:2], 1):
                    preview = citation[:60] + ('...' if len(citation) > 60 else '')
                    print(f"        {i}. {preview}")
            
            all_citation_results[page_name] = {
                'page': page_name,
                'revision_id': revision_id,
                'date': page_date,
                'content_length': content_length,
                'twitter_citations_count': citations_count,
                'twitter_citations': twitter_citations,
                'analysis_success': True
            }
        else:
            print(f"      âŒ Failed: {content_result['error']}")
            all_citation_results[page_name] = {
                'page': page_name,
                'revision_id': revision_id,
                'date': page_date,
                'twitter_citations_count': 0,
                'twitter_citations': [],
                'analysis_success': False,
                'error': content_result['error']
            }
        
        processed_count += 1
        
        # Add delay to be respectful to Wikipedia's servers
        time.sleep(1.5)
    
    # Progress update
    elapsed = (datetime.now() - start_time).total_seconds()
    remaining = len(successful_pages) - processed_count
    
    print(f"\n  Batch {batch_num + 1} complete:")
    print(f"    Processed: {processed_count}/{len(successful_pages)} pages")
    print(f"    Total citations found so far: {total_citations_found}")
    print(f"    Elapsed: {elapsed:.1f}s")
    
    if remaining > 0:
        estimated_remaining = (elapsed / processed_count) * remaining
        print(f"    Estimated time remaining: {estimated_remaining:.1f}s")

print(f"\n=== COMPREHENSIVE CITATION ANALYSIS COMPLETE ===\n")

total_elapsed = (datetime.now() - start_time).total_seconds()
print(f"Total processing time: {total_elapsed:.1f} seconds")
print(f"Pages processed: {len(all_citation_results)}")
print(f"Total Twitter/X citations found: {total_citations_found}")

# Compile final results
print(f"\n=== FINAL RESULTS BY PAGE ===\n")

pages_with_citations = []
pages_without_citations = []

for page_name, result in all_citation_results.items():
    if result['analysis_success']:
        citations_count = result['twitter_citations_count']
        if citations_count > 0:
            pages_with_citations.append((page_name, citations_count))
            print(f"âœ“ {page_name}: {citations_count} Twitter/X citations ({result['date']})")
        else:
            pages_without_citations.append(page_name)
            print(f"â—‹ {page_name}: 0 Twitter/X citations ({result['date']})")
    else:
        print(f"âŒ {page_name}: Analysis failed - {result.get('error', 'Unknown error')}")

print(f"\n=== SUMMARY STATISTICS ===\n")
print(f"Total August pages analyzed: {len(all_citation_results)}")
print(f"Pages with Twitter/X citations: {len(pages_with_citations)}")
print(f"Pages without Twitter/X citations: {len(pages_without_citations)}")
print(f"Total Twitter/X citations found: {total_citations_found}")

if pages_with_citations:
    print(f"\nPages with citations (sorted by count):")
    pages_with_citations.sort(key=lambda x: x[1], reverse=True)
    for page, count in pages_with_citations:
        print(f"  {page}: {count} citations")

if len(all_citation_results) > 0:
    avg_citations = total_citations_found / len(all_citation_results)
    print(f"\nAverage citations per page: {avg_citations:.2f}")

# Save final comprehensive results
final_results = {
    'analysis_metadata': {
        'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'phase': 'complete_twitter_citation_analysis',
        'target_archive_date': 'June 2023',
        'total_pages_processed': len(all_citation_results),
        'processing_time_seconds': total_elapsed
    },
    'summary_statistics': {
        'total_august_pages_analyzed': len(all_citation_results),
        'pages_with_twitter_citations': len(pages_with_citations),
        'pages_without_twitter_citations': len(pages_without_citations),
        'total_twitter_citations_found': total_citations_found,
        'average_citations_per_page': round(avg_citations, 2) if len(all_citation_results) > 0 else 0
    },
    'detailed_results': all_citation_results,
    'pages_with_citations': pages_with_citations,
    'pages_without_citations': pages_without_citations
}

final_file = os.path.join(workspace_dir, 'august_pages_twitter_citations_final_results.json')
with open(final_file, 'w', encoding='utf-8') as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False)

print(f"\nâœ… Final results saved to: {os.path.basename(final_file)}")
print(f"   File size: {os.path.getsize(final_file):,} bytes")

# Create summary report
summary_file = os.path.join(workspace_dir, 'twitter_citations_summary_report.txt')
with open(summary_file, 'w', encoding='utf-8') as f:
    f.write(f"AUGUST WIKIPEDIA PAGES - TWITTER CITATION ANALYSIS\n")
    f.write(f"={'='*55}\n\n")
    f.write(f"Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Archive date: June 2023 (last versions from June 2023)\n\n")
    f.write(f"FINAL RESULTS:\n")
    f.write(f"- Total August pages analyzed: {len(all_citation_results)}\n")
    f.write(f"- Pages with Twitter/X citations: {len(pages_with_citations)}\n")
    f.write(f"- Pages without Twitter/X citations: {len(pages_without_citations)}\n")
    f.write(f"- Total Twitter/X citations found: {total_citations_found}\n")
    f.write(f"- Average citations per page: {avg_citations:.2f}\n\n")
    
    f.write(f"DETAILED BREAKDOWN BY PAGE:\n")
    for page_name, result in all_citation_results.items():
        if result['analysis_success']:
            f.write(f"- {page_name}: {result['twitter_citations_count']} citations ({result['date']})\n")
        else:
            f.write(f"- {page_name}: Analysis failed\n")
    
    if pages_with_citations:
        f.write(f"\nPAGES WITH CITATIONS (sorted by count):\n")
        for page, count in pages_with_citations:
            f.write(f"- {page}: {count} citations\n")

print(f"âœ… Summary report saved to: {os.path.basename(summary_file)}")

print(f"\nðŸŽ¯ ANALYSIS COMPLETE - FINAL ANSWER:")
print(f"Found {total_citations_found} Twitter/X citations across {len(all_citation_results)} August day pages")
print(f"from their last versions in June 2023")

if pages_with_citations:
    print(f"\nðŸ“‹ Pages with Twitter citations:")
    for page, count in pages_with_citations[:5]:  # Show top 5
        print(f"  - {page}: {count} citations")
else:
    print(f"\nðŸ“‹ No Twitter/X citations found in any August day pages from June 2023")
```