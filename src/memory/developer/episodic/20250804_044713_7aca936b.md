### Development Step 42: Scrape Survivor (US) seasons 1–44 winners from Wikipedia and save into JSON file.

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated update of an entertainment news site’s Survivor winners archive to ensure the site always displays the latest data for seasons 1 through 44 without manual editing
- Integration into a TV trivia mobile app’s backend to dynamically fetch and store Survivor winners, powering daily quiz questions and leaderboards
- Data journalism analysis pipeline that extracts Survivor winner data to visualize trends in gender, age, and demographics across the first 44 seasons
- Academic research workflow for media studies students needing structured winner data to correlate reality TV outcomes with cultural and social factors
- Marketing analytics for a TV network using Survivor winner data to model the impact of winner profiles on viewer engagement and advertising revenue
- Chatbot enhancement for a fan community platform that retrieves and responds with accurate Survivor winner information on demand
- Social media automation tool that pulls Survivor winner names each season anniversary to generate scheduled celebratory posts and increase audience interaction

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
page_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/wiki/{page_title}"
print(f"Fetching Survivor page...\nURL: {url}\n")
response = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
})
response.raise_for_status()
print(f"Page fetched successfully (status {response.status_code})\n")

# 3) Parse the HTML with BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')

# 4) Attempt to find the exact two-column Winners table under the "Winners" heading
print("Locating the two-column winners table under the 'Winners' section...\n")
span_winners = soup.find('span', id='Winners')
target_table = None

if span_winners:
    # The <span id="Winners"> is inside the <h2>; we find its parent then look for the next table sibling
    h2 = span_winners.find_parent('h2')
    if h2:
        for sib in h2.find_next_siblings():
            if sib.name == 'table':
                # Inspect header row
                first_tr = sib.find('tr')
                if first_tr:
                    headers = [th.get_text(strip=True).lower() for th in first_tr.find_all(['th','td'], recursive=False)]
                    print(f"  Found table after 'Winners' heading with headers: {headers}")
                    if headers == ['season', 'winner']:
                        target_table = sib
                        print("→ Selected this table as the two-column winners list.\n")
                        break
        if not target_table:
            print("  Table after 'Winners' heading did not match exactly ['Season','Winner']. Falling back.\n")
else:
    print("No 'Winners' section found. Falling back to scanning all tables.\n")

# 5) If still not found, scan every <table> tag for exact 2-column header match
if not target_table:
    print("Scanning ALL <table> tags for an exact two-column ['Season','Winner'] header...\n")
    all_tables = soup.find_all('table')
    print(f"Total tables on page: {len(all_tables)}")
    for idx, tbl in enumerate(all_tables, start=1):
        first_tr = tbl.find('tr')
        if not first_tr:
            continue
        hdr_texts = [th.get_text(strip=True).lower() for th in first_tr.find_all(['th','td'], recursive=False)]
        if len(hdr_texts) == 2:
            print(f"  Table #{idx} header: {hdr_texts}")
            if hdr_texts == ['season', 'winner']:
                target_table = tbl
                print(f"→ Selected table #{idx} as the exact two-column winners table.\n")
                break
    if not target_table:
        print("No exact two-column table found in full scan. Falling back to .wikitable search.\n")

# 6) Final fallback: scan .wikitable tables containing both 'season' & 'winner'
if not target_table:
    print("Fallback: scanning .wikitable tables for headers containing 'season' & 'winner'...\n")
    wiki_tables = soup.find_all('table', class_=lambda c: c and 'wikitable' in c)
    for idx, tbl in enumerate(wiki_tables, start=1):
        first_tr = tbl.find('tr')
        if not first_tr:
            continue
        hdr_texts = [th.get_text(strip=True).lower() for th in first_tr.find_all(['th','td'], recursive=False)]
        print(f"  Wikitable #{idx} header: {hdr_texts}")
        if 'season' in hdr_texts and 'winner' in hdr_texts:
            target_table = tbl
            print(f"→ Selected wikitable #{idx} containing Season & Winner.\n")
            break

# 7) Abort if no appropriate table is found
def bail():
    print("❌ Could not find a suitable winners table. Exiting.")
    sys.exit(1)

if not target_table:
    bail()

# 8) Determine the indices of the Season and Winner columns dynamically
def get_header_indices(table):
    first_tr = table.find('tr')
    cells = first_tr.find_all(['th','td'], recursive=False)
    texts = [cell.get_text(strip=True).lower() for cell in cells]
    return texts.index('season'), texts.index('winner')

season_idx, winner_idx = get_header_indices(target_table)
print(f"Identified column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 9) Extract season→winner pairs for seasons 1 through 44
winners = []
for tr in target_table.find_all('tr')[1:]:  # skip header row
    cells = tr.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # Parse season number
    season_text = cells[season_idx].get_text(strip=True)
    m = re.match(r"^(\d+)", season_text)
    if not m:
        continue
    season_num = int(m.group(1))
    if not (1 <= season_num <= 44):
        continue
    # Extract winner name(s)
    winner_cell = cells[winner_idx]
    link_tags = winner_cell.find_all('a')
    if link_tags:
        names = [a.get_text(strip=True) for a in link_tags if a.get_text(strip=True)]
        winner_name = ' & '.join(names) if names else winner_cell.get_text(strip=True)
    else:
        winner_name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season_num} → Winner: '{winner_name}'")
    winners.append({'season': season_num, 'winner': winner_name})

# 10) Sort by season and verify count
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: Extracted count != 44. Please verify table selection and page structure.")
else:
    print("✅ Correct number of winners found.\n")

# 11) Save the results to JSON
output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"✅ Winners list saved to: {output_path}")
```