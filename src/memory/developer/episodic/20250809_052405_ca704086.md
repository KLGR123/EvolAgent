### Development Step 1: **Title:**  
Identify Astronauts in NASA's APOD for January 21, 2006 and Extract Full Entry Details

**Description**: Search for NASA's Astronomy Picture of the Day from January 21, 2006 to identify the two astronauts visible in the image, with particular focus on determining which astronaut appears smaller. Extract the complete APOD entry including title, description, image details, and any caption information that identifies the astronauts by name.

**Use Cases**:
- Historical space exploration research and astronaut identification for museum exhibit curation
- Automated extraction of astronaut names and image metadata for educational STEM lesson planning
- Archival verification of NASA APOD entries for publishing in astronomy magazines or books
- Media production fact-checking for documentaries featuring specific spacewalks and astronaut appearances
- AI training dataset creation for image recognition models focused on astronaut suits and EVA scenarios
- Space agency internal documentation and mission timeline reconstruction for astronaut activity logs
- Automated content aggregation for astronomy news portals highlighting notable APOD entries
- Data-driven analysis of astronaut representation in public outreach materials for diversity studies

```
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json

print("=== NASA ASTRONOMY PICTURE OF THE DAY - JANUARY 21, 2006 ===\n")
print("Objective: Find APOD from January 21, 2006 showing two astronauts and identify which appears smaller\n")

# Create workspace directory if needed
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# NASA APOD URL format: https://apod.nasa.gov/apod/apYYMMDD.html
# For January 21, 2006: ap060121.html
target_date = "January 21, 2006"
apod_url = "https://apod.nasa.gov/apod/ap060121.html"

print(f"Target date: {target_date}")
print(f"Direct APOD URL: {apod_url}")

print("\n=== APPROACH 1: DIRECT ACCESS TO NASA APOD ARCHIVE ===\n")

try:
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print(f"Accessing: {apod_url}")
    response = requests.get(apod_url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        print(f"✓ Successfully accessed APOD for January 21, 2006")
        print(f"Content length: {len(response.content):,} bytes")
        
        # Parse content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_element = soup.find('b')
        title = title_element.get_text().strip() if title_element else 'No title found'
        print(f"\nTitle: {title}")
        
        # Extract the main content text
        content_text = soup.get_text()
        
        # Save the HTML content
        html_filename = 'workspace/nasa_apod_january_21_2006.html'
        with open(html_filename, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        print(f"✓ HTML content saved to: {html_filename}")
        
        # Extract image information
        print(f"\n=== EXTRACTING IMAGE INFORMATION ===\n")
        
        # Look for image tags
        img_tags = soup.find_all('img')
        print(f"Found {len(img_tags)} image tags")
        
        for i, img in enumerate(img_tags, 1):
            src = img.get('src', 'No src')
            alt = img.get('alt', 'No alt text')
            print(f"  Image {i}: {src}")
            print(f"    Alt text: {alt}")
        
        # Look for links to larger images
        links = soup.find_all('a')
        image_links = []
        
        for link in links:
            href = link.get('href', '')
            if any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):
                image_links.append({
                    'href': href,
                    'text': link.get_text().strip()
                })
        
        if image_links:
            print(f"\nFound {len(image_links)} image links:")
            for i, link in enumerate(image_links, 1):
                print(f"  Link {i}: {link['text']} -> {link['href']}")
        
        # Extract the main description/explanation
        print(f"\n=== EXTRACTING DESCRIPTION AND ASTRONAUT INFORMATION ===\n")
        
        # APOD pages typically have the explanation after the image
        # Look for paragraphs or text blocks
        paragraphs = soup.find_all('p')
        
        if paragraphs:
            print(f"Found {len(paragraphs)} paragraph elements")
            for i, p in enumerate(paragraphs, 1):
                p_text = p.get_text().strip()
                if len(p_text) > 50:  # Only show substantial paragraphs
                    print(f"\nParagraph {i}: {p_text}")
        
        # Search for astronaut-related keywords
        astronaut_keywords = ['astronaut', 'spacewalk', 'EVA', 'extravehicular', 'space suit', 'spacesuit']
        found_keywords = []
        
        for keyword in astronaut_keywords:
            if keyword.lower() in content_text.lower():
                found_keywords.append(keyword)
        
        print(f"\n=== ASTRONAUT KEYWORD ANALYSIS ===\n")
        print(f"Found astronaut-related keywords: {found_keywords}")
        
        # Search for specific astronaut names or references
        # Look for patterns like names in the text
        import re
        
        # Look for potential astronaut names (capitalized words that might be names)
        name_pattern = r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b'
        potential_names = re.findall(name_pattern, content_text)
        
        if potential_names:
            print(f"\nPotential astronaut names found:")
            for name in set(potential_names):  # Remove duplicates
                print(f"  - {name}")
        
        # Look for size-related descriptions
        size_keywords = ['smaller', 'larger', 'bigger', 'tiny', 'distant', 'close', 'near', 'far']
        size_references = []
        
        for keyword in size_keywords:
            if keyword.lower() in content_text.lower():
                # Find context around the keyword
                keyword_pos = content_text.lower().find(keyword.lower())
                if keyword_pos != -1:
                    context_start = max(0, keyword_pos - 100)
                    context_end = min(len(content_text), keyword_pos + 200)
                    context = content_text[context_start:context_end]
                    size_references.append({
                        'keyword': keyword,
                        'context': context.strip()
                    })
        
        if size_references:
            print(f"\n=== SIZE-RELATED DESCRIPTIONS ===\n")
            for ref in size_references:
                print(f"Keyword '{ref['keyword']}' context:")
                print(f"  ...{ref['context']}...")
                print()
        
        # Extract all text content for detailed analysis
        # Clean up the text by removing extra whitespace
        lines = (line.strip() for line in content_text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        clean_text = '\n'.join(chunk for chunk in chunks if chunk)
        
        # Save clean text content
        text_filename = 'workspace/nasa_apod_january_21_2006_text.txt'
        with open(text_filename, 'w', encoding='utf-8') as f:
            f.write(f"NASA APOD - January 21, 2006\n")
            f.write(f"Title: {title}\n")
            f.write(f"URL: {apod_url}\n")
            f.write(f"Extraction Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n")
            f.write(clean_text)
        
        print(f"✓ Clean text saved to: {text_filename}")
        print(f"Clean text length: {len(clean_text):,} characters")
        
        # Create structured data summary
        apod_data = {
            'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'target_date': target_date,
            'apod_url': apod_url,
            'title': title,
            'html_filename': html_filename,
            'text_filename': text_filename,
            'content_length_chars': len(clean_text),
            'image_tags_count': len(img_tags),
            'image_links': image_links,
            'astronaut_keywords_found': found_keywords,
            'potential_names': list(set(potential_names)) if potential_names else [],
            'size_references': size_references,
            'paragraphs_count': len(paragraphs)
        }
        
        # Save structured data
        json_filename = 'workspace/nasa_apod_january_21_2006_data.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(apod_data, f, indent=2, ensure_ascii=False)
        
        print(f"✓ Structured data saved to: {json_filename}")
        
    elif response.status_code == 404:
        print(f"✗ APOD not found (404) - may not exist for this date")
    else:
        print(f"✗ Failed to access APOD (Status: {response.status_code})")
        print(f"Response text preview: {response.text[:200]}...")
        
except Exception as e:
    print(f"✗ Exception occurred while accessing APOD: {str(e)}")

print("\n=== APPROACH 2: SEARCH ENGINE BACKUP ===\n")

# If direct access fails or we need more information, use search
api_key = os.getenv("SERPAPI_API_KEY")

if api_key:
    print(f"API key found: {api_key[:8]}...")
    
    search_queries = [
        'NASA APOD "January 21, 2006" astronauts spacewalk',
        'site:apod.nasa.gov "January 21 2006" two astronauts',
        '"NASA Astronomy Picture of the Day" "January 21, 2006" EVA',
        'NASA APOD 2006 January 21 space suit smaller astronaut'
    ]
    
    print(f"Executing {len(search_queries)} backup search queries...\n")
    
    all_search_results = []
    
    for i, query in enumerate(search_queries, 1):
        print(f"Query {i}: {query}")
        
        params = {
            "q": query,
            "api_key": api_key,
            "engine": "google",
            "google_domain": "google.com",
            "safe": "off",
            "num": 10
        }
        
        try:
            search_response = requests.get("https://serpapi.com/search.json", params=params)
            
            if search_response.status_code == 200:
                results = search_response.json()
                
                if results.get("organic_results"):
                    print(f"  Found {len(results['organic_results'])} results")
                    
                    for j, result in enumerate(results['organic_results'], 1):
                        result_info = {
                            'query_number': i,
                            'result_number': j,
                            'title': result.get('title', 'No title'),
                            'link': result.get('link', 'No link'),
                            'snippet': result.get('snippet', 'No snippet'),
                            'is_nasa_apod': 'apod.nasa.gov' in result.get('link', '').lower(),
                            'has_january_2006': any(term in result.get('snippet', '').lower() + result.get('title', '').lower() 
                                                   for term in ['january 2006', 'jan 2006', '2006 january', 'january 21']),
                            'has_astronaut_reference': any(term in result.get('snippet', '').lower() + result.get('title', '').lower() 
                                                         for term in ['astronaut', 'spacewalk', 'eva', 'space suit'])
                        }
                        
                        all_search_results.append(result_info)
                        
                        if result_info['is_nasa_apod'] and result_info['has_january_2006']:
                            print(f"  ★ RELEVANT NASA APOD RESULT {j}:")
                            print(f"    Title: {result_info['title']}")
                            print(f"    Link: {result_info['link']}")
                            print(f"    Snippet: {result_info['snippet'][:150]}...")
                        elif result_info['has_january_2006'] and result_info['has_astronaut_reference']:
                            print(f"  ◆ RELEVANT RESULT {j}: {result_info['title'][:60]}...")
                        else:
                            print(f"  Result {j}: {result_info['title'][:40]}...")
                else:
                    print(f"  No organic results found")
            else:
                print(f"  Search API Error: {search_response.status_code}")
                
        except Exception as e:
            print(f"  Exception during search: {str(e)}")
        
        print()  # Space between queries
    
    # Save search results if any were found
    if all_search_results:
        search_summary = {
            'search_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'target_date': target_date,
            'search_queries': search_queries,
            'total_results': len(all_search_results),
            'nasa_apod_results': [r for r in all_search_results if r['is_nasa_apod']],
            'all_results': all_search_results
        }
        
        search_filename = 'workspace/nasa_apod_january_21_2006_search.json'
        with open(search_filename, 'w', encoding='utf-8') as f:
            json.dump(search_summary, f, indent=2)
        
        print(f"Search results saved to: {search_filename}")
else:
    print("No SERPAPI_API_KEY found, skipping search backup")

print("\n=== INITIAL ACCESS PHASE COMPLETE ===\n")

# List all created files
if os.path.exists('workspace'):
    print("Files created in workspace:")
    for file in os.listdir('workspace'):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")

print(f"\nNext step: Analyze the downloaded APOD content to identify the two astronauts and determine which appears smaller")
```