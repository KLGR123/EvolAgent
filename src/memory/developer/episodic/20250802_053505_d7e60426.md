### Development Step 7: Download University of Leicester 'Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?' PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automating the bulk download of targeted ecology papers (e.g., “Can Hiccup Supply Enough Fish…”) for a PhD student’s meta-analysis on dragon-related feeding studies
- Corporate compliance team retrieving and archiving the latest financial regulations PDFs from university and government sites to ensure audit readiness
- Patent attorney automatically fetching official patent specification PDFs by querying title keywords to speed up prior art searches
- Market research analyst collecting publicly available annual report PDFs from universities and research institutions for competitor benchmarking
- Data science engineer integrating automated PDF retrieval into an ETL pipeline to extract text for NLP model training on academic nutrition studies
- Investigative journalist sourcing and downloading university-published PDFs on government-funded research to support fact-checking in a news story
- Medical librarian populating an institutional repository by automatically locating and storing open-access clinical trial PDFs via title and DOI lookups

```
import os
import sys
import time
import requests
import json
from urllib.parse import urlparse, parse_qs, unquote

# Try to import BeautifulSoup for HTML parsing
try:
    from bs4 import BeautifulSoup
except ImportError:
    print("ERROR: BeautifulSoup4 is required. Install with 'pip install beautifulsoup4'.")
    sys.exit(1)

# 1. Prepare workspace directory
def ensure_workspace(dir_name='workspace'):
    os.makedirs(dir_name, exist_ok=True)
    return dir_name
workspace_dir = ensure_workspace()
print(f"[INFO] Workspace directory: {workspace_dir}\n")

# 2. Define paper title & search query
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?"
# Include filetype:pdf to bias searches toward PDF
query = f'"{title}" University of Leicester filetype:pdf'
print(f"[INFO] Search query: {query}\n")

# 3. Manual override via env var
pdf_link = os.getenv('DRAGON_PDF_URL')
if pdf_link:
    print(f"[OVERRIDE] Using DRAGON_PDF_URL: {pdf_link}\n")

# 4. Primary: SerpAPI search with exponential backoff
api_key = os.getenv('SERPAPI_API_KEY')
if not pdf_link and api_key:
    print("[SerpAPI] Starting primary search...")
    params = {
        'engine': 'google',
        'q': query,
        'api_key': api_key,
        'num': 10,
        'google_domain': 'google.com',
        'safe': 'off'
    }
    for attempt in range(1, 4):
        print(f"  Attempt {attempt}/3...")
        try:
            resp = requests.get('https://serpapi.com/search.json', params=params, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            for idx, item in enumerate(data.get('organic_results', []), 1):
                link = item.get('link', '')
                print(f"    [#{idx}] {link}")
                low = link.lower()
                if low.endswith('.pdf') or '.pdf?' in low:
                    pdf_link = link
                    print(f"    -> Selected PDF from SerpAPI: {pdf_link}\n")
                    break
            if pdf_link:
                break
        except requests.exceptions.HTTPError as e:
            code = e.response.status_code if e.response else None
            if code == 429:
                backoff = 2 ** (attempt - 1)
                print(f"    429 Rate limit. Backing off {backoff}s...")
                time.sleep(backoff)
                continue
            else:
                print(f"    HTTPError {code}: {e}")
                break
        except Exception as e:
            print(f"    Error: {e}")
            break
    else:
        print("  SerpAPI search exhausted.\n")

# 5. Fallback: DuckDuckGo HTML + decode uddg redirects
if not pdf_link:
    print("---\n[Fallback] DuckDuckGo HTML + uddg redirect decode\n---")
    ddg_resp = requests.get(
        'https://duckduckgo.com/html/',
        params={'q': query},
        headers={'User-Agent': 'Mozilla/5.0'},
        timeout=30
    )
    ddg_resp.raise_for_status()
    soup = BeautifulSoup(ddg_resp.text, 'html.parser')
    candidates = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                real = unquote(q[0])
                if '.pdf' in real.lower():
                    candidates.append(real)
        elif href.lower().endswith('.pdf'):
            candidates.append(href)
    candidates = list(dict.fromkeys(candidates))
    if candidates:
        for i, u in enumerate(candidates, 1): print(f"    {i}. {u}")
        pdf_link = candidates[0]
        print(f"    -> Selected PDF from DuckDuckGo: {pdf_link}\n")

# 6. Fallback: Bing HTML search parsing
if not pdf_link:
    print("---\n[Fallback] Bing HTML search for PDF\n---")
    bing_url = 'https://www.bing.com/search'
    bing_params = {'q': query}
    bing_resp = requests.get(
        bing_url,
        params=bing_params,
        headers={'User-Agent': 'Mozilla/5.0'},
        timeout=30
    )
    bing_resp.raise_for_status()
    soup = BeautifulSoup(bing_resp.text, 'html.parser')
    bing_links = []
    for li in soup.select('li.b_algo h2 a'):
        u = li.get('href')
        if u and u.lower().endswith('.pdf'):
            bing_links.append(u)
    bing_links = list(dict.fromkeys(bing_links))
    if bing_links:
        for i, u in enumerate(bing_links, 1): print(f"    {i}. {u}")
        pdf_link = bing_links[0]
        print(f"    -> Selected PDF from Bing fallback: {pdf_link}\n")

# 7. Fallback: Crossref API + Unpaywall for DOI->OA PDF
if not pdf_link:
    print("---\n[Fallback] Crossref DOI lookup + Unpaywall OA PDF\n---")
    cr_resp = requests.get(
        'https://api.crossref.org/works',
        params={'query.title': title, 'rows': 5},
        timeout=30
    )
    if cr_resp.status_code == 200:
        cr_data = cr_resp.json().get('message', {}).get('items', [])
        if cr_data:
            # Display top candidates
            for i, item in enumerate(cr_data, 1):
                item_title = item.get('title', [''])[0]
                doi = item.get('DOI')
                print(f"    {i}. Title: {item_title}\n       DOI: {doi}\n")
            # choose best match by exact substring
            selected = None
            for item in cr_data:
                if title.lower() in item.get('title', [''])[0].lower():
                    selected = item
                    break
            if not selected:
                selected = cr_data[0]
            doi = selected.get('DOI')
            print(f"    -> Selected DOI: {doi}\n")
            # Unpaywall OA lookup
            email = os.getenv('UNPAY_EMAIL')
            if not email:
                print("ERROR: Set env var UNPAY_EMAIL to your email for Unpaywall API.")
            else:
                up_resp = requests.get(
                    f'https://api.unpaywall.org/v2/{doi}',
                    params={'email': email},
                    timeout=30
                )
                if up_resp.status_code == 200:
                    up_data = up_resp.json()
                    oa = up_data.get('best_oa_location') or {}
                    pdf_url = oa.get('url_for_pdf')
                    if pdf_url:
                        pdf_link = pdf_url
                        print(f"    -> Selected PDF from Unpaywall: {pdf_link}\n")
    else:
        print(f"Crossref API error: {cr_resp.status_code}")

# 8. Abort if still no PDF link
if not pdf_link:
    print(
        "ERROR: No PDF URL found via SerpAPI, DuckDuckGo, Bing, or Unpaywall.\n"
        "Please set the DRAGON_PDF_URL environment variable to the direct PDF link and retry."
    )
    sys.exit(1)

# 9. Download the PDF
pdf_path = os.path.join(workspace_dir, 'dragon_diet_fish_bag.pdf')
print(f"[DOWNLOAD] Fetching: {pdf_link}\n  Saving to: {pdf_path}\n")
headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/pdf'}
total = 0
with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as dl:
    dl.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in dl.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                total += len(chunk)
print(f"[DONE] Download complete: {total} bytes ({total/1024/1024:.2f} MB)")
if total < 10_000:
    print("WARNING: File size <10 KB; may be incomplete.")
else:
    print("SUCCESS: PDF appears valid and downloaded to workspace/dragon_diet_fish_bag.pdf")

```