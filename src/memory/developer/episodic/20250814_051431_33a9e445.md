### Development Step 2: **Title:**  
Locate and Analyze Footnote 397 in Federico Lauria’s 2014 Dissertation for Bibliographic Reference

**Description**: Search for Federico Lauria's 2014 dissertation to locate and examine footnote 397. Extract the complete bibliographic information and identify the specific work referenced in this footnote. Focus on finding the dissertation title, institution, and accessing the full text or at least the section containing footnote 397 to determine what literary or historical work is being cited.

**Use Cases**:
- Academic research verification for scholars needing to trace and confirm specific citations (e.g., footnote 397) in dissertations for literature reviews or meta-analyses
- Library and information science workflows automating the extraction of bibliographic references from dissertations to enhance institutional repository metadata
- Automated legal research tools identifying and extracting precedent citations or referenced works in legal theses for case preparation
- Digital humanities projects mapping intertextual references across historical dissertations to analyze intellectual influence networks
- Editorial and peer review processes where journal editors or reviewers must validate the accuracy and completeness of cited works in submitted manuscripts
- University administrative audits ensuring doctoral dissertations comply with citation standards and that referenced works are properly documented

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, quote
import time
import re

print('=== ACCESSING FEDERICO LAURIA 2014 DISSERTATION - TARGETED APPROACH ===')
print('Target: "The Logic of the Liver: A Deontic View of the Intentionality of Desire"')
print('Author: Federico Lauria (2014)')
print('Objective: Locate footnote 397 and extract bibliographic information')
print('\n' + '='*80 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# First, let's inspect the existing search results to understand what we have
print('=== STEP 1: ANALYZING EXISTING SEARCH RESULTS ===')

# Check what files we have from the previous search
if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    print(f'Found {len(workspace_files)} files in workspace:')
    for file in workspace_files:
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f'- {file} ({file_size:,} bytes)')
else:
    print('No workspace directory found')

# Load and inspect the Federico Lauria matches
matches_file = 'workspace/federico_lauria_matches.json'
if os.path.exists(matches_file):
    print(f'\nInspecting Federico Lauria matches file...')
    with open(matches_file, 'r', encoding='utf-8') as f:
        matches_data = json.load(f)
    
    print(f'Found {len(matches_data)} matches in the file')
    print('\nStructure of first match:')
    if matches_data:
        first_match = matches_data[0]
        print(f'Keys: {list(first_match.keys())}')
        print(f'Sample match:')
        for key, value in first_match.items():
            print(f'  {key}: {str(value)[:100]}...' if len(str(value)) > 100 else f'  {key}: {value}')
else:
    print('No Federico Lauria matches file found')

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}

print('\n=== STEP 2: ACCESSING PHILPAPERS DISSERTATION DIRECTLY ===')

# The most promising result from the search was the PhilPapers entry
philpapers_url = 'https://philpapers.org/rec/LAUQLO'
print(f'Accessing PhilPapers URL: {philpapers_url}')

try:
    response = requests.get(philpapers_url, headers=headers, timeout=30)
    print(f'Status: {response.status_code}')
    print(f'Content type: {response.headers.get("content-type", "unknown")}')
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Save the PhilPapers page
        philpapers_path = 'workspace/philpapers_lauria_dissertation.html'
        with open(philpapers_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f'✓ PhilPapers page saved to: {philpapers_path}')
        
        # Extract key information from the PhilPapers page
        print('\n--- EXTRACTING DISSERTATION METADATA ---')
        
        # Get title
        title_elem = soup.find('h1') or soup.find('title')
        if title_elem:
            title = title_elem.get_text().strip()
            print(f'Title: {title}')
        
        # Look for author information
        author_links = soup.find_all('a', href=lambda x: x and '/profile/' in x)
        authors = []
        for link in author_links:
            author_text = link.get_text().strip()
            if author_text and len(author_text) > 2:
                authors.append(author_text)
        
        if authors:
            print(f'Authors: {", ".join(authors[:3])}')
        
        # Look for publication year
        year_pattern = r'\b(19|20)\d{2}\b'
        page_text = soup.get_text()
        years = re.findall(year_pattern, page_text)
        if '2014' in years:
            print('✓ Confirmed: 2014 publication year found')
        
        # Look for institution information
        institution_keywords = ['university', 'college', 'institut', 'école', 'universit']
        text_lower = page_text.lower()
        institutions = []
        for keyword in institution_keywords:
            if keyword in text_lower:
                # Extract context around institution mentions
                lines = page_text.split('\n')
                for line in lines:
                    if keyword in line.lower() and len(line.strip()) > 10:
                        institutions.append(line.strip())
        
        if institutions:
            print('Potential institutions mentioned:')
            for inst in institutions[:3]:
                print(f'  - {inst}')
        
        # Look for PDF download links
        pdf_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            link_text = link.get_text().strip().lower()
            
            if href and ('.pdf' in href.lower() or 'download' in link_text or 'pdf' in link_text):
                if href.startswith('/'):
                    href = urljoin('https://philpapers.org', href)
                pdf_links.append({
                    'url': href,
                    'text': link.get_text().strip()
                })
        
        print(f'\nFound {len(pdf_links)} potential PDF links:')
        for i, link in enumerate(pdf_links, 1):
            print(f'  {i}. "{link["text"]}" -> {link["url"]}')
        
        # Try to download the dissertation PDF
        if pdf_links:
            for i, pdf_link in enumerate(pdf_links[:2], 1):  # Try first 2 PDF links
                print(f'\nAttempting to download PDF {i}: {pdf_link["url"]}')
                
                try:
                    pdf_response = requests.get(pdf_link['url'], headers=headers, timeout=60)
                    print(f'PDF download status: {pdf_response.status_code}')
                    print(f'Content type: {pdf_response.headers.get("content-type", "unknown")}')
                    
                    if pdf_response.status_code == 200:
                        content_type = pdf_response.headers.get('content-type', '').lower()
                        
                        if 'pdf' in content_type:
                            pdf_filename = f'lauria_dissertation_philpapers_{i}.pdf'
                            pdf_path = f'workspace/{pdf_filename}'
                            
                            with open(pdf_path, 'wb') as pdf_file:
                                pdf_file.write(pdf_response.content)
                            
                            file_size = os.path.getsize(pdf_path)
                            print(f'✓ PDF downloaded: {pdf_path} ({file_size:,} bytes)')
                            
                            # Analyze the PDF for footnote 397
                            print(f'\nAnalyzing PDF for footnote 397...')
                            
                            try:
                                from langchain_community.document_loaders import PyPDFLoader
                                
                                loader = PyPDFLoader(pdf_path)
                                pages = loader.load_and_split()
                                print(f'✓ PDF loaded: {len(pages)} pages')
                                
                                # Search for footnote 397 with multiple patterns
                                footnote_patterns = [
                                    r'footnote\s*397',
                                    r'note\s*397',
                                    r'\b397\.',
                                    r'\b397\s',
                                    r'\b397:',
                                    r'\b397\)',
                                    r'\(397\)',
                                    r'\[397\]',
                                    r'397\s*[A-Z]',  # 397 followed by capital letter
                                    r'\n\s*397',     # 397 at start of line
                                ]
                                
                                footnote_found = False
                                for page_num, page in enumerate(pages, 1):
                                    page_text = page.page_content
                                    page_text_lower = page_text.lower()
                                    
                                    for pattern in footnote_patterns:
                                        matches = re.finditer(pattern, page_text_lower, re.IGNORECASE)
                                        
                                        for match in matches:
                                            print(f'\n🎯 POTENTIAL FOOTNOTE 397 FOUND ON PAGE {page_num}!')
                                            print(f'Pattern matched: {pattern}')
                                            print(f'Match text: "{page_text[match.start():match.end()]}"')
                                            
                                            # Extract substantial context around the footnote
                                            context_start = max(0, match.start() - 1000)
                                            context_end = min(len(page_text), match.end() + 1500)
                                            context = page_text[context_start:context_end]
                                            
                                            print('\n*** FOOTNOTE 397 CONTEXT ***')
                                            print('='*100)
                                            print(context)
                                            print('='*100)
                                            
                                            # Save footnote context
                                            footnote_path = f'workspace/footnote_397_context_philpapers_{i}.txt'
                                            with open(footnote_path, 'w', encoding='utf-8') as f:
                                                f.write(f'FOOTNOTE 397 FOUND IN FEDERICO LAURIA DISSERTATION\n')
                                                f.write(f'Title: The Logic of the Liver: A Deontic View of the Intentionality of Desire\n')
                                                f.write(f'Author: Federico Lauria (2014)\n')
                                                f.write(f'Source: PhilPapers ({pdf_link["url"]})\n')
                                                f.write(f'Page: {page_num}\n')
                                                f.write(f'Pattern: {pattern}\n\n')
                                                f.write('CONTEXT:\n')
                                                f.write(context)
                                                f.write('\n\n')
                                                f.write('FULL PAGE TEXT:\n')
                                                f.write(page_text)
                                            
                                            print(f'\n✓ Footnote context saved to: {footnote_path}')
                                            footnote_found = True
                                            break
                                    
                                    if footnote_found:
                                        break
                                
                                if not footnote_found:
                                    print('⚠ Footnote 397 not found with standard patterns')
                                    print('Searching for any occurrence of "397" in the text...')
                                    
                                    # Broader search for any "397"
                                    all_397_occurrences = []
                                    for page_num, page in enumerate(pages, 1):
                                        page_text = page.page_content
                                        if '397' in page_text:
                                            # Find all occurrences of 397 on this page
                                            for match in re.finditer(r'397', page_text):
                                                context_start = max(0, match.start() - 200)
                                                context_end = min(len(page_text), match.end() + 200)
                                                context = page_text[context_start:context_end]
                                                
                                                all_397_occurrences.append({
                                                    'page': page_num,
                                                    'context': context.strip(),
                                                    'position': match.start()
                                                })
                                    
                                    print(f'Found {len(all_397_occurrences)} occurrences of "397" in the document:')
                                    for i, occ in enumerate(all_397_occurrences[:10], 1):  # Show first 10
                                        print(f'\n  Occurrence {i} (Page {occ["page"]}):') 
                                        print(f'    Context: {occ["context"][:150]}...')
                                    
                                    # Save all 397 occurrences
                                    if all_397_occurrences:
                                        all_397_path = f'workspace/all_397_occurrences_philpapers_{i}.json'
                                        with open(all_397_path, 'w', encoding='utf-8') as f:
                                            json.dump(all_397_occurrences, f, indent=2, ensure_ascii=False)
                                        print(f'\n✓ All 397 occurrences saved to: {all_397_path}')
                            
                            except ImportError:
                                print('⚠ PyPDFLoader not available - PDF saved but not analyzed')
                            except Exception as pdf_error:
                                print(f'❌ PDF analysis error: {str(pdf_error)}')
                            
                            break  # Successfully downloaded and analyzed one PDF
                        
                        else:
                            print(f'⚠ Downloaded content is not PDF: {content_type}')
                    
                    else:
                        print(f'❌ PDF download failed: {pdf_response.status_code}')
                
                except Exception as download_error:
                    print(f'❌ Error downloading PDF {i}: {str(download_error)}')
        
        else:
            print('⚠ No PDF download links found on PhilPapers page')
    
    else:
        print(f'❌ Failed to access PhilPapers: {response.status_code}')

except Exception as e:
    print(f'❌ Error accessing PhilPapers: {str(e)}')

print('\n=== STEP 3: RE-ANALYZING EXISTING DOWNLOADED PDF ===')

# Check if we have the previously downloaded PDF and analyze it more thoroughly
existing_pdf = None
for file in os.listdir('workspace') if os.path.exists('workspace') else []:
    if file.endswith('.pdf') and 'lauria' in file.lower():
        existing_pdf = os.path.join('workspace', file)
        break

if existing_pdf:
    print(f'Found existing PDF: {existing_pdf}')
    file_size = os.path.getsize(existing_pdf)
    print(f'File size: {file_size:,} bytes')
    
    try:
        from langchain_community.document_loaders import PyPDFLoader
        
        loader = PyPDFLoader(existing_pdf)
        pages = loader.load_and_split()
        print(f'✓ Existing PDF loaded: {len(pages)} pages')
        
        # More comprehensive search for footnote 397
        print('\nPerforming comprehensive search for footnote 397...')
        
        # Enhanced footnote patterns
        enhanced_patterns = [
            r'footnote\s*397',
            r'note\s*397', 
            r'\b397\.',
            r'\b397\s+[A-Z]',  # 397 followed by space and capital letter
            r'\b397\)',
            r'\(397\)',
            r'\[397\]',
            r'^\s*397',       # 397 at start of line
            r'\n\s*397',     # 397 after newline
            r'397\s*[–—-]',  # 397 followed by dash
            r'397\s*:',      # 397 followed by colon
            r'397\s*;',      # 397 followed by semicolon
            r'397\s*,',      # 397 followed by comma
            r'\b397\b',      # 397 as whole word
        ]
        
        all_matches = []
        for page_num, page in enumerate(pages, 1):
            page_text = page.page_content
            
            for pattern in enhanced_patterns:
                matches = list(re.finditer(pattern, page_text, re.IGNORECASE | re.MULTILINE))
                
                for match in matches:
                    # Extract more context
                    context_start = max(0, match.start() - 1500)
                    context_end = min(len(page_text), match.end() + 2000)
                    context = page_text[context_start:context_end]
                    
                    match_info = {
                        'page': page_num,
                        'pattern': pattern,
                        'match_text': page_text[match.start():match.end()],
                        'context': context,
                        'position': match.start()
                    }
                    
                    all_matches.append(match_info)
                    
                    print(f'\n🎯 MATCH FOUND ON PAGE {page_num}!')
                    print(f'Pattern: {pattern}')
                    print(f'Match: "{match_info["match_text"]}"')
                    print(f'Context preview: {context[:200]}...')
        
        if all_matches:
            print(f'\n✓ Found {len(all_matches)} potential footnote 397 matches!')
            
            # Save all matches
            matches_path = 'workspace/footnote_397_all_matches.json'
            with open(matches_path, 'w', encoding='utf-8') as f:
                json.dump(all_matches, f, indent=2, ensure_ascii=False)
            print(f'✓ All matches saved to: {matches_path}')
            
            # Save the most promising match
            if all_matches:
                best_match = all_matches[0]  # Take the first match
                best_match_path = 'workspace/footnote_397_best_match.txt'
                with open(best_match_path, 'w', encoding='utf-8') as f:
                    f.write(f'FOOTNOTE 397 - BEST MATCH\n')
                    f.write(f'Source: {existing_pdf}\n')
                    f.write(f'Page: {best_match["page"]}\n')
                    f.write(f'Pattern: {best_match["pattern"]}\n')
                    f.write(f'Match Text: {best_match["match_text"]}\n\n')
                    f.write('FULL CONTEXT:\n')
                    f.write(best_match['context'])
                
                print(f'✓ Best match saved to: {best_match_path}')
                
                # Display the best match
                print('\n*** BEST FOOTNOTE 397 MATCH ***')
                print('='*100)
                print(best_match['context'])
                print('='*100)
        
        else:
            print('⚠ No footnote 397 matches found with enhanced patterns')
            
            # Final fallback: search for any number around 397
            print('\nSearching for numbers near 397 (395-399)...')
            nearby_numbers = []
            for num in range(395, 400):
                for page_num, page in enumerate(pages, 1):
                    if str(num) in page.page_content:
                        nearby_numbers.append((num, page_num))
            
            if nearby_numbers:
                print(f'Found nearby numbers: {nearby_numbers[:10]}')
            else:
                print('No nearby numbers found')
    
    except ImportError:
        print('⚠ PyPDFLoader not available for existing PDF analysis')
    except Exception as existing_pdf_error:
        print(f'❌ Error analyzing existing PDF: {str(existing_pdf_error)}')

else:
    print('No existing Lauria PDF found in workspace')

print('\n=== STEP 4: ALTERNATIVE SEARCH STRATEGIES ===')

# Try searching for the dissertation with the full title
full_title_queries = [
    '"The Logic of the Liver: A Deontic View of the Intentionality of Desire" Federico Lauria',
    '"Logic of the Liver" Lauria 2014 dissertation',
    'Federico Lauria "deontic view" dissertation 2014',
    '"intentionality of desire" Lauria PhD thesis'
]

print('Trying alternative searches with full dissertation title...')

for i, query in enumerate(full_title_queries, 1):
    print(f'\nAlternative search {i}: {query}')
    
    # Try Google Scholar with the full title
    scholar_url = f'https://scholar.google.com/scholar?q={quote(query)}&hl=en'
    print(f'Scholar URL: {scholar_url}')
    
    try:
        time.sleep(2)  # Be respectful
        response = requests.get(scholar_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            results = soup.find_all('div', class_='gs_r gs_or gs_scl')
            
            print(f'Found {len(results)} results')
            
            for j, result in enumerate(results[:3], 1):  # Top 3 results
                title_elem = result.find('h3', class_='gs_rt')
                if title_elem:
                    title_link = title_elem.find('a')
                    title = title_link.get_text() if title_link else title_elem.get_text()
                    url = title_link.get('href') if title_link else None
                    
                    print(f'  Result {j}: {title.strip()}')
                    if url:
                        print(f'    URL: {url}')
                        
                        # If this looks like a direct PDF link, try to download it
                        if url and '.pdf' in url.lower():
                            print(f'    Attempting to download PDF...')
                            try:
                                pdf_response = requests.get(url, headers=headers, timeout=60)
                                if pdf_response.status_code == 200 and 'pdf' in pdf_response.headers.get('content-type', '').lower():
                                    pdf_filename = f'lauria_alternative_search_{i}_{j}.pdf'
                                    pdf_path = f'workspace/{pdf_filename}'
                                    
                                    with open(pdf_path, 'wb') as pdf_file:
                                        pdf_file.write(pdf_response.content)
                                    
                                    file_size = os.path.getsize(pdf_path)
                                    print(f'    ✓ PDF downloaded: {pdf_path} ({file_size:,} bytes)')
                            except Exception as alt_download_error:
                                print(f'    ❌ PDF download failed: {str(alt_download_error)}')
        
        else:
            print(f'Alternative search failed: {response.status_code}')
    
    except Exception as alt_search_error:
        print(f'Alternative search error: {str(alt_search_error)}')

print('\n=== SEARCH SUMMARY ===')
print('Files in workspace:')
if os.path.exists('workspace'):
    for file in os.listdir('workspace'):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f'- {file} ({file_size:,} bytes)')

print('\n=== RECOMMENDATIONS ===')
print('1. ✓ Identified the likely dissertation title: "The Logic of the Liver: A Deontic View of the Intentionality of Desire"')
print('2. ✓ Found the PhilPapers entry for Federico Lauria\'s 2014 work')
print('3. ✓ Conducted comprehensive search for footnote 397 in available documents')
print('4. If footnote 397 was found, check the saved context files for bibliographic information')
print('5. If not found, the dissertation may need to be accessed through:')
print('   - University of Geneva library (likely institution)')
print('   - Direct contact with Federico Lauria')
print('   - Institutional repository access')
print('   - Interlibrary loan services')
```