### Development Step 8: Identify year & gold medalists’ birth countries of Cortina Nordic Worlds without Finland or Norway

**Description**: Search for information about FIS Nordic World Ski Championships held in Cortina d'Ampezzo that did not include participants from Finland or Norway. Focus on identifying which specific year this championship took place, then find the complete list of gold medalists from that event. Extract the birth countries of all gold medalists to identify which country produced a gold medalist at this particular championship.

**Use Cases**:
- Historical sports research for a publisher compiling a book on wartime championships, using automated web scraping to pinpoint a FIS Nordic World Ski event in Cortina d’Ampezzo without Finland or Norway and extract the gold medalists’ birth countries
- Data journalism investigation into national representation gaps at major sporting events, applying targeted scraping and keyword analysis to uncover the specific year Cortina hosted a Finland/Norway-free championship and list its winners
- Museum exhibition planning for an alpine sports hall of fame, leveraging automated context extraction to identify medalist origins from the unique Cortina championship and enrich biographical displays
- Academic study on geopolitics and sports participation during World War II, using precise HTML parsing and table analysis to validate that the 1941 Cortina championships excluded Finnish and Norwegian athletes and document medalist nationalities
- Sports analytics startup building a predictor model for athlete performance by nationality trends, integrating scrapers to harvest historical championship data where key nations were absent and mapping medalist birth countries
- Travel agency designing a niche “Nordic Ski Legends” tour in Cortina d’Ampezzo, using the research script to discover the year without Finnish/Norwegian competitors and craft storytelling around the golden medalists’ homelands
- TV documentary production for a sports network, employing automated scraping and detailed sentence-level analysis to secure accurate contexts for a segment on an anomalous Cortina ski event and its gold medal heroes
- Mobile app developer creating an interactive quiz on ski championship trivia, using extracted years and medalist birth countries from the Cortina event lacking Finland/Norway as dynamic quiz content

```
import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== FIS NORDIC WORLD SKI CHAMPIONSHIPS RESEARCH (VARIABLE SCOPE FIXED) ===\n")
print("Objective: Find Cortina d'Ampezzo championship WITHOUT Finland or Norway participants")
print("Then: Extract all gold medalists and their birth countries\n")

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# First, let's search for information about FIS Nordic World Ski Championships in Cortina d'Ampezzo
print("Step 1: Searching for FIS Nordic World Ski Championships in Cortina d'Ampezzo...\n")

# Key search URLs - start with Wikipedia which usually has comprehensive championship data
search_urls = [
    "https://en.wikipedia.org/wiki/FIS_Nordic_World_Ski_Championships",
    "https://en.wikipedia.org/wiki/Cortina_d%27Ampezzo",
    "https://en.wikipedia.org/wiki/1956_FIS_Nordic_World_Ski_Championships",
    "https://en.wikipedia.org/wiki/1941_FIS_Nordic_World_Ski_Championships"
]

successful_sources = []
failed_sources = []

for url in search_urls:
    print(f"Accessing: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=20)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            # Save the content first
            filename = url.split('/')[-1].replace('%27', '_').replace('%', '_') + '.html'
            filepath = f'workspace/{filename}'
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # Parse and analyze content with proper variable scoping
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Get title safely
            title_element = soup.find('title')
            if title_element:
                title_text = title_element.get_text().strip()
            else:
                title_text = 'No title found'
            
            # Get text content in a safe way
            all_text = soup.get_text()
            lowercase_text = all_text.lower()
            
            # Count occurrences manually to avoid variable scope issues
            cortina_count = 0
            finland_count = 0
            norway_count = 0
            has_championship_data = False
            
            # Count occurrences word by word
            for word in lowercase_text.split():
                if 'cortina' in word:
                    cortina_count += 1
                if 'finland' in word:
                    finland_count += 1
                if 'norway' in word:
                    norway_count += 1
            
            # Check for championship indicators
            championship_keywords = ['championship', 'gold medal', 'winner', 'result']
            for keyword in championship_keywords:
                if keyword in lowercase_text:
                    has_championship_data = True
                    break
            
            # Store results
            source_info = {
                'url': url,
                'title': title_text,
                'filename': filepath,
                'cortina_mentions': cortina_count,
                'has_championship_info': has_championship_data,
                'finland_mentions': finland_count,
                'norway_mentions': norway_count,
                'content_length': len(response.text)
            }
            
            successful_sources.append(source_info)
            
            print(f"  ✓ Title: {title_text}")
            print(f"  ✓ Cortina mentions: {cortina_count}")
            print(f"  ✓ Championship info: {has_championship_data}")
            print(f"  ✓ Finland mentions: {finland_count}")
            print(f"  ✓ Norway mentions: {norway_count}")
            print(f"  ✓ Content length: {len(response.text)} characters")
            print(f"  ✓ Saved to: {filepath}\n")
            
        else:
            failed_sources.append({'url': url, 'status': response.status_code})
            print(f"  ✗ Failed - Status: {response.status_code}\n")
            
    except Exception as e:
        failed_sources.append({'url': url, 'error': str(e)})
        print(f"  ✗ Error: {str(e)}\n")
    
    time.sleep(2)  # Be respectful to servers

print(f"=== INITIAL SEARCH RESULTS ===\n")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources\n")

# Analyze the most promising sources
if successful_sources:
    print("=== ANALYZING SUCCESSFUL SOURCES ===\n")
    
    # Sort sources by relevance
    def source_priority(source):
        return (source['cortina_mentions'], source['has_championship_info'])
    
    priority_sources = sorted(successful_sources, key=source_priority, reverse=True)
    
    for i, source in enumerate(priority_sources, 1):
        print(f"{i}. {source['url']}")
        print(f"   Title: {source['title']}")
        print(f"   Cortina mentions: {source['cortina_mentions']}")
        print(f"   Championship info: {source['has_championship_info']}")
        print(f"   Finland/Norway mentions: {source['finland_mentions']}/{source['norway_mentions']}")
        
        if source['cortina_mentions'] > 0 and source['has_championship_info']:
            print(f"   *** HIGH PRIORITY SOURCE ***")
        print()
    
    # Now let's examine the content of high-priority sources in detail
    print("=== DETAILED CONTENT ANALYSIS ===\n")
    
    analysis_results = []
    all_cortina_contexts = []  # Store all contexts for saving to file
    
    for idx, source in enumerate(priority_sources[:3], 1):  # Analyze top 3 sources
        print(f"Analyzing: {source['url']}\n")
        
        # Read the saved HTML file
        with open(source['filename'], 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        print("Searching for Cortina d'Ampezzo championship years...")
        
        # Get all text and find Cortina mentions
        page_content = soup.get_text()
        
        # Find sentences mentioning Cortina
        sentences = re.split(r'[.!?]', page_content)
        cortina_sentences = []
        
        for sentence in sentences:
            if 'cortina' in sentence.lower():
                clean_sentence = sentence.strip()
                if len(clean_sentence) > 10:  # Filter out very short fragments
                    cortina_sentences.append(clean_sentence)
        
        print(f"Found {len(cortina_sentences)} sentences mentioning Cortina:\n")
        
        potential_years = []
        for j, sentence in enumerate(cortina_sentences[:15], 1):  # Show first 15
            print(f"{j}. {sentence}")
            
            # Store context for file output
            all_cortina_contexts.append(f"Source {idx}: {sentence}")
            
            # Look for years in this sentence
            year_matches = re.findall(r'\b(19\d{2}|20\d{2})\b', sentence)
            if year_matches:
                print(f"   Years found: {year_matches}")
            
            # Check for Finland/Norway absence - this is the key criterion!
            sentence_lower = sentence.lower()
            has_finland = 'finland' in sentence_lower
            has_norway = 'norway' in sentence_lower
            
            if not has_finland and not has_norway and year_matches:
                print(f"   *** POTENTIAL TARGET: {year_matches} - NO FINLAND/NORWAY MENTIONED ***")
                for year in year_matches:
                    potential_years.append({
                        'year': year,
                        'context': sentence,
                        'sentence_number': j
                    })
            print()
        
        # Analyze tables for championship results - FIXED VARIABLE SCOPE
        tables = soup.find_all('table')
        print(f"Found {len(tables)} tables in this source")
        
        relevant_tables = []
        for table_idx, table in enumerate(tables):
            # CRITICAL FIX: Define table_content variable properly within loop scope
            current_table_content = table.get_text().lower()
            
            # Check for championship/medal content using the properly scoped variable
            medal_words = ['gold', 'medal', 'winner', 'champion', 'result']
            has_medals = any(word in current_table_content for word in medal_words)
            
            # Check for Cortina
            has_cortina_ref = 'cortina' in current_table_content
            
            # Check for Finland/Norway
            has_finland_ref = 'finland' in current_table_content
            has_norway_ref = 'norway' in current_table_content
            
            if has_medals or has_cortina_ref:
                table_info = {
                    'table_index': table_idx,
                    'has_medal_info': has_medals,
                    'has_cortina': has_cortina_ref,
                    'has_finland': has_finland_ref,
                    'has_norway': has_norway_ref
                }
                relevant_tables.append(table_info)
                
                print(f"  Table {table_idx}: Medal info={has_medals}, Cortina={has_cortina_ref}, Finland={has_finland_ref}, Norway={has_norway_ref}")
                
                # Highlight tables with Cortina but no Finland/Norway
                if has_cortina_ref and not has_finland_ref and not has_norway_ref:
                    print(f"    *** HIGHLY RELEVANT - Cortina mentioned, no Finland/Norway ***")
        
        print(f"Found {len(relevant_tables)} relevant tables\n")
        
        # Save analysis for this source
        source_analysis = {
            'url': source['url'],
            'cortina_sentences': cortina_sentences,
            'potential_target_years': potential_years,
            'relevant_tables_count': len(relevant_tables),
            'total_tables': len(tables),
            'table_details': relevant_tables
        }
        
        analysis_filename = f'workspace/cortina_detailed_analysis_{idx}.json'
        with open(analysis_filename, 'w') as f:
            json.dump(source_analysis, f, indent=2)
        
        analysis_results.append(source_analysis)
        
        print(f"Detailed analysis saved to: {analysis_filename}\n")
        print("-" * 50 + "\n")
    
    # Save all Cortina contexts to file as recommended by tester
    contexts_filename = 'workspace/cortina_championship_contexts.txt'
    with open(contexts_filename, 'w', encoding='utf-8') as f:
        f.write("=== CORTINA D'AMPEZZO CHAMPIONSHIP CONTEXTS ===\n\n")
        f.write(f"Total contexts found: {len(all_cortina_contexts)}\n\n")
        for i, context in enumerate(all_cortina_contexts, 1):
            f.write(f"{i}. {context}\n\n")
    
    print(f"All Cortina contexts saved to: {contexts_filename}\n")
    
    # Summary of findings
    print("=== SUMMARY OF FINDINGS ===\n")
    
    all_potential_years = []
    for analysis in analysis_results:
        all_potential_years.extend(analysis['potential_target_years'])
    
    if all_potential_years:
        print(f"Found {len(all_potential_years)} potential target years:")
        unique_years = set()
        for target in all_potential_years:
            year = target['year']
            if year not in unique_years:
                unique_years.add(year)
                print(f"  Year: {year} - Context: {target['context'][:100]}...")
        
        print(f"\nUnique potential target years: {sorted(unique_years)}")
        print("\nThese years represent Cortina d'Ampezzo events with no Finland/Norway mentions.")
        print("Next step: Focus on championship-specific years and extract gold medalist data.")
    else:
        print("No clear target years identified in initial analysis.")
        print("May need to examine specific championship pages or use different search strategy.")
else:
    print("No sources successfully accessed. Need to try alternative search methods.\n")

# Save comprehensive research summary
research_summary = {
    'research_target': 'FIS Nordic World Ski Championships in Cortina d\'Ampezzo without Finland/Norway',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'successful_sources': len(successful_sources),
    'failed_sources': len(failed_sources),
    'sources_data': successful_sources,
    'failed_attempts': failed_sources,
    'analysis_completed': len(successful_sources) > 0,
    'potential_target_years_found': len([item for sublist in [analysis.get('potential_target_years', []) for analysis in analysis_results] for item in sublist]) if 'analysis_results' in locals() else 0,
    'next_steps': [
        'Identify specific year of Cortina championship without Finland/Norway',
        'Extract complete gold medalist list from that championship',
        'Research birth countries of all gold medalists'
    ]
}

with open('workspace/cortina_research_summary.json', 'w') as f:
    json.dump(research_summary, f, indent=2)

print("=== PHASE 1 COMPLETE ===\n")
print(f"Research summary saved to: workspace/cortina_research_summary.json")
print(f"Successfully gathered {len(successful_sources)} sources for analysis")
print(f"Cortina contexts saved to: workspace/cortina_championship_contexts.txt")
print("Next phase: Deep analysis to identify the specific championship year and gold medalists")
```