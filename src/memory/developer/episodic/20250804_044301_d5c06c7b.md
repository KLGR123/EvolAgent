### Development Step 26: Extract Survivor US seasons 1–44 winners into workspace JSON file

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Reality TV network automating anniversary posts by extracting Survivor season–winner pairs to schedule social media campaigns
- Fan-run website generating dynamic winner profile pages for each Survivor season (1–44) from the JSON data
- Trivia app populating its quiz database with season numbers and winners for automated question generation
- Streaming service enriching episode metadata with correct Survivor winner tags to improve user recommendations
- Data journalism project analyzing gender and age trends among Survivor winners across seasons 1–44
- Academic research team importing season winners into statistical models to study competitive reality TV outcomes
- Marketing analytics dashboard correlating Survivor winner announcements with spikes in merchandise sales
- Chatbot integration that fetches and replies with Survivor winners on demand for fan Q&A interactions

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable Wikipedia page for Survivor (American TV series)
page_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={page_title}&printable=yes"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching Survivor printable page...\nURL: {url}\n")
response = requests.get(url, headers=headers)
response.raise_for_status()
print(f"Page fetched successfully (status code: {response.status_code})\n")

# 3) Parse HTML with BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')

# 4) Find the table with both 'Season' and 'Winner' headers
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} tables with class 'wikitable'.\n")
target_table = None
for idx, tbl in enumerate(tables, start=1):
    first_row = tbl.find('tr')
    headers = [cell.get_text(strip=True).lower() for cell in first_row.find_all(['th','td'], recursive=False)]
    print(f"Table {idx} headers: {headers}")
    if 'season' in headers and 'winner' in headers:
        target_table = tbl
        print(f"→ Selected table {idx} for parsing.\n")
        break

if not target_table:
    print("❌ Could not find a table containing both 'Season' and 'Winner'. Exiting.")
    sys.exit(1)

# 5) Determine indices for 'Season' and 'Winner' columns
header_cells = target_table.find('tr').find_all(['th','td'], recursive=False)
col_names = [c.get_text(strip=True).lower() for c in header_cells]
season_idx = col_names.index('season')
winner_idx = col_names.index('winner')
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 6) Helper to clean and extract winner name

def clean_winner(cell):
    # remove any footnotes
    for sup in cell.find_all('sup'):
        sup.decompose()
    # a) prefer <a> link texts with letters
    for a in cell.find_all('a'):
        text = a.get_text(strip=True)
        if re.search(r'[A-Za-z]', text):
            return text
    # b) next prefer <b> tag texts with letters
    for b in cell.find_all('b'):
        text = b.get_text(strip=True)
        if re.search(r'[A-Za-z]', text):
            return text
    # c) fallback: full text minus trailing vote counts
    full_text = cell.get_text(' ', strip=True)
    return re.sub(r"\s*\d+(?:[–-]\d+)*$", "", full_text).strip()

# 7) Iterate rows and extract season→winner for 1–44
winners = []
for row in target_table.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    season_text = cells[season_idx].get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        continue
    if not (1 <= season_num <= 44):
        continue
    winner_name = clean_winner(cells[winner_idx])
    print(f"Parsed Season {season_num} → Winner: {winner_name}")
    winners.append({'season': season_num, 'winner': winner_name})

# 8) Sort by season and verify count
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Please verify the logic.")

# 9) Write to JSON
output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"✅ Winners list saved to: {output_path}")
```