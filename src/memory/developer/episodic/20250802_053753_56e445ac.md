### Development Step 16: Download University of Leicester 'Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?' PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic librarian automating the discovery and download of University of Leicester research PDFs from saved search result pages for digital repository curation
- Research group conducting a systematic literature review by extracting and validating “le.ac.uk” URLs from DuckDuckGo and Bing archives to ensure comprehensive paper coverage
- SEO strategist monitoring competitor academic domain (le.ac.uk) appearances in search result snapshots to optimize institutional backlink profiles
- Compliance officer auditing archived web searches for unauthorized or inaccurate references to the university’s website to enforce citation policies
- Data engineer feeding scraped “le.ac.uk” URLs into a knowledge graph pipeline for enhanced educational content indexing and retrieval
- Research software developer integrating link-inspection scripts into CI/CD to automatically flag missing or broken university domain links before deployment
- Academic project manager verifying that core domain-specific papers (e.g., “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?”) have been found and placed in the shared workspace for analysis
- Information security analyst scanning DuckDuckGo redirect patterns in offline search dumps to detect potential phishing or malicious redirects targeting the university domain

```
import os
import sys
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# Step 1: Ensure workspace exists and contains the search result HTML files
dir_workspace = 'workspace'
if not os.path.isdir(dir_workspace):
    print(f"ERROR: Workspace directory '{dir_workspace}' does not exist.")
    print("Please run the search scripts first to save the DuckDuckGo and Bing HTML files.")
    sys.exit(1)

file_ddg = os.path.join(dir_workspace, 'site_le_duckduckgo_search.html')
file_bing = os.path.join(dir_workspace, 'site_le_bing_search.html')
for fpath in (file_ddg, file_bing):
    if not os.path.isfile(fpath):
        print(f"ERROR: Required file not found: {fpath}")
        print("Make sure you have saved the search result pages into workspace/ before running this.")
        sys.exit(1)

print(f"[INFO] Inspecting for 'le.ac.uk' links in:\n - {file_ddg}\n - {file_bing}\n")

# Container for all detected le.ac.uk URLs
leicester_urls = []

def inspect_html(path, label):
    """
    Parse the given HTML file for any links to le.ac.uk, including
    direct <a href> and DuckDuckGo uddg redirect links, plus raw text occurrences.
    """
    print(f"[INSPECT] {label}")
    with open(path, 'r', encoding='utf-8') as f:
        raw = f.read()
    soup = BeautifulSoup(raw, 'html.parser')

    # 1) Check all <a> tags
    for a in soup.find_all('a', href=True):
        href = a['href']
        # handle DuckDuckGo redirect pattern (/l/?uddg=...)
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                decoded = unquote(q[0])
                if 'le.ac.uk' in urlparse(decoded).netloc.lower():
                    print(f"  [REDIRECT] decoded uddg -> {decoded}")
                    leicester_urls.append(decoded)
        # direct link
        net = urlparse(href).netloc.lower()
        if 'le.ac.uk' in net:
            print(f"  [DIRECT] {href}")
            leicester_urls.append(href)

    # 2) Raw text scan for any 'le.ac.uk' not in an <a> tag
    lines = raw.splitlines()
    for idx, line in enumerate(lines):
        if 'le.ac.uk' in line and '<a ' not in line:
            start = max(0, idx - 2)
            end = min(len(lines), idx + 3)
            print(f"  [TEXT] Context around line {idx+1}:")
            for i in range(start, end):
                print(f"    {i+1:4d}: {lines[i].strip()}")
            print('')

# Run inspection on both files
inspect_html(file_ddg, 'DuckDuckGo results')
print('-' * 60)
inspect_html(file_bing, 'Bing results')

# Deduplicate
unique = []
for u in leicester_urls:
    if u not in unique:
        unique.append(u)

# Step 4: Write to output report
out_path = os.path.join(dir_workspace, 'leicester_link_inspection.txt')
with open(out_path, 'w', encoding='utf-8') as out_f:
    out_f.write('Detected University of Leicester (le.ac.uk) landing URLs and contexts:\n\n')
    for url in unique:
        out_f.write(url + '\n')

print(f"[DONE] Found {len(unique)} unique Leicester-related URLs.")
print(f"[INFO] Detailed list written to: {out_path}")
print("Next: Open and review 'leicester_link_inspection.txt' to identify the correct landing-page URL(s) on le.ac.uk for the target paper.")
```