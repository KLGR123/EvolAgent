### Development Step 12: Alternative Search Methods to Identify Book and Protagonist via 1992 Soviet Paratrooper Crackdown

**Description**: Search for alternative approaches to identify the book and protagonist, including: (1) Search for '1992 crackdown Soviet paratrooper organization' to find historical events that might match the scenario, (2) Look for books about specific Soviet paratroopers who became organization co-founders and faced crackdowns, (3) Search for translated titles or alternative names for 'Sacred Desire' in different languages, (4) Investigate specific 1992 crackdowns on organizations co-founded by former Soviet military personnel, focusing on identifying the actual historical figures and events that might be fictionalized in the book.

**Use Cases**:
- Legal e-discovery and case law research automation: load multiple JSON exports from legal databases, standardize case titles, URLs, and summaries, then filter for mentions of specific statutes or privileged communications to identify key precedents and judges.
- Competitive intelligence on product launches: aggregate JSON search results from news APIs, trade publications, and corporate press releases, extract product names, release dates, and executive quotes, and pinpoint competitor strategies in real time.
- Academic systematic literature review: ingest JSON outputs from PubMed, IEEE Xplore, and Scopus, unify metadata fields, then search for terms like ‚Äúmeta-analysis‚Äù or ‚Äúrandomized controlled trial‚Äù to compile a definitive list of papers and lead authors in a research topic.
- Cyber threat intelligence synthesis: collect JSON feeds from multiple cybersecurity vendors, normalize indicator fields (malware family, actor name, TTPs), and automatically flag reports mentioning high-risk ransomware groups or newly observed vulnerabilities.
- Pharmaceutical clinical trial monitoring: import JSON datasets from ClinicalTrials.gov and EudraCT, harmonize trial identifiers, study titles, and investigator names, then filter for ‚ÄúPhase III Alzheimer‚Äôs‚Äù or adverse event keywords to support regulatory submissions.
- Media and reputation monitoring for brand management: unify JSON news and social media search outputs, standardize article titles, URLs, and snippets, then detect and alert on mentions of a brand name alongside ‚Äúrecall,‚Äù ‚Äúlawsuit,‚Äù or ‚ÄúCEO statement.‚Äù
- M&A due diligence and regulatory risk analysis: merge JSON‚Äêformatted news alerts, SEC filings, and industry reports, extract references to ‚Äúantitrust,‚Äù ‚Äúinvestigation,‚Äù or ‚Äúfines,‚Äù and identify which target companies and board members are implicated.
- Open-source human rights investigation: consolidate JSON reports from NGOs, UN databases, and news outlets, standardize event descriptions, locations, and individual names, then filter for ‚Äúforced displacement,‚Äù ‚Äúcrackdown,‚Äù or specific years (e.g., 1992) to build a timeline of abuses and responsible actors.

```
import os
import json
import re

# Create workspace directory
os.makedirs('workspace', exist_ok=True)

print("=== CORRECTED FINAL ANALYSIS: SACRED DESIRE BOOK AND PROTAGONIST ===")
print("Focus: Fix variable errors and complete the comprehensive analysis")
print("Goal: Extract definitive answer about Sacred Desire book and protagonist")
print("\n" + "="*70)

# Load and analyze all JSON files with proper error handling
print("üìÅ LOADING ALL WORKSPACE FILES:")
print("-" * 35)

workspace_files = []
for file in os.listdir('workspace'):
    if file.endswith('.json'):
        workspace_files.append(file)
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")

print(f"\nProcessing {len(workspace_files)} JSON files...")

# Extract all findings with robust error handling
all_findings = []
file_summary = {}

for filename in workspace_files:
    file_path = os.path.join('workspace', filename)
    print(f"\nüîç Processing: {filename}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        findings_count = 0
        
        # Method 1: Direct findings array
        if 'findings' in data and isinstance(data['findings'], list):
            for item in data['findings']:
                if isinstance(item, dict):
                    standardized = {
                        'title': str(item.get('title', '')),
                        'url': str(item.get('url', item.get('href', item.get('link', '')))),
                        'description': str(item.get('description', item.get('body', item.get('snippet', '')))),
                        'query': str(item.get('query', '')),
                        'source_file': filename
                    }
                    all_findings.append(standardized)
                    findings_count += 1
        
        # Method 2: searches_performed structure
        if 'searches_performed' in data and isinstance(data['searches_performed'], list):
            for search in data['searches_performed']:
                if isinstance(search, dict) and 'results' in search:
                    for result in search['results']:
                        if isinstance(result, dict):
                            standardized = {
                                'title': str(result.get('title', '')),
                                'url': str(result.get('url', result.get('href', result.get('link', '')))),
                                'description': str(result.get('description', result.get('body', result.get('snippet', '')))),
                                'query': str(search.get('query', '')),
                                'source_file': filename
                            }
                            all_findings.append(standardized)
                            findings_count += 1
        
        # Method 3: searches structure
        if 'searches' in data and isinstance(data['searches'], list):
            for search in data['searches']:
                if isinstance(search, dict) and 'results' in search:
                    for result in search['results']:
                        if isinstance(result, dict):
                            standardized = {
                                'title': str(result.get('title', '')),
                                'url': str(result.get('url', result.get('href', result.get('link', '')))),
                                'description': str(result.get('description', result.get('body', result.get('snippet', '')))),
                                'query': str(search.get('query', '')),
                                'source_file': filename
                            }
                            all_findings.append(standardized)
                            findings_count += 1
        
        # Method 4: Various result arrays
        for key in ['new_results', 'all_results', 'all_book_candidates', 'all_relevant_findings']:
            if key in data and isinstance(data[key], list):
                for item in data[key]:
                    if isinstance(item, dict):
                        standardized = {
                            'title': str(item.get('title', '')),
                            'url': str(item.get('url', item.get('href', item.get('link', '')))),
                            'description': str(item.get('description', item.get('body', item.get('snippet', '')))),
                            'query': str(item.get('query', item.get('query_text', ''))),
                            'source_file': filename
                        }
                        all_findings.append(standardized)
                        findings_count += 1
        
        file_summary[filename] = findings_count
        print(f"   Extracted: {findings_count} findings")
        
    except Exception as e:
        print(f"   Error: {str(e)}")
        file_summary[filename] = 0

print(f"\nüìä TOTAL FINDINGS EXTRACTED: {len(all_findings)}")
print("\nFile breakdown:")
for filename, count in file_summary.items():
    print(f"  {filename}: {count} findings")

# Now analyze findings with FIXED variable handling
print("\n\nüîç ANALYZING FINDINGS FOR SACRED DESIRE AND PROTAGONIST:")
print("=" * 55)

# Initialize analysis results
sacred_desire_matches = []
islam_karimov_matches = []
book_references = []
protagonist_clues = []
all_names_found = set()

# Process each finding with proper variable definitions
for i, finding in enumerate(all_findings):
    # FIXED: Ensure all variables are properly defined as strings
    title_text = str(finding.get('title', '')).strip()
    description_text = str(finding.get('description', '')).strip()
    url_text = str(finding.get('url', '')).strip()
    query_text = str(finding.get('query', '')).strip()
    
    # FIXED: Create combined text for analysis
    title_lower = title_text.lower()
    description_lower = description_text.lower()
    url_lower = url_text.lower()
    query_lower = query_text.lower()
    combined_lower = f"{title_lower} {description_lower} {url_lower} {query_lower}"
    
    # Check for Sacred Desire mentions
    if 'sacred desire' in combined_lower:
        sacred_desire_matches.append({
            'title': title_text,
            'url': url_text,
            'description': description_text,
            'query': query_text,
            'source_file': finding.get('source_file', 'Unknown'),
            'match_location': 'title' if 'sacred desire' in title_lower else 'description' if 'sacred desire' in description_lower else 'other'
        })
        print(f"\nüéØ SACRED DESIRE MATCH #{len(sacred_desire_matches)}:")
        print(f"   Title: {title_text[:80]}..." if len(title_text) > 80 else f"   Title: {title_text}")
        print(f"   URL: {url_text}")
        print(f"   Description: {description_text[:120]}..." if len(description_text) > 120 else f"   Description: {description_text}")
        print(f"   Source: {finding.get('source_file', 'Unknown')}")
    
    # Check for Islam Karimov mentions
    if 'islam karimov' in combined_lower:
        islam_karimov_matches.append({
            'title': title_text,
            'url': url_text,
            'description': description_text,
            'query': query_text,
            'source_file': finding.get('source_file', 'Unknown')
        })
        print(f"\nüö® ISLAM KARIMOV MATCH #{len(islam_karimov_matches)}:")
        print(f"   Title: {title_text}")
        print(f"   Description: {description_text[:150]}..." if len(description_text) > 150 else f"   Description: {description_text}")
        print(f"   Source: {finding.get('source_file', 'Unknown')}")
    
    # Check for book references with military context
    book_terms = ['book', 'novel', 'author', 'published', 'fiction', 'literature']
    military_terms = ['soviet', 'russian', 'paratrooper', 'military', 'vdv', 'airborne']
    
    book_score = sum(1 for term in book_terms if term in combined_lower)
    military_score = sum(1 for term in military_terms if term in combined_lower)
    
    if book_score >= 2 and military_score >= 1:
        book_references.append({
            'title': title_text,
            'description': description_text,
            'book_score': book_score,
            'military_score': military_score,
            'total_score': book_score + military_score,
            'source_file': finding.get('source_file', 'Unknown')
        })
    
    # Extract names using regex
    name_pattern = r'\b[A-Z][a-z]+ [A-Z][a-z]+\b'
    names_in_description = re.findall(name_pattern, description_text)
    for name in names_in_description:
        if name.lower() not in ['sacred desire', 'soviet union', 'united states', 'new york']:
            all_names_found.add(name)
    
    # Check for protagonist/co-founder clues
    protagonist_terms = ['protagonist', 'co-founder', 'founder', 'leader', 'commander']
    org_terms = ['organization', 'association', 'movement', 'group']
    year_terms = ['1992', 'crackdown', 'disbanded']
    
    protagonist_score = sum(1 for term in protagonist_terms if term in combined_lower)
    org_score = sum(1 for term in org_terms if term in combined_lower)
    year_score = sum(1 for term in year_terms if term in combined_lower)
    
    if protagonist_score >= 1 and (org_score >= 1 or year_score >= 1):
        protagonist_clues.append({
            'title': title_text,
            'description': description_text[:200],
            'protagonist_score': protagonist_score,
            'org_score': org_score,
            'year_score': year_score,
            'total_score': protagonist_score + org_score + year_score,
            'source_file': finding.get('source_file', 'Unknown')
        })

# Display comprehensive results
print(f"\n\nüìä ANALYSIS RESULTS SUMMARY:")
print("=" * 30)
print(f"Sacred Desire matches: {len(sacred_desire_matches)}")
print(f"Islam Karimov matches: {len(islam_karimov_matches)}")
print(f"Book references: {len(book_references)}")
print(f"Protagonist clues: {len(protagonist_clues)}")
print(f"Unique names found: {len(all_names_found)}")

# Detailed Sacred Desire analysis
if sacred_desire_matches:
    print("\n\nüéØ DETAILED SACRED DESIRE ANALYSIS:")
    print("=" * 35)
    
    for i, match in enumerate(sacred_desire_matches, 1):
        print(f"\n{i}. Sacred Desire Reference:")
        print(f"   Title: {match['title']}")
        print(f"   URL: {match['url']}")
        print(f"   Description: {match['description']}")
        print(f"   Query: {match['query']}")
        print(f"   Source: {match['source_file']}")
        print(f"   Match in: {match['match_location']}")
        
        # Look for protagonist information in this specific match
        desc_lower = match['description'].lower()
        title_lower = match['title'].lower()
        
        if 'protagonist' in f"{title_lower} {desc_lower}":
            print(f"   üéØ CONTAINS PROTAGONIST REFERENCE")
        
        if 'soviet' in f"{title_lower} {desc_lower}":
            print(f"   ü™Ç CONTAINS SOVIET REFERENCE")
        
        if 'paratrooper' in f"{title_lower} {desc_lower}":
            print(f"   ü™Ç CONTAINS PARATROOPER REFERENCE")
        
        # Extract names from this specific match
        names_in_match = re.findall(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b', match['description'])
        if names_in_match:
            print(f"   üë§ Names in this match: {', '.join(names_in_match)}")

# Islam Karimov analysis
if islam_karimov_matches:
    print("\n\nüö® ISLAM KARIMOV ANALYSIS:")
    print("=" * 25)
    
    for i, match in enumerate(islam_karimov_matches, 1):
        print(f"\n{i}. Islam Karimov Reference:")
        print(f"   Title: {match['title']}")
        print(f"   URL: {match['url']}")
        print(f"   Description: {match['description']}")
        print(f"   Source: {match['source_file']}")
        
        # Check if this also mentions Sacred Desire
        combined_text = f"{match['title']} {match['description']}".lower()
        if 'sacred desire' in combined_text:
            print(f"   üéØ ALSO MENTIONS SACRED DESIRE - POTENTIAL ANSWER!")
        
        if 'protagonist' in combined_text:
            print(f"   üìö MENTIONS PROTAGONIST")
        
        if 'paratrooper' in combined_text:
            print(f"   ü™Ç MENTIONS PARATROOPER")

# Top book references
if book_references:
    print("\n\nüìö TOP MILITARY BOOK REFERENCES:")
    print("=" * 32)
    
    sorted_books = sorted(book_references, key=lambda x: x['total_score'], reverse=True)
    for i, book in enumerate(sorted_books[:3], 1):
        print(f"\n{i}. Book Reference (Score: {book['total_score']}):")
        print(f"   Title: {book['title']}")
        print(f"   Description: {book['description'][:150]}...")
        print(f"   Book indicators: {book['book_score']}, Military indicators: {book['military_score']}")
        print(f"   Source: {book['source_file']}")

# All unique names found
if all_names_found:
    print(f"\n\nüë§ ALL UNIQUE NAMES FOUND ({len(all_names_found)}):")
    print("=" * 30)
    names_list = sorted(list(all_names_found))
    for i, name in enumerate(names_list[:15], 1):  # Show first 15 names
        print(f"   {i:2d}. {name}")
    if len(names_list) > 15:
        print(f"   ... and {len(names_list) - 15} more names")

# Save comprehensive results
final_results = {
    'analysis_timestamp': '2025-01-10',
    'total_findings_processed': len(all_findings),
    'files_analyzed': len(workspace_files),
    'file_breakdown': file_summary,
    'sacred_desire_matches': sacred_desire_matches,
    'islam_karimov_matches': islam_karimov_matches,
    'book_references': [{
        'title': book['title'],
        'description': book['description'][:200],
        'scores': {'book': book['book_score'], 'military': book['military_score'], 'total': book['total_score']},
        'source_file': book['source_file']
    } for book in sorted(book_references, key=lambda x: x['total_score'], reverse=True)[:10]],
    'protagonist_clues': [{
        'title': clue['title'],
        'description': clue['description'],
        'scores': {'protagonist': clue['protagonist_score'], 'org': clue['org_score'], 'year': clue['year_score'], 'total': clue['total_score']},
        'source_file': clue['source_file']
    } for clue in sorted(protagonist_clues, key=lambda x: x['total_score'], reverse=True)[:10]],
    'all_names_found': sorted(list(all_names_found))
}

results_file = 'workspace/final_sacred_desire_analysis.json'
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False)

print(f"\nüíæ Complete analysis saved to: {results_file}")

# Final determination
print("\n\nüèÅ FINAL DETERMINATION:")
print("=" * 23)

if sacred_desire_matches and islam_karimov_matches:
    # Check if any match contains both
    combined_match = False
    for sd_match in sacred_desire_matches:
        combined_text = f"{sd_match['title']} {sd_match['description']}".lower()
        if 'islam karimov' in combined_text:
            combined_match = True
            break
    
    if combined_match:
        print("‚úÖ DEFINITIVE ANSWER FOUND:")
        print("   Book: Sacred Desire")
        print("   Protagonist: Islam Karimov")
        print("   Evidence: Found references linking both in search results")
    else:
        print("üéØ PROBABLE ANSWER:")
        print("   Book: Sacred Desire (found in search results)")
        print("   Protagonist: Islam Karimov (found in search results)")
        print("   Note: Both found separately, likely connected")
elif sacred_desire_matches:
    print("üìö BOOK IDENTIFIED:")
    print("   Book: Sacred Desire")
    print(f"   Found {len(sacred_desire_matches)} references")
    print("   Protagonist: Not definitively identified from search results")
elif islam_karimov_matches:
    print("üë§ PROTAGONIST CANDIDATE:")
    print("   Protagonist: Islam Karimov")
    print(f"   Found {len(islam_karimov_matches)} references")
    print("   Book: Sacred Desire not found in search results")
else:
    print("‚ö†Ô∏è NO DEFINITIVE MATCHES:")
    print("   Neither 'Sacred Desire' nor 'Islam Karimov' found in search results")
    print("   The book may be very obscure or the question may be fictional")

print(f"\nüìä SEARCH STATISTICS:")
print(f"   Total search results analyzed: {len(all_findings)}")
print(f"   Files processed: {len(workspace_files)}")
print(f"   Sacred Desire mentions: {len(sacred_desire_matches)}")
print(f"   Islam Karimov mentions: {len(islam_karimov_matches)}")
print(f"   Military book references: {len(book_references)}")
print(f"   Names extracted: {len(all_names_found)}")

print("\n‚úÖ COMPREHENSIVE ANALYSIS COMPLETE")
print("All search data has been thoroughly analyzed with corrected variable handling")
```