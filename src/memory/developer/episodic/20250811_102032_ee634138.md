### Development Step 4: Counting Twitter/X Citations in August Wikipedia Day Pages from June 2023 Archived Versions

**Description**: Search for and access the archived Wikipedia pages for each day of August (August 1st through August 31st) as they appeared in the last versions from June 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific versions of these 31 daily pages from late June 2023. Extract all reference citations from each page and identify those that cite Twitter/X posts (looking for twitter.com, x.com, or @username patterns in the reference URLs). Count the total number of Twitter/X citations for each individual August day page and compile the results.

**Use Cases**:
- Media studies research analyzing the prevalence of Twitter/X citations in Wikipedia’s daily news coverage for August, to assess social media’s influence on historical documentation.
- Academic investigation into the reliability and persistence of social media sources in Wikipedia’s reference sections, supporting digital preservation studies.
- Automated compliance auditing for organizations tracking the use of Twitter/X links in public knowledge bases, to monitor adherence to citation policies or copyright guidelines.
- Journalism retrospectives examining how breaking news from August was sourced and referenced on Wikipedia, highlighting the role of Twitter/X as a primary source.
- Development of tools for digital archivists to quantify and visualize the impact of social media platform changes (e.g., Twitter to X rebranding) on citation patterns in Wikipedia.
- Fact-checking initiatives that require identifying and validating Twitter/X references in Wikipedia’s daily event pages for transparency and misinformation analysis.
- Business intelligence teams monitoring the frequency of brand or executive Twitter/X mentions in Wikipedia’s daily summaries to inform PR and reputation management strategies.
- Legal or regulatory reviews assessing the use of ephemeral social media content as references in public records, using Wikipedia’s August day pages as a case study.

```
import os
import requests
import json
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
import re

print("=== SCALING WIKIPEDIA API APPROACH TO ALL 31 AUGUST PAGES ===\n")
print("Success confirmed: Wikipedia API found June 2023 revisions for 2/3 test pages")
print("Now scaling to all 31 August pages to get complete coverage\n")

# First, inspect existing workspace files to understand structure
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using existing workspace: {workspace_dir}\n")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created new workspace: {workspace_dir}\n")

# Inspect the API test results file to understand structure
api_test_file = os.path.join(workspace_dir, 'august_pages_wikipedia_api_test.json')
if os.path.exists(api_test_file):
    print("=== INSPECTING API TEST RESULTS FILE ===\n")
    
    # First check file size and basic info
    file_size = os.path.getsize(api_test_file)
    print(f"API test file size: {file_size:,} bytes")
    
    # Read and inspect structure
    with open(api_test_file, 'r', encoding='utf-8') as f:
        api_test_data = json.load(f)
    
    print(f"\nAPI test file structure:")
    for key in api_test_data.keys():
        print(f"  {key}: {type(api_test_data[key]).__name__}")
    
    # Extract key information
    if 'test_results' in api_test_data:
        test_results = api_test_data['test_results']
        print(f"\nTest results structure:")
        for page, result in test_results.items():
            if result:
                print(f"  {page}: Available - {result.get('formatted_date', 'Unknown date')}")
            else:
                print(f"  {page}: Not available")
    
    # Get the August pages list
    august_pages = []
    if 'analysis_metadata' in api_test_data and 'total_august_pages' in api_test_data['analysis_metadata']:
        # Generate the full list since we know there are 31 pages
        august_pages = [f"August {day}" for day in range(1, 32)]
        print(f"\nGenerated full list of {len(august_pages)} August pages")
else:
    print("❌ API test file not found, generating August pages list")
    august_pages = [f"August {day}" for day in range(1, 32)]

print(f"\nTotal August pages to process: {len(august_pages)}")
print(f"Sample pages: {august_pages[:5]}... (showing first 5)\n")

# Wikipedia API configuration
api_url = "https://en.wikipedia.org/w/api.php"

def get_june_2023_revision(page_title):
    """Get the latest revision of a Wikipedia page from June 2023"""
    print(f"  Searching: {page_title}")
    
    params = {
        'action': 'query',
        'format': 'json',
        'prop': 'revisions',
        'titles': page_title,
        'rvlimit': 50,
        'rvprop': 'timestamp|ids|user|comment|size',
        'rvdir': 'older',
        'rvstart': '2023-07-01T00:00:00Z',
        'rvend': '2023-06-01T00:00:00Z'
    }
    
    try:
        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if 'query' in data and 'pages' in data['query']:
            pages = data['query']['pages']
            page_id = list(pages.keys())[0]
            
            if page_id == '-1':
                print(f"    ❌ Page not found")
                return None
            
            if 'revisions' in pages[page_id]:
                revisions = pages[page_id]['revisions']
                
                if revisions:
                    latest_june_rev = revisions[0]
                    timestamp = latest_june_rev['timestamp']
                    revid = latest_june_rev['revid']
                    
                    rev_date = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    
                    if rev_date.year == 2023 and rev_date.month == 6:
                        print(f"    ✓ Found: {rev_date.strftime('%Y-%m-%d')} (ID: {revid})")
                        return {
                            'available': True,
                            'page_title': page_title,
                            'page_id': page_id,
                            'revision_id': revid,
                            'timestamp': timestamp,
                            'formatted_date': rev_date.strftime('%Y-%m-%d'),
                            'user': latest_june_rev.get('user', 'Unknown'),
                            'comment': latest_june_rev.get('comment', ''),
                            'size': latest_june_rev.get('size', 0)
                        }
                    else:
                        print(f"    ⚠️ Latest is from {rev_date.strftime('%Y-%m')}, not June 2023")
                        return None
                else:
                    print(f"    ❌ No June 2023 revisions")
                    return None
            else:
                print(f"    ❌ No revision data")
                return None
        else:
            print(f"    ❌ No page data")
            return None
            
    except Exception as e:
        print(f"    ❌ Error: {str(e)}")
        return None

print("=== STEP 1: COMPREHENSIVE JUNE 2023 REVISION SEARCH ===\n")
print("Processing all 31 August pages to find June 2023 revisions...\n")

# Process all August pages
all_results = {}
success_count = 0
start_time = datetime.now()

# Process in batches to show progress
batch_size = 5
total_batches = (len(august_pages) + batch_size - 1) // batch_size

for batch_num in range(total_batches):
    start_idx = batch_num * batch_size
    end_idx = min(start_idx + batch_size, len(august_pages))
    batch_pages = august_pages[start_idx:end_idx]
    
    print(f"\n--- Batch {batch_num + 1}/{total_batches}: Processing pages {start_idx + 1}-{end_idx} ---")
    
    for page_title in batch_pages:
        result = get_june_2023_revision(page_title)
        all_results[page_title] = result
        
        if result and result.get('available', False):
            success_count += 1
        
        # Add delay to be respectful to Wikipedia's servers
        time.sleep(1)
    
    # Progress update
    elapsed = (datetime.now() - start_time).total_seconds()
    processed = end_idx
    remaining = len(august_pages) - processed
    
    print(f"\n  Batch {batch_num + 1} complete:")
    print(f"    Processed: {processed}/{len(august_pages)} pages")
    print(f"    Found: {success_count} June 2023 revisions")
    print(f"    Elapsed: {elapsed:.1f}s")
    print(f"    Remaining: {remaining} pages")
    
    if remaining > 0:
        estimated_remaining_time = (elapsed / processed) * remaining
        print(f"    Estimated time remaining: {estimated_remaining_time:.1f}s")

print(f"\n=== COMPREHENSIVE SEARCH COMPLETE ===\n")

total_elapsed = (datetime.now() - start_time).total_seconds()
print(f"Total processing time: {total_elapsed:.1f} seconds")
print(f"Pages with June 2023 revisions: {success_count}/{len(august_pages)}")
print(f"Success rate: {(success_count/len(august_pages)*100):.1f}%")

# Analyze the results
print(f"\n=== DETAILED RESULTS ANALYSIS ===\n")

successful_pages = []
failed_pages = []

for page_title, result in all_results.items():
    if result and result.get('available', False):
        successful_pages.append({
            'page': page_title,
            'date': result['formatted_date'],
            'revision_id': result['revision_id'],
            'size': result['size']
        })
        print(f"✓ {page_title}: {result['formatted_date']} (ID: {result['revision_id']}, {result['size']:,} bytes)")
    else:
        failed_pages.append(page_title)
        print(f"❌ {page_title}: No June 2023 revision found")

print(f"\n=== SUMMARY STATISTICS ===\n")
print(f"Successful pages: {len(successful_pages)}")
print(f"Failed pages: {len(failed_pages)}")

if successful_pages:
    # Analyze successful pages
    dates = [page['date'] for page in successful_pages]
    sizes = [page['size'] for page in successful_pages]
    
    print(f"\nSuccessful pages date range:")
    print(f"  Earliest: {min(dates)}")
    print(f"  Latest: {max(dates)}")
    
    print(f"\nPage size statistics:")
    print(f"  Average: {sum(sizes)/len(sizes):,.0f} bytes")
    print(f"  Smallest: {min(sizes):,} bytes")
    print(f"  Largest: {max(sizes):,} bytes")

if failed_pages:
    print(f"\nFailed pages: {failed_pages[:10]}{'...' if len(failed_pages) > 10 else ''}")

# Save comprehensive results
comprehensive_results = {
    'analysis_metadata': {
        'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'approach': 'wikipedia_revision_history_api_comprehensive',
        'date_range_searched': '2023-06-01 to 2023-07-01',
        'total_pages_processed': len(august_pages),
        'processing_time_seconds': total_elapsed,
        'batch_size': batch_size
    },
    'search_parameters': {
        'rvstart': '2023-07-01T00:00:00Z',
        'rvend': '2023-06-01T00:00:00Z',
        'rvlimit': 50
    },
    'results_summary': {
        'pages_with_june_revisions': success_count,
        'pages_without_june_revisions': len(august_pages) - success_count,
        'success_rate_percent': round((success_count/len(august_pages)*100), 1)
    },
    'all_results': all_results,
    'successful_pages': successful_pages,
    'failed_pages': failed_pages
}

comprehensive_file = os.path.join(workspace_dir, 'august_pages_comprehensive_june_2023.json')
with open(comprehensive_file, 'w', encoding='utf-8') as f:
    json.dump(comprehensive_results, f, indent=2, ensure_ascii=False)

print(f"\n✅ Comprehensive results saved to: {os.path.basename(comprehensive_file)}")
print(f"   File size: {os.path.getsize(comprehensive_file):,} bytes")

print(f"\n=== NEXT PHASE READY ===\n")
if success_count > 0:
    print(f"🎯 SUCCESS: Found {success_count} August pages with June 2023 revisions!")
    print(f"📋 Ready for next phase: Extract content and analyze Twitter citations")
    print(f"🔄 Next steps:")
    print(f"   1. Retrieve full content for each successful revision")
    print(f"   2. Parse content to extract reference citations")
    print(f"   3. Identify Twitter/X citations (twitter.com, x.com, @username)")
    print(f"   4. Count citations per page and compile final results")
else:
    print(f"❌ NO SUCCESS: No August pages found with June 2023 revisions")
    print(f"🤔 Consider expanding date range or alternative approach")

print(f"\n📊 Current progress: {success_count}/{len(august_pages)} pages ready for citation analysis")
```