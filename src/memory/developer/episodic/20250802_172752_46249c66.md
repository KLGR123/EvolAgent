### Development Step 2: Search English Wikipedia Featured Dinosaur Articles Promoted November 2016

**Description**: Search for Featured Articles on English Wikipedia that were promoted in November 2016, specifically focusing on dinosaur-related articles. Use targeted web searches with queries like 'Wikipedia Featured Articles November 2016 dinosaur', 'site:en.wikipedia.org Featured Article candidates November 2016 dinosaur', and 'Wikipedia FAC promoted November 2016 paleontology'. Look for the Wikipedia Featured Article log, archives, or candidate pages that show articles promoted during that specific month. Extract information about any dinosaur articles that achieved Featured Article status in November 2016, including the article title and nomination details.

**Use Cases**:
- Paleontology research group compiling a timeline of dinosaur article promotions on Wikipedia in November 2016 to analyze shifts in public interest
- Museum exhibit curator automating collection of dinosaur Featured Article logs from November 2016 to schedule promotional events around peak attention periods
- SEO consultant extracting historical featured status of dinosaur pages to inform content optimization strategy for a natural history website
- Educational publisher integrating high-quality dinosaur entries from the November 2016 Featured Article archive into interactive e-textbooks
- Data journalist analyzing communication patterns by mining the Wikipedia Featured Article log for dinosaur topics in late 2016
- Machine learning engineer creating a labeled dataset of quality-classified Wikipedia dinosaur articles based on their November 2016 promotion status
- Digital archivist preserving snapshots of Wikipedia‚Äôs dinosaur content from November 2016 to document evolution of online paleontology resources
- Social media analyst correlating spikes in dinosaur-related Featured Article promotions in November 2016 with Twitter engagement campaigns

```
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import os

print("=== SEARCHING FOR WIKIPEDIA FEATURED ARTICLES FROM NOVEMBER 2016 ===\n")
print("Objective: Find dinosaur-related Featured Articles promoted in November 2016\n")

# Create workspace directory
workspace_dir = f"workspace_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
os.makedirs(workspace_dir, exist_ok=True)
print(f"Created workspace directory: {workspace_dir}\n")

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Strategy 1: Search Wikipedia Featured Article log for November 2016
print("=== STRATEGY 1: ACCESSING WIKIPEDIA FEATURED ARTICLE LOG ===\n")

# Try to access the Featured Article log page
fa_log_urls = [
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Featured_log/November_2016",
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_articles/2016",
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Featured_log/2016"
]

fa_data = []
for i, url in enumerate(fa_log_urls, 1):
    print(f"Attempting to access FA log URL {i}: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=20)
        if response.status_code == 200:
            print(f"‚úÖ Successfully accessed: {url}")
            print(f"Content length: {len(response.content):,} bytes")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Save the raw HTML for analysis
            html_file = os.path.join(workspace_dir, f'fa_log_{i}.html')
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"Saved HTML to: {os.path.basename(html_file)}")
            
            # Look for November 2016 content
            page_text = soup.get_text().lower()
            if 'november 2016' in page_text or 'nov 2016' in page_text:
                print(f"üéØ Found November 2016 content in this page!")
                
                # Extract relevant sections
                # Look for headings containing November 2016
                november_sections = []
                for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                    heading_text = heading.get_text().lower()
                    if 'november' in heading_text and '2016' in heading_text:
                        print(f"Found November 2016 heading: {heading.get_text().strip()}")
                        november_sections.append(heading)
                
                # Look for lists or tables that might contain featured articles
                lists_and_tables = soup.find_all(['ul', 'ol', 'table'])
                print(f"Found {len(lists_and_tables)} lists and tables to analyze")
                
                # Search for dinosaur-related terms in the content
                dinosaur_terms = ['dinosaur', 'paleontology', 'fossil', 'cretaceous', 'jurassic', 'triassic', 'prehistoric', 'extinct', 'reptile']
                dinosaur_matches = []
                
                for term in dinosaur_terms:
                    if term in page_text:
                        print(f"ü¶ï Found dinosaur-related term: '{term}'")
                        dinosaur_matches.append(term)
                
                fa_data.append({
                    'url': url,
                    'status': 'success',
                    'has_november_2016': True,
                    'november_sections': len(november_sections),
                    'dinosaur_terms_found': dinosaur_matches,
                    'content_length': len(response.content)
                })
                
            else:
                print(f"‚ö†Ô∏è No November 2016 content found in this page")
                fa_data.append({
                    'url': url,
                    'status': 'success',
                    'has_november_2016': False,
                    'content_length': len(response.content)
                })
            
        elif response.status_code == 404:
            print(f"‚ùå Page not found: {url}")
            fa_data.append({'url': url, 'status': 'not_found'})
        else:
            print(f"‚ùå HTTP error {response.status_code}: {url}")
            fa_data.append({'url': url, 'status': f'http_error_{response.status_code}'})
            
    except Exception as e:
        print(f"‚ùå Error accessing {url}: {str(e)}")
        fa_data.append({'url': url, 'status': 'error', 'error': str(e)})
    
    print()  # Add spacing between attempts
    time.sleep(2)  # Be respectful to Wikipedia servers

print("=== STRATEGY 2: SEARCH WIKIPEDIA FEATURED ARTICLE CANDIDATES ARCHIVE ===\n")

# Try to access the Featured Article Candidates archive
fac_urls = [
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/archive/November_2016",
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Archived_nominations/November_2016",
    "https://en.wikipedia.org/wiki/Category:Featured_article_candidates_promoted_in_November_2016"
]

fac_data = []
for i, url in enumerate(fac_urls, 1):
    print(f"Attempting to access FAC archive URL {i}: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=20)
        if response.status_code == 200:
            print(f"‚úÖ Successfully accessed: {url}")
            print(f"Content length: {len(response.content):,} bytes")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Save the raw HTML for analysis
            html_file = os.path.join(workspace_dir, f'fac_archive_{i}.html')
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"Saved HTML to: {os.path.basename(html_file)}")
            
            # Search for dinosaur-related content
            page_text = soup.get_text().lower()
            dinosaur_terms = ['dinosaur', 'paleontology', 'fossil', 'cretaceous', 'jurassic', 'triassic', 'prehistoric', 'extinct reptile']
            dinosaur_matches = []
            
            for term in dinosaur_terms:
                if term in page_text:
                    print(f"ü¶ï Found dinosaur-related term: '{term}'")
                    dinosaur_matches.append(term)
            
            # Look for article links and titles
            article_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if href.startswith('/wiki/') and ':' not in href.split('/')[-1]:
                    link_text = link.get_text().strip()
                    if link_text and any(term in link_text.lower() for term in dinosaur_terms):
                        article_links.append({
                            'title': link_text,
                            'href': href,
                            'full_url': f"https://en.wikipedia.org{href}"
                        })
                        print(f"üîó Found potential dinosaur article link: {link_text}")
            
            fac_data.append({
                'url': url,
                'status': 'success',
                'dinosaur_terms_found': dinosaur_matches,
                'potential_dinosaur_articles': article_links,
                'content_length': len(response.content)
            })
            
        elif response.status_code == 404:
            print(f"‚ùå Page not found: {url}")
            fac_data.append({'url': url, 'status': 'not_found'})
        else:
            print(f"‚ùå HTTP error {response.status_code}: {url}")
            fac_data.append({'url': url, 'status': f'http_error_{response.status_code}'})
            
    except Exception as e:
        print(f"‚ùå Error accessing {url}: {str(e)}")
        fac_data.append({'url': url, 'status': 'error', 'error': str(e)})
    
    print()  # Add spacing between attempts
    time.sleep(2)  # Be respectful to Wikipedia servers

print("=== STRATEGY 3: DIRECT SEARCH FOR SPECIFIC DINOSAUR FEATURED ARTICLES ===\n")

# Search for specific dinosaur articles that might have been promoted in November 2016
known_dinosaur_fas = [
    "Allosaurus",
    "Tyrannosaurus",
    "Triceratops",
    "Stegosaurus",
    "Diplodocus",
    "Velociraptor",
    "Spinosaurus",
    "Carnotaurus",
    "Therizinosaurus",
    "Parasaurolophus"
]

# Fixed variable name: changed from 'dinosuar_fa_check' to 'dinosaur_fa_check'
dinosaur_fa_check = []
for dinosaur in known_dinosaur_fas:
    print(f"Checking Featured Article status for: {dinosaur}")
    
    try:
        # Check the article's talk page for FA status
        talk_url = f"https://en.wikipedia.org/wiki/Talk:{dinosaur.replace(' ', '_')}"
        response = requests.get(talk_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text().lower()  # Fixed: Define page_text variable properly
            
            # Look for Featured Article indicators
            fa_indicators = ['featured article', 'fa-class', 'featured star', 'promoted to featured']
            is_fa = any(indicator in page_text for indicator in fa_indicators)
            
            # Look for November 2016 promotion date
            has_nov_2016 = 'november 2016' in page_text or 'nov 2016' in page_text
            
            if is_fa:
                print(f"  ‚úÖ {dinosaur} is a Featured Article")
                if has_nov_2016:
                    print(f"  üéØ Found November 2016 reference for {dinosaur}!")
                else:
                    print(f"  üìÖ No November 2016 reference found")
            else:
                print(f"  ‚ùå {dinosaur} is not a Featured Article")
            
            dinosaur_fa_check.append({
                'dinosaur': dinosaur,
                'is_featured_article': is_fa,
                'has_november_2016_reference': has_nov_2016,
                'talk_page_url': talk_url
            })
            
        else:
            print(f"  ‚ùå Could not access talk page for {dinosaur}")
            dinosaur_fa_check.append({
                'dinosaur': dinosaur,
                'status': 'talk_page_not_accessible'
            })
            
    except Exception as e:
        print(f"  ‚ùå Error checking {dinosaur}: {str(e)}")
        dinosaur_fa_check.append({
            'dinosaur': dinosaur,
            'status': 'error',
            'error': str(e)
        })
    
    time.sleep(1)  # Be respectful to servers

print("\n=== COMPILING SEARCH RESULTS ===\n")

# Compile all results
search_results = {
    'search_metadata': {
        'search_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'objective': 'Find dinosaur-related Featured Articles promoted in November 2016',
        'strategies_used': [
            'Wikipedia Featured Article log search',
            'Featured Article Candidates archive search', 
            'Direct dinosaur article FA status check'
        ]
    },
    'featured_article_log_results': fa_data,
    'fac_archive_results': fac_data,
    'dinosaur_fa_status_check': dinosaur_fa_check,
    'summary': {
        'fa_log_pages_accessed': len([d for d in fa_data if d.get('status') == 'success']),
        'fac_archive_pages_accessed': len([d for d in fac_data if d.get('status') == 'success']),
        'dinosaur_articles_checked': len(dinosaur_fa_check),
        'potential_matches_found': len([d for d in dinosaur_fa_check if d.get('has_november_2016_reference', False)])
    }
}

# Save comprehensive results
results_file = os.path.join(workspace_dir, 'wikipedia_fa_november_2016_search_results.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(search_results, f, indent=2, ensure_ascii=False, default=str)

print(f"üìÅ Search results saved to: {os.path.basename(results_file)}")
print(f"File size: {os.path.getsize(results_file):,} bytes")

print("\n=== SEARCH SUMMARY ===\n")
print(f"Featured Article log pages accessed: {search_results['summary']['fa_log_pages_accessed']}")
print(f"FAC archive pages accessed: {search_results['summary']['fac_archive_pages_accessed']}")
print(f"Dinosaur articles checked: {search_results['summary']['dinosaur_articles_checked']}")
print(f"Potential November 2016 matches: {search_results['summary']['potential_matches_found']}")

# Show any potential matches found
if search_results['summary']['potential_matches_found'] > 0:
    print("\nüéØ POTENTIAL MATCHES FOUND:\n")
    for check in dinosaur_fa_check:
        if check.get('has_november_2016_reference', False):
            print(f"  - {check['dinosaur']}: Featured Article with November 2016 reference")
else:
    print("\n‚ö†Ô∏è No direct matches found in initial search")
    print("Next steps: Manual analysis of saved HTML files for detailed examination")

print(f"\n‚úÖ Search completed. All data saved to workspace: {workspace_dir}")
```