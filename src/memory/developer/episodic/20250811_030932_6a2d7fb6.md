### Development Step 1: Book Identification via DOI 10.2307/j.ctv9b2xdv: Title, Author, Publisher, and Bibliographic Details

**Description**: Search for the book with DOI 10.2307/j.ctv9b2xdv to identify the title, author, and publisher. Use multiple search approaches: (1) Direct DOI resolution through doi.org, (2) Academic database searches using the DOI, (3) Google Scholar search with the DOI string, (4) Publisher database searches focusing on the '10.2307' prefix which typically indicates JSTOR publications. Extract complete bibliographic information including title, author, publication year, publisher, and any available abstract or description to understand the book's content and structure.

**Use Cases**:
- University librarian verifying bibliographic details for a faculty member’s reading list to ensure accurate citation of a JSTOR book using its DOI
- Academic researcher compiling a literature review and needing to extract full metadata (title, author, publisher, year, abstract) for a referenced book found only by DOI
- Institutional repository manager automating the ingestion of book metadata into a digital library system by resolving DOIs and extracting structured bibliographic information
- Journal editor cross-checking references in submitted manuscripts to validate the existence and details of cited books via DOI lookups across multiple databases
- Graduate student preparing a thesis bibliography and using the tool to retrieve publisher and author information for books referenced only by DOI
- Library acquisitions specialist evaluating new book purchases by searching for publisher and publication year details using DOIs to avoid duplicate orders
- Corporate knowledge manager integrating academic book metadata into an internal knowledge base by automatically extracting and verifying bibliographic records from DOIs
- Data analyst conducting citation network analysis who needs to batch-extract structured metadata for books referenced by DOI in scholarly datasets

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import time
import re

print('=== COMPREHENSIVE BOOK SEARCH VIA DOI 10.2307/j.ctv9b2xdv ===')
print('DOI: 10.2307/j.ctv9b2xdv')
print('Publisher prefix: 10.2307 (JSTOR)')
print('Target: Complete bibliographic information including title, author, publisher, year')
print('\n' + '='*100 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# DOI to search for
doi = '10.2307/j.ctv9b2xdv'
print(f'Searching for book with DOI: {doi}')
print('='*60)

# Method 1: Direct DOI resolution through doi.org
print('\nMethod 1: Direct DOI Resolution')
print('-' * 40)
try:
    doi_url = f'https://doi.org/{doi}'
    print(f'Attempting to resolve DOI: {doi_url}')
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }
    
    response = requests.get(doi_url, headers=headers, timeout=15, allow_redirects=True)
    print(f'DOI resolution status: {response.status_code}')
    print(f'Final URL after redirect: {response.url}')
    print(f'Content length: {len(response.content):,} bytes')
    
    if response.status_code == 200:
        # Save the DOI resolution page content
        with open('workspace/doi_resolution_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print('✓ DOI resolution page saved to workspace/doi_resolution_page.html')
        
        # Parse the resolved page
        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = soup.get_text()
        page_text_lower = page_text.lower()
        
        # Extract key bibliographic information
        print('\n--- EXTRACTING BIBLIOGRAPHIC METADATA ---')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            title_text = page_title.get_text().strip()
            print(f'Page title: {title_text}')
        
        # Look for book title patterns
        title_patterns = [
            r'<title[^>]*>([^<]+)</title>',
            r'<h1[^>]*>([^<]+)</h1>',
            r'"title"\s*:\s*"([^"]+)"',
            r'<meta[^>]*name=["\']title["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'<meta[^>]*property=["\']dc:title["\'][^>]*content=["\']([^"\'\>]+)["\']'
        ]
        
        book_title_candidates = []
        for pattern in title_patterns:
            matches = re.findall(pattern, response.text, re.IGNORECASE | re.DOTALL)
            if matches:
                for match in matches[:3]:  # Take first 3 matches
                    clean_match = re.sub(r'\s+', ' ', match.strip())
                    if len(clean_match) > 5 and clean_match not in book_title_candidates:
                        book_title_candidates.append(clean_match)
        
        if book_title_candidates:
            print(f'Book title candidates found: {book_title_candidates}')
        
        # Look for author patterns
        author_patterns = [
            r'<meta[^>]*name=["\']author["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'<meta[^>]*property=["\']dc:creator["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'"author"\s*:\s*"([^"]+)"',
            r'by\s+([A-Z][a-zA-Z\s,\.\-]+)',
            r'<span[^>]*class=["\'][^"\'\>]*author[^"\'\>]*["\'][^>]*>([^<]+)</span>',
            r'<div[^>]*class=["\'][^"\'\>]*author[^"\'\>]*["\'][^>]*>([^<]+)</div>'
        ]
        
        author_candidates = []
        for pattern in author_patterns:
            matches = re.findall(pattern, response.text, re.IGNORECASE)
            if matches:
                for match in matches[:3]:  # Take first 3 matches
                    clean_match = re.sub(r'\s+', ' ', match.strip())
                    if len(clean_match) > 2 and clean_match not in author_candidates:
                        author_candidates.append(clean_match)
        
        if author_candidates:
            print(f'Author candidates found: {author_candidates}')
        
        # Look for publisher information
        publisher_patterns = [
            r'<meta[^>]*name=["\']publisher["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'<meta[^>]*property=["\']dc:publisher["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'"publisher"\s*:\s*"([^"]+)"',
            r'Published by\s+([^\n\r<]+)',
            r'Publisher:\s*([^\n\r<]+)'
        ]
        
        publisher_candidates = []
        for pattern in publisher_patterns:
            matches = re.findall(pattern, response.text, re.IGNORECASE)
            if matches:
                for match in matches[:3]:  # Take first 3 matches
                    clean_match = re.sub(r'\s+', ' ', match.strip())
                    if len(clean_match) > 2 and clean_match not in publisher_candidates:
                        publisher_candidates.append(clean_match)
        
        if publisher_candidates:
            print(f'Publisher candidates found: {publisher_candidates}')
        
        # Look for publication year
        year_patterns = [
            r'<meta[^>]*name=["\']date["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'<meta[^>]*property=["\']dc:date["\'][^>]*content=["\']([^"\'\>]+)["\']',
            r'"datePublished"\s*:\s*"([^"]+)"',
            r'Published:\s*(\d{4})',
            r'Copyright\s*©?\s*(\d{4})'
        ]
        
        year_candidates = []
        for pattern in year_patterns:
            matches = re.findall(pattern, response.text, re.IGNORECASE)
            if matches:
                for match in matches[:3]:  # Take first 3 matches
                    # Extract 4-digit year
                    year_match = re.search(r'(\d{4})', match)
                    if year_match:
                        year = year_match.group(1)
                        if year not in year_candidates and 1900 <= int(year) <= 2024:
                            year_candidates.append(year)
        
        if year_candidates:
            print(f'Publication year candidates found: {year_candidates}')
        
        # Check if this is JSTOR (based on 10.2307 prefix)
        if 'jstor.org' in response.url or 'jstor' in page_text_lower:
            print('*** CONFIRMED: This is a JSTOR publication ***')
            
        # Look for abstract or description
        abstract_selectors = ['.abstract', '.description', '.summary', '[name="description"]']
        abstract_text = None
        
        for selector in abstract_selectors:
            if selector.startswith('['):
                # Handle attribute selector
                elem = soup.select_one(selector)
            else:
                elem = soup.select_one(selector)
            
            if elem:
                if elem.name == 'meta':
                    abstract_text = elem.get('content', '')
                else:
                    abstract_text = elem.get_text().strip()
                
                if abstract_text and len(abstract_text) > 50:
                    print(f'\nAbstract/Description found ({selector}):')
                    print('='*80)
                    print(abstract_text[:500] + ('...' if len(abstract_text) > 500 else ''))
                    print('='*80)
                    break
        
        # Save initial bibliographic data
        initial_biblio = {
            'doi': doi,
            'doi_url': doi_url,
            'resolved_url': response.url,
            'page_title': page_title.get_text().strip() if page_title else None,
            'title_candidates': book_title_candidates,
            'author_candidates': author_candidates,
            'publisher_candidates': publisher_candidates,
            'year_candidates': year_candidates,
            'abstract': abstract_text,
            'is_jstor': 'jstor.org' in response.url or 'jstor' in page_text_lower,
            'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open('workspace/initial_bibliographic_data.json', 'w', encoding='utf-8') as f:
            json.dump(initial_biblio, f, indent=2, ensure_ascii=False)
        print('\n✓ Initial bibliographic data saved to workspace/initial_bibliographic_data.json')
            
    else:
        print(f'DOI resolution failed with status {response.status_code}')
        
except Exception as e:
    print(f'Error in DOI resolution: {str(e)}')

time.sleep(1)  # Brief pause between requests

# Method 2: Google Scholar search with DOI
print('\n' + '='*100)
print('Method 2: Google Scholar Search')
print('-' * 40)

# Check if SerpAPI key is available
api_key = os.getenv('SERPAPI_API_KEY')
if api_key:
    print(f'SerpAPI key available: {api_key[:10]}...')
    
    # Search Google Scholar for the DOI
    scholar_query = f'"10.2307/j.ctv9b2xdv" OR "doi:10.2307/j.ctv9b2xdv"'
    
    params = {
        'q': scholar_query,
        'api_key': api_key,
        'engine': 'google_scholar',
        'num': 10
    }
    
    try:
        print(f'Searching Google Scholar for: {scholar_query}')
        response = requests.get('https://serpapi.com/search.json', params=params)
        
        if response.status_code == 200:
            scholar_results = response.json()
            
            if scholar_results.get('organic_results'):
                print(f'Found {len(scholar_results["organic_results"])} results on Google Scholar')
                
                for i, result in enumerate(scholar_results['organic_results'][:5]):
                    title = result.get('title', 'No title')
                    authors = result.get('authors', 'No authors')
                    publication_info = result.get('publication_info', {}).get('summary', 'No publication info')
                    link = result.get('link', 'No link')
                    snippet = result.get('snippet', 'No snippet')
                    
                    print(f'\nScholar Result {i+1}:')
                    print(f'Title: {title}')
                    print(f'Authors: {authors}')
                    print(f'Publication: {publication_info}')
                    print(f'Link: {link}')
                    if snippet:
                        print(f'Snippet: {snippet}')
                    print('-' * 50)
                
                # Save Google Scholar results
                with open('workspace/google_scholar_results.json', 'w', encoding='utf-8') as f:
                    json.dump(scholar_results, f, indent=2, ensure_ascii=False)
                print('✓ Google Scholar results saved to workspace/google_scholar_results.json')
                
            else:
                print('No results found on Google Scholar')
                if 'error' in scholar_results:
                    print(f'Scholar API Error: {scholar_results["error"]}')
                    
        else:
            print(f'Google Scholar search failed with status {response.status_code}')
            
    except Exception as e:
        print(f'Error in Google Scholar search: {str(e)}')
else:
    print('SerpAPI key not available - skipping Google Scholar search')

time.sleep(1)  # Brief pause between requests

# Method 3: General Google search with DOI
print('\n' + '='*100)
print('Method 3: General Google Search')
print('-' * 40)

if api_key:
    # Search regular Google for the DOI
    google_query = f'"10.2307/j.ctv9b2xdv" book title author publisher'
    
    params = {
        'q': google_query,
        'api_key': api_key,
        'engine': 'google',
        'num': 15
    }
    
    try:
        print(f'Searching Google for: {google_query}')
        response = requests.get('https://serpapi.com/search.json', params=params)
        
        if response.status_code == 200:
            google_results = response.json()
            
            if google_results.get('organic_results'):
                print(f'Found {len(google_results["organic_results"])} results on Google')
                
                # Look for academic/book-related results
                academic_results = []
                
                for i, result in enumerate(google_results['organic_results']):
                    title = result.get('title', 'No title')
                    link = result.get('link', 'No link')
                    snippet = result.get('snippet', 'No snippet')
                    
                    # Check for academic indicators
                    academic_indicators = [
                        'book', 'author', 'publisher', 'jstor.org', 'jstor',
                        'university press', 'academic', 'doi', 'bibliography',
                        'publication', 'press', 'isbn'
                    ]
                    
                    is_academic = any(indicator in (title + link + snippet).lower() for indicator in academic_indicators)
                    
                    print(f'\nGoogle Result {i+1}:')
                    print(f'Title: {title}')
                    print(f'Link: {link}')
                    print(f'Snippet: {snippet}')
                    
                    if is_academic:
                        print('*** POTENTIALLY RELEVANT ACADEMIC SOURCE ***')
                        academic_results.append(result)
                    
                    print('-' * 50)
                    
                    if i >= 7:  # Show first 8 results in detail
                        break
                
                print(f'\nTotal potentially relevant academic results: {len(academic_results)}')
                
                # Save Google search results
                with open('workspace/google_search_results.json', 'w', encoding='utf-8') as f:
                    json.dump(google_results, f, indent=2, ensure_ascii=False)
                print('✓ Google search results saved to workspace/google_search_results.json')
                
            else:
                print('No results found on Google')
                if 'error' in google_results:
                    print(f'Google API Error: {google_results["error"]}')
                    
        else:
            print(f'Google search failed with status {response.status_code}')
            
    except Exception as e:
        print(f'Error in Google search: {str(e)}')

time.sleep(1)  # Brief pause between requests

# Method 4: JSTOR specific search
print('\n' + '='*100)
print('Method 4: JSTOR Specific Search')
print('-' * 40)

if api_key:
    # Search specifically for JSTOR with this DOI
    jstor_query = f'site:jstor.org "10.2307/j.ctv9b2xdv" OR "j.ctv9b2xdv"'
    
    params = {
        'q': jstor_query,
        'api_key': api_key,
        'engine': 'google',
        'num': 10
    }
    
    try:
        print(f'Searching JSTOR for: {jstor_query}')
        response = requests.get('https://serpapi.com/search.json', params=params)
        
        if response.status_code == 200:
            jstor_results = response.json()
            
            if jstor_results.get('organic_results'):
                print(f'Found {len(jstor_results["organic_results"])} results on JSTOR')
                
                for i, result in enumerate(jstor_results['organic_results']):
                    title = result.get('title', 'No title')
                    link = result.get('link', 'No link')
                    snippet = result.get('snippet', 'No snippet')
                    
                    print(f'\nJSTOR Result {i+1}:')
                    print(f'Title: {title}')
                    print(f'Link: {link}')
                    print(f'Snippet: {snippet}')
                    print('-' * 50)
                    
                # Save JSTOR results
                with open('workspace/jstor_search_results.json', 'w', encoding='utf-8') as f:
                    json.dump(jstor_results, f, indent=2, ensure_ascii=False)
                print('✓ JSTOR search results saved to workspace/jstor_search_results.json')
                
            else:
                print('No results found on JSTOR')
                if 'error' in jstor_results:
                    print(f'JSTOR API Error: {jstor_results["error"]}')
                    
        else:
            print(f'JSTOR search failed with status {response.status_code}')
            
    except Exception as e:
        print(f'Error in JSTOR search: {str(e)}')

# Method 5: Alternative DOI lookup services
print('\n' + '='*100)
print('Method 5: Alternative DOI Lookup Services')
print('-' * 40)

# Try CrossRef API for DOI metadata
try:
    crossref_url = f'https://api.crossref.org/works/{doi}'
    print(f'Querying CrossRef API: {crossref_url}')
    
    crossref_headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; DOI-Lookup/1.0; mailto:research@example.com)'
    }
    
    crossref_response = requests.get(crossref_url, headers=crossref_headers, timeout=10)
    print(f'CrossRef API status: {crossref_response.status_code}')
    
    if crossref_response.status_code == 200:
        crossref_data = crossref_response.json()
        
        if 'message' in crossref_data:
            work = crossref_data['message']
            
            print('\n*** CROSSREF METADATA FOUND ***')
            
            # Extract title
            if 'title' in work and work['title']:
                crossref_title = work['title'][0]
                print(f'Title: {crossref_title}')
            
            # Extract authors
            if 'author' in work:
                authors = []
                for author in work['author']:
                    if 'given' in author and 'family' in author:
                        full_name = f"{author['given']} {author['family']}"
                        authors.append(full_name)
                    elif 'family' in author:
                        authors.append(author['family'])
                
                if authors:
                    print(f'Authors: {", ".join(authors)}')
            
            # Extract publisher
            if 'publisher' in work:
                print(f'Publisher: {work["publisher"]}')
            
            # Extract publication date
            if 'published-print' in work:
                date_parts = work['published-print'].get('date-parts', [])
                if date_parts and date_parts[0]:
                    year = date_parts[0][0]
                    print(f'Publication Year: {year}')
            elif 'published-online' in work:
                date_parts = work['published-online'].get('date-parts', [])
                if date_parts and date_parts[0]:
                    year = date_parts[0][0]
                    print(f'Publication Year (online): {year}')
            
            # Extract type
            if 'type' in work:
                print(f'Publication Type: {work["type"]}')
            
            # Extract URL
            if 'URL' in work:
                print(f'URL: {work["URL"]}')
            
            # Save CrossRef data
            with open('workspace/crossref_metadata.json', 'w', encoding='utf-8') as f:
                json.dump(crossref_data, f, indent=2, ensure_ascii=False)
            print('\n✓ CrossRef metadata saved to workspace/crossref_metadata.json')
        
    else:
        print(f'CrossRef API returned status {crossref_response.status_code}')
        if crossref_response.status_code == 404:
            print('DOI not found in CrossRef database')
        
except Exception as e:
    print(f'Error querying CrossRef API: {str(e)}')

# Summary of search approach
print('\n' + '='*100)
print('COMPREHENSIVE SEARCH SUMMARY')
print('='*100)
print(f'DOI searched: {doi}')
print(f'Publisher prefix: 10.2307 (JSTOR)')
print('\nSearch methods completed:')
print('1. Direct DOI resolution through doi.org')
print('2. Google Scholar search with DOI')
print('3. General Google search with DOI + keywords')
print('4. JSTOR specific search')
print('5. CrossRef API metadata lookup')
print('\nResults saved to workspace/ directory for analysis')
print('Files created:')
if os.path.exists('workspace'):
    for file in os.listdir('workspace'):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f'- {file} ({file_size:,} bytes)')

print('\n*** NEXT STEP: Analyze collected data to compile final bibliographic record ***')
```