### Development Step 15: Extract Survivor winners (Seasons 1–44) from Wikipedia into JSON file

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment analytics team automating extraction of Survivor winners for seasons 1–44 to feed a Tableau dashboard on gender and age trends among reality-TV champions
- Media research department compiling historical reality-TV winner data for a published study on diversity progression across prime-time competition shows
- Social media marketing agency integrating an automated script to update a weekly “Flashback Friday” post template with Survivor season numbers and champions’ names
- Fantasy reality-TV league platform pulling the official winners list into JSON to dynamically generate trivia quizzes and leaderboard predictions
- Academic data science project performing statistical analysis on winner geographic distribution by scraping the Survivor winners table for model input
- TV network M&A team gathering structured winner data across multiple reality formats—starting with Survivor—to benchmark show performance metrics
- Chatbot developer incorporating the JSON of Survivor winners to allow fans to query “Who won season X?” directly in messaging apps

```
import os
import sys
import requests
from bs4 import BeautifulSoup
import json

# 1) Locate the dynamic workspace directory
candidates = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not candidates:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
# pick the most recently modified
workspace_dir = max(candidates, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable desktop version of the Survivor page
title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={title}&printable=yes"
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/114.0.5735.199 Safari/537.36'
    ),
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching printable page with headers:\n  {headers}\n  URL: {url}\n")
response = requests.get(url, headers=headers)
response.raise_for_status()
print(f"Page fetched successfully (status code: {response.status_code})")

# 3) Save printable HTML
html_printable = os.path.join(workspace_dir, 'survivor_page_printable.html')
with open(html_printable, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"Saved printable HTML to: {html_printable}\n")

# 4) Parse printable HTML
tree = BeautifulSoup(response.text, 'html.parser')

# 5) Find the "Winners by season" headline span
target_span = None
for span in tree.select('span.mw-headline'):
    sid = span.get('id', '')
    text = span.get_text(strip=True)
    if 'winners_by_season' in sid.lower() or 'winners by season' in text.lower():
        target_span = span
        print(f"✓ Found target span: id={sid}, text='{text}'")
        break

if not target_span:
    print("❌ Could not locate a span with id or text 'Winners by season'. Please inspect 'survivor_page_printable.html'.")
    sys.exit(1)

# 6) Locate the heading tag (h2 or h3) that contains this span
heading = target_span.find_parent(['h2', 'h3', 'h4'])
if not heading:
    print("❌ Could not find a parent heading for the winners section.")
    sys.exit(1)
print(f"Parent heading tag: {heading.name}\n")

# 7) Find the next wikitable after this heading
winners_table = None
for sib in heading.find_next_siblings():
    if sib.name == 'table' and 'wikitable' in (sib.get('class') or []):
        winners_table = sib
        print("Found the winners table following the heading.\n")
        break
    # stop if another heading arrives
    if sib.name in ['h2', 'h3', 'h4']:
        break

if not winners_table:
    print("❌ No wikitable found immediately after the winners heading.")
    sys.exit(1)

# 8) Parse header row to find column indices for Season and Winner
top_tr = winners_table.find('tr')
cols = [th.get_text(strip=True).lower() for th in top_tr.find_all(['th','td'], recursive=False)]
print(f"Table headers detected: {cols}")
try:
    season_idx = cols.index('season')
    winner_idx = cols.index('winner')
except ValueError:
    print("❌ 'Season' or 'Winner' column not found in headers.")
    sys.exit(1)
print(f"Indices -> season: {season_idx}, winner: {winner_idx}\n")

# 9) Iterate rows and collect seasons 1–44
data = []
for row in winners_table.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # parse season number
    season_cell = cells[season_idx]
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    s_txt = season_cell.get_text(strip=True)
    try:
        s_num = int(s_txt)
    except:
        continue
    if not (1 <= s_num <= 44):
        continue
    # parse winner name
    winner_cell = cells[winner_idx]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    w_name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {s_num} -> Winner: {w_name}")
    data.append({'season': s_num, 'winner': w_name})

# 10) Validate count and save results
data_sorted = sorted(data, key=lambda x: x['season'])
print(f"\nTotal winners extracted for seasons 1–44: {len(data_sorted)}")
if len(data_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Please verify the table structure.")

out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(data_sorted, f, indent=2)
print(f"Saved winners list to: {out_file}")
```