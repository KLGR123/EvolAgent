### Development Step 15: Fetch and Save University of Leicester’s Dragon Diet Fish Supply Paper PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic researcher automating retrieval of University of Leicester publications (le.ac.uk) to compile a comprehensive literature review on dragon ecology.
- Patent analyst extracting PDF documents from the European Patent Office’s site to maintain an offline database of recent biotech patent filings.
- Legal team gathering downloadable court decision PDFs from government judiciary websites to prepare case briefs ahead of trial.
- Digital archivist harvesting historical manuscripts in PDF format from the National Archives domain for preservation in a cultural heritage repository.
- SEO professional monitoring competitor domain links in search results to audit backlink profiles and optimize outreach strategies.
- Marketing analyst collecting vendor whitepapers in PDF format from technology provider sites to build a curated content library for sales teams.
- Cyber threat intelligence team extracting vendor post-mortem PDF reports from security researchers’ domains to enrich threat models.
- Business intelligence analyst scraping quarterly investor presentation PDFs from corporate IR websites to feed financial forecasting models.

```
import os
import sys
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# Step 1: Ensure workspace directory exists
workspace = 'workspace'
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    print("Please run the search scripts first to generate HTML files.")
    sys.exit(1)
print(f"[INFO] Using workspace: {workspace}\n")

# Step 2: Define paths to the saved search result HTML files
ddg_html = os.path.join(workspace, 'site_le_duckduckgo_search.html')
bing_html = os.path.join(workspace, 'site_le_bing_search.html')

# Step 3: Verify HTML files exist
for path in (ddg_html, bing_html):
    if not os.path.isfile(path):
        print(f"ERROR: Missing required file: {path}")
        print("Run the site-specific search step first to save HTML files.")
        sys.exit(1)
print(f"[INFO] Found HTML files:\n - {ddg_html}\n - {bing_html}\n")

# Container for detected Leicester URLs
leicester_links = []

# Function to inspect a single HTML file for le.ac.uk links and contexts
def inspect_file(file_path, label):
    print(f"[INSPECT] Searching for 'le.ac.uk' links in {label}...")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    soup = BeautifulSoup(content, 'html.parser')

    # 1. Look at all <a> tags
    for a in soup.find_all('a', href=True):
        href = a['href']
        # Handle DuckDuckGo redirect pattern (/l/?uddg=)
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                real = unquote(q[0])
                if 'le.ac.uk' in urlparse(real).netloc.lower():
                    print(f"  [REDIRECT] {label} uddg decoded --> {real}")
                    leicester_links.append(real)
        # Direct le.ac.uk link
        if 'le.ac.uk' in urlparse(href).netloc.lower():
            print(f"  [DIRECT] {label} --> {href}")
            leicester_links.append(href)

    # 2. Raw text scan for any 'le.ac.uk' occurrences not in <a>
    lines = content.splitlines()
    for i, line in enumerate(lines):
        if 'le.ac.uk' in line and '<a ' not in line:
            # Print a small context window
            start = max(0, i - 2)
            end = min(len(lines), i + 3)
            print(f"  [TEXT] Context around line {i+1}:")
            for j in range(start, end):
                print(f"    {j+1}: {lines[j].strip()}")
            print("")

# Inspect both HTML files
try:
    inspect_file(ddg_html, 'DuckDuckGo HTML')
    print("\n" + "-"*60 + "\n")
    inspect_file(bing_html, 'Bing HTML')
except Exception as e:
    print(f"ERROR during inspection: {e}")
    sys.exit(1)

# Deduplicate collected URLs
unique_links = []
for url in leicester_links:
    if url not in unique_links:
        unique_links.append(url)

# Step 4: Save results to a text file
output_path = os.path.join(workspace, 'leicester_link_inspection.txt')
with open(output_path, 'w', encoding='utf-8') as out:
    out.write('Detected University of Leicester (le.ac.uk) URLs and contexts:\n\n')
    for url in unique_links:
        out.write(f"{url}\n")
print(f"[DONE] Inspection complete. Found {len(unique_links)} unique Leicester-related links.")
print(f"[INFO] Detailed report saved to: {output_path}")
print("Next: Review 'leicester_link_inspection.txt' to identify the correct landing-page URL(s) on 'le.ac.uk'.")
```