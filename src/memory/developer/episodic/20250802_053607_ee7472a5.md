### Development Step 10: Download ‘Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?’ PDF from Leicester University

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Legal research automation for law firms: scraping DuckDuckGo and Bing search results to locate and download court opinions or case law PDFs for due diligence.
- Financial audit streamlining: extracting and downloading publicly available annual reports and financial statements in PDF format from corporate websites and search engine results for audit teams.
- Academic literature collection: retrieving specific journal articles or conference papers in PDF form (e.g., “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?”) for university researchers building comprehensive bibliographies.
- Patent analysis workflow: searching for patent specification PDFs across multiple search engines and automatically downloading them into a local repository for IP analysts.
- Regulatory compliance gathering: harvesting and downloading government regulations or industry guidelines (e.g., FDA or EPA PDF documents) from search engine results to ensure up-to-date policy adherence.
- Healthcare data curation: locating and downloading clinical trial reports or medical guidelines in PDF format from public databases via search engine scraping for evidence-based medicine teams.
- Environmental monitoring reports aggregation: scraping and collecting PDF technical reports on water quality, air pollution studies, or wildlife assessments from academic and governmental sources for environmental scientists.

```
import os
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# 1. Ensure workspace directory
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)
print(f"[INFO] Using workspace: {workspace}\n")

# 2. Define paths to saved HTML files
ddg_html = os.path.join(workspace, 'site_le_duckduckgo_search.html')
bing_html = os.path.join(workspace, 'site_le_bing_search.html')

# 3. Verify files exist
for path in (ddg_html, bing_html):
    if not os.path.exists(path):
        print(f"ERROR: Missing required file: {path}")
        print("Please run the site-specific search step first to save HTML files.")
        sys.exit(1)
print(f"[INFO] Found HTML files:\n - {ddg_html}\n - {bing_html}\n")

candidates = []

# 4. Parse DuckDuckGo HTML for .pdf links
print("[PARSING] DuckDuckGo results for PDF links...")
with open(ddg_html, 'r', encoding='utf-8') as f:
    soup = BeautifulSoup(f, 'html.parser')

# Look for uddg redirects and direct .pdf hrefs
for a in soup.find_all('a', href=True):
    href = a['href']
    # uddg redirect
    if '/l/?uddg=' in href:
        parsed = urlparse(href)
        q = parse_qs(parsed.query).get('uddg', [])
        if q:
            url = unquote(q[0])
            if url.lower().endswith('.pdf') or '.pdf?' in url.lower():
                candidates.append(url)
                print(f"  -> Found via uddg: {url}")
    # direct link
    elif href.lower().endswith('.pdf'):
        candidates.append(href)
        print(f"  -> Found direct PDF href: {href}")

# 5. Parse Bing HTML for .pdf links
print("\n[PARSING] Bing results for PDF links...")
with open(bing_html, 'r', encoding='utf-8') as f:
    soup = BeautifulSoup(f, 'html.parser')
# Bing results: li.b_algo h2 a
for li in soup.select('li.b_algo h2 a'):
    href = li.get('href')
    if href and href.lower().endswith('.pdf'):
        candidates.append(href)
        print(f"  -> Found in Bing: {href}")

# Deduplicate while preserving order
seen = set()
unique = []
for url in candidates:
    if url not in seen:
        seen.add(url)
        unique.append(url)
candidates = unique

# 6. Evaluate findings
if not candidates:
    print("ERROR: No PDF links found in both DuckDuckGo and Bing HTML.")
    print("Please inspect the HTML files manually or set DRAGON_PDF_URL to the direct PDF link.")
    sys.exit(1)

print(f"\n[INFO] Total PDF candidates found: {len(candidates)}")
for i, url in enumerate(candidates, 1):
    print(f"  {i}. {url}")

# 7. Select the first candidate for download
pdf_link = candidates[0]
print(f"\n[DOWNLOAD] Selected for download: {pdf_link}\n")

# 8. Download the PDF
pdf_path = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')
headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/pdf'}
print(f"[DOWNLOAD] Fetching PDF and saving to {pdf_path}...")
total = 0
try:
    with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as r:
        r.raise_for_status()
        with open(pdf_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    total += len(chunk)
except Exception as e:
    print(f"ERROR: Failed to download PDF: {e}")
    sys.exit(1)

# 9. Final report
print(f"[DONE] Download complete: {total} bytes ({total/1024/1024:.2f} MB)")
if total < 10_000:
    print("WARNING: File size <10 KB; may be incomplete.")
else:
    print(f"SUCCESS: PDF saved at {pdf_path}")
```