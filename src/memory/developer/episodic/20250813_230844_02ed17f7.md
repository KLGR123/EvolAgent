### Development Step 6: Identifying the Federal Rules of Evidence Article with Most 'Witnesses' in Rule Titles and Analyzing Amendments

**Description**: Access the Federal Rules of Evidence section on Cornell Law School's Legal Information Institute website and systematically analyze all rules to identify which article has 'witnesses' appearing in the most rule titles as of 2021. Count the occurrences of 'witnesses' in titles across all Federal Rules of Evidence articles, then identify the article with the highest count for further analysis of its first rule's amendment history.

**Use Cases**:
- Legal research automation for law firms preparing motions that require detailed amendment histories of witness-related evidence rules
- Compliance auditing in corporate legal departments to ensure internal policies align with the latest amendments to federal witness testimony standards
- Academic legal scholarship analyzing the evolution of evidentiary rules regarding witnesses for law review articles or historical studies
- Judicial clerks compiling legislative and amendment histories for judges’ reference in complex evidentiary hearings
- Legal technology companies developing AI-powered tools that extract and summarize rule changes for case law databases and legal analytics platforms
- Continuing legal education (CLE) course developers creating up-to-date training materials on the Federal Rules of Evidence, focusing on witness testimony amendments

```
import requests
from bs4 import BeautifulSoup
import os
import json
from datetime import datetime
import re

print('=== FEDERAL RULES OF EVIDENCE RULE 701 AMENDMENT HISTORY ANALYSIS ===')
print('Objective: Access Rule 701 (first rule in Article 7) and analyze its amendment history')
print('Context: Article 7 has the most "witnesses" in rule titles (3 mentions)\n')

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# Rule 701 URL (first rule in Article 7 - Opinion Testimony by Lay Witnesses)
rule_701_url = 'https://www.law.cornell.edu/rules/fre/rule_701'
print(f'Accessing Rule 701: {rule_701_url}')

print('\n=== STEP 1: ACCESSING RULE 701 PAGE ===')

try:
    response = requests.get(rule_701_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f'Successfully accessed Rule 701 page')
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    
    # Parse the page
    soup = BeautifulSoup(response.content, 'html.parser')
    
    page_title = soup.find('title').get_text() if soup.find('title') else 'No title found'
    print(f'Page title: {page_title}')
    
    # Save the raw HTML for inspection
    html_file = os.path.join(workspace_dir, 'rule_701_page.html')
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(response.text)
    print(f'Raw HTML saved to: {html_file}')
    
    print('\n=== STEP 2: EXTRACTING RULE 701 CONTENT AND STRUCTURE ===')
    
    # Get the main rule text
    print('Extracting rule text and structure...')
    
    # Look for the rule content
    rule_content = soup.get_text()
    
    # Extract rule title and text
    rule_title_match = re.search(r'Rule 701[.:]?\s*([^\n\r]+)', rule_content, re.IGNORECASE)
    if rule_title_match:
        rule_title = rule_title_match.group(1).strip()
        print(f'Rule 701 Title: {rule_title}')
    else:
        rule_title = 'Title not found'
        print('Rule title not found in content')
    
    # Look for the current rule text
    print('\nExtracting current rule text...')
    
    # Find main content areas that might contain the rule text
    main_content_areas = soup.find_all(['div', 'section', 'article'], class_=re.compile(r'content|main|body|rule', re.I))
    
    rule_text = ''
    for content_area in main_content_areas:
        content_text = content_area.get_text()
        if 'opinion testimony' in content_text.lower() and len(content_text) > 100:
            rule_text = content_text
            break
    
    if not rule_text:
        rule_text = rule_content  # Use full page content as fallback
    
    print(f'Rule text length: {len(rule_text):,} characters')
    print(f'Rule text preview: {rule_text[:500]}...')
    
    print('\n=== STEP 3: SEARCHING FOR AMENDMENT HISTORY ===')
    
    # Look for amendment history sections
    amendment_patterns = [
        r'amendment[s]?\s+history',
        r'historical?\s+notes?',
        r'notes?\s+of\s+advisory\s+committee',
        r'committee\s+notes?',
        r'amendments?',
        r'effective\s+date',
        r'amended\s+\d{4}',
        r'revised\s+\d{4}',
        r'adopted\s+\d{4}'
    ]
    
    amendment_sections = []
    
    for pattern in amendment_patterns:
        matches = re.finditer(pattern, rule_content, re.IGNORECASE)
        for match in matches:
            # Get context around the match
            start_pos = max(0, match.start() - 200)
            end_pos = min(len(rule_content), match.end() + 500)
            context = rule_content[start_pos:end_pos]
            
            amendment_sections.append({
                'pattern': pattern,
                'match_text': match.group(),
                'context': context,
                'position': match.start()
            })
    
    print(f'Found {len(amendment_sections)} potential amendment references:')
    
    for i, section in enumerate(amendment_sections[:10], 1):  # Show first 10
        print(f'\n{i}. Pattern: {section["pattern"]}')
        print(f'   Match: "{section["match_text"]}"')
        print(f'   Context: {section["context"][:300]}...')
    
    # Look for specific amendment dates and years
    print('\n=== STEP 4: EXTRACTING AMENDMENT DATES AND DETAILS ===')
    
    # Search for year patterns that might indicate amendments
    year_patterns = re.findall(r'\b(19\d{2}|20\d{2})\b', rule_content)
    unique_years = sorted(set(year_patterns))
    
    print(f'Years mentioned in Rule 701 content: {unique_years}')
    
    # Look for more specific amendment language
    amendment_details = []
    
    # Pattern for "amended [date]" or "effective [date]"
    date_amendment_patterns = [
        r'amended\s+([A-Za-z]+\s+\d{1,2},?\s+\d{4})',
        r'effective\s+([A-Za-z]+\s+\d{1,2},?\s+\d{4})',
        r'adopted\s+([A-Za-z]+\s+\d{1,2},?\s+\d{4})',
        r'revised\s+([A-Za-z]+\s+\d{1,2},?\s+\d{4})'
    ]
    
    for pattern in date_amendment_patterns:
        matches = re.finditer(pattern, rule_content, re.IGNORECASE)
        for match in matches:
            amendment_details.append({
                'type': pattern.split('\\s+')[0],  # Get the action (amended, effective, etc.)
                'date': match.group(1),
                'full_match': match.group(0),
                'context': rule_content[max(0, match.start()-100):match.end()+100]
            })
    
    print(f'\nSpecific amendment details found: {len(amendment_details)}')
    
    for i, detail in enumerate(amendment_details, 1):
        print(f'\n{i}. {detail["type"].title()}: {detail["date"]}')
        print(f'   Full match: "{detail["full_match"]}"')
        print(f'   Context: {detail["context"]}...')
    
    # Look for advisory committee notes or explanatory text
    print('\n=== STEP 5: SEARCHING FOR ADVISORY COMMITTEE NOTES ===')
    
    advisory_patterns = [
        r'advisory\s+committee\s+note[s]?',
        r'committee\s+note[s]?',
        r'explanatory\s+note[s]?',
        r'drafters?[\']?\s+note[s]?'
    ]
    
    advisory_notes = []
    
    for pattern in advisory_patterns:
        matches = re.finditer(pattern, rule_content, re.IGNORECASE)
        for match in matches:
            # Get substantial context around advisory notes
            start_pos = max(0, match.start() - 100)
            end_pos = min(len(rule_content), match.end() + 1000)  # Get more context for notes
            context = rule_content[start_pos:end_pos]
            
            advisory_notes.append({
                'pattern': pattern,
                'match_text': match.group(),
                'context': context,
                'position': match.start()
            })
    
    print(f'Found {len(advisory_notes)} advisory committee note references:')
    
    for i, note in enumerate(advisory_notes[:5], 1):  # Show first 5
        print(f'\n{i}. Pattern: {note["pattern"]}')
        print(f'   Match: "{note["match_text"]}"')
        print(f'   Context: {note["context"][:500]}...')
    
    # Save the extracted content for further analysis
    rule_701_analysis = {
        'analysis_date': datetime.now().isoformat(),
        'source_url': rule_701_url,
        'page_title': page_title,
        'rule_title': rule_title,
        'rule_text_length': len(rule_text),
        'years_mentioned': unique_years,
        'amendment_references_found': len(amendment_sections),
        'specific_amendment_details': amendment_details,
        'advisory_committee_notes_found': len(advisory_notes),
        'amendment_sections': amendment_sections[:10],  # First 10 for storage
        'advisory_notes': advisory_notes[:5]  # First 5 for storage
    }
    
    # Save full rule text separately
    rule_text_file = os.path.join(workspace_dir, 'rule_701_full_text.txt')
    with open(rule_text_file, 'w', encoding='utf-8') as f:
        f.write(f'Federal Rules of Evidence - Rule 701\n')
        f.write(f'Title: {rule_title}\n')
        f.write(f'Source: {rule_701_url}\n')
        f.write(f'Extracted: {datetime.now().isoformat()}\n')
        f.write('='*80 + '\n\n')
        f.write(rule_text)
    
    print(f'\nRule 701 full text saved to: {rule_text_file}')
    
    # Save analysis results
    analysis_file = os.path.join(workspace_dir, 'rule_701_amendment_analysis.json')
    with open(analysis_file, 'w') as f:
        json.dump(rule_701_analysis, f, indent=2)
    
    print(f'Rule 701 analysis saved to: {analysis_file}')
    
    print('\n=== STEP 6: SUMMARY OF AMENDMENT HISTORY FINDINGS ===')
    print('='*70)
    
    print(f'Rule 701: {rule_title}')
    print(f'Years mentioned in content: {len(unique_years)} ({unique_years})')
    print(f'Amendment references found: {len(amendment_sections)}')
    print(f'Specific amendment details: {len(amendment_details)}')
    print(f'Advisory committee notes: {len(advisory_notes)}')
    
    if amendment_details:
        print('\nAmendment Timeline:')
        for detail in sorted(amendment_details, key=lambda x: x['date']):
            print(f'  • {detail["type"].title()}: {detail["date"]}')
    
    if unique_years:
        print(f'\nKey years in Rule 701 history: {unique_years}')
    
except requests.exceptions.RequestException as e:
    print(f'Error accessing Rule 701 page: {e}')
except Exception as e:
    print(f'Unexpected error during analysis: {e}')
    import traceback
    traceback.print_exc()

print('\n=== RULE 701 AMENDMENT HISTORY ANALYSIS COMPLETE ===')
print('Next step: Analyze the extracted content for detailed amendment history')
```