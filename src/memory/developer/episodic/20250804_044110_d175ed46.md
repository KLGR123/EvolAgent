### Development Step 19: Extract Survivor (American) Seasons 1–44 Winners from Wikipedia to JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated updating of a reality TV fan site’s winner archive by extracting Survivor champions to keep season summaries current
- Academic media studies research on diversity trends by pulling winner names to analyze gender and age representation across Survivor seasons
- Backend data feed for a mobile trivia app that dynamically generates “Name the Survivor season X winner” questions from the extracted JSON
- SEO-driven blog automation that inserts the latest Survivor winner details into scheduled articles and metadata for improved search rankings
- Conversational AI enhancement for an entertainment chatbot, using the winners list to accurately answer fan questions about past Survivor champions
- Data journalism visualization project that builds interactive timelines of Survivor winners to illustrate show longevity and changing contest dynamics
- Social media calendar automation for a TV network, scheduling celebratory posts on each Survivor season’s winner anniversary based on the extracted dates and names
- Machine learning feature engineering in a TV recommendation engine, incorporating past Survivor winners to personalize show suggestions for reality-TV enthusiasts

```
import os
import sys

# 1) Locate the active workspace directory dynamically
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Path to the printable HTML saved earlier
html_path = os.path.join(workspace_dir, 'survivor_page_printable.html')
if not os.path.exists(html_path):
    print(f"❌ File not found: {html_path}")
    sys.exit(1)
print(f"Inspecting HTML file: {html_path}\n")

# 3) Read file contents
with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
    lines = f.readlines()
    text = ''.join(lines)

# 4) Preview first 30 lines with line numbers
preview_file = os.path.join(workspace_dir, 'html_preview.txt')
with open(preview_file, 'w', encoding='utf-8') as out:
    out.write('# Preview of first 30 lines of survivor_page_printable.html\n')
    for idx, line in enumerate(lines[:30], start=1):
        prefix = f"{idx:02d}: "
        out.write(prefix + line)
print(f"Wrote preview of first 30 lines to: {preview_file}\n")

print("=== First 30 lines preview ===")
for idx, line in enumerate(lines[:30], start=1):
    print(f"{idx:02d}: {line.rstrip()}")

# 5) Search for key patterns in the entire file
patterns = ['<!DOCTYPE', '<html', '<head', 'Wikipedia', 'mw-headline', 'Survivor']
print("\n=== Pattern occurrences in HTML file ===")
for pat in patterns:
    count = text.count(pat)
    print(f"Pattern '{pat}': found {count} times")

# 6) Find and print first 5 occurrences of <span class
span_lines = []
for idx, line in enumerate(lines, start=1):
    if '<span' in line.lower() and 'mw-headline' in line.lower():
        span_lines.append((idx, line.strip()))
        if len(span_lines) >= 5:
            break
print("\n=== First 5 lines containing '<span' and 'mw-headline' ===")
if span_lines:
    for ln, txt in span_lines:
        print(f"Line {ln}: {txt}")
else:
    print("No '<span class=\'mw-headline\'>' lines found in preview.")

print("\nInspection complete. Please open 'html_preview.txt' to confirm this is the correct Wikipedia page printable HTML and contains expected sections and spans.")
```