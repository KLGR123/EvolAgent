### Development Step 49: Extract Survivor US Seasons 1–44 Winners to JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Pop culture journalism and database integration for entertainment news websites to automatically update Survivor winner lists
- Development of a trivia mobile app that pulls the latest Survivor champions for interactive quizzes
- Television marketing campaign analysis and trend identification for network promotional teams tracking winner demographics
- Academic behavioral research on reality TV outcomes and demographic correlations using structured Survivor data
- Automated content scraping pipeline for aggregator platforms maintaining up-to-date Survivor winner archives
- Sports and entertainment betting trend analysis leveraging historical Survivor winner information for predictive models
- Archival metadata generation for streaming services organizing reality TV catalogs by season and champion

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# Marker so tester knows this script ran
print("=== RUNNING Survivor HEADINGS DEBUG SCRIPT ===")

# 1) Locate workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
wiki_url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page... URL: {wiki_url}\n")
resp = requests.get(wiki_url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html',
    'Accept-Language': 'en-US'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP {resp.status_code} OK\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML
essence = resp.text
soup = BeautifulSoup(essence, 'html.parser')

# 4) Iterate through h2/h3/h4 headings and capture span.mw-headline info
debug_lines = []
headers = soup.find_all(['h2', 'h3', 'h4'])
print(f"Found {len(headers)} total heading tags (h2/h3/h4). Scanning for .mw-headline spans...\n")
for heading in headers:
    span = heading.find('span', class_='mw-headline')
    if not span:
        continue
    hid = span.get('id', '')
    text = span.get_text(strip=True)
    tag = heading.name
n    # Count tables until next same/higher-level heading
    count = 0
    for sib in heading.find_next_siblings():
        if sib.name in ['h2', 'h3', 'h4']:
            break
        if sib.name == 'table':
            count += 1
    line = f"{tag} id='{hid}' text='{text}' → {count} table(s)"
    debug_lines.append(line)

# 5) Write debug info to file
dbg_path = os.path.join(workspace_dir, 'survivor_debug_headings.txt')
with open(dbg_path, 'w', encoding='utf-8') as f:
    f.write("# Debug: section headings (h2/h3/h4) with .mw-headline and count of tables until next heading\n")
    for l in debug_lines:
        f.write(l + "\n")

# 6) Print results
good = len(debug_lines) > 0
print(f"✅ Wrote {len(debug_lines)} heading entries to {dbg_path}\n")
print("=== Contents of survivor_debug_headings.txt ===")
for l in debug_lines:
    print(l)
print("=== End debug info ===")
if not good:
    print("⚠️ Warning: No headings captured. Check selectors or page structure.")
```