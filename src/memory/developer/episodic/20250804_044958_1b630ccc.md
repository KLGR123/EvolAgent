### Development Step 54: Extract Survivor US Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated fan website update for a Survivor enthusiast portal, pulling the latest winner list into the site’s database for season-by-season profiles
- Backend data ingestion for a mobile trivia app, ensuring the Survivor winners JSON is always current for quiz question generation
- Media analytics dashboard for a TV research firm, tracking winner demographics across seasons 1–44 to visualize voting trends and cast diversity
- Content pipeline for a sports and entertainment newsletter, auto-extracting new Survivor winners each season and embedding them into weekly email blasts
- Academic study on reality TV outcomes, converting the winners table into structured JSON for statistical analysis on gender and alliance patterns
- Social media automation for a TV network’s Twitter bot, posting anniversary reminders of each season’s winner with accurate historical data
- Archival workflow in a digital library project, harvesting page tables into JSON to preserve Survivor winner records for future cultural research

```
import os
import sys
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch Survivor page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP Status: {resp.status_code} OK\n")
    print(f"→ Final URL: {resp.url}\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML with BeautifulSoup
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Locate the Winners section by its heading ID and grab the next <table>
winners_span = soup.find('span', id='Winners')
if not winners_span:
    print("❌ Could not find <span id='Winners'>. The page structure may have changed.")
    sys.exit(1)
# Parent is the <h2> tag
heading = winners_span.find_parent(['h2', 'h3', 'h4'])
# Find the next sibling table (skip non-tag siblings)
winners_table = None
for sib in heading.next_siblings:
    if getattr(sib, 'name', None) == 'table':
        winners_table = sib
        break
if not winners_table:
    print("❌ Could not locate the winners table after the Winners heading.")
    sys.exit(1)
print("→ Located Winners table under the Winners section.\n")

# 5) Extract rows: each <tr> after the header
rows = winners_table.find_all('tr')
print(f"Table has {len(rows)-1} data rows (excluding header).\n")
data = []
for i, row in enumerate(rows[1:], start=1):  # skip header
    cells = row.find_all(['th', 'td'])
    if len(cells) < 2:
        print(f"  Skipping row {i}: fewer than 2 cells.")
        continue
    # Season number in first cell
    season_text = cells[0].get_text(' ', strip=True).split()[0]
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"  Skipping row {i}: cannot parse season '{season_text}'.")
        continue
    if season_num < 1 or season_num > 44:
        print(f"  Skipping season {season_num}: outside 1–44 range.")
        continue
    # Winner name in second cell
    winner_cell = cells[1]
    # Prefer link text if present
    link = winner_cell.find('a')
    if link and link.get_text(strip=True):
        winner_name = link.get_text(strip=True)
    else:
        winner_name = winner_cell.get_text(' ', strip=True)
    data.append({'season': season_num, 'winner': winner_name})
    print(f"Parsed Season {season_num}: {winner_name}")

if not data:
    print("⚠️ No winners data parsed. Check the table structure and parsing logic.")
    sys.exit(1)

# 6) Sort and save JSON
data_sorted = sorted(data, key=lambda x: x['season'])
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(data_sorted, f, ensure_ascii=False, indent=2)
print(f"\n✅ Successfully wrote {len(data_sorted)} entries to: {out_path}")
```