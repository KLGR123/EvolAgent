### Development Step 47: Extract Survivor (US) Winners for Seasons 1–44 and Save to JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Trivia game engine integration for a TV quiz app that auto-populates weekly Survivor winner questions from the JSON list
- Media analytics dashboard for broadcast networks to visualize winner trends and demographic shifts across Survivor seasons
- Academic sociology research compiling Survivor winner data to study patterns in age, gender, and contestant backgrounds
- Automated content pipeline for entertainment blogs that pulls winner names into season recap articles and newsletters
- Subscriber outreach tool for streaming services to send anniversary highlights featuring past Survivor champions
- Chatbot development for fan communities, enabling real-time Q&A about Survivor seasons and their winners
- Machine learning dataset assembly for predictive models aiming to forecast future reality TV show outcomes

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page for debug generation
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page for debug generation...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
})
try:
    resp.raise_for_status()
except Exception as e:
    print(f"❌ Error fetching page: {e}")
    sys.exit(1)

# 3) Parse HTML and collect all section headings
soup = BeautifulSoup(resp.text, 'html.parser')
debug_lines = []

# For each <span class="mw-headline">, record its id, text, parent heading tag, and number of tables until next heading
for span in soup.select('span.mw-headline'):
    hid = span.get('id', '')
    text = span.get_text(strip=True)
    parent = span.find_parent(['h2', 'h3', 'h4'])
    parent_tag = parent.name if parent else 'UNKNOWN'
    # Count tables until the next heading of same/higher level
    tbl_count = 0
    if parent:
        for sib in parent.find_next_siblings():
            if sib.name in ['h2', 'h3', 'h4']:
                break
            if sib.name == 'table':
                tbl_count += 1
    debug_lines.append(f"{parent_tag} id='{hid}' text='{text}' → {tbl_count} table(s)")

# 4) Write debug info to file
dbg_path = os.path.join(workspace_dir, 'survivor_debug_headings.txt')
with open(dbg_path, 'w', encoding='utf-8') as f:
    f.write("# Debug: section headings and count of tables until next heading\n")
    for line in debug_lines:
        f.write(line + "\n")

# 5) Print summary for tester
good = len(debug_lines) > 0
print(f"✅ Generated debug info: {dbg_path}")
print("=== Contents of survivor_debug_headings.txt ===")
for line in debug_lines:
    print(line)
print("=== End of debug info ===")
if not good:
    print("⚠️ Warning: No headings were captured. Check selectors or page structure.")
```