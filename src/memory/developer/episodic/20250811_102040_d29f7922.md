### Development Step 5: Counting Twitter/X Citations in Archived Wikipedia August Day Pages from June 2023 Using Wayback Machine

**Description**: Search for and access the archived Wikipedia pages for each day of August (August 1st through August 31st) as they appeared in the last versions from June 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific versions of these 31 daily pages from late June 2023. Extract all reference citations from each page and identify those that cite Twitter/X posts (looking for twitter.com, x.com, or @username patterns in the reference URLs). Count the total number of Twitter/X citations for each individual August day page and compile the results.

**Use Cases**:
- Media studies research analyzing the prevalence of Twitter/X citations in Wikipediaâ€™s daily news coverage to assess social mediaâ€™s influence on public knowledge in August 2023
- Academic investigations into the reliability of Wikipedia references by quantifying the use of social media sources in historical page versions
- Fact-checking organizations auditing how often breaking news on Wikipedia relied on Twitter/X posts during a specific period
- Digital preservation teams tracking the persistence and accessibility of Twitter/X-linked citations in Wikipediaâ€™s archived pages
- Journalism educators illustrating the evolution of citation practices by comparing the frequency of Twitter/X references before and after major platform changes
- Automated tools for Wikipedia editors to identify and flag Twitter/X citations for review or replacement, improving citation quality
- Data journalists creating visualizations of social mediaâ€™s role in Wikipediaâ€™s daily event documentation for public reports
- Legal researchers examining the use of ephemeral social media content as evidence or reference in public knowledge repositories

```
import os
import requests
import json
from datetime import datetime
import time
from bs4 import BeautifulSoup
import re

print("=== CONTINUING FROM INTERRUPTED COMPREHENSIVE SEARCH ===\n")
print("Based on execution history: Found 19 June 2023 revisions from 30 processed pages")
print("Now completing the search and moving to citation analysis phase\n")

# First, inspect workspace to understand current state
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using existing workspace: {workspace_dir}\n")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created new workspace: {workspace_dir}\n")

print("=== INSPECTING EXISTING WORKSPACE FILES ===\n")
existing_files = [f for f in os.listdir(workspace_dir) if f.endswith('.json')]
print(f"Found {len(existing_files)} JSON files in workspace:")

for file in existing_files:
    file_path = os.path.join(workspace_dir, file)
    file_size = os.path.getsize(file_path)
    print(f"  - {file} ({file_size:,} bytes)")

# Check if comprehensive results file exists from the interrupted run
comprehensive_file = os.path.join(workspace_dir, 'august_pages_comprehensive_june_2023.json')

if os.path.exists(comprehensive_file):
    print(f"\n=== FOUND EXISTING COMPREHENSIVE RESULTS FILE ===\n")
    
    # First inspect the file structure
    with open(comprehensive_file, 'r', encoding='utf-8') as f:
        content = f.read()
        print(f"Comprehensive file size: {len(content):,} characters")
    
    # Now load and inspect structure
    with open(comprehensive_file, 'r', encoding='utf-8') as f:
        comprehensive_data = json.load(f)
    
    print(f"\nComprehensive file structure:")
    for key in comprehensive_data.keys():
        print(f"  {key}: {type(comprehensive_data[key]).__name__}")
    
    # Extract successful pages
    if 'successful_pages' in comprehensive_data:
        successful_pages = comprehensive_data['successful_pages']
        print(f"\nFound {len(successful_pages)} successful pages with June 2023 revisions")
        
        # Show first few successful pages
        print(f"\nFirst 5 successful pages:")
        for i, page in enumerate(successful_pages[:5], 1):
            print(f"  {i}. {page['page']}: {page['date']} (ID: {page['revision_id']}, {page['size']:,} bytes)")
    
    existing_successful = successful_pages if 'successful_pages' in comprehensive_data else []
else:
    print(f"\n=== NO COMPREHENSIVE RESULTS FILE FOUND ===\n")
    print("Need to reconstruct successful pages from execution history")
    
    # Based on the execution history, reconstruct the successful pages
    # From the output, I can see these successful pages:
    existing_successful = [
        {'page': 'August 1', 'date': '2023-06-27', 'revision_id': 1162212811, 'size': 54328},
        {'page': 'August 3', 'date': '2023-06-21', 'revision_id': 1161173535, 'size': 43523},
        {'page': 'August 5', 'date': '2023-06-14', 'revision_id': 1160055600, 'size': 0},  # Size unknown from history
        {'page': 'August 6', 'date': '2023-06-21', 'revision_id': 1161173545, 'size': 0},
        {'page': 'August 7', 'date': '2023-06-23', 'revision_id': 1161549076, 'size': 0},
        {'page': 'August 8', 'date': '2023-06-22', 'revision_id': 1161374666, 'size': 0},
        {'page': 'August 9', 'date': '2023-06-04', 'revision_id': 1158557601, 'size': 0},
        {'page': 'August 12', 'date': '2023-06-21', 'revision_id': 1161200055, 'size': 0},
        {'page': 'August 13', 'date': '2023-06-20', 'revision_id': 1161023889, 'size': 0},
        {'page': 'August 15', 'date': '2023-06-25', 'revision_id': 1161888362, 'size': 0},
        {'page': 'August 17', 'date': '2023-06-24', 'revision_id': 1161732805, 'size': 0},
        {'page': 'August 18', 'date': '2023-06-30', 'revision_id': 1162729014, 'size': 0},
        {'page': 'August 19', 'date': '2023-06-29', 'revision_id': 1162559583, 'size': 0},
        {'page': 'August 20', 'date': '2023-06-13', 'revision_id': 1159917358, 'size': 0},
        {'page': 'August 23', 'date': '2023-06-04', 'revision_id': 1158447489, 'size': 0},
        {'page': 'August 24', 'date': '2023-06-26', 'revision_id': 1161975431, 'size': 0},
        {'page': 'August 25', 'date': '2023-06-15', 'revision_id': 1160306747, 'size': 0},
        {'page': 'August 26', 'date': '2023-06-03', 'revision_id': 1158350124, 'size': 0},
        {'page': 'August 27', 'date': '2023-06-21', 'revision_id': 1161252298, 'size': 0}
    ]
    
    print(f"Reconstructed {len(existing_successful)} successful pages from execution history")

print(f"\n=== PROCEEDING WITH CITATION ANALYSIS ===\n")
print(f"Working with {len(existing_successful)} August pages with June 2023 revisions")
print(f"Next phase: Extract content and analyze Twitter/X citations\n")

# Wikipedia API configuration
api_url = "https://en.wikipedia.org/w/api.php"

def get_revision_content(revision_id):
    """Get the full content of a specific Wikipedia revision"""
    params = {
        'action': 'query',
        'format': 'json',
        'prop': 'revisions',
        'revids': revision_id,
        'rvprop': 'content|timestamp|ids'
    }
    
    try:
        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if 'query' in data and 'pages' in data['query']:
            pages = data['query']['pages']
            page_id = list(pages.keys())[0]
            
            if 'revisions' in pages[page_id] and len(pages[page_id]['revisions']) > 0:
                revision = pages[page_id]['revisions'][0]
                if '*' in revision:  # Content is in the '*' field
                    content = revision['*']
                    return {
                        'success': True,
                        'content': content,
                        'content_length': len(content)
                    }
        
        return {'success': False, 'error': 'No content found'}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

def extract_twitter_citations(content):
    """Extract Twitter/X citations from Wikipedia content"""
    twitter_citations = []
    
    # Patterns to match Twitter/X citations
    patterns = [
        r'https?://(?:www\.)?twitter\.com/[^\s\]\}\|]+',  # twitter.com URLs
        r'https?://(?:www\.)?x\.com/[^\s\]\}\|]+',        # x.com URLs
        r'\|\s*url\s*=\s*https?://(?:www\.)?twitter\.com/[^\s\]\}\|]+',  # Citation template URLs
        r'\|\s*url\s*=\s*https?://(?:www\.)?x\.com/[^\s\]\}\|]+',
        r'@[A-Za-z0-9_]+(?=\s|\]|\}|\||$)',  # @username patterns (more restrictive)
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        for match in matches:
            # Clean up the match
            clean_match = match.strip()
            if clean_match and clean_match not in twitter_citations:
                twitter_citations.append(clean_match)
    
    return twitter_citations

print("=== STEP 1: EXTRACTING CONTENT AND ANALYZING CITATIONS ===\n")
print("Processing first 3 successful pages to test the citation extraction...\n")

# Test citation extraction on first 3 pages
test_pages = existing_successful[:3]
citation_results = {}

for page_info in test_pages:
    page_name = page_info['page']
    revision_id = page_info['revision_id']
    
    print(f"\n--- Processing: {page_name} (Revision {revision_id}) ---")
    
    # Get revision content
    print(f"  Fetching content...")
    content_result = get_revision_content(revision_id)
    
    if content_result['success']:
        content = content_result['content']
        content_length = content_result['content_length']
        print(f"    âœ“ Retrieved content: {content_length:,} characters")
        
        # Extract Twitter citations
        print(f"  Analyzing Twitter/X citations...")
        twitter_citations = extract_twitter_citations(content)
        
        print(f"    Found {len(twitter_citations)} Twitter/X citations")
        
        # Show first few citations if found
        if twitter_citations:
            print(f"    Sample citations:")
            for i, citation in enumerate(twitter_citations[:3], 1):
                preview = citation[:80] + ('...' if len(citation) > 80 else '')
                print(f"      {i}. {preview}")
        else:
            print(f"    No Twitter/X citations found")
        
        citation_results[page_name] = {
            'page': page_name,
            'revision_id': revision_id,
            'date': page_info['date'],
            'content_length': content_length,
            'twitter_citations_count': len(twitter_citations),
            'twitter_citations': twitter_citations,
            'analysis_success': True
        }
    else:
        print(f"    âŒ Failed to retrieve content: {content_result['error']}")
        citation_results[page_name] = {
            'page': page_name,
            'revision_id': revision_id,
            'date': page_info['date'],
            'twitter_citations_count': 0,
            'twitter_citations': [],
            'analysis_success': False,
            'error': content_result['error']
        }
    
    # Add delay to be respectful to Wikipedia's servers
    time.sleep(2)

print(f"\n=== CITATION EXTRACTION TEST RESULTS ===\n")

total_citations = 0
for page_name, result in citation_results.items():
    if result['analysis_success']:
        citations_count = result['twitter_citations_count']
        total_citations += citations_count
        print(f"âœ“ {page_name}: {citations_count} Twitter/X citations ({result['content_length']:,} chars)")
    else:
        print(f"âŒ {page_name}: Analysis failed - {result.get('error', 'Unknown error')}")

print(f"\nTest summary:")
print(f"  Pages analyzed: {len(citation_results)}")
print(f"  Total Twitter/X citations found: {total_citations}")
print(f"  Average citations per page: {total_citations/len(citation_results):.1f}")

# Save test results
test_results = {
    'analysis_metadata': {
        'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'phase': 'citation_extraction_test',
        'pages_tested': len(test_pages),
        'total_successful_pages_available': len(existing_successful)
    },
    'test_results': citation_results,
    'test_summary': {
        'total_citations_found': total_citations,
        'average_citations_per_page': round(total_citations/len(citation_results), 1)
    }
}

test_file = os.path.join(workspace_dir, 'twitter_citation_extraction_test.json')
with open(test_file, 'w', encoding='utf-8') as f:
    json.dump(test_results, f, indent=2, ensure_ascii=False)

print(f"\nâœ… Citation extraction test results saved to: {os.path.basename(test_file)}")

print(f"\n=== NEXT STEPS ===\n")
if total_citations > 0:
    print(f"ğŸ¯ SUCCESS: Found {total_citations} Twitter/X citations in test pages!")
    print(f"ğŸ“‹ Ready to scale to all {len(existing_successful)} successful pages")
    print(f"ğŸ”„ Next: Process all pages and compile final results")
elif len([r for r in citation_results.values() if r['analysis_success']]) > 0:
    print(f"âœ“ Content extraction working, but no Twitter citations in test pages")
    print(f"ğŸ“‹ This is normal - not all pages may have Twitter citations")
    print(f"ğŸ”„ Next: Process all pages to get complete picture")
else:
    print(f"âŒ Content extraction failed - need to debug approach")
    print(f"ğŸ”§ Check API parameters and error handling")

print(f"\nğŸ“Š Current progress: Citation extraction method validated")
print(f"ğŸ“ Ready to process all {len(existing_successful)} pages with June 2023 revisions")
```