### Development Step 6: Title:  
Download and Extract Full Content of Galactic Center Filaments Paper for NASA Award Number Details

**Description**: Access the research paper at https://iopscience.iop.org/article/10.3847/2041-8213/acd54b titled 'The Population of the Galactic Center Filaments: Position Angle Distribution Reveals a Degree-scale Collimated Outflow from Sgr A* along the Galactic Plane' and download it to the workspace folder. Extract the full paper content including author information, acknowledgments section, and funding details to locate R. G. Arendt's NASA award number information.

**Use Cases**:
- Grant compliance auditing for university research offices to verify NASA funding attribution in published astrophysics papers
- Automated extraction of funding acknowledgments for bibliometric analysis in research impact studies
- Building a searchable database of NASA-funded projects by mining published literature for award numbers and PI associations
- Supporting science communication teams in highlighting institutional contributions by extracting author and funding details from high-impact publications
- Streamlining proposal writing by quickly retrieving prior NASA award numbers linked to specific researchers or topics
- Assisting librarians in cataloging and metadata enrichment by programmatically extracting author affiliations and funding sources from journal articles
- Enabling research administrators to monitor faculty participation in federally funded projects through automated publication scanning
- Facilitating due diligence for technology transfer offices by identifying NASA-funded intellectual property in recent scientific papers

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime

print("=== ACCESSING RESEARCH PAPER: GALACTIC CENTER FILAMENTS ===\n")
print("Objective: Download and extract R. G. Arendt's NASA award information from the research paper\n")

# Create workspace directory if needed
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Paper details from previous analysis
paper_url = "https://iopscience.iop.org/article/10.3847/2041-8213/acd54b"
paper_title = "The Population of the Galactic Center Filaments: Position Angle Distribution Reveals a Degree-scale Collimated Outflow from Sgr A* along the Galactic Plane"
paper_doi = "10.3847/2041-8213/acd54b"

print(f"Target paper URL: {paper_url}")
print(f"Paper title: {paper_title}")
print(f"DOI: {paper_doi}")

# First, let's check if we already have workspace files from previous attempts
workspace_files = []
if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    print(f"\nFound {len(workspace_files)} existing files in workspace:")
    for file in workspace_files:
        file_size = os.path.getsize(os.path.join('workspace', file))
        print(f"  - {file} ({file_size:,} bytes)")

# Strategy: Try multiple approaches to access the paper content
print("\n=== APPROACH 1: TRY ARXIV PREPRINT ACCESS ===\n")

# Many astrophysics papers are available on ArXiv as preprints
# Let's search for this specific paper on ArXiv
try:
    # ArXiv search API
    arxiv_api_url = "http://export.arxiv.org/api/query"
    
    # Search using parts of the title and author names
    search_queries = [
        'ti:"Galactic Center Filaments" AND au:"Yusef-Zadeh"',
        'ti:"Position Angle Distribution" AND au:"Arendt"',
        'all:"Sgr A*" AND all:"filaments" AND au:"Arendt"',
        'all:"2306.01071"'  # This might be the ArXiv ID based on the ADS link
    ]
    
    for i, query in enumerate(search_queries, 1):
        print(f"ArXiv Search Query {i}: {query}")
        
        params = {
            'search_query': query,
            'start': 0,
            'max_results': 5
        }
        
        time.sleep(2)  # Be respectful to ArXiv API
        
        response = requests.get(arxiv_api_url, params=params, timeout=30)
        
        if response.status_code == 200:
            print(f"  ✓ ArXiv API responded successfully")
            
            # Parse XML response
            from xml.etree import ElementTree as ET
            
            try:
                root = ET.fromstring(response.text)
                
                # ArXiv uses Atom namespace
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                
                entries = root.findall('atom:entry', ns)
                print(f"  Found {len(entries)} ArXiv entries")
                
                if entries:
                    for j, entry in enumerate(entries, 1):
                        title_elem = entry.find('atom:title', ns)
                        id_elem = entry.find('atom:id', ns)
                        
                        if title_elem is not None and id_elem is not None:
                            arxiv_title = title_elem.text.strip()
                            arxiv_id = id_elem.text.strip()
                            
                            print(f"    Entry {j}:")
                            print(f"      Title: {arxiv_title[:100]}...")
                            print(f"      ID: {arxiv_id}")
                            
                            # Check if this matches our target paper
                            if ('galactic center' in arxiv_title.lower() and 
                                'filaments' in arxiv_title.lower()):
                                
                                print(f"      ★ POTENTIAL MATCH FOUND!")
                                
                                # Extract ArXiv ID from the URL
                                arxiv_paper_id = arxiv_id.split('/')[-1]
                                
                                # Try to download the PDF
                                arxiv_pdf_url = f"https://arxiv.org/pdf/{arxiv_paper_id}.pdf"
                                print(f"      Attempting to download PDF: {arxiv_pdf_url}")
                                
                                try:
                                    headers = {
                                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                                    }
                                    
                                    pdf_response = requests.get(arxiv_pdf_url, headers=headers, timeout=60)
                                    
                                    if pdf_response.status_code == 200:
                                        # Save the PDF
                                        pdf_filename = f'workspace/galactic_center_filaments_arxiv_{arxiv_paper_id}.pdf'
                                        with open(pdf_filename, 'wb') as f:
                                            f.write(pdf_response.content)
                                        
                                        pdf_size = len(pdf_response.content)
                                        print(f"      ✓ PDF downloaded successfully: {pdf_filename} ({pdf_size:,} bytes)")
                                        
                                        # Now try to extract text from the PDF
                                        print(f"      Extracting text from PDF...")
                                        
                                        try:
                                            from langchain_community.document_loaders import PyPDFLoader
                                            
                                            # Load and extract text from PDF
                                            loader = PyPDFLoader(pdf_filename)
                                            pages = loader.load_and_split()
                                            
                                            if pages:
                                                print(f"      ✓ Extracted {len(pages)} pages from PDF")
                                                
                                                # Combine all pages
                                                full_text = "\n".join([page.page_content for page in pages])
                                                
                                                # Save the extracted text
                                                text_filename = 'workspace/galactic_center_filaments_arxiv_text.txt'
                                                with open(text_filename, 'w', encoding='utf-8') as f:
                                                    f.write(f"ArXiv Paper Text Extraction\n")
                                                    f.write(f"Paper ID: {arxiv_paper_id}\n")
                                                    f.write(f"PDF URL: {arxiv_pdf_url}\n")
                                                    f.write(f"Extraction Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                                                    f.write("=" * 80 + "\n")
                                                    f.write(full_text)
                                                
                                                print(f"      ✓ Text saved to: {text_filename} ({len(full_text):,} characters)")
                                                
                                                # Search for R. G. Arendt and NASA award information
                                                print(f"\n      === SEARCHING FOR R. G. ARENDT AND NASA AWARDS ===\n")
                                                
                                                # Look for Arendt
                                                arendt_patterns = ['R. G. Arendt', 'R.G. Arendt', 'Arendt']
                                                arendt_found = False
                                                
                                                for pattern in arendt_patterns:
                                                    if pattern in full_text:
                                                        arendt_found = True
                                                        print(f"      ★ Found '{pattern}' in PDF text")
                                                        
                                                        # Get context around Arendt's name
                                                        import re
                                                        matches = list(re.finditer(pattern, full_text, re.IGNORECASE))
                                                        for match in matches[:2]:  # Show first 2 matches
                                                            start = max(0, match.start() - 300)
                                                            end = min(len(full_text), match.end() + 300)
                                                            context = full_text[start:end]
                                                            print(f"        Context: ...{context[:200]}...")
                                                        break
                                                
                                                # Look for acknowledgments section
                                                acknowledgment_keywords = ['acknowledgment', 'acknowledgement', 'funding', 'support', 'grant', 'award']
                                                
                                                for keyword in acknowledgment_keywords:
                                                    if keyword.lower() in full_text.lower():
                                                        print(f"      ★ Found '{keyword}' in PDF text")
                                                        
                                                        # Find acknowledgments section
                                                        import re
                                                        
                                                        # Look for acknowledgments section
                                                        ack_pattern = rf'{keyword}[^.]*?NASA[^.]*?\d[^.]*?'
                                                        ack_matches = re.findall(ack_pattern, full_text, re.IGNORECASE | re.DOTALL)
                                                        
                                                        if ack_matches:
                                                            print(f"        NASA award references found:")
                                                            for match in ack_matches[:3]:
                                                                print(f"          - {match[:150]}...")
                                                
                                                # Look specifically for NASA award numbers
                                                nasa_award_patterns = [
                                                    r'NASA[\s\w]*?award[\s#]*([A-Z0-9\-]+)',
                                                    r'award[\s#]*([A-Z0-9\-]+)[\s\w]*?NASA',
                                                    r'grant[\s#]*([A-Z0-9\-]+)',
                                                    r'80NSSC[A-Z0-9]+',
                                                    r'NAS[A-Z0-9\-]+'
                                                ]
                                                
                                                award_numbers_found = []
                                                for pattern in nasa_award_patterns:
                                                    matches = re.findall(pattern, full_text, re.IGNORECASE)
                                                    if matches:
                                                        award_numbers_found.extend(matches)
                                                        print(f"      ★ Found potential NASA award numbers: {matches}")
                                                
                                                # Save extraction results
                                                extraction_data = {
                                                    'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                                    'source': 'ArXiv PDF',
                                                    'arxiv_id': arxiv_paper_id,
                                                    'pdf_url': arxiv_pdf_url,
                                                    'pdf_filename': pdf_filename,
                                                    'text_filename': text_filename,
                                                    'pdf_size_bytes': pdf_size,
                                                    'text_length_chars': len(full_text),
                                                    'pages_extracted': len(pages),
                                                    'arendt_found': arendt_found,
                                                    'award_numbers_found': award_numbers_found
                                                }
                                                
                                                extraction_json = 'workspace/arxiv_extraction_results.json'
                                                with open(extraction_json, 'w') as f:
                                                    json.dump(extraction_data, f, indent=2)
                                                
                                                print(f"      ✓ Extraction results saved to: {extraction_json}")
                                                
                                            else:
                                                print(f"      ✗ No pages extracted from PDF")
                                                
                                        except ImportError:
                                            print(f"      ⚠️ PyPDFLoader not available, cannot extract PDF text")
                                        except Exception as e:
                                            print(f"      ✗ Error extracting PDF text: {str(e)}")
                                        
                                    else:
                                        print(f"      ✗ Failed to download PDF (Status: {pdf_response.status_code})")
                                        
                                except Exception as e:
                                    print(f"      ✗ Exception downloading PDF: {str(e)}")
                                
                                break  # Found our paper, no need to check other entries
                        
                else:
                    print(f"  No entries found for this query")
                    
            except ET.ParseError as e:
                print(f"  ✗ Error parsing ArXiv XML response: {str(e)}")
                
        else:
            print(f"  ✗ ArXiv API request failed (Status: {response.status_code})")
        
        print()  # Add space between queries
        
        # If we found and downloaded the paper, break out of the search loop
        if any('arxiv' in f and '.pdf' in f for f in os.listdir('workspace') if os.path.exists('workspace')):
            print("✓ Successfully downloaded paper from ArXiv, stopping search")
            break
            
except Exception as e:
    print(f"✗ Exception during ArXiv search: {str(e)}")

print("\n=== APPROACH 2: DIRECT DOI RESOLVER ACCESS ===\n")

# Try the DOI resolver with different strategies
try:
    doi_urls = [
        f"https://doi.org/{paper_doi}",
        f"https://dx.doi.org/{paper_doi}",
        f"https://www.doi.org/{paper_doi}"
    ]
    
    for i, doi_url in enumerate(doi_urls, 1):
        print(f"Trying DOI resolver {i}: {doi_url}")
        
        # Use different headers for each attempt
        headers_variants = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Connection': 'keep-alive'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/pdf,text/html,application/xhtml+xml,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9'
            }
        ]
        
        for j, headers in enumerate(headers_variants, 1):
            print(f"  Attempt {j} with headers variant {j}")
            
            time.sleep(3)  # Respectful delay
            
            try:
                response = requests.get(doi_url, headers=headers, timeout=30, allow_redirects=True)
                
                if response.status_code == 200:
                    print(f"  ✓ Success! (Status: {response.status_code})")
                    print(f"  Final URL: {response.url}")
                    print(f"  Content-Type: {response.headers.get('content-type', 'unknown')}")
                    print(f"  Content length: {len(response.content):,} bytes")
                    
                    # Check if we got a PDF
                    content_type = response.headers.get('content-type', '').lower()
                    
                    if 'pdf' in content_type:
                        print(f"  ★ Received PDF content!")
                        
                        # Save the PDF
                        pdf_filename = 'workspace/galactic_center_filaments_doi.pdf'
                        with open(pdf_filename, 'wb') as f:
                            f.write(response.content)
                        
                        print(f"  ✓ PDF saved to: {pdf_filename}")
                        
                        # Extract text from PDF (same as ArXiv approach)
                        # [PDF text extraction code would go here - similar to ArXiv approach]
                        
                    elif 'html' in content_type:
                        print(f"  Received HTML content")
                        
                        # Check if it's a CAPTCHA page
                        soup = BeautifulSoup(response.content, 'html.parser')
                        page_title = soup.find('title')
                        
                        if page_title:
                            title_text = page_title.get_text().strip()
                            print(f"  Page title: {title_text}")
                            
                            if 'captcha' in title_text.lower() or 'bot' in title_text.lower():
                                print(f"  ⚠️ CAPTCHA detected, skipping this approach")
                            else:
                                print(f"  ✓ Valid HTML content received")
                                
                                # Save the HTML
                                html_filename = 'workspace/galactic_center_filaments_doi.html'
                                with open(html_filename, 'w', encoding='utf-8') as f:
                                    f.write(response.text)
                                
                                print(f"  ✓ HTML saved to: {html_filename}")
                    
                    # If we got valid content, break out of loops
                    if ('pdf' in content_type or 
                        ('html' in content_type and 'captcha' not in response.text.lower())):
                        print(f"  ✓ Successfully accessed paper via DOI resolver")
                        break
                        
                else:
                    print(f"  ✗ Failed (Status: {response.status_code})")
                    
            except Exception as e:
                print(f"  ✗ Exception: {str(e)}")
        
        # If successful, break out of URL loop
        if any('doi' in f for f in os.listdir('workspace') if os.path.exists('workspace')):
            break
            
except Exception as e:
    print(f"✗ Exception during DOI resolver access: {str(e)}")

print("\n=== FINAL SUMMARY ===\n")

# List all files created
if os.path.exists('workspace'):
    final_files = os.listdir('workspace')
    print(f"Files created in workspace ({len(final_files)} total):")
    
    for file in sorted(final_files):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")
        
        # Highlight important files
        if '.pdf' in file:
            print(f"    ★ PDF FILE - Contains full paper content")
        elif 'text' in file and file.endswith('.txt'):
            print(f"    ★ TEXT EXTRACTION - Searchable paper content")
        elif 'results' in file and file.endswith('.json'):
            print(f"    ★ ANALYSIS RESULTS - Structured extraction data")

print(f"\nNext step: Parse the successfully downloaded content to extract R. G. Arendt's NASA award number")
```