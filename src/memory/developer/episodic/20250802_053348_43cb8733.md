### Development Step 3: Download Leicester’s “Can Hiccup Supply Enough Fish for Dragon’s Diet?” PDF into workspace

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated retrieval of competitor whitepapers and industry reports in PDF for market intelligence teams
- Batch downloading of SEC EDGAR PDF filings (10-K, 10-Q) for financial analysts and portfolio managers
- Scraping and archiving legal case opinions and statutes in PDF for law firms and legal researchers
- Automated collection of clinical trial protocols and medical guidelines in PDF for pharmaceutical R&D teams
- Bulk acquisition of engineering datasheets and technical manuals in PDF for product development engineers
- Real-time download of government policy documents and regulatory standards in PDF for compliance officers
- Automated gathering of academic literature PDFs for systematic literature reviews in graduate research labs
- Collection of NGO and UN reports in PDF format for policy analysts and non-profit organizations

```
import os
import sys
import time
import requests

# Purpose: Search and download the University of Leicester paper
# "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?"
# Save PDF to workspace/dragon_diet_fish_bag.pdf with primary SerpAPI + fallback DuckDuckGo scraping

# 1. Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2. Prepare search query and API key
query = '"Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" University of Leicester PDF'
api_key = os.getenv('SERPAPI_API_KEY')

print("Starting search for the paper PDF...")
print(f"Search query: {query}")

# 3. Try SerpAPI search with exponential backoff (if API key present)
results = None
if api_key:
    params = {
        'engine': 'google',
        'q': query,
        'api_key': api_key,
        'num': 10,
        'google_domain': 'google.com',
        'safe': 'off'
    }
    max_retries = 3
    for attempt in range(1, max_retries + 1):
        print(f"SerpAPI request attempt {attempt}/{max_retries}...")
        try:
            resp = requests.get('https://serpapi.com/search.json', params=params, timeout=30)
            resp.raise_for_status()
            results = resp.json()
            print("✅ SerpAPI response received.")
            break
        except requests.exceptions.HTTPError as e:
            status = e.response.status_code if e.response else None
            if status == 429:
                backoff = 2 ** (attempt - 1)
                print(f"⚠️ 429 Too Many Requests. Backing off for {backoff}s...")
                time.sleep(backoff)
                continue
            else:
                print(f"❌ SerpAPI HTTPError {status}: {e}")
                break
        except Exception as e:
            print(f"❌ Error querying SerpAPI: {e}")
            break
    else:
        print(f"❌ All {max_retries} SerpAPI attempts failed.")

# 4. Function to extract PDF link from SerpAPI results
def extract_pdf_from_serpapi(res_json):
    for idx, item in enumerate(res_json.get('organic_results', []), start=1):
        link = item.get('link', '')
        title = item.get('title', '')
        snippet = item.get('snippet', '')
        print(f"[SerpAPI Result #{idx}] Title: {title}\n  URL: {link}\n  Snippet: {snippet[:100]}{'...' if len(snippet)>100 else ''}\n")
        low = link.lower()
        if low.endswith('.pdf') or '.pdf?' in low:
            return link
    return None

pdf_link = None
if results:
    print(f"Total organic results from SerpAPI: {len(results.get('organic_results', []))}")
    pdf_link = extract_pdf_from_serpapi(results)
    if pdf_link:
        print(f"✅ Selected PDF link from SerpAPI: {pdf_link}")

# 5. Fallback: DuckDuckGo HTML search if SerpAPI failed or no PDF link found
if not pdf_link:
    print("\n---\nNo PDF link from SerpAPI. Falling back to DuckDuckGo HTML search...\n---")
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("ERROR: BeautifulSoup4 is required for fallback search. Please install with 'pip install beautifulsoup4'.")
        sys.exit(1)

    ddg_url = 'https://duckduckgo.com/html/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
        'Accept-Language': 'en-US,en;q=0.5'
    }
    print(f"Querying DuckDuckGo HTML: {query}")
    resp = requests.get(ddg_url, params={'q': query}, headers=headers, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')

    # Collect all candidate links
    candidates = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        # DuckDuckGo wraps redirect URLs, skip internal results
        if href.startswith('/l/'):
            continue
        # Look for .pdf in the URL
        if '.pdf' in href.lower():
            candidates.append(href)
    candidates = list(dict.fromkeys(candidates))  # unique

    if candidates:
        print(f"Found {len(candidates)} candidate PDF URLs via DuckDuckGo:")
        for i, link in enumerate(candidates, start=1):
            print(f"  {i}. {link}")
        pdf_link = candidates[0]
        print(f"\n✅ Selected first fallback PDF link: {pdf_link}")
    else:
        print("❌ No PDF links found via DuckDuckGo fallback search.")
        sys.exit(1)

# 6. Download the PDF
pdf_path = os.path.join(workspace_dir, 'dragon_diet_fish_bag.pdf')
print(f"\nDownloading PDF to: {pdf_path}")
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'application/pdf'
}
total_bytes = 0
with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as dl:
    dl.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in dl.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                total_bytes += len(chunk)

print(f"Download complete: {pdf_path}")
print(f"File size: {total_bytes} bytes ({total_bytes/1024/1024:.2f} MB)")

# 7. Sanity check
if total_bytes < 10000:
    print("WARNING: The downloaded file is very small (<10 KB). It may not be the full PDF.")
else:
    print("✅ PDF appears to have been downloaded successfully.")

print("Script finished.")
```