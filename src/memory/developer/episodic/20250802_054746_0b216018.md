### Development Step 50: Download University of Leicester PDF ‘Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?’

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated academic literature collection for a PhD student: fetch and save PDFs of exact-titled journal articles into a reference manager for offline review
- Clinical trial dossier assembly in pharmaceutical R&D: download specific trial reports (e.g. “Phase III Cardiovascular Drug Study Results”) from regulatory portals into a secure project folder
- Legal eDiscovery workflow for law firms: retrieve and archive court opinions or statutes by exact title (e.g. “Smith v. Jones 2022 Ruling”) into document management systems
- Market intelligence gathering for competitive analysis: auto-download vendor whitepapers titled “2024 Cloud Adoption Trends” from corporate sites into a research repository
- Regulatory compliance audit support: collect named compliance guidelines (e.g. “GDPR Article 30 Record-Keeping Guidance”) from official government sites for audit evidence
- Technical documentation build in manufacturing: fetch versioned equipment manuals like “XYZ Sensor User Guide v2.1” from supplier domains into the product documentation pipeline
- NLP corpus creation for data science teams: accumulate domain-specific reports (e.g. “Global Energy Outlook 2050”) by exact title into a training dataset for language models

```
import os
import sys
import re
import requests
from bs4 import BeautifulSoup

# --- Configuration ---
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Exact paper title to search for
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
# DuckDuckGo HTML interface URL
ddg_url = "https://html.duckduckgo.com/html/"
# Browser-like headers to avoid simple blocks
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# Three progressive search queries\queries = [
    f'filetype:pdf "{title}" site:lra.le.ac.uk',
    f'filetype:pdf "{title}" site:le.ac.uk',
    f'filetype:pdf "{title}"'
]

# Output file paths
ddg_html_path  = os.path.join(workspace, 'ddg_search.html')
pdf_list_path = os.path.join(workspace, 'ddg_pdf_urls.txt')
pdf_out_path  = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

all_candidates = []
print("[INFO] Starting DuckDuckGo GET-based PDF search...")

for q in queries:
    print(f"[INFO] Query: {q}")
    try:
        resp = requests.get(ddg_url, params={'q': q}, headers=headers, timeout=30)
        print(f"[INFO] HTTP status: {resp.status_code}")
    except Exception as e:
        print(f"[WARN] Request error for '{q}': {e}")
        continue
    if resp.status_code != 200:
        print(f"[WARN] Non-200 status for '{q}', skipping.\n")
        continue

    # Save the first successful HTML for manual inspection
    if not os.path.exists(ddg_html_path):
        with open(ddg_html_path, 'w', encoding='utf-8') as f:
            f.write(resp.text)
        print(f"[SAVE] DuckDuckGo HTML -> {ddg_html_path}")

    soup = BeautifulSoup(resp.text, 'html.parser')
    candidates = set()

    # 1) DuckDuckGo result links: <a class="result__a"> entries
    for a in soup.find_all('a', class_='result__a', href=True):
        href = a['href']
        if '.pdf' in href.lower():
            candidates.add(href)
            print(f"    [FOUND] PDF link (result__a): {href}")

    # 2) Any <a href> ending with .pdf or containing .pdf?
    for a in soup.find_all('a', href=True):
        href = a['href']
        low = href.lower()
        if low.endswith('.pdf') or '.pdf?' in low:
            candidates.add(href)
            print(f"    [FOUND] PDF link (href): {href}")

    # 3) Regex fallback for raw PDF URLs
    pattern = r'https?://[^\s"\'<>]+?\.pdf(?:\?[^\s"\'<>]*)?'
    for match in re.findall(pattern, resp.text, flags=re.IGNORECASE):
        candidates.add(match)
        print(f"    [FOUND] PDF via regex: {match}")

    # Decode any DuckDuckGo redirect wrappers (uddg=)
    for link in candidates:
        if 'uddg=' in link:
            m = re.search(r'uddg=([^&]+)', link)
            url = requests.utils.unquote(m.group(1)) if m else link
        else:
            url = link
        all_candidates.append(url)

    if all_candidates:
        print(f"[INFO] Found {len(all_candidates)} candidate(s); stopping queries.")
        break
    else:
        print(f"[INFO] No PDF links for '{q}'. Trying next.\n")

# If no candidates found, exit with error
if not all_candidates:
    print("[ERROR] No PDF candidates found after all queries.")
    print(f"Inspect '{ddg_html_path}' and refine your parsing logic.")
    sys.exit(1)

# Deduplicate and save candidate URLs
unique = []
for url in all_candidates:
    if url not in unique:
        unique.append(url)
with open(pdf_list_path, 'w', encoding='utf-8') as f:
    for u in unique:
        f.write(u + '\n')
print(f"[SAVE] Candidate PDF URLs -> {pdf_list_path}")

# Function to download a PDF until success
def download_pdf(url, out_path):
    print(f"[INFO] Downloading PDF from: {url}")
    try:
        r = requests.get(url, headers=headers, timeout=60)
        print(f"[INFO] Download status: {r.status_code}")
    except Exception as e:
        print(f"[ERROR] Download error: {e}")
        return False
    if r.status_code == 200 and len(r.content) > 10000:
        with open(out_path, 'wb') as f:
            f.write(r.content)
        size = os.path.getsize(out_path)
        print(f"[SUCCESS] Saved PDF -> {out_path} ({size} bytes)")
        return True
    else:
        print(f"[WARN] Bad response or file too small ({len(r.content)} bytes)")
        return False

# Attempt to download the first candidate
if download_pdf(unique[0], pdf_out_path):
    sys.exit(0)
else:
    print(f"[ERROR] Could not download first URL. Check other links in {pdf_list_path}")
    sys.exit(1)
```