### Development Step 41: Download PDF of "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" from Leicester

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic researcher automates batch download of specialized ecology PDFs from multiple university DSpace repositories to build a training dataset for machine learning species distribution models
- Digital humanities scholar retrieves medieval manuscript transcriptions in PDF from national library DSpace servers for high-accuracy OCR and text analysis
- Intellectual property analyst fetches technical patent and invention disclosure PDFs from institutional repositories to cross-check claims during patent examination
- Market intelligence team collects automotive engineering whitepapers in PDF from manufacturer-hosted DSpace archives for competitive benchmarking and trend forecasting
- Compliance officer downloads environmental impact assessment reports in PDF from governmental open-access repositories to verify regulatory adherence across projects
- E-learning platform automatically syncs lecture notes and course handouts in PDF from multiple university DSpace archives to keep online curricula up to date
- Enterprise knowledge management system ingests R&D technical manuals and specification PDFs from corporate DSpace-like storage for full-text search indexing
- Public policy researcher aggregates urban planning and infrastructure study PDFs from city council digital archives to support evidence-based policy recommendations

```
import os
import sys
import requests
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

# --- Suppress SSL warnings and bypass verification ---
import requests.packages.urllib3
requests.packages.urllib3.disable_warnings()

# --- Configuration ---
workspace = 'workspace'
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# Base for Leicester Research Archive (DSpace)
base_url = 'https://lra.le.ac.uk'
# Paper title (using ASCII apostrophe)
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
# Build search URL (DSpace simple-search)
encoded = quote(f'"{title}"')
search_url = f'{base_url}/simple-search?query={encoded}'

# Paths for saving intermediate files
search_html = os.path.join(workspace, 'lra_search.html')
candidates_txt = os.path.join(workspace, 'lra_pdf_urls.txt')
pdf_out = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

print(f"[INFO] Searching DSpace for paper: {title}")
print(f"[INFO] GET {search_url} (verify=False)")
try:
    resp = requests.get(search_url, verify=False, timeout=30)
    print(f"[INFO] Search page response status: {resp.status_code}")
except requests.exceptions.RequestException as e:
    print(f"ERROR: Failed to fetch search page due to: {e}")
    sys.exit(1)

if resp.status_code != 200:
    print(f"ERROR: Unexpected status code {resp.status_code} for search page.")
    sys.exit(1)

# Save search HTML for inspection
with open(search_html, 'w', encoding='utf-8') as f:
    f.write(resp.text)
print(f"[SAVE] Search HTML → {search_html}")

# Parse HTML and extract PDF links
soup = BeautifulSoup(resp.text, 'html.parser')
pdf_links = set()

# Strategy: find anchors that either end in .pdf or whose text mentions PDF or contain 'bitstream'
for a in soup.find_all('a', href=True):
    href = a['href']
    txt = a.get_text(strip=True).lower()
    # detect direct PDF files or DSpace bitstream links
    if href.lower().endswith('.pdf') or ('bitstream' in href.lower() and '.pdf' in href.lower()):
        pdf_links.add(href)
    # also check link text for 'pdf'
    elif 'pdf' in txt:
        pdf_links.add(href)

# Normalize and filter full URLs
full_links = []
for link in pdf_links:
    if link.startswith('http'):
        full = link
    else:
        full = urljoin(base_url, link)
    full_links.append(full)

# Write candidates to file
with open(candidates_txt, 'w', encoding='utf-8') as f:
    for u in full_links:
        f.write(u + '\n')
print(f"[SAVE] Candidate PDF URLs → {candidates_txt}")

# If found, download the first candidate
if full_links:
    first = full_links[0]
    print(f"[INFO] Attempting to download paper PDF from: {first} (verify=False)")
    try:
        pdf_resp = requests.get(first, verify=False, timeout=60)
        print(f"[INFO] PDF download status: {pdf_resp.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"ERROR: PDF download failed due to: {e}")
        sys.exit(1)

    if pdf_resp.status_code == 200 and len(pdf_resp.content) > 10000:
        with open(pdf_out, 'wb') as f:
            f.write(pdf_resp.content)
        size = os.path.getsize(pdf_out)
        print(f"[SUCCESS] Saved PDF → {pdf_out} ({size} bytes)")
    else:
        print("ERROR: Download failed or file too small. Check link and try again.")
        print(f"Content length: {len(pdf_resp.content)} bytes")
else:
    print("WARN: No PDF links found on the repository search page.")
    print("Please inspect 'lra_search.html' and adjust the selection logic accordingly.")
```