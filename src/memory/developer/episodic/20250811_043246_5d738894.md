### Development Step 3: Title:  
Extract and Analyze Unique Food Items and Duplicates from Excel Food Inventory Spreadsheet

**Description**: Parse and analyze the attached Excel file data/gaia/2023/validation/9b54f9d9-35ee-4a14-b62f-d130ea00317f/food_duplicates.xls to extract all food items listed in the spreadsheet. Identify the complete inventory of food names and examine for any duplicates or variations of the same food item appearing under different names. Create a comprehensive list of all unique food items and identify which single food item appears only once without any duplicate entries or alternative naming variations.

**Use Cases**:
- Restaurant chain menu standardization by detecting duplicate or variant food names across multiple branch inventory spreadsheets
- Food safety compliance audits to ensure all unique food items are accounted for and no mislabeled duplicates exist in supplier records
- Nutrition database curation to identify and merge semantically equivalent food entries, improving accuracy for diet tracking apps
- Automated cleaning of grocery store product lists to eliminate redundancies and unify naming conventions for inventory systems
- Academic research in food science to analyze survey data for unique dietary items and avoid double-counting due to naming variations
- Food delivery platform onboarding to reconcile restaurant menu uploads and prevent duplicate listings of the same dish under different names
- Supply chain optimization for catering companies by consolidating ingredient lists and identifying unique procurement needs
- Quality assurance in recipe publishing platforms to detect and flag duplicate or synonym food entries submitted by different users

```
import os
import json
import pandas as pd
from collections import Counter

# First, let's inspect the workspace files to understand the current analysis
print("=== INSPECTING WORKSPACE FILES ===")

# Check what files exist in workspace
workspace_files = []
if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    print(f"Files in workspace: {workspace_files}")
else:
    print("No workspace directory found")

# Inspect the semantic analysis JSON file if it exists
semantic_file = 'workspace/semantic_food_analysis.json'
if os.path.exists(semantic_file):
    print(f"\n=== INSPECTING {semantic_file} ===")
    with open(semantic_file, 'r') as f:
        semantic_data = json.load(f)
    
    print("Keys in semantic analysis:")
    for key in semantic_data.keys():
        print(f"  - {key}")
    
    print(f"\nNumber of true unique items found: {len(semantic_data.get('true_unique_items', []))}")
    print("True unique items:")
    for item in semantic_data.get('true_unique_items', [])[:10]:  # Show first 10
        print(f"  - {item}")
    if len(semantic_data.get('true_unique_items', [])) > 10:
        print(f"  ... and {len(semantic_data.get('true_unique_items', [])) - 10} more")
    
    print(f"\nSemantic duplicates found: {len(semantic_data.get('semantic_duplicates', {}))}")
    print("Sample semantic duplicates:")
    for main_term, variants in list(semantic_data.get('semantic_duplicates', {}).items())[:5]:
        print(f"  {main_term}: {variants}")
else:
    print(f"File {semantic_file} not found")

# Now let's reload the original data and expand our synonym detection
print("\n" + "="*60)
print("EXPANDING SEMANTIC DUPLICATE DETECTION")
print("="*60)

# Load the Excel file
xls_path = "data/gaia/2023/validation/9b54f9d9-35ee-4a14-b62f-d130ea00317f/food_duplicates.xls"
df = pd.read_excel(xls_path)

# Extract all food items (including column headers)
all_food_items = []
for col in df.columns:
    food_values = [col] + df[col].dropna().tolist()
    for item in food_values:
        item_str = str(item).strip().lower()
        if item_str:
            all_food_items.append(item_str)

print(f"Total food items: {len(all_food_items)}")
print(f"Unique food items: {len(set(all_food_items))}")

# Expanded synonym dictionary with more comprehensive food relationships
expanded_food_synonyms = {
    # Vegetables
    'zucchini': ['courgette'],
    'eggplant': ['aubergine'],
    'bell pepper': ['capsicum'],
    'beet': ['beetroot'],
    'cilantro': ['coriander'],
    'arugula': ['rocket'],
    'broccoli rabe': ['rapini'],
    
    # Legumes
    'chickpea': ['garbanzo bean'],
    
    # Fruits
    'avocado': ['alligator pear'],
    
    # Meat and Poultry
    'beef': ['cow meat'],
    'veal': ['calf meat'],
    'pigeon': ['squab'],
    'foie gras': ['fatty goose liver'],
    
    # Seafood and Shellfish
    'crawdad': ['mudbug'],
    'clam': ['geoduck'],  # Both are types of clams
    
    # Grains/Cereals
    'rice krispies': ['puffed rice'],
    'congee': ['rice porridge'],
    
    # Sweets/Desserts
    'cotton candy': ['candy floss'],
    'confectioner\'s sugar': ['icing sugar'],
    'jelly donut': ['jam doughnut'],
    'cupcake': ['fairy cake'],
    'candy': ['bonbon'],
    
    # Beverages
    'soda': ['pop'],
    'coffee': ['java'],
    
    # Dried fruits
    'dried cranberries': ['craisins'],
    'golden raisin': ['sultana'],
    
    # Sandwiches and Bread
    'sandwich': ['hoagie'],
    
    # Potatoes
    'fries': ['chips'],
    
    # Pasta
    'angel hair pasta': ['capellini'],
    
    # Dairy and Protein
    'soy curds': ['tofu'],
    
    # Seasonings
    'fleur de sel': ['salt'],
    
    # Additional potential relationships
    'hot wings': ['buffalo wings'],  # Both are chicken wings
    'mac and cheese': ['kraft dinner'],  # Both are boxed mac and cheese
    'pancake': ['flapjack'],  # Both are flat cakes
    'rasher': ['bacon strip'],  # Both are bacon
    'hand pies': ['pasties'],  # Both are small filled pastries
    'deer meat': ['venison'],  # Same meat
    'goat meat': ['mutton'],  # Wait, mutton is sheep, not goat - remove this
    'stomach': ['tripe'],  # Both are organ meat from stomach
    'sweetbread': ['calf thymus'],  # Sweetbread includes thymus
    'cottage cheese': ['farmer\'s cheese'],  # Similar fresh cheeses
    'relish': ['pickle'],  # Both are pickled vegetables
    'peas': ['sugar snaps'],  # Sugar snap peas are a type of pea
    'squash': ['pumpkin'],  # Pumpkin is a type of squash
    'nectar': ['agave'],  # Agave nectar
    'turtle soup': ['boba'],  # This doesn't make sense - remove
    'shish kebab': ['skewer'],  # Kebab is food on a skewer
    'granola': ['oat cereal'],  # Both are oat-based cereals
    'lizardfish': ['bombay duck'],  # Bombay duck is actually a type of lizardfish
}

# Remove incorrect relationships
if 'goat meat' in expanded_food_synonyms:
    del expanded_food_synonyms['goat meat']  # mutton is sheep, not goat
if 'turtle soup' in expanded_food_synonyms:
    del expanded_food_synonyms['turtle soup']  # boba is not turtle soup

print(f"\nExpanded synonym groups: {len(expanded_food_synonyms)}")

# Create reverse mapping
synonym_groups = {}
for main_term, synonyms in expanded_food_synonyms.items():
    group = [main_term] + synonyms
    for term in group:
        synonym_groups[term] = tuple(sorted(group))

# Group items by semantic equivalence
semantic_groups = {}
ungrouped_items = []

for item in all_food_items:
    if item in synonym_groups:
        group_key = synonym_groups[item]
        if group_key not in semantic_groups:
            semantic_groups[group_key] = []
        semantic_groups[group_key].append(item)
    else:
        ungrouped_items.append(item)

print(f"\nSemantic groups found: {len(semantic_groups)}")
print(f"Ungrouped items: {len(ungrouped_items)}")

# Analyze groups
groups_with_duplicates = []
groups_without_duplicates = []

for group_key, items in semantic_groups.items():
    unique_items = list(set(items))
    if len(unique_items) > 1:
        groups_with_duplicates.append((group_key, unique_items))
    else:
        groups_without_duplicates.append((group_key, unique_items))

# Count ungrouped items
ungrouped_counter = Counter(ungrouped_items)
ungrouped_appearing_once = [item for item, count in ungrouped_counter.items() if count == 1]

# Find truly unique items
true_unique_items = ungrouped_appearing_once + [items[0] for _, items in groups_without_duplicates]

print(f"\n=== EXPANDED ANALYSIS RESULTS ===")
print(f"Groups with semantic duplicates: {len(groups_with_duplicates)}")
print(f"Items appearing only once: {len(true_unique_items)}")

print(f"\nItems that appear only once:")
for item in sorted(true_unique_items):
    print(f"  - {item}")

print(f"\nSemantic duplicate groups found:")
for group_key, unique_items in groups_with_duplicates:
    print(f"  {list(group_key)}: {unique_items}")

# Save expanded analysis
expanded_analysis = {
    'total_items': len(all_food_items),
    'unique_items_count': len(true_unique_items),
    'true_unique_items': sorted(true_unique_items),
    'semantic_duplicates': {group_key[0]: unique_items for group_key, unique_items in groups_with_duplicates},
    'expanded_synonyms_used': len(expanded_food_synonyms)
}

os.makedirs('workspace', exist_ok=True)
with open('workspace/expanded_food_analysis.json', 'w') as f:
    json.dump(expanded_analysis, f, indent=2)

print(f"\nExpanded analysis saved to: workspace/expanded_food_analysis.json")

if len(true_unique_items) == 1:
    print(f"\n*** ANSWER: The single food item that appears only once is: '{true_unique_items[0]}' ***")
else:
    print(f"\n*** Still found {len(true_unique_items)} items appearing only once. Need further analysis. ***")
```