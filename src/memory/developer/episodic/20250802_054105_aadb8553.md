### Development Step 27: Search and Download Leicester University Dragon Fish Diet Paper as PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragonâ€™s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated compliance monitoring by downloading the latest regulatory guideline PDFs (e.g., Basel III standards) from government websites into a risk management workflow
- Financial analysis pipeline fetching quarterly and annual SEC filings (10-K/10-Q) in PDF format from sec.gov for automated ratio calculation and trend modeling
- Academic library curation retrieving open-access theses and dissertations PDFs from university repositories (e.g., le.ac.uk) for institutional archiving
- Data science research gathering machine learning conference papers in PDF from ACM and IEEE digital libraries to build a custom NLP training corpus
- Patent prior-art review automating bulk download of patent specification PDFs from USPTO/EPO sites by patent number list for legal analysis
- Investigative journalism workflow scraping corporate press release PDFs from investor relations portals to validate public statements
- Healthcare meta-analysis collecting clinical guideline and systematic review PDFs from WHO or PubMed Central for evidence synthesis
- Real estate due diligence tool pulling municipal zoning map and building code PDFs from city planning websites to assess property development constraints

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# --- Configuration ---
WORKSPACE = 'workspace'
PDF_DEST = os.path.join(WORKSPACE, 'dragon_diet_fish_bag.pdf')
TITLE = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
QUERY = f'"{TITLE}" filetype:pdf'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept': 'text/html'
}

# --- Helpers ---
def ensure_workspace():
    if not os.path.isdir(WORKSPACE):
        print(f"ERROR: Workspace directory '{WORKSPACE}' not found.")
        sys.exit(1)
    print(f"[INFO] Workspace: {WORKSPACE}")


def save_html(name, html):
    path = os.path.join(WORKSPACE, name)
    with open(path, 'w', encoding='utf-8') as f:
        f.write(html)
    print(f"[SAVE] HTML saved to {path}")
    return path


def extract_pdf_links(html):
    soup = BeautifulSoup(html, 'html.parser')
    urls = set()
    for a in soup.find_all('a', href=True):
        href = a['href']
        # normalize relative URLs
        if href.lower().endswith('.pdf') or '.pdf?' in href.lower():
            urls.add(href)
    print(f"[PARSE] Found {len(urls)} PDF links via <a> tags.")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    return list(urls)


def download_pdf(url, dest):
    print(f"[DOWNLOAD] Downloading PDF from: {url}")
    r = requests.get(url, headers=HEADERS, stream=True, timeout=60)
    r.raise_for_status()
    with open(dest, 'wb') as f:
        for chunk in r.iter_content(8192):
            if chunk:
                f.write(chunk)
    size = os.path.getsize(dest)
    print(f"[SUCCESS] Saved PDF ({size} bytes) to {dest}")
    return size

# --- Main Flow ---
if __name__ == '__main__':
    ensure_workspace()

    # 1) Bing Search
    print(f"[SEARCH] Bing for: {QUERY}")
    bing_resp = requests.get('https://www.bing.com/search', params={'q': QUERY}, headers=HEADERS, timeout=30)
    bing_resp.raise_for_status()
    html_bing = bing_resp.text
    save_html('bing_search.html', html_bing)
    pdf_links = extract_pdf_links(html_bing)

    # 2) DuckDuckGo if needed
    if not pdf_links:
        print("[INFO] No PDFs from Bing, falling back to DuckDuckGo...")
        ddg_resp = requests.post('https://duckduckgo.com/html/', data={'q': QUERY}, headers=HEADERS, timeout=30)
        ddg_resp.raise_for_status()
        html_ddg = ddg_resp.text
        save_html('ddg_search.html', html_ddg)
        pdf_links = extract_pdf_links(html_ddg)

    if not pdf_links:
        print("ERROR: No PDF URLs found on either search. Cannot proceed.")
        sys.exit(1)

    # 3) Filter candidates by university domain hints
    keywords = ['le.ac.uk', 'lra.le.ac.uk', 'core.ac.uk', 'hdl.handle.net']
    filtered = [u for u in pdf_links if any(k in u.lower() for k in keywords)]
    chosen = filtered[0] if filtered else pdf_links[0]
    print(f"[SELECT] Chosen PDF URL: {chosen}\n")

    # 4) Download the selected PDF
    try:
        size = download_pdf(chosen, PDF_DEST)
        if size < 10 * 1024:
            print("WARNING: Downloaded PDF is smaller than 10KB. It may not be correct.")
            sys.exit(1)
        print("[COMPLETE] Paper downloaded successfully.")
        sys.exit(0)
    except Exception as e:
        print(f"ERROR: Download failed: {e}")
        if os.path.exists(PDF_DEST):
            os.remove(PDF_DEST)
        sys.exit(1)

```