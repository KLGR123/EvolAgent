### Development Step 2: Surrey Lodge History: Victorian Beginnings, Temperance Hall, 1935 Rotating Stage, WWII Bombing, Burnley Relocation

**Description**: Conduct a comprehensive web search to identify Surrey Lodge in South London and trace its historical development during the Victorian era. Focus on finding information about: (1) the original organization that developed Surrey Lodge, (2) its transformation into a temperance hall, (3) the installation of the UK's first rotating stage in 1935, (4) World War II bomb damage, and (5) the company's relocation to Burnley. Use search terms like 'Surrey Lodge South London Victorian era temperance hall', 'first rotating stage UK 1935 temperance hall', 'Surrey Lodge bomb damage World War II Burnley', and 'Victorian temperance organization Surrey Lodge founder'.

**Use Cases**:
- Municipal heritage teams automating extraction of Victorian-era building histories from archived city council websites for preservation grant applications
- Academic history researchers compiling precise timelines of 19th-century social movement venues by analyzing saved HTML search results from digital archives
- Theatre technology historians identifying early rotating stage installations by scanning periodicals and hall websites for 1930s date references
- Insurance risk assessors evaluating wartime bomb damage liabilities by mining online council bomb-damage records and compiled archive pages
- Corporate due diligence analysts tracking legacy company relocations by parsing historical press releases and relocation notices from archived web pages
- Digital librarians structuring metadata on temperance movement meeting halls by extracting organizational founding details from library web archives
- Real estate developers reviewing heritage constraints by retrieving structural transformation records and wartime impact data from municipal HTML archives
- Cultural tourism planners building interactive timelines of historical London landmarks by consolidating extracted event dates and site transformations from saved HTML

```
import os
import json
from bs4 import BeautifulSoup
import re
from collections import Counter

print('=== SURREY LODGE HISTORICAL ANALYSIS - FIXING SEARCH RESULTS PROCESSING ===')
print('Objective: Analyze the successfully downloaded HTML files for Surrey Lodge historical information')
print('Previous issue: Variable scope error prevented analysis - now processing saved HTML files\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# First, let's check what files we have from the previous search
print('=== PHASE 1: EXAMINING AVAILABLE SEARCH RESULT FILES ===')
print('=' * 70)

html_files = [f for f in os.listdir('workspace') if f.startswith('surrey_lodge_search_') and f.endswith('.html')]
print(f'Found {len(html_files)} Surrey Lodge search result HTML files:')

for i, filename in enumerate(html_files, 1):
    filepath = os.path.join('workspace', filename)
    file_size = os.path.getsize(filepath)
    print(f'  {i}. {filename} ({file_size:,} bytes)')

if not html_files:
    print('‚ùå No Surrey Lodge HTML files found - need to re-run searches')
    exit()

# Initialize corrected analysis results
analysis_results = {
    'timestamp': '2024-12-19',
    'objective': 'Analyze Surrey Lodge South London historical development from saved HTML search results',
    'focus_areas': [
        'Original organization that developed Surrey Lodge',
        'Transformation into temperance hall', 
        'Installation of UK first rotating stage in 1935',
        'World War II bomb damage',
        'Company relocation to Burnley'
    ],
    'html_files_analyzed': [],
    'historical_findings': [],
    'timeline_information': [],
    'evidence_summary': {}
}

print('\n=== PHASE 2: SYSTEMATIC HTML CONTENT ANALYSIS ===')
print('=' * 80)

# Define key terms with weights for relevance scoring
key_terms = {
    'surrey lodge': 10,
    'south london': 8,
    'temperance': 8,
    'temperance hall': 10,
    'victorian': 6,
    'rotating stage': 10,
    '1935': 8,
    'first rotating stage': 12,
    'bomb damage': 8,
    'world war': 6,
    'wwii': 6,
    'burnley': 8,
    'relocation': 6,
    'moved to burnley': 10,
    'theatre': 5,
    'organization': 5,
    'founder': 6,
    'developed': 4,
    'transformation': 5,
    'installation': 5,
    'company': 4
}

# Process each HTML file
for i, html_file in enumerate(html_files, 1):
    print(f'\n--- Analyzing File {i}/{len(html_files)}: {html_file} ---')
    
    try:
        filepath = os.path.join('workspace', html_file)
        
        # Read HTML content
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f'File size: {len(html_content):,} characters')
        
        # Parse HTML content - FIXED: Proper variable scope
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract text content - FIXED: Ensure page_text is properly defined
        page_text = soup.get_text().lower()
        print(f'Extracted text: {len(page_text):,} characters')
        
        # Calculate relevance score
        found_terms = []
        relevance_score = 0
        
        for term, weight in key_terms.items():
            if term in page_text:
                found_terms.append(term)
                relevance_score += weight
        
        print(f'Relevance score: {relevance_score}')
        print(f'Found terms ({len(found_terms)}): {', '.join(found_terms[:8])}')
        
        # Extract specific historical indicators
        historical_indicators = []
        
        # Look for Surrey Lodge mentions
        if 'surrey lodge' in page_text:
            historical_indicators.append('Surrey Lodge confirmed')
            print('üéØ SURREY LODGE CONFIRMED!')
        
        # Look for temperance hall references
        if 'temperance hall' in page_text or ('temperance' in page_text and 'hall' in page_text):
            historical_indicators.append('Temperance hall connection found')
            print('üèõÔ∏è TEMPERANCE HALL CONNECTION FOUND!')
        
        # Look for rotating stage references
        if 'rotating stage' in page_text:
            historical_indicators.append('Rotating stage reference found')
            print('üé≠ ROTATING STAGE REFERENCE FOUND!')
            
            if '1935' in page_text:
                historical_indicators.append('1935 rotating stage date confirmed')
                print('üìÖ 1935 DATE CONFIRMED!')
        
        # Look for WWII/bomb damage
        if any(term in page_text for term in ['bomb damage', 'bombed', 'war damage', 'blitz']):
            historical_indicators.append('WWII damage reference found')
            print('üí• WWII DAMAGE REFERENCE FOUND!')
        
        # Look for Burnley connection
        if 'burnley' in page_text:
            historical_indicators.append('Burnley connection found')
            print('üè≠ BURNLEY CONNECTION FOUND!')
        
        # Extract relevant text snippets for context
        text_snippets = []
        sentences = page_text.split('.')
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20 and len(sentence) < 400:
                # Look for sentences containing key terms
                if 'surrey lodge' in sentence:
                    text_snippets.append(sentence)
                elif any(term in sentence for term in ['temperance hall', 'rotating stage', 'burnley', '1935']):
                    text_snippets.append(sentence)
        
        # Remove duplicates and limit
        unique_snippets = list(set(text_snippets))[:5]
        
        if unique_snippets:
            print(f'Key text snippets found ({len(unique_snippets)}):') 
            for j, snippet in enumerate(unique_snippets[:3], 1):
                print(f'  {j}. {snippet[:200]}...')
        
        # Look for dates and timeline information
        timeline_info = []
        date_patterns = ['1935', '193[0-9]', '19[0-4][0-9]', 'victorian', 'world war', 'wwii']
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 15 and len(sentence) < 300:
                # Check if sentence contains date-related information
                if any(pattern.replace('[0-9]', '') in sentence for pattern in date_patterns):
                    timeline_info.append(sentence)
        
        # Remove duplicates from timeline info
        unique_timeline = list(set(timeline_info))[:3]
        
        # Store analysis results
        file_analysis = {
            'filename': html_file,
            'file_size': len(html_content),
            'text_length': len(page_text),
            'relevance_score': relevance_score,
            'found_terms': found_terms,
            'historical_indicators': historical_indicators,
            'text_snippets': unique_snippets,
            'timeline_information': unique_timeline
        }
        
        analysis_results['html_files_analyzed'].append(file_analysis)
        analysis_results['historical_findings'].extend(historical_indicators)
        analysis_results['timeline_information'].extend(unique_timeline)
        
        # Show high-relevance findings
        if relevance_score >= 15 or len(historical_indicators) >= 2:
            print(f'üìö HIGH RELEVANCE FINDING - Score: {relevance_score}, Indicators: {len(historical_indicators)}')
        elif relevance_score > 0:
            print(f'üìù Moderate relevance - Score: {relevance_score}')
        else:
            print('‚ùå Low relevance - no key terms found')
    
    except Exception as e:
        print(f'Error processing {html_file}: {str(e)}')
        # Continue with next file instead of stopping
        continue

print('\n=== PHASE 3: COMPREHENSIVE EVIDENCE ANALYSIS ===')
print('=' * 80)

total_files = len(analysis_results['html_files_analyzed'])
print(f'Successfully analyzed files: {total_files}')

if total_files > 0:
    # Sort files by relevance score
    analysis_results['html_files_analyzed'].sort(key=lambda x: x['relevance_score'], reverse=True)
    
    print('\nüìä RELEVANCE RANKING:')
    print('-' * 40)
    
    high_relevance = [f for f in analysis_results['html_files_analyzed'] if f['relevance_score'] >= 15]
    moderate_relevance = [f for f in analysis_results['html_files_analyzed'] if 5 <= f['relevance_score'] < 15]
    low_relevance = [f for f in analysis_results['html_files_analyzed'] if f['relevance_score'] < 5]
    
    print(f'High relevance files (15+ points): {len(high_relevance)}')
    print(f'Moderate relevance files (5-14 points): {len(moderate_relevance)}')
    print(f'Low relevance files (0-4 points): {len(low_relevance)}')
    
    if high_relevance:
        print('\nüéØ TOP HIGH-RELEVANCE FILES:')
        for i, file_data in enumerate(high_relevance[:3], 1):
            print(f'\n{i}. {file_data["filename"]}:')
            print(f'   Score: {file_data["relevance_score"]}')
            print(f'   Terms: {', '.join(file_data["found_terms"][:6])}')
            print(f'   Indicators: {', '.join(file_data["historical_indicators"])}')
            if file_data.get('text_snippets'):
                print(f'   Sample text: {file_data["text_snippets"][0][:150]}...')
    
    # Analyze all historical indicators
    all_indicators = analysis_results['historical_findings']
    indicator_counts = Counter(all_indicators)
    
    print('\nüìà HISTORICAL EVIDENCE FREQUENCY:')
    print('-' * 45)
    for indicator, count in indicator_counts.most_common():
        print(f'  ‚Ä¢ {indicator}: {count} files')
    
    # Evidence analysis for each focus area
    evidence_analysis = {
        'surrey_lodge_identified': 'Surrey Lodge confirmed' in all_indicators,
        'temperance_hall_connection': 'Temperance hall connection found' in all_indicators,
        'rotating_stage_1935': 'Rotating stage reference found' in all_indicators and '1935 rotating stage date confirmed' in all_indicators,
        'wwii_damage_confirmed': 'WWII damage reference found' in all_indicators,
        'burnley_relocation': 'Burnley connection found' in all_indicators
    }
    
    print('\nüîç FOCUS AREA EVIDENCE ANALYSIS:')
    print('-' * 45)
    for area, confirmed in evidence_analysis.items():
        status = '‚úÖ' if confirmed else '‚ùå'
        print(f'{status} {area.replace("_", " ").title()}: {confirmed}')
    
    analysis_results['evidence_summary'] = evidence_analysis
    
    # Calculate research success rate
    confirmed_count = sum(evidence_analysis.values())
    success_percentage = (confirmed_count / len(evidence_analysis)) * 100
    
    print(f'\nüìà RESEARCH SUCCESS RATE: {success_percentage:.1f}% ({confirmed_count}/{len(evidence_analysis)} areas confirmed)')
    analysis_results['success_percentage'] = success_percentage
    
else:
    print('‚ùå No files successfully analyzed')

print('\n=== PHASE 4: TIMELINE COMPILATION ===')
print('=' * 60)

# Compile and deduplicate timeline information
all_timeline = analysis_results['timeline_information']
unique_timeline = list(set(all_timeline))

print(f'Timeline references collected: {len(unique_timeline)}')

if unique_timeline:
    print('\nüìÖ HISTORICAL TIMELINE INFORMATION:')
    print('-' * 50)
    
    # Sort timeline info by relevance (contains dates or key terms)
    timeline_with_scores = []
    for info in unique_timeline:
        score = 0
        if '1935' in info: score += 10
        if 'victorian' in info: score += 8
        if any(term in info for term in ['surrey lodge', 'temperance', 'rotating stage']): score += 5
        if any(term in info for term in ['bomb', 'war', 'burnley']): score += 5
        timeline_with_scores.append((info, score))
    
    timeline_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    for i, (info, score) in enumerate(timeline_with_scores[:8], 1):
        print(f'\n{i}. (Score: {score}) {info[:300]}...')
    
    analysis_results['timeline_information'] = [info for info, score in timeline_with_scores[:10]]
else:
    print('‚ùå No timeline information found')

# Save corrected analysis results
results_file = os.path.join('workspace', 'surrey_lodge_corrected_analysis_results.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)

print(f'\nüíæ CORRECTED ANALYSIS RESULTS SAVED TO: {results_file}')

# Create detailed findings report
report_content = f'''SURREY LODGE SOUTH LONDON HISTORICAL RESEARCH
CORRECTED ANALYSIS REPORT
Generated: {analysis_results["timestamp"]}

RESEARCH OBJECTIVE:
Trace Surrey Lodge historical development during Victorian era focusing on:
'''

for area in analysis_results['focus_areas']:
    report_content += f'‚Ä¢ {area}\n'

report_content += f'''
ANALYSIS EXECUTION:
‚Ä¢ HTML files analyzed: {len(analysis_results["html_files_analyzed"])}
‚Ä¢ Historical indicators found: {len(set(analysis_results["historical_findings"]))}
‚Ä¢ Timeline references collected: {len(analysis_results["timeline_information"])}
'''

if analysis_results.get('success_percentage'):
    report_content += f'‚Ä¢ Research success rate: {analysis_results["success_percentage"]:.1f}%\n'

report_content += f'''
EVIDENCE ANALYSIS RESULTS:
'''

if analysis_results.get('evidence_summary'):
    for area, confirmed in analysis_results['evidence_summary'].items():
        status = 'CONFIRMED' if confirmed else 'NOT FOUND'
        report_content += f'‚Ä¢ {area.replace("_", " ").title()}: {status}\n'

report_content += f'''
HIGH-RELEVANCE FINDINGS:
'''

high_relevance_files = [f for f in analysis_results['html_files_analyzed'] if f['relevance_score'] >= 15]
for i, file_data in enumerate(high_relevance_files[:3], 1):
    report_content += f'''{i}. {file_data["filename"]} (Score: {file_data["relevance_score"]})
   Terms found: {', '.join(file_data["found_terms"][:8])}
   Historical indicators: {', '.join(file_data["historical_indicators"])}

'''

report_content += f'''
TIMELINE INFORMATION:
'''

for i, info in enumerate(analysis_results['timeline_information'][:5], 1):
    report_content += f'{i}. {info[:250]}...\n\n'

report_content += f'''
FILES GENERATED:
‚Ä¢ surrey_lodge_corrected_analysis_results.json - Complete analysis data
‚Ä¢ surrey_lodge_detailed_findings_report.txt - This detailed report
‚Ä¢ Original HTML search result files available for further analysis

CONCLUSIONS:
'''

if analysis_results.get('success_percentage', 0) >= 60:
    report_content += 'Research successful - multiple historical elements confirmed\n'
elif analysis_results.get('success_percentage', 0) >= 30:
    report_content += 'Partial success - some historical elements identified\n'
else:
    report_content += 'Limited success - further research needed with alternative approaches\n'

report_content += f'''
RECOMMENDATIONS:
1. Review high-relevance HTML files for detailed information extraction
2. Cross-reference findings with historical archives and databases
3. Conduct targeted searches for specific organizational names
4. Research Victorian temperance movement records
5. Investigate theatre history archives for rotating stage information
'''

# Save detailed report
report_file = os.path.join('workspace', 'surrey_lodge_detailed_findings_report.txt')
with open(report_file, 'w', encoding='utf-8') as f:
    f.write(report_content)

print(f'üìã DETAILED FINDINGS REPORT SAVED TO: {report_file}')

print('\n' + '=' * 90)
print('SURREY LODGE HISTORICAL ANALYSIS - CORRECTED PROCESSING COMPLETE')
print('=' * 90)

if analysis_results.get('success_percentage', 0) >= 50:
    print('‚úÖ ANALYSIS SUCCESSFUL - Historical elements identified')
elif analysis_results.get('success_percentage', 0) >= 20:
    print('‚ö†Ô∏è PARTIAL SUCCESS - Some elements found, further investigation needed')
else:
    print('‚ùå LIMITED SUCCESS - Alternative research approaches recommended')

print('\nüìä FINAL ANALYSIS STATISTICS:')
print(f'   ‚Ä¢ HTML files successfully processed: {len(analysis_results["html_files_analyzed"])}')
print(f'   ‚Ä¢ Files with high relevance (15+): {len([f for f in analysis_results["html_files_analyzed"] if f["relevance_score"] >= 15])}')
print(f'   ‚Ä¢ Files with moderate relevance (5-14): {len([f for f in analysis_results["html_files_analyzed"] if 5 <= f["relevance_score"] < 15])}')
print(f'   ‚Ä¢ Unique historical indicators: {len(set(analysis_results["historical_findings"]))}')
print(f'   ‚Ä¢ Timeline references collected: {len(analysis_results["timeline_information"])}')

if analysis_results.get('evidence_summary'):
    confirmed_areas = [area.replace('_', ' ').title() for area, confirmed in analysis_results['evidence_summary'].items() if confirmed]
    if confirmed_areas:
        print('\nüéØ CONFIRMED HISTORICAL ELEMENTS:')
        for area in confirmed_areas:
            print(f'   ‚úÖ {area}')

print('\nüìã NEXT STEPS: Review detailed findings report and high-relevance HTML files for specific historical details')
print('=== SURREY LODGE CORRECTED HISTORICAL ANALYSIS COMPLETE ===')
```