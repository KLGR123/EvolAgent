### Development Step 5: Identifying the Federal Rules of Evidence Article with Most 'Witnesses' in Rule Titles and Analyzing Amendments

**Description**: Access the Federal Rules of Evidence section on Cornell Law School's Legal Information Institute website and systematically analyze all rules to identify which article has 'witnesses' appearing in the most rule titles as of 2021. Count the occurrences of 'witnesses' in titles across all Federal Rules of Evidence articles, then identify the article with the highest count for further analysis of its first rule's amendment history.

**Use Cases**:
- Legal research automation for law firms seeking to quickly identify which sections of the Federal Rules of Evidence most frequently address witnesses, streamlining case preparation and citation.
- Academic analysis by legal scholars studying the evolution and emphasis on witness-related rules across different articles for publication or curriculum development.
- Compliance auditing for court administrators verifying that local procedures align with the most witness-focused articles of the Federal Rules of Evidence.
- Development of legal tech tools that provide targeted guidance or alerts to attorneys when drafting motions or preparing for trial, based on the prominence of witness rules in specific articles.
- Judicial training program content creation, focusing on the articles with the highest concentration of witness-related rules to enhance judge and clerk education.
- Automated updating of legal knowledge bases for in-house counsel teams, ensuring that internal resources highlight the most relevant and frequently amended witness rules.

```
import os
import json
from datetime import datetime
import re
from collections import defaultdict

print('=== PARSING FEDERAL RULES OF EVIDENCE FOR WITNESSES ANALYSIS ===')
print('Objective: Parse saved content to identify articles and count "witnesses" in rule titles')
print('Goal: Find which article has the most "witnesses" occurrences in rule titles\n')

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

print('=== STEP 1: INSPECTING SAVED ANALYSIS FILES ===')

# First, let's inspect the structure of saved files
analysis_file = None
content_file = None
html_file = None

# Find the saved files
for file in os.listdir(workspace_dir):
    if 'fre_initial_structure_analysis.json' in file:
        analysis_file = os.path.join(workspace_dir, file)
    elif 'fre_main_content.txt' in file:
        content_file = os.path.join(workspace_dir, file)
    elif 'federal_rules_evidence_page.html' in file:
        html_file = os.path.join(workspace_dir, file)

print(f'Analysis file: {analysis_file}')
print(f'Content file: {content_file}')
print(f'HTML file: {html_file}')

# Inspect the analysis file structure first
if analysis_file and os.path.exists(analysis_file):
    print('\nInspecting analysis file structure...')
    
    with open(analysis_file, 'r') as f:
        analysis_data = json.load(f)
    
    print('Analysis file keys:')
    for key, value in analysis_data.items():
        if isinstance(value, list):
            print(f'  {key}: List with {len(value)} items')
        elif isinstance(value, dict):
            print(f'  {key}: Dictionary with {len(value)} keys')
        else:
            print(f'  {key}: {value}')
    
    # Examine the rule links sample to understand structure
    if 'rule_links_sample' in analysis_data:
        rule_links = analysis_data['rule_links_sample']
        print(f'\nRule links sample ({len(rule_links)} items):')
        for i, link in enumerate(rule_links[:10], 1):
            print(f'  {i}. "{link.get("text", "")}" -> {link.get("href", "")}')

print('\n=== STEP 2: PARSING MAIN CONTENT FOR ARTICLE STRUCTURE ===')

# Read and analyze the main content
if content_file and os.path.exists(content_file):
    print(f'Reading main content file: {content_file}')
    
    with open(content_file, 'r', encoding='utf-8') as f:
        content_text = f.read()
    
    print(f'Content length: {len(content_text):,} characters')
    
    # Look for article structure in the content
    print('\nSearching for article structure...')
    
    # Strategy 1: Look for "Article" patterns
    article_patterns = re.findall(r'Article\s+([IVXLC]+|\d+)\s*[:-]?\s*([^\n\r]{1,100})', content_text, re.IGNORECASE)
    print(f'Found {len(article_patterns)} "Article" patterns:')
    for i, (num, title) in enumerate(article_patterns[:10], 1):
        print(f'  {i}. Article {num}: {title.strip()}')
    
    # Strategy 2: Look for rule numbering patterns to infer articles
    print('\nAnalyzing rule numbering patterns...')
    
    # Extract all rule numbers and their contexts
    rule_patterns = re.findall(r'Rule\s+(\d+)\s*[:-]?\s*([^\n\r]{1,200})', content_text, re.IGNORECASE)
    print(f'Found {len(rule_patterns)} rule patterns')
    
    # Group rules by their hundreds digit (which typically indicates article)
    rules_by_article = defaultdict(list)
    
    for rule_num, rule_title in rule_patterns:
        rule_number = int(rule_num)
        article_num = rule_number // 100  # 101-199 = Article 1, 201-299 = Article 2, etc.
        
        rules_by_article[article_num].append({
            'number': rule_number,
            'title': rule_title.strip(),
            'full_text': f'Rule {rule_num}: {rule_title.strip()}'
        })
    
    print(f'\nRules grouped by inferred articles:')
    for article_num in sorted(rules_by_article.keys()):
        rules = rules_by_article[article_num]
        print(f'\nArticle {article_num} (inferred): {len(rules)} rules')
        for rule in rules[:3]:  # Show first 3 rules per article
            print(f'  Rule {rule["number"]}: {rule["title"][:100]}...')
        if len(rules) > 3:
            print(f'  ... and {len(rules) - 3} more rules')

else:
    print('Content file not found, trying to parse HTML directly')

print('\n=== STEP 3: PARSING HTML FOR COMPLETE RULE STRUCTURE ===')

# Parse the HTML file for more comprehensive structure
if html_file and os.path.exists(html_file):
    print(f'Parsing HTML file: {html_file}')
    
    from bs4 import BeautifulSoup
    
    with open(html_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Look for rule links with their full titles
    print('\nExtracting rule links with full titles...')
    
    all_rule_data = []
    rule_links = soup.find_all('a', href=re.compile(r'/rules/fre/rule_\d+'))
    
    print(f'Found {len(rule_links)} rule links in HTML')
    
    for link in rule_links:
        href = link.get('href')
        text = link.get_text().strip()
        
        # Extract rule number from href
        rule_match = re.search(r'rule_(\d+)', href)
        if rule_match:
            rule_number = int(rule_match.group(1))
            article_num = rule_number // 100
            
            # Try to get the full rule title from surrounding context
            parent = link.parent
            if parent:
                parent_text = parent.get_text().strip()
                # Look for title after the rule number
                title_match = re.search(rf'Rule\s+{rule_number}\s*[:-]?\s*([^\n\r]+)', parent_text, re.IGNORECASE)
                if title_match:
                    rule_title = title_match.group(1).strip()
                else:
                    rule_title = text
            else:
                rule_title = text
            
            all_rule_data.append({
                'number': rule_number,
                'article': article_num,
                'title': rule_title,
                'href': href,
                'link_text': text
            })
    
    # Group by article for analysis
    html_rules_by_article = defaultdict(list)
    for rule in all_rule_data:
        html_rules_by_article[rule['article']].append(rule)
    
    print(f'\nRules from HTML grouped by article:')
    for article_num in sorted(html_rules_by_article.keys()):
        rules = html_rules_by_article[article_num]
        print(f'\nArticle {article_num}: {len(rules)} rules')
        for rule in rules[:5]:  # Show first 5 rules per article
            print(f'  Rule {rule["number"]}: {rule["title"]}')
        if len(rules) > 5:
            print(f'  ... and {len(rules) - 5} more rules')

print('\n=== STEP 4: COUNTING "WITNESSES" IN RULE TITLES BY ARTICLE ===')

# Now count "witnesses" occurrences in rule titles
witnesses_count_by_article = defaultdict(int)
witnesses_rules_by_article = defaultdict(list)

# Use the HTML data if available, otherwise use content parsing
if 'html_rules_by_article' in locals():
    rules_data = html_rules_by_article
    data_source = 'HTML parsing'
else:
    rules_data = rules_by_article
    data_source = 'Content parsing'

print(f'Using {data_source} for witnesses analysis')
print(f'Analyzing {sum(len(rules) for rules in rules_data.values())} total rules')

for article_num, rules in rules_data.items():
    article_witnesses_count = 0
    article_witnesses_rules = []
    
    for rule in rules:
        rule_title = rule.get('title', '') if isinstance(rule, dict) else rule.get('title', '')
        
        # Count occurrences of "witnesses" (case-insensitive)
        witnesses_in_title = len(re.findall(r'\bwitnesses?\b', rule_title, re.IGNORECASE))
        
        if witnesses_in_title > 0:
            article_witnesses_count += witnesses_in_title
            article_witnesses_rules.append({
                'rule_number': rule.get('number', 0) if isinstance(rule, dict) else rule.get('number', 0),
                'rule_title': rule_title,
                'witnesses_count': witnesses_in_title
            })
    
    witnesses_count_by_article[article_num] = article_witnesses_count
    witnesses_rules_by_article[article_num] = article_witnesses_rules

print('\n=== WITNESSES COUNT RESULTS BY ARTICLE ===')
print('=' * 60)

total_witnesses_mentions = 0
for article_num in sorted(witnesses_count_by_article.keys()):
    count = witnesses_count_by_article[article_num]
    rules_with_witnesses = witnesses_rules_by_article[article_num]
    
    print(f'\nArticle {article_num}:')
    print(f'  Total "witnesses" mentions in rule titles: {count}')
    print(f'  Rules containing "witnesses": {len(rules_with_witnesses)}')
    
    if rules_with_witnesses:
        print(f'  Rules with "witnesses" in title:')
        for rule in rules_with_witnesses:
            print(f'    Rule {rule["rule_number"]}: {rule["rule_title"]} ({rule["witnesses_count"]} mentions)')
    
    total_witnesses_mentions += count

# Find the article with the most "witnesses" mentions
max_witnesses_article = max(witnesses_count_by_article.items(), key=lambda x: x[1]) if witnesses_count_by_article else (None, 0)

print(f'\n=== FINAL RESULTS ===')
print('=' * 50)
print(f'Total "witnesses" mentions across all rule titles: {total_witnesses_mentions}')
print(f'Articles analyzed: {len(witnesses_count_by_article)}')

if max_witnesses_article[0] is not None:
    article_num, max_count = max_witnesses_article
    print(f'\n*** ARTICLE WITH MOST "WITNESSES" IN RULE TITLES: ***')
    print(f'Article {article_num} with {max_count} total mentions')
    
    # Show the specific rules
    max_article_rules = witnesses_rules_by_article[article_num]
    print(f'\nRules in Article {article_num} containing "witnesses":')
    for rule in max_article_rules:
        print(f'  â€¢ Rule {rule["rule_number"]}: {rule["rule_title"]}')
else:
    print('\nNo "witnesses" mentions found in any rule titles')

# Save comprehensive analysis
final_analysis = {
    'analysis_date': datetime.now().isoformat(),
    'data_source': data_source,
    'total_rules_analyzed': sum(len(rules) for rules in rules_data.values()),
    'total_witnesses_mentions': total_witnesses_mentions,
    'articles_analyzed': len(witnesses_count_by_article),
    'witnesses_count_by_article': dict(witnesses_count_by_article),
    'witnesses_rules_by_article': dict(witnesses_rules_by_article),
    'article_with_most_witnesses': {
        'article_number': max_witnesses_article[0],
        'witnesses_count': max_witnesses_article[1],
        'rules_with_witnesses': witnesses_rules_by_article.get(max_witnesses_article[0], [])
    } if max_witnesses_article[0] is not None else None
}

final_file = os.path.join(workspace_dir, 'fre_witnesses_analysis_final.json')
with open(final_file, 'w') as f:
    json.dump(final_analysis, f, indent=2)

print(f'\nFinal analysis saved to: {final_file}')
print('\n=== FEDERAL RULES OF EVIDENCE WITNESSES ANALYSIS COMPLETE ===')
```