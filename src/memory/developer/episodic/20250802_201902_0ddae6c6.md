### Development Step 4: Search 2011 Bulgarian Census Data on Tertiary Education Completion by Gender

**Description**: Search for the 2011 Bulgarian census data on tertiary education completion by gender. Look for official census results from Bulgaria's National Statistical Institute or other authoritative sources that provide the specific breakdown of men and women who completed tertiary education in 2011.

**Use Cases**:
- Academic research on gender parity in higher education: automating the extraction of Bulgarian 2011 census figures to compare male vs. female tertiary attainment in a peer-reviewed journal article.
- Government policy evaluation for the Ministry of Education: scraping official census data to assess progress in closing the gender gap for university degrees and inform new scholarship programs.
- NGO program design for women’s empowerment: collecting precise statistics on female tertiary graduates in Bulgaria to target vocational training grants in under-served regions.
- Data journalism investigation into Balkan education trends: harvesting census tables and sentences with tertiary education metrics to publish an interactive online story on gender disparities.
- HR workforce planning for a multinational corporation: integrating Bulgarian census counts of university-educated men and women to model local talent availability and diversity hiring targets.
- International development grant proposal: compiling authoritative education statistics by gender from Bulgaria’s 2011 census to justify funding allocations for STEM outreach programs.
- EdTech product localization and marketing: analyzing regional tertiary completion rates from official census tables to tailor online course offerings and advertising campaigns to Bulgarian male and female learners.

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
import re

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== BULGARIAN 2011 CENSUS: COMPREHENSIVE SEARCH FOR TERTIARY EDUCATION DATA ===")
print("Objective: Find official data on men and women who completed tertiary education in 2011")
print("Strategy: Multiple data sources with robust error handling\n")

# Search headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Comprehensive list of potential data sources
search_sources = [
    "https://en.wikipedia.org/wiki/Education_in_Bulgaria",
    "https://en.wikipedia.org/wiki/Demographics_of_Bulgaria", 
    "https://en.wikipedia.org/wiki/Bulgaria",
    "https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Educational_attainment_statistics",
    "https://data.worldbank.org/country/bulgaria"
]

print("Step 1: Systematic data source access and analysis...\n")

successful_sources = []
failed_sources = []

for source_index, url in enumerate(search_sources, 1):
    print(f"Source {source_index}/{len(search_sources)}: {url}")
    
    try:
        # Make request with timeout
        response = requests.get(url, headers=headers, timeout=15)
        print(f"  HTTP Status: {response.status_code}")
        
        if response.status_code == 200:
            print(f"  ✓ Successfully accessed")
            
            # Create safe filename
            filename = f"source_{source_index}_" + url.replace('https://', '').replace('http://', '').replace('/', '_').replace('.', '_').replace('?', '_').replace('=', '_')[:100] + '.html'
            filepath = f'workspace/{filename}'
            
            # Save content
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"  Content saved to: {filename}")
            
            # Parse and analyze content
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Get page title
            title_element = soup.find('title')
            page_title = title_element.get_text().strip() if title_element else 'No title found'
            
            # Get all text content for analysis
            full_text_content = soup.get_text()
            content_lower = full_text_content.lower()
            
            print(f"  Page title: {page_title}")
            print(f"  Content length: {len(full_text_content)} characters")
            
            # Analyze content relevance
            bulgaria_terms = ['bulgaria', 'bulgarian']
            census_terms = ['2011', 'census', 'population']
            education_terms = ['tertiary', 'education', 'university', 'higher education', 'degree', 'bachelor', 'master']
            gender_terms = ['gender', 'men', 'women', 'male', 'female', 'sex']
            
            # Check for presence of key terms
            has_bulgaria = any(term in content_lower for term in bulgaria_terms)
            has_census = any(term in content_lower for term in census_terms)
            has_education = any(term in content_lower for term in education_terms)
            has_gender = any(term in content_lower for term in gender_terms)
            
            relevance_score = sum([has_bulgaria, has_census, has_education, has_gender])
            
            print(f"  Bulgaria content: {has_bulgaria}")
            print(f"  Census/2011 content: {has_census}")
            print(f"  Education content: {has_education}")
            print(f"  Gender content: {has_gender}")
            print(f"  Relevance score: {relevance_score}/4")
            
            # Store successful source data
            source_data = {
                'index': source_index,
                'url': url,
                'title': page_title,
                'filename': filename,
                'filepath': filepath,
                'has_bulgaria': has_bulgaria,
                'has_census': has_census,
                'has_education': has_education,
                'has_gender': has_gender,
                'relevance_score': relevance_score,
                'content_length': len(full_text_content),
                'status': 'success'
            }
            
            successful_sources.append(source_data)
            
        else:
            print(f"  ✗ Failed with HTTP {response.status_code}")
            failed_sources.append({
                'url': url,
                'status_code': response.status_code,
                'error_type': 'http_error'
            })
            
    except requests.exceptions.Timeout:
        print(f"  ✗ Request timeout")
        failed_sources.append({
            'url': url,
            'error_type': 'timeout',
            'error': 'Request timeout'
        })
    except requests.exceptions.RequestException as e:
        print(f"  ✗ Request error: {str(e)}")
        failed_sources.append({
            'url': url,
            'error_type': 'request_exception',
            'error': str(e)
        })
    except Exception as e:
        print(f"  ✗ Unexpected error: {str(e)}")
        failed_sources.append({
            'url': url,
            'error_type': 'unexpected_error',
            'error': str(e)
        })
    
    print()  # Add spacing between sources
    time.sleep(1)  # Be respectful to servers

print(f"=== INITIAL SEARCH SUMMARY ===")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources")
print(f"High relevance sources (score 3+): {len([s for s in successful_sources if s['relevance_score'] >= 3])}")
print(f"Medium relevance sources (score 2+): {len([s for s in successful_sources if s['relevance_score'] >= 2])}\n")

# Analyze the most promising sources in detail
if successful_sources:
    # Sort by relevance score
    successful_sources.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    print("=== DETAILED ANALYSIS OF TOP SOURCES ===\n")
    
    for source in successful_sources:
        if source['relevance_score'] >= 2:  # Focus on relevant sources
            print(f"ANALYZING: {source['url']} (Score: {source['relevance_score']}/4)")
            print(f"Title: {source['title']}")
            
            # Load and analyze the saved content
            try:
                with open(source['filepath'], 'r', encoding='utf-8') as f:
                    html_content = f.read()
                
                soup = BeautifulSoup(html_content, 'html.parser')
                text_content = soup.get_text()
                
                # Look for tables that might contain statistical data
                tables = soup.find_all('table')
                print(f"  Tables found: {len(tables)}")
                
                # Search for sentences containing relevant information
                sentences = text_content.split('.')
                relevant_sentences = []
                
                for sentence in sentences:
                    sentence_clean = sentence.strip().lower()
                    if len(sentence_clean) > 30:  # Skip very short sentences
                        # Check for Bulgaria + education + numbers
                        has_bulgaria_ref = 'bulgaria' in sentence_clean
                        has_education_ref = any(term in sentence_clean for term in ['tertiary', 'education', 'university', 'higher', 'degree'])
                        has_numbers = bool(re.search(r'\d+', sentence_clean))
                        has_year_2011 = '2011' in sentence_clean
                        
                        if has_bulgaria_ref and has_education_ref and (has_numbers or has_year_2011):
                            relevant_sentences.append(sentence.strip())
                
                print(f"  Relevant sentences found: {len(relevant_sentences)}")
                
                # Look for numerical data in educational context
                education_statistics = []
                
                # Search for patterns like "X% of men" or "Y% of women" with education terms
                stat_patterns = [
                    r'\d+[.,]?\d*\s*%.*?(?:men|women|male|female)',
                    r'(?:men|women|male|female).*?\d+[.,]?\d*\s*%',
                    r'tertiary.*?\d+[.,]?\d*\s*%',
                    r'university.*?\d+[.,]?\d*\s*%',
                    r'higher education.*?\d+[.,]?\d*\s*%'
                ]
                
                for pattern in stat_patterns:
                    matches = re.finditer(pattern, text_content, re.IGNORECASE)
                    for match in matches:
                        # Get context around the match
                        start = max(0, match.start() - 150)
                        end = min(len(text_content), match.end() + 150)
                        context = text_content[start:end].strip()
                        
                        # Check if context mentions Bulgaria
                        if 'bulgaria' in context.lower():
                            education_statistics.append({
                                'pattern': pattern,
                                'match': match.group(),
                                'context': context
                            })
                
                print(f"  Education statistics found: {len(education_statistics)}")
                
                # Analyze tables for potential census data
                relevant_tables = []
                for table_idx, table in enumerate(tables):
                    table_text = table.get_text().lower()
                    
                    # Check if table contains education or census terms
                    if any(term in table_text for term in ['education', 'tertiary', 'university', 'census', '2011']):
                        # Extract table structure
                        headers = []
                        header_cells = table.find_all('th')
                        for header in header_cells:
                            headers.append(header.get_text().strip())
                        
                        # Get sample data rows
                        rows = table.find_all('tr')
                        sample_data = []
                        for row in rows[1:4]:  # First 3 data rows
                            cells = []
                            for cell in row.find_all(['td', 'th']):
                                cells.append(cell.get_text().strip())
                            if cells:
                                sample_data.append(cells)
                        
                        relevant_tables.append({
                            'table_index': table_idx,
                            'headers': headers,
                            'sample_data': sample_data,
                            'total_rows': len(rows)
                        })
                
                print(f"  Relevant tables found: {len(relevant_tables)}")
                
                # Save detailed analysis for this source
                source_analysis = {
                    'source_info': source,
                    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'tables_total': len(tables),
                    'relevant_tables': relevant_tables,
                    'relevant_sentences': relevant_sentences[:15],  # Top 15 sentences
                    'education_statistics': education_statistics[:10],  # Top 10 statistics
                    'content_preview': text_content[:2000]  # First 2000 characters
                }
                
                analysis_filename = f'workspace/analysis_source_{source["index"]}.json'
                with open(analysis_filename, 'w', encoding='utf-8') as f:
                    json.dump(source_analysis, f, indent=2, ensure_ascii=False)
                
                print(f"  Detailed analysis saved to: analysis_source_{source['index']}.json")
                
                # Display key findings
                if relevant_sentences:
                    print(f"  Sample finding: {relevant_sentences[0][:200]}...")
                
                if education_statistics:
                    print(f"  Sample statistic: {education_statistics[0]['match']}")
                    print(f"  Context: {education_statistics[0]['context'][:150]}...")
                
                if relevant_tables and relevant_tables[0]['headers']:
                    print(f"  Sample table headers: {relevant_tables[0]['headers'][:5]}")
                
            except Exception as e:
                print(f"  ✗ Error analyzing source: {str(e)}")
            
            print()

# Create comprehensive summary
summary_data = {
    'search_objective': 'Bulgarian 2011 census data on tertiary education completion by gender',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'sources_attempted': len(search_sources),
    'sources_successful': len(successful_sources),
    'sources_failed': len(failed_sources),
    'high_relevance_sources': len([s for s in successful_sources if s['relevance_score'] >= 3]),
    'medium_relevance_sources': len([s for s in successful_sources if s['relevance_score'] >= 2]),
    'successful_sources': successful_sources,
    'failed_sources': failed_sources,
    'next_steps': [
        'Review detailed analysis files for each relevant source',
        'Extract specific numerical data on tertiary education by gender',
        'Verify data authenticity and source reliability',
        'Compile final statistics on men vs women tertiary education completion'
    ]
}

with open('workspace/bulgarian_census_comprehensive_search.json', 'w', encoding='utf-8') as f:
    json.dump(summary_data, f, indent=2, ensure_ascii=False)

print("=== COMPREHENSIVE SEARCH COMPLETE ===\n")
print(f"Summary saved to: workspace/bulgarian_census_comprehensive_search.json")
print(f"Total sources accessed: {len(successful_sources)}/{len(search_sources)}")
print(f"Relevant sources identified: {len([s for s in successful_sources if s['relevance_score'] >= 2])}")
print(f"Detailed analysis files created: {len([s for s in successful_sources if s['relevance_score'] >= 2])}")
print("\nReady for data extraction phase: analyzing saved content for specific tertiary education statistics by gender")
```