### Development Step 11: Identify the Journal Named After Hreidmarâ€™s Son Hosting Emily Midkiffâ€™s June 2014 Dragon Article

**Description**: Search for information about Emily Midkiff's June 2014 article, focusing on identifying the journal named after one of Hreidmar's sons from Norse mythology. Research Hreidmar's sons (Fafnir, Regin, and Otr) to determine which one has a journal named after them, then locate Midkiff's specific article from June 2014 that discusses dragon depictions and contains quotes from two different authors expressing distaste.

**Use Cases**:
- Academic researchers automating retrieval of specific articles like Emily Midkiffâ€™s June 2014 dragon analysis for literature reviews
- Digital humanities teams extracting thematic and sentiment data from Fafnir issues to map dragon depiction trends over time
- University library systems auto-harvesting metadata and author quotes from Nordic fantasy research journals for enriched catalog entries
- Editorial staff scheduling social media posts by detecting new Emily Midkiffâ€“style articles and extracting compelling dragon-distaste quotes
- Plagiarism detection services verifying author quotes and references in fantasy research publications against original Fafnir content
- Game narrative designers analyzing academic sentiments on dragon tropes in Fafnir to inform authentic lore and character design
- Scholarly indexing platforms auto-capturing June 2014 articles in Fafnir for real-time database updates and citation linking
- University syllabus coordinators sourcing month-specific journal articles like Midkiffâ€™s for targeted course reading lists and seminar discussions

```
import os
import json
import requests
from bs4 import BeautifulSoup

print('=== ACCESSING FAFNIR JOURNAL - SYNTAX ERROR FIXED ===')
print('Objective: Navigate Fafnir journal to find Emily Midkiff June 2014 article')
print('Strategy: Fix regex syntax error and complete the research\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# The Fafnir journal URL discovered from previous analysis
fafnir_journal_url = 'https://fafnir.journal.fi/'
print(f'Target URL: {fafnir_journal_url}')
print('Confirmed: This is "Fafnir â€“ Nordic Journal of Science Fiction and Fantasy Research"')
print('Named after Fafnir, the dragon son of Hreidmar from Norse mythology\n')

# Set up headers to mimic a real browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

print('=== STEP 1: ACCESSING FAFNIR JOURNAL HOMEPAGE ===\n')

try:
    print(f'Making request to: {fafnir_journal_url}')
    response = requests.get(fafnir_journal_url, headers=headers, timeout=30)
    
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    print(f'Content type: {response.headers.get("Content-Type", "unknown")}')
    
    if response.status_code == 200:
        # Save the homepage for analysis
        homepage_path = os.path.join(workspace, 'fafnir_journal_homepage.html')
        with open(homepage_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f'âœ“ Homepage saved to: {homepage_path}')
        
        # Parse the homepage
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'\nPage title: {page_title.get_text().strip()}')
            print('âœ“ CONFIRMED: This is the Fafnir - Nordic Journal of Science Fiction and Fantasy Research')
            print('âœ“ This journal is named after Fafnir, the dragon son of Hreidmar from Norse mythology')
        else:
            print('No page title found, but proceeding with navigation')
        
        print('\n=== STEP 2: SEARCHING FOR ARCHIVE OR NAVIGATION LINKS ===\n')
        
        # Look for navigation links that might lead to archives or past issues
        navigation_patterns = [
            'archive', 'archives', 'past issues', 'back issues', 'volumes', 'issues',
            '2014', 'browse', 'all issues', 'previous', 'older', 'current issue'
        ]
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Found {len(all_links)} total links on homepage')
        
        # Filter for navigation/archive links
        archive_links = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                full_url = f'https://fafnir.journal.fi{href}'
            elif not href.startswith('http'):
                full_url = f'https://fafnir.journal.fi/{href}'
            else:
                full_url = href
            
            # Check if this link might lead to archives
            link_text_lower = text.lower()
            href_lower = href.lower()
            
            relevance_score = 0
            for pattern in navigation_patterns:
                if pattern in link_text_lower:
                    relevance_score += 2
                if pattern in href_lower:
                    relevance_score += 1
            
            if relevance_score > 0 and len(text) > 2:  # Avoid empty or very short links
                archive_links.append({
                    'url': full_url,
                    'text': text[:100],  # Limit text length
                    'score': relevance_score,
                    'contains_2014': '2014' in link_text_lower or '2014' in href_lower
                })
        
        # Sort by relevance score
        archive_links.sort(key=lambda x: x['score'], reverse=True)
        
        print(f'Found {len(archive_links)} potential archive/navigation links:')
        for i, link in enumerate(archive_links[:10], 1):  # Show top 10
            print(f'  {i}. Score {link["score"]}: "{link["text"]}"')
            print(f'     URL: {link["url"]}')
            if link['contains_2014']:
                print('     *** CONTAINS 2014 REFERENCE ***')
            print()
        
        # Also search the homepage content for any direct 2014 or Midkiff references
        homepage_text = soup.get_text().lower()
        
        print('=== STEP 3: SEARCHING HOMEPAGE FOR KEY TERMS ===\n')
        
        # Count key terms on homepage
        key_terms = {
            '2014': homepage_text.count('2014'),
            'midkiff': homepage_text.count('midkiff'),
            'emily': homepage_text.count('emily'),
            'june': homepage_text.count('june'),
            'dragon': homepage_text.count('dragon'),
            'current issue': homepage_text.count('current issue'),
            'latest': homepage_text.count('latest')
        }
        
        print('Key term occurrences on homepage:')
        for term, count in key_terms.items():
            if count > 0:
                print(f'  {term}: {count}')
        
        # Try to access archive links systematically
        if archive_links:
            # Try the highest scoring archive link
            target_link = archive_links[0]
            print(f'\n=== STEP 4: ACCESSING HIGHEST PRIORITY NAVIGATION LINK ===\n')
            print(f'Target: "{target_link["text"]}" (Score: {target_link["score"]})')
            print(f'URL: {target_link["url"]}')
            
            try:
                print('\nMaking request to navigation page...')
                nav_response = requests.get(target_link['url'], headers=headers, timeout=30)
                print(f'Navigation page status: {nav_response.status_code}')
                
                if nav_response.status_code == 200:
                    # Save navigation page
                    nav_path = os.path.join(workspace, 'fafnir_navigation_page.html')
                    with open(nav_path, 'w', encoding='utf-8') as f:
                        f.write(nav_response.text)
                    print(f'âœ“ Navigation page saved to: {nav_path}')
                    
                    # Parse navigation page
                    nav_soup = BeautifulSoup(nav_response.content, 'html.parser')
                    nav_text = nav_soup.get_text().lower()
                    
                    print(f'\n=== STEP 5: ANALYZING NAVIGATION PAGE ===\n')
                    
                    # Count key terms on navigation page
                    nav_key_terms = {
                        '2014': nav_text.count('2014'),
                        'midkiff': nav_text.count('midkiff'),
                        'emily': nav_text.count('emily'),
                        'june': nav_text.count('june'),
                        'volume': nav_text.count('volume'),
                        'issue': nav_text.count('issue'),
                        'dragon': nav_text.count('dragon')
                    }
                    
                    print('Key terms on navigation page:')
                    for term, count in nav_key_terms.items():
                        if count > 0:
                            print(f'  {term}: {count}')
                    
                    # Look for any links containing 2014, June, or Midkiff
                    nav_links = nav_soup.find_all('a', href=True)
                    relevant_links = []
                    
                    for link in nav_links:
                        href = link.get('href', '')
                        text = link.get_text().strip()
                        
                        # Convert to absolute URL
                        if href.startswith('/'):
                            full_url = f'https://fafnir.journal.fi{href}'
                        elif not href.startswith('http'):
                            full_url = f'https://fafnir.journal.fi/{href}'
                        else:
                            full_url = href
                        
                        # Check for relevant terms
                        text_lower = text.lower()
                        href_lower = href.lower()
                        
                        relevance_indicators = ['2014', 'june', 'midkiff', 'emily', 'dragon']
                        is_relevant = any(indicator in text_lower or indicator in href_lower 
                                        for indicator in relevance_indicators)
                        
                        if is_relevant and len(text) > 3:
                            relevant_links.append({
                                'url': full_url,
                                'text': text[:200],  # Limit text length
                                'contains_2014': '2014' in text_lower or '2014' in href_lower,
                                'contains_june': 'june' in text_lower or 'june' in href_lower,
                                'contains_midkiff': 'midkiff' in text_lower or 'midkiff' in href_lower
                            })
                    
                    if relevant_links:
                        print(f'\n*** FOUND {len(relevant_links)} RELEVANT LINKS ***')
                        for i, link in enumerate(relevant_links[:5], 1):
                            print(f'  {i}. "{link["text"]}"')
                            print(f'     URL: {link["url"]}')
                            if link['contains_2014']:
                                print('     *** CONTAINS 2014 ***')
                            if link['contains_june']:
                                print('     *** CONTAINS JUNE ***')
                            if link['contains_midkiff']:
                                print('     *** CONTAINS MIDKIFF ***')
                            print()
                        
                        # Try to access the most promising link
                        # Prioritize Midkiff > June + 2014 > 2014 only
                        priority_link = None
                        for link in relevant_links:
                            if link['contains_midkiff']:
                                priority_link = link
                                break
                        
                        if not priority_link:
                            for link in relevant_links:
                                if link['contains_june'] and link['contains_2014']:
                                    priority_link = link
                                    break
                        
                        if not priority_link:
                            for link in relevant_links:
                                if link['contains_2014']:
                                    priority_link = link
                                    break
                        
                        if not priority_link:
                            priority_link = relevant_links[0]
                        
                        print(f'=== STEP 6: ACCESSING MOST RELEVANT LINK ===\n')
                        print(f'Target: "{priority_link["text"]}"')
                        print(f'URL: {priority_link["url"]}')
                        
                        try:
                            print('\nMaking request to target page...')
                            target_response = requests.get(priority_link['url'], headers=headers, timeout=30)
                            print(f'Target page status: {target_response.status_code}')
                            
                            if target_response.status_code == 200:
                                # Save target page
                                target_path = os.path.join(workspace, 'fafnir_target_page.html')
                                with open(target_path, 'w', encoding='utf-8') as f:
                                    f.write(target_response.text)
                                print(f'âœ“ Target page saved to: {target_path}')
                                
                                # Parse target page for Emily Midkiff
                                target_soup = BeautifulSoup(target_response.content, 'html.parser')
                                target_text = target_soup.get_text().lower()
                                
                                print(f'\n=== STEP 7: SEARCHING TARGET PAGE FOR EMILY MIDKIFF ===\n')
                                
                                # Count key terms on target page
                                target_key_terms = {
                                    'midkiff': target_text.count('midkiff'),
                                    'emily': target_text.count('emily'),
                                    'june': target_text.count('june'),
                                    'dragon': target_text.count('dragon'),
                                    'depiction': target_text.count('depiction'),
                                    'distaste': target_text.count('distaste')
                                }
                                
                                print('Key terms on target page:')
                                for term, count in target_key_terms.items():
                                    if count > 0:
                                        print(f'  {term}: {count}')
                                
                                if target_key_terms['midkiff'] > 0:
                                    print('\nðŸŽ¯ *** EMILY MIDKIFF FOUND ON TARGET PAGE! ***')
                                    
                                    # Extract contexts around Midkiff mentions
                                    full_target_text = target_soup.get_text()
                                    midkiff_contexts = []
                                    start_pos = 0
                                    
                                    while True:
                                        pos = target_text.find('midkiff', start_pos)
                                        if pos == -1:
                                            break
                                        
                                        context_start = max(0, pos - 300)
                                        context_end = min(len(full_target_text), pos + 400)
                                        context = full_target_text[context_start:context_end].strip()
                                        midkiff_contexts.append(context)
                                        start_pos = pos + 1
                                    
                                    print(f'Found {len(midkiff_contexts)} Midkiff contexts:')
                                    for i, context in enumerate(midkiff_contexts, 1):
                                        print(f'\n{i}. Context around Midkiff mention:')
                                        print(f'   ...{context}...')
                                    
                                    # Look for dragon-related content and quotes
                                    if target_key_terms['dragon'] > 0:
                                        print('\n*** DRAGON CONTENT FOUND - SEARCHING FOR QUOTES ***')
                                        
                                        # Look for quote patterns - FIXED: Proper string escaping
                                        import re
                                        
                                        # Simple quote patterns to avoid syntax errors
                                        quote_patterns = [
                                            r'"[^"]{20,200}"',  # Double quotes with substantial content
                                            r'"[^"]{20,200}
```