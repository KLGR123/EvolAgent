### Development Step 35: Extract Survivor Seasons 1–44 Winners to JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Television analytics team automating the ingestion of Survivor winners per season to correlate champion demographics with Nielsen ratings and optimize future programming decisions
- Reality TV fan site updating its historical champions database daily by extracting the latest Survivor winner entries for interactive timeline features
- Data scientist training a predictive model on 44 seasons of Survivor outcomes, using the winners JSON alongside game mechanics to forecast likely future champions
- Marketing analyst cross-referencing Survivor winners with their post-show endorsement deals to evaluate influencer potential and tailor brand partnership strategies
- Academic researcher performing a longitudinal gender‐representation study on reality TV champions by processing the season-winner list for statistical significance tests
- Mobile trivia app developer automating question generation by importing Survivor winners into the question bank, ensuring up-to-date challenges for quiz players
- Data journalist crafting an interactive infographic of Survivor season finales by feeding the extracted winners JSON into a visualization pipeline
- Chatbot integration for streaming platforms, leveraging the Survivor winners list to answer user queries about past champions in real time

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
page_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/wiki/{page_title}"
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
})
resp.raise_for_status()
print(f"Page fetched successfully (status {resp.status_code})\n")

# 3) Parse HTML
doc = BeautifulSoup(resp.text, 'html.parser')

target = None
# 4) Try to select the first table immediately after the 'Winners' section heading
print("Locating 'Winners' section then its first following <table>...\n")
span = doc.find('span', id='Winners')
if span:
    h2 = span.find_parent('h2')
    if h2:
        tbl = h2.find_next('table')
        if tbl:
            target = tbl
            print("→ Selected table following the 'Winners' section.\n")

# 5) Fallback: scan all .wikitable tables for headers containing both 'season' & 'winner'
if not target:
    print("Fallback: scanning all .wikitable tables for headers containing 'Season' & 'Winner'...\n")
    wikitables = doc.find_all('table', class_=lambda v: v and 'wikitable' in v)
    for idx, tbl in enumerate(wikitables, 1):
        first_row = tbl.find('tr')
        if not first_row:
            continue
        hdrs = [th.get_text(strip=True).lower() for th in first_row.find_all(['th','td'], recursive=False)]
        print(f"Table {idx} headers: {hdrs}")
        if 'season' in hdrs and 'winner' in hdrs:
            target = tbl
            print(f"→ Selected wikitable #{idx} with headers containing Season & Winner.\n")
            break

if not target:
    print("❌ Could not find any suitable winners table. Exiting.")
    sys.exit(1)

# 6) Determine which columns are Season and Winner
def get_header_indices(tbl):
    first = tbl.find('tr')
    texts = [c.get_text(strip=True).lower() for c in first.find_all(['th','td'], recursive=False)]
    return texts.index('season'), texts.index('winner')

season_idx, winner_idx = get_header_indices(target)
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 7) Extract (season -> winner) pairs
winners = []
for row in target.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # parse season number
    s_text = cells[season_idx].get_text(strip=True)
    m = re.match(r"^(\d+)", s_text)
    if not m:
        continue
    season = int(m.group(1))
    if not (1 <= season <= 44):
        continue
    # extract winner name(s)
    winner_cell = cells[winner_idx]
    a_tags = winner_cell.find_all('a')
    if a_tags:
        # join all hyperlink texts (handles joint winners)
        names = [a.get_text(strip=True) for a in a_tags if re.search(r"[A-Za-z]", a.get_text())]
        name = " & ".join(names) if names else winner_cell.get_text(strip=True)
    else:
        name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season} → Winner: '{name}'")
    winners.append({'season': season, 'winner': name})

# 8) Sort and verify
ewinners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Verify table structure.")
else:
    missing = [w['season'] for w in winners_sorted if not w['winner']]
    if missing:
        print(f"⚠️ Seasons with missing winner names: {missing}")

# 9) Write to JSON
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {out_path}")
```