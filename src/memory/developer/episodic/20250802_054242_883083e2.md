### Development Step 33: Retrieve and Save University of Leicester’s Dragon Diet Fish Supply Paper PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic literature aggregation for systematic reviews by automatically fetching full-text PDF articles from university domains
- R&D patent monitoring and bulk download of engineering whitepapers in PDF format for competitive intelligence teams
- Regulatory compliance automation by scraping and archiving PDF guidelines, standards, and policy documents from government websites
- Financial analysis workflows that auto-retrieve quarterly and annual reports in PDF from corporate investor relations pages
- E-learning content management through scheduled crawling of university sites for PDF syllabi, lecture slides, and reading materials
- Legal department automation for collecting court opinions, statutes, and legal briefs in PDF from judiciary and legislative portals
- Construction project coordination via automated download of architectural blueprints, building codes, and safety manuals in PDF from municipal archives

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
bing_html_path = os.path.join(workspace, 'bing_search.html')
found_urls_path = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure workspace exists ---
if not os.path.isdir(workspace):
    print(f"[INFO] Creating workspace directory: {workspace}")
    os.makedirs(workspace, exist_ok=True)

# --- Step 1: Perform Bing search for the target paper ---
query = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet? filetype:pdf site:le.ac.uk"
bing_url = 'https://www.bing.com/search'
params = {'q': query}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:104.0) Gecko/20100101 Firefox/104.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"[INFO] Sending Bing search request with query:\n      {query}\n")
response = requests.get(bing_url, params=params, headers=headers, timeout=30)
print(f"[INFO] Bing response status: {response.status_code}")
if response.status_code != 200:
    print(f"ERROR: Bing search failed ({response.status_code}). Exiting.")
    sys.exit(1)

# Save the fetched HTML for inspection
with open(bing_html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"[SAVE] Bing search page saved to: {bing_html_path}\n")

# --- Step 2: Parse HTML and extract PDF URLs ---
print("[INFO] Parsing saved HTML for PDF URLs...")
with open(bing_html_path, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()

# 2a) Direct <a href> tags ending with .pdf or containing .pdf?
print("[STEP] Scanning <a> tags for literal .pdf links...")
for a in soup.find_all('a', href=True):
    href = a['href']
    href_lower = href.lower()
    if href_lower.endswith('.pdf') or '.pdf?' in href_lower:
        pdf_urls.add(href)
print(f"    Found {len(pdf_urls)} literal PDF href(s)")

# 2b) Bing redirect links (/url?q=...)
print("[STEP] Parsing Bing redirect links (/url?q=...pdf)...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q', 'u', 'url'):
            if key in qs:
                real_url = unquote(qs[key][0])
                ru_lower = real_url.lower()
                if ru_lower.endswith('.pdf') or '.pdf?' in ru_lower:
                    pdf_urls.add(real_url)
                break
print(f"    Total after redirect parsing: {len(pdf_urls)} candidate(s)")

# 2c) Regex fallback on raw HTML
print("[STEP] Running regex fallback for PDF URLs in page text...")
# Use a double-quoted raw string and escape the double-quote inside
regex = r"https?://[^\s\"']+?\.pdf(?:\?[^\"']*)?"
matches = re.findall(regex, html, flags=re.IGNORECASE)
for m in matches:
    pdf_urls.add(m)
print(f"    Total after regex fallback: {len(pdf_urls)} candidate(s)")

# 2d) Filter for Leicester-related domains
print("[STEP] Filtering for Leicester domains (le.ac.uk, lra.le.ac.uk)...")
le_domains = ('le.ac.uk', 'lra.le.ac.uk', 'core.ac.uk', 'hdl.handle.net')
leicester_pdfs = [u for u in pdf_urls if any(d in u.lower() for d in le_domains)]

if leicester_pdfs:
    print(f"    Found {len(leicester_pdfs)} Leicester-specific PDF URL(s):")
    for url in leicester_pdfs:
        print(f"      - {url}")
else:
    print("    No Leicester-specific PDFs found; listing all PDF candidates:")
    for url in sorted(pdf_urls):
        print(f"      - {url}")

# --- Save all candidates to file ---
with open(found_urls_path, 'w', encoding='utf-8') as f:
    for url in sorted(pdf_urls):
        f.write(url + '\n')
print(f"\n[SAVE] All PDF URL candidates written to: {found_urls_path}")
print("[COMPLETE] Extraction finished. Inspect the file for the correct PDF link.")
```