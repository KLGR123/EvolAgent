### Development Step 4: **Title:**  
Extract Wikipedia Access Date from Endnote on Page 11 of 'The Responsibility of Intellectuals' (2019)

**Description**: Access and download the full text of 'The Responsibility of Intellectuals' (DOI 10.2307/j.ctv9b2xdv) from JSTOR or UCL Press. Since this is a 2019 UCL Press publication available through JSTOR, retrieve the complete book content and save it to the workspace. Focus on locating page 11, identifying the second-to-last paragraph on that page, and extracting the specific endnote referenced in that paragraph. The endnote should contain a Wikipedia article citation with a November access date - extract the exact day of the month when the Wikipedia article was accessed.

**Use Cases**:
- Academic integrity auditing by university librarians to verify the accuracy and recency of Wikipedia citations in scholarly books and ensure proper referencing standards are met
- Automated extraction of citation metadata for digital humanities researchers analyzing how Wikipedia is referenced in modern academic monographs
- Legal compliance checks by publishers to confirm that open-access book content and endnotes are correctly attributed and accessible, especially for digital distribution
- Research reproducibility validation for peer reviewers who need to trace and confirm the exact sources and access dates of online references cited in academic texts
- Large-scale content ingestion and knowledge graph enrichment for AI systems that require granular bibliographic data, including access dates of web-based sources
- Workflow automation for academic editors who need to extract, review, and cross-check endnote details (such as Wikipedia access dates) across multiple chapters of a book
- Historical citation trend analysis by information scientists studying the evolution of Wikipedia usage in scholarly literature over time
- Quality assurance for digital archives ensuring that digitized books from platforms like JSTOR/UCL Press have complete, accurate, and machine-readable endnote information for future reference

```
import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import re

print('=== ALTERNATIVE ACCESS APPROACH: TRYING SPECIFIC CHAPTER PDF LINKS ===')
print('DOI: 10.2307/j.ctv9b2xdv')
print('Objective: Access individual chapter PDFs to locate page 11 and Wikipedia endnote')
print('\n' + '='*100 + '\n')

# First, inspect the final bibliographic record to get chapter URLs
final_record_path = 'workspace/final_bibliographic_record.json'
if os.path.exists(final_record_path):
    print('=== INSPECTING BIBLIOGRAPHIC RECORD FOR CHAPTER URLS ===')
    with open(final_record_path, 'r', encoding='utf-8') as f:
        biblio_data = json.load(f)
    
    print('Checking chapters_sections structure...')
    if 'chapters_sections' in biblio_data:
        chapters = biblio_data['chapters_sections']
        print(f'Found {len(chapters)} chapters/sections')
        
        # Extract PDF links specifically
        pdf_links = []
        for i, chapter in enumerate(chapters, 1):
            chapter_url = chapter.get('url', '')
            chapter_title = chapter.get('title', f'Chapter {i}')
            
            print(f'{i}. {chapter_title}')
            print(f'   URL: {chapter_url}')
            
            if '.pdf' in chapter_url.lower():
                pdf_links.append({
                    'title': chapter_title,
                    'url': chapter_url,
                    'index': i
                })
                print('   *** PDF LINK DETECTED ***')
        
        print(f'\nFound {len(pdf_links)} direct PDF links:')
        for pdf_link in pdf_links:
            print(f'- {pdf_link["title"]} -> {pdf_link["url"]}')
else:
    print('Final bibliographic record not found')
    exit()

# Set up headers for requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': 'https://www.jstor.org/'
}

# Try accessing each PDF link
successful_pdfs = []

print('\n=== ATTEMPTING TO ACCESS INDIVIDUAL CHAPTER PDFS ===')

for i, pdf_link in enumerate(pdf_links, 1):
    print(f'\n{i}. Trying: {pdf_link["title"]}')
    print(f'   URL: {pdf_link["url"]}')
    
    try:
        response = requests.get(pdf_link['url'], headers=headers, timeout=30)
        print(f'   Status: {response.status_code}')
        print(f'   Content-Type: {response.headers.get("content-type", "unknown")}')
        print(f'   Content-Length: {len(response.content):,} bytes')
        
        if response.status_code == 200:
            content_type = response.headers.get('content-type', '').lower()
            
            if 'pdf' in content_type or len(response.content) > 10000:  # Likely PDF if large
                print('   *** SUCCESS: PDF CONTENT RETRIEVED ***')
                
                # Save the PDF
                pdf_filename = f'workspace/chapter_{i}_{pdf_link["index"]}.pdf'
                with open(pdf_filename, 'wb') as pdf_file:
                    pdf_file.write(response.content)
                
                file_size = os.path.getsize(pdf_filename)
                print(f'   ‚úì PDF saved to: {pdf_filename}')
                print(f'   File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                
                successful_pdfs.append({
                    'title': pdf_link['title'],
                    'filename': pdf_filename,
                    'size': file_size,
                    'original_url': pdf_link['url']
                })
            else:
                print(f'   ‚ö† Content does not appear to be PDF: {content_type}')
                # Save as HTML for inspection
                html_filename = f'workspace/chapter_{i}_response.html'
                with open(html_filename, 'w', encoding='utf-8') as html_file:
                    html_file.write(response.text)
                print(f'   Saved response as HTML: {html_filename}')
        
        elif response.status_code == 403:
            print('   ‚ùå Access forbidden (403) - authentication may be required')
        elif response.status_code == 404:
            print('   ‚ùå Not found (404) - URL may be invalid')
        else:
            print(f'   ‚ùå Request failed with status {response.status_code}')
    
    except Exception as e:
        print(f'   ‚ùå Error accessing PDF: {str(e)}')
    
    time.sleep(1)  # Brief pause between requests

print(f'\n=== PDF ACCESS RESULTS: {len(successful_pdfs)} SUCCESSFUL DOWNLOADS ===')

if successful_pdfs:
    for pdf in successful_pdfs:
        print(f'‚úì {pdf["title"]} - {pdf["size"]:,} bytes')
        print(f'  File: {pdf["filename"]}')
    
    print('\n=== EXTRACTING TEXT FROM DOWNLOADED PDFS ===')
    
    # Try to extract text from each PDF
    try:
        from langchain_community.document_loaders import PyPDFLoader
        
        all_pages = []
        pdf_page_mapping = []  # Track which PDF each page comes from
        
        for pdf_info in successful_pdfs:
            print(f'\nProcessing: {pdf_info["title"]}')
            
            try:
                loader = PyPDFLoader(pdf_info['filename'])
                pages = loader.load_and_split()
                
                print(f'‚úì Extracted {len(pages)} pages from {pdf_info["title"]}')
                
                # Add pages to our collection with source tracking
                start_page_num = len(all_pages) + 1
                for page in pages:
                    all_pages.append(page)
                    pdf_page_mapping.append({
                        'pdf_title': pdf_info['title'],
                        'pdf_filename': pdf_info['filename'],
                        'page_in_collection': len(all_pages),
                        'original_page_num': len(all_pages) - start_page_num + 1
                    })
                
                # Show preview of first page from this PDF
                if pages:
                    preview = pages[0].page_content[:200].replace('\n', ' ')
                    print(f'   First page preview: {preview}...')
            
            except Exception as pdf_error:
                print(f'‚ùå Error processing {pdf_info["filename"]}: {str(pdf_error)}')
        
        print(f'\n*** TOTAL PAGES COLLECTED: {len(all_pages)} ***')
        
        if len(all_pages) >= 11:
            print('\n=== ANALYZING PAGE 11 FOR TARGET CONTENT ===')
            
            # Get page 11 (index 10)
            page_11 = all_pages[10]
            page_11_source = pdf_page_mapping[10]
            
            print(f'Page 11 source: {page_11_source["pdf_title"]}')
            print(f'Page 11 content length: {len(page_11.page_content):,} characters')
            
            # Save page 11 content
            with open('workspace/page_11_extracted.txt', 'w', encoding='utf-8') as f:
                f.write(f'PAGE 11 CONTENT\n')
                f.write(f'Source: {page_11_source["pdf_title"]}\n')
                f.write(f'PDF File: {page_11_source["pdf_filename"]}\n')
                f.write('='*80 + '\n\n')
                f.write(page_11.page_content)
            
            print('‚úì Page 11 content saved to workspace/page_11_extracted.txt')
            
            # Analyze page 11 for paragraphs
            page_11_text = page_11.page_content
            
            # Split into paragraphs (handle different paragraph separators)
            paragraphs = []
            
            # Try different paragraph splitting methods
            if '\n\n' in page_11_text:
                paragraphs = [p.strip() for p in page_11_text.split('\n\n') if p.strip()]
            elif '\n' in page_11_text:
                # Split by single newlines and group consecutive non-empty lines
                lines = [line.strip() for line in page_11_text.split('\n')]
                current_para = []
                for line in lines:
                    if line:
                        current_para.append(line)
                    else:
                        if current_para:
                            paragraphs.append(' '.join(current_para))
                            current_para = []
                if current_para:
                    paragraphs.append(' '.join(current_para))
            else:
                # Fallback: treat entire content as one paragraph
                paragraphs = [page_11_text.strip()]
            
            print(f'\nFound {len(paragraphs)} paragraphs on page 11')
            
            if len(paragraphs) >= 2:
                second_to_last_para = paragraphs[-2]
                print(f'\n=== SECOND-TO-LAST PARAGRAPH ON PAGE 11 ===')
                print('='*80)
                print(second_to_last_para)
                print('='*80)
                
                # Save the specific paragraph
                with open('workspace/page_11_second_to_last_paragraph.txt', 'w', encoding='utf-8') as f:
                    f.write('SECOND-TO-LAST PARAGRAPH FROM PAGE 11\n')
                    f.write('='*50 + '\n\n')
                    f.write(second_to_last_para)
                
                print('\n‚úì Second-to-last paragraph saved to workspace/page_11_second_to_last_paragraph.txt')
                
                # Look for endnote references in this paragraph
                print('\n=== SEARCHING FOR ENDNOTE REFERENCES ===')
                
                endnote_patterns = [
                    r'\b(\d+)\b',  # Simple numbers
                    r'\[(\d+)\]',  # Numbers in brackets
                    r'\((\d+)\)',  # Numbers in parentheses
                    r'\b(\d+)\.',  # Numbers with periods
                    r'see note (\d+)',  # "see note X" format
                    r'note (\d+)',  # "note X" format
                    r'footnote (\d+)',  # "footnote X" format
                ]
                
                found_endnotes = []
                for pattern in endnote_patterns:
                    matches = re.findall(pattern, second_to_last_para, re.IGNORECASE)
                    if matches:
                        for match in matches:
                            if match.isdigit() and 1 <= int(match) <= 200:  # Reasonable endnote range
                                found_endnotes.append(int(match))
                
                # Remove duplicates and sort
                found_endnotes = sorted(list(set(found_endnotes)))
                
                if found_endnotes:
                    print(f'*** FOUND ENDNOTE REFERENCES: {found_endnotes} ***')
                    
                    # Now search for the actual endnotes in all collected pages
                    print('\n=== SEARCHING ALL PAGES FOR ENDNOTES SECTION ===')
                    
                    # Combine all pages to search for endnotes
                    full_text = '\n\n'.join([page.page_content for page in all_pages])
                    
                    print(f'Total text to search: {len(full_text):,} characters')
                    
                    # Search for Wikipedia citations with November access dates
                    print('\n=== SEARCHING FOR WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATE ===')
                    
                    # Comprehensive Wikipedia citation patterns
                    wikipedia_patterns = [
                        r'wikipedia[^\n]{0,200}november[^\n]{0,100}\d{1,2}[^\n]{0,50}',
                        r'en\.wikipedia\.org[^\n]{0,200}november[^\n]{0,100}\d{1,2}[^\n]{0,50}',
                        r'accessed[^\n]{0,100}november[^\n]{0,50}\d{1,2}[^\n]{0,100}wikipedia[^\n]{0,100}',
                        r'november[^\n]{0,50}\d{1,2}[^\n]{0,100}wikipedia[^\n]{0,200}',
                        r'\d{1,2}[^\n]{0,20}november[^\n]{0,100}wikipedia[^\n]{0,200}',
                        r'wikipedia[^\n]{0,300}accessed[^\n]{0,100}november[^\n]{0,50}\d{1,2}[^\n]{0,50}'
                    ]
                    
                    wikipedia_citations = []
                    for pattern in wikipedia_patterns:
                        matches = re.finditer(pattern, full_text, re.IGNORECASE | re.DOTALL)
                        for match in matches:
                            citation_text = match.group(0)
                            
                            # Extract the day from November date
                            day_patterns = [
                                r'november\s+(\d{1,2})',
                                r'(\d{1,2})\s+november',
                                r'november\s+(\d{1,2})(?:st|nd|rd|th)?',
                                r'(\d{1,2})(?:st|nd|rd|th)?\s+november'
                            ]
                            
                            day_found = None
                            for day_pattern in day_patterns:
                                day_match = re.search(day_pattern, citation_text, re.IGNORECASE)
                                if day_match:
                                    day_found = day_match.group(1)
                                    break
                            
                            if day_found:
                                # Check if this citation is near any of our endnote numbers
                                citation_context = full_text[max(0, match.start()-500):match.end()+500]
                                
                                related_endnotes = []
                                for endnote_num in found_endnotes:
                                    if str(endnote_num) in citation_context:
                                        related_endnotes.append(endnote_num)
                                
                                wikipedia_citations.append({
                                    'citation': citation_text,
                                    'november_day': day_found,
                                    'position': match.start(),
                                    'context': citation_context,
                                    'related_endnotes': related_endnotes
                                })
                    
                    # Remove duplicates based on citation text
                    unique_citations = []
                    seen_citations = set()
                    for citation in wikipedia_citations:
                        citation_key = citation['citation'].strip().lower()
                        if citation_key not in seen_citations:
                            seen_citations.add(citation_key)
                            unique_citations.append(citation)
                    
                    if unique_citations:
                        print(f'\nüéØ FOUND {len(unique_citations)} UNIQUE WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATES:')
                        
                        for i, citation in enumerate(unique_citations, 1):
                            print(f'\nCitation {i}:')
                            print(f'November day: {citation["november_day"]}')
                            print(f'Position in text: {citation["position"]:,}')
                            if citation['related_endnotes']:
                                print(f'Related endnotes: {citation["related_endnotes"]}')
                            print('Citation text:')
                            print('='*60)
                            print(citation['citation'])
                            print('='*60)
                            
                            # Show some context
                            context_preview = citation['context'][:300] + '...' if len(citation['context']) > 300 else citation['context']
                            print(f'Context: {context_preview}')
                            print('-'*60)
                        
                        # Save the analysis
                        analysis_data = {
                            'source_pdfs': [pdf['filename'] for pdf in successful_pdfs],
                            'total_pages_analyzed': len(all_pages),
                            'page_11_source': page_11_source,
                            'page_11_paragraph_count': len(paragraphs),
                            'second_to_last_paragraph': second_to_last_para,
                            'endnote_references_found': found_endnotes,
                            'wikipedia_citations': unique_citations,
                            'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                        
                        with open('workspace/wikipedia_endnote_analysis.json', 'w', encoding='utf-8') as f:
                            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
                        
                        print('\n‚úì Complete analysis saved to workspace/wikipedia_endnote_analysis.json')
                        
                        # Determine the most likely answer
                        if len(unique_citations) == 1:
                            answer_day = unique_citations[0]['november_day']
                            print(f'\n*** ANSWER FOUND: The Wikipedia article was accessed on November {answer_day} ***')
                        elif len(unique_citations) > 1:
                            # Look for citations most closely related to our endnote references
                            best_citation = None
                            max_related_endnotes = 0
                            
                            for citation in unique_citations:
                                if len(citation['related_endnotes']) > max_related_endnotes:
                                    max_related_endnotes = len(citation['related_endnotes'])
                                    best_citation = citation
                            
                            if best_citation:
                                answer_day = best_citation['november_day']
                                print(f'\n*** MOST LIKELY ANSWER: November {answer_day} ***')
                                print(f'(This citation is related to endnotes: {best_citation["related_endnotes"]})')
                            else:
                                print(f'\n*** MULTIPLE CANDIDATES FOUND - Manual review needed ***')
                                for i, citation in enumerate(unique_citations, 1):
                                    print(f'Option {i}: November {citation["november_day"]}')
                    else:
                        print('\n‚ö† No Wikipedia citations with November access dates found')
                        
                        # Broader search for any Wikipedia references
                        print('\nSearching for any Wikipedia references...')
                        wiki_matches = re.finditer(r'wikipedia[^\n]{0,100}', full_text, re.IGNORECASE)
                        wiki_refs = [match.group(0) for match in wiki_matches]
                        
                        if wiki_refs:
                            print(f'Found {len(wiki_refs)} general Wikipedia references:')
                            for i, ref in enumerate(wiki_refs[:5], 1):
                                print(f'{i}. {ref}')
                        else:
                            print('No Wikipedia references found at all')
                else:
                    print('\n‚ö† No endnote references found in second-to-last paragraph')
                    print('Paragraph content for manual inspection:')
                    print(second_to_last_para)
            else:
                print(f'\n‚ö† Page 11 has fewer than 2 paragraphs ({len(paragraphs)} found)')
                if paragraphs:
                    print('Available paragraphs:')
                    for i, para in enumerate(paragraphs, 1):
                        print(f'{i}. {para[:150]}...')
        else:
            print(f'\n‚ö† Only {len(all_pages)} pages collected, page 11 not available')
            print('Available pages:')
            for i, page in enumerate(all_pages[:10], 1):
                source = pdf_page_mapping[i-1]
                preview = page.page_content[:100].replace('\n', ' ')
                print(f'Page {i} (from {source["pdf_title"]}): {preview}...')
    
    except ImportError:
        print('‚ùå PyPDFLoader not available - cannot extract text from PDFs')
        print('PDFs have been downloaded but text extraction is not possible')
    except Exception as extraction_error:
        print(f'‚ùå Error during text extraction: {str(extraction_error)}')
else:
    print('\n‚ùå No PDFs were successfully downloaded')
    print('Cannot proceed with page 11 analysis')

print('\n' + '='*100)
print('ALTERNATIVE ACCESS ATTEMPT COMPLETE')
print('='*100)
print('Summary:')
print(f'- Attempted to access {len(pdf_links) if "pdf_links" in locals() else 0} PDF links')
print(f'- Successfully downloaded {len(successful_pdfs)} PDFs')
if successful_pdfs:
    print('- Extracted text and analyzed for page 11 content')
    print('- Searched for Wikipedia citations with November access dates')
print('\nObjective: Find exact day in November when Wikipedia article was accessed')
```