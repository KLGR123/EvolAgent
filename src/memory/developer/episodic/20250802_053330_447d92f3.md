### Development Step 2: Download PDF of University of Leicester’s “Can Hiccup Supply Enough Fish to Maintain Dragon’s Diet?”

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated academic literature retrieval for PhD students conducting a systematic review in neuroscience, downloading target PDFs into their citation manager
- Batch download of SEC filings (10-K, annual reports) for financial analysts to feed into quantitative valuation models for investment research
- Automated ingestion of ISO and IEEE standard documents into a manufacturing firm’s quality management system for compliance audits
- Bulk retrieval of industry whitepapers and market intelligence reports (e.g., consumer electronics trends) for consulting teams to prepare client presentations
- Collection and archiving of GDPR, HIPAA, and other regulatory guideline PDFs for legal and compliance teams to maintain up-to-date policy libraries
- Automated fetching of grant proposal templates and funding call documents (e.g., NIH R01, Horizon Europe) for university research administration offices
- Downloading clinical trial protocols and FDA approval documentation for pharmaceutical R&D teams to mine data on drug efficacy studies
- Automated acquisition of supplier technical datasheets (e.g., semiconductor component PDFs) for e-commerce platforms to enrich product catalog metadata

```
import os
import sys
import time
import requests
import json

# Purpose: Search and download the University of Leicester paper
# "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?"
# Save PDF to workspace/dragon_diet_fish_bag.pdf with retry logic on SerpAPI rate limits

# 1. Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2. Prepare search query and API key
query = '"Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" University of Leicester PDF'
api_key = os.getenv('SERPAPI_API_KEY')

print("Starting search for the paper PDF...")
print(f"Search query: {query}")

if not api_key:
    print("ERROR: Environment variable SERPAPI_API_KEY not set. Cannot perform SerpAPI search.")
    sys.exit(1)

# 3. Set up SerpAPI parameters
params = {
    'engine': 'google',
    'q': query,
    'api_key': api_key,
    'num': 10,
    'google_domain': 'google.com',
    'safe': 'off'
}

# 4. Attempt SerpAPI search with exponential backoff on 429 errors
max_retries = 3
results = None
for attempt in range(1, max_retries + 1):
    print(f"SerpAPI request attempt {attempt}/{max_retries}...")
    try:
        resp = requests.get('https://serpapi.com/search.json', params=params, timeout=30)
        resp.raise_for_status()
        results = resp.json()
        print("SerpAPI response received successfully.")
        break
    except requests.exceptions.HTTPError as e:
        status = e.response.status_code if e.response is not None else None
        if status == 429:
            backoff = 2 ** (attempt - 1)
            print(f"Received 429 Too Many Requests (status {status}). Backing off for {backoff} seconds...")
            time.sleep(backoff)
            continue
        else:
            print(f"HTTPError on SerpAPI request (status {status}): {e}")
            sys.exit(1)
    except Exception as e:
        print(f"Error on SerpAPI request: {e}")
        sys.exit(1)
else:
    print(f"ERROR: All {max_retries} SerpAPI attempts failed due to rate limiting.")
    sys.exit(1)

# 5. Inspect results for direct PDF link
pdf_link = None
organic = results.get('organic_results', [])
print(f"Total organic results retrieved: {len(organic)}")
for idx, item in enumerate(organic, start=1):
    title = item.get('title', '')
    link = item.get('link', '')
    snippet = item.get('snippet', '')
    print(f"Result #{idx}:")
    print(f"  Title: {title}")
    print(f"  URL: {link}")
    print(f"  Snippet: {snippet[:100]}{'...' if len(snippet)>100 else ''}")
    # Identify direct PDF URLs
    low = link.lower()
    if low.endswith('.pdf') or '.pdf?' in low:
        pdf_link = link
        print(f"--> Selected PDF link: {pdf_link}")
        break

if not pdf_link:
    print("ERROR: No direct PDF link found in top SerpAPI results.")
    sys.exit(1)

# 6. Download the PDF
pdf_path = os.path.join(workspace_dir, 'dragon_diet_fish_bag.pdf')
print(f"Downloading PDF from: {pdf_link}")
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'application/pdf'
}
total_bytes = 0
with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as dl:
    dl.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in dl.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                total_bytes += len(chunk)

print(f"Download complete: {pdf_path}")
print(f"File size: {total_bytes} bytes ({total_bytes/1024/1024:.2f} MB)")

# 7. Basic sanity check
if total_bytes < 10000:
    print("WARNING: The downloaded file is very small (<10 KB). It may not be the full PDF.")
else:
    print("PDF appears to have been downloaded successfully.")

print("Script finished.")

```