### Development Step 37: Download University of Leicester’s “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic marine biology research: automating the retrieval of “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF to analyse fish-to-predator biomass ratios in mythical ecosystems
- Pharmaceutical patent intelligence: scanning search results for patent PDFs on novel drug delivery systems and auto-downloading them for competitive analysis
- Legal eDiscovery and compliance review: extracting court filing PDFs from search engine results to build a document corpus for due-diligence investigations
- Government policy monitoring: locating and downloading the latest fisheries regulation PDFs from official sites for automated policy-tracking and impact assessment
- Market research in seafood industry: gathering industry report PDFs on global fish supply chains directly from search results for real-time trend analysis
- Environmental NGO field studies: pulling down PDF datasets on water quality and fish population surveys for automated GIS and ecological modelling
- News fact-checking workflows: finding and downloading primary source PDFs (e.g., whitepapers, government reports) cited in breaking stories to verify claims

```
import os
from bs4 import BeautifulSoup

# --- Paths ---
workspace = 'workspace'
bing_html = os.path.join(workspace, 'bing_search.html')
inspection_out = os.path.join(workspace, 'search_results_inspection.txt')

# --- Ensure input exists ---
if not os.path.isfile(bing_html):
    print(f"ERROR: Cannot find Bing HTML at {bing_html}. Please run the search extraction first.")
    exit(1)

print(f"[INFO] Inspecting Bing search HTML: {bing_html}\n")
with open(bing_html, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

# 1) Extract main result anchors (li.b_algo > h2 > a)
anchors = soup.select('li.b_algo h2 a')
print(f"[INSPECT] Found {len(anchors)} main result links (li.b_algo h2 a). Listing top 10:\n")

with open(inspection_out, 'w', encoding='utf-8') as out:
    out.write(f"Main result links (total: {len(anchors)})\n")
    out.write("="*60 + "\n")
    for i, a in enumerate(anchors[:10], 1):
        title = a.get_text(strip=True)
        href = a.get('href')
        line = f"{i}. Title: {title}\n   URL:  {href}\n"
        print(line)
        out.write(line + "\n")

# 2) Scan for any <a> elements whose text or href contains '.pdf'
pdf_links = []
for a in soup.find_all('a', href=True):
    href = a['href']
    txt = a.get_text()
    if '.pdf' in href.lower() or '.pdf' in txt.lower():
        pdf_links.append((txt.strip(), href))

print(f"[INSPECT] Found {len(pdf_links)} <a> tags with '.pdf' in text or href. Listing top 10:\n")
out.write(f"\nPDF-like <a> tags (total: {len(pdf_links)})\n")
out.write("="*60 + "\n")
for i, (txt, href) in enumerate(pdf_links[:10], 1):
    line = f"{i}. Text: {txt or '[no text]'}\n   Href: {href}\n"
    print(line)
    out.write(line + "\n")

# 3) Summarize
summary = (
    f"\n=== SUMMARY ===\n"
    f"Total main result anchors (li.b_algo h2 a): {len(anchors)}\n"
    f"Total PDF-like <a> tags: {len(pdf_links)}\n"
)
print(summary)
out.write(summary)

print(f"[SAVE] Inspection details written to: {inspection_out}")
print("[COMPLETE] Please open the inspection file to understand the HTML structure and adjust your extraction logic accordingly.")
```