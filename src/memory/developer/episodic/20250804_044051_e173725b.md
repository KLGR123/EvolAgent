### Development Step 18: Extract Survivor Winners for Seasons 1–44 from Wikipedia into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automating a reality-TV fan site update by scraping the Survivor winners table to power a season-by-season winners page with up-to-date JSON data.
- Populating a sports analytics dashboard by extracting World Series champions from Wikipedia tables into a structured feed for trend and streak analysis.
- Compiling Nobel Prize laureate lists for an academic research portal by harvesting the “Year” and “Laureate” columns from the official Nobel page and saving as JSON.
- Building an interactive corporate leaderboard by scraping Fortune 500 ranking tables to feed a BI tool that tracks annual shifts in company standings.
- Enriching a film advocacy newsletter by collecting Academy Award Best Picture winners from Wikipedia into a JSON file to highlight historical trends in film themes.
- Powering a product-comparison widget for an e-commerce site by extracting “Model” and “Rating” columns from consumer electronics tables into machine-readable JSON.
- Streamlining compliance reporting for finance teams by scraping regulatory fine tables from government sites (e.g., CFPB enforcement actions) into structured JSON for audit trails.
- Generating a historical timeline feature for an educational history app by pulling “President” and “Term” data from the U.S. Presidents table on Wikipedia into a sorted JSON list.

```
import os
import sys
import json
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory dynamically
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Load the printable HTML saved earlier
html_path = os.path.join(workspace_dir, 'survivor_page_printable.html')
if not os.path.exists(html_path):
    print(f"❌ File not found: {html_path}")
    sys.exit(1)
with open(html_path, 'r', encoding='utf-8') as f:
    html_text = f.read()
soup = BeautifulSoup(html_text, 'html.parser')

# 3) Find all wikitable tables and select the one with 'Season' & 'Winner' headers
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} tables with class 'wikitable'.\n")
target_table = None
for idx, table in enumerate(tables, start=1):
    header_row = table.find('tr')
    headers_txt = [th.get_text(strip=True).lower() for th in header_row.find_all(['th','td'], recursive=False)]
    print(f"Table {idx} headers: {headers_txt}")
    if 'season' in headers_txt and 'winner' in headers_txt:
        target_table = table
        print(f"→ Selected table {idx} for parsing.\n")
        break

if not target_table:
    print("❌ Could not find a table containing both 'Season' and 'Winner'. Exiting.")
    sys.exit(1)

# 4) Determine column indices
first_row = target_table.find('tr')
cols = [th.get_text(strip=True).lower() for th in first_row.find_all(['th','td'], recursive=False)]
season_idx = cols.index('season')
winner_idx = cols.index('winner')
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 5) Extract data rows, refine winner extraction using bold/link fallback
winners = []
for row in target_table.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # remove footnotes
    for sup in cells[season_idx].find_all('sup') + cells[winner_idx].find_all('sup'):
        sup.decompose()
    # parse season number
    season_text = cells[season_idx].get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        continue
    if not (1 <= season_num <= 44):
        continue
    # refined winner name extraction
    winner_cell = cells[winner_idx]
    bold_tag = winner_cell.find('b')
    if bold_tag:
        w_name = bold_tag.get_text(strip=True)
    else:
        first_link = winner_cell.find('a')
        if first_link:
            w_name = first_link.get_text(strip=True)
        else:
            w_name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season_num} -> Winner: {w_name}")
    winners.append({'season': season_num, 'winner': w_name})

# 6) Validate count, sort, and save
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted for seasons 1–44: {len(winners_sorted)}")
if len(winners_sorted) != 44:
    print("⚠️ Warning: extracted count != 44. Please verify extraction logic.")

out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as out_file:
    json.dump(winners_sorted, out_file, indent=2)
print(f"Saved winners list to: {out_path}")
```