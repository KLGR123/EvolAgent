### Development Step 12: Collect US Presidents’ Official Birth Cities, State Names, and Geographic Coordinates into JSON

**Description**: Collect a complete list of all United States presidents with their official birth city names and state names, then obtain the geographic coordinates (longitude and latitude) for each birth city using authoritative sources such as official presidential biographies or Wikipedia infoboxes. Save the results in a structured format (e.g., CSV or JSON) at workspace/presidential_birthplaces.json.

**Use Cases**:
- K-12 educational platform integrating an interactive US map that plots each president’s birth city and state coordinates for geography and history lessons in the classroom
- Heritage tourism operator generating a “Presidential Birthplaces Trail” road-trip itinerary with turn-by-turn directions to each president’s birthplace landmark
- Political science researcher mapping birthplaces against election results to analyze regional representation and party affiliation trends over U.S. history
- Data journalism team creating an online infographic and heatmap that visualizes presidents’ birth cities to illustrate shifting population centers and cultural origins
- Civics mobile app sending location-based alerts and historical profiles when users approach a president’s birthplace marker in their vicinity
- National Park Service GIS integration to catalog, monitor, and manage preservation efforts at historic presidential birth sites using precise latitude/longitude
- Educational game developer building a location-aware mobile quiz that prompts players to identify presidents based on proximity to their birth city coordinates

```
import os
import re
import time
import json
import requests
from bs4 import BeautifulSoup, NavigableString
from geopy.geocoders import Nominatim
from requests.exceptions import SSLError, RequestException

# Ensure workspace directory exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# 1. Load or fetch the presidents list page
list_html_path = os.path.join(workspace, 'presidents_list.html')
if os.path.exists(list_html_path):
    print(f"Loading cached HTML from {list_html_path}")
    with open(list_html_path, 'r', encoding='utf-8') as f:
        list_html = f.read()
else:
    list_url = 'https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States'
    print(f"Fetching presidents list: {list_url}")
    resp = requests.get(list_url)
    resp.raise_for_status()
    list_html = resp.text
    with open(list_html_path, 'w', encoding='utf-8') as f:
        f.write(list_html)
    print(f"Saved HTML to {list_html_path}")

# 2. Parse the presidents table
soup = BeautifulSoup(list_html, 'html.parser')
table = soup.find('table', class_='wikitable')
rows = table.find_all('tr')[1:]
print(f"Found {len(rows)} rows in presidents table")

# 3. Collect each president's name and Wikipedia URL
presidents = []
for row in rows:
    cols = row.find_all('td')
    if len(cols) < 2:
        continue
    link = cols[1].find('a', href=True)
    if not link:
        continue
    name = link.get_text(strip=True)
    url = 'https://en.wikipedia.org' + link['href']
    presidents.append({'name': name, 'url': url})
print(f"Collected {len(presidents)} president entries to process")

# 4. Initialize geocoder and HTTP session
geolocator = Nominatim(user_agent='presidential_birth_locator')
session = requests.Session()
results = []

# 5. Helper: fetch page with retry on SSLError

def fetch_page(url, max_retries=2, delay=1):
    for attempt in range(1, max_retries + 1):
        try:
            resp = session.get(url)
            resp.raise_for_status()
            return resp.text
        except SSLError as e:
            print(f"  SSLError on attempt {attempt} for {url}: {e}")
            time.sleep(delay)
        except RequestException as e:
            print(f"  RequestException for {url}: {e}")
            break
    print(f"  Failed to fetch {url} after {max_retries} attempts")
    return None

# 6. Process each president
for idx, pres in enumerate(presidents, start=1):
    name = pres['name']
    url = pres['url']
    print(f"\n[{idx}/{len(presidents)}] {name}\nFetching page...")
    page_html = fetch_page(url)
    if not page_html:
        print(f"  Skipping {name} due to fetch failure")
        results.append({'name': name, 'birth_city': None, 'birth_state': None, 'latitude': None, 'longitude': None})
        continue
    page = BeautifulSoup(page_html, 'html.parser')
    time.sleep(1)

    # 7. Try <span class='birthplace'>
    birth_place = ''
    span_bp = page.select_one('span.birthplace')
    if span_bp and span_bp.get_text(strip=True):
        birth_place = span_bp.get_text(strip=True)
        print(f"  Found span.birthplace: '{birth_place}'")
    else:
        # Fallback: locate 'Born' row and split on <br>
        infobox = page.find('table', class_=lambda c: c and 'infobox' in c)
        if infobox:
            born_tr = infobox.find(lambda t: t.name=='tr' and t.th and t.th.get_text(strip=True).startswith('Born'))
            if born_tr:
                td = born_tr.find('td')
                if td:
                    # Split by <br> HTML tag
                    parts = re.split(r'<br\s*/?>', str(td), flags=re.IGNORECASE)
                    if len(parts) >= 2:
                        place_html = parts[1]
                        # Remove small tags
                        place_html = re.sub(r'<small[^>]*>.*?</small>', '', place_html, flags=re.DOTALL|re.IGNORECASE)
                        birth_place = BeautifulSoup(place_html, 'html.parser').get_text(strip=True)
                        print(f"  Fallback Born->extracted place: '{birth_place}'")
    if not birth_place:
        print(f"  Warning: no birthplace extracted for {name}")

    # 8. Clean birthplace text
    #    - remove parentheses
    #    - remove citation brackets
    clean = re.sub(r'\([^)]*\)', '', birth_place)
    clean = re.sub(r'\[.*?\]', '', clean)
    clean = clean.strip()
    # 9. Split into tokens, drop date-like tokens
    tokens = [t.strip() for t in clean.split(',') if t.strip()]
    while tokens and re.search(r'\d', tokens[0]):
        tokens.pop(0)
    # Drop trailing country tokens
    unwanted = re.compile(r'^(British America|United States|U\.?S\.?A?\.?|USA)$', re.I)
    while tokens and unwanted.match(tokens[-1]):
        tokens.pop()
    birth_clean = ', '.join(tokens)
    print(f"  Cleaned birthplace: '{birth_clean}'")

    # 10. Derive city and state
    city = tokens[0] if len(tokens) >= 1 else None
    state = tokens[1] if len(tokens) >= 2 else None
    print(f"  Parsed city='{city}', state='{state}'")

    # 11. Geocode
    latitude = longitude = None
    if city and state:
        for query in [f"{city}, {state}, USA", f"{birth_clean}, USA"]:
            try:
                print(f"  Geocoding: '{query}'")
                loc = geolocator.geocode(query, timeout=10)
                time.sleep(1)
                if loc:
                    latitude, longitude = loc.latitude, loc.longitude
                    print(f"   -> Got coords: ({latitude}, {longitude})")
                    break
            except Exception as e:
                print(f"   -> Geocoding error for '{query}': {e}")
    else:
        print(f"  Insufficient tokens to geocode for {name}")

    # 12. Append result
    results.append({
        'name': name,
        'birth_city': city,
        'birth_state': state,
        'latitude': latitude,
        'longitude': longitude
    })

# 13. Save to JSON
out_path = os.path.join(workspace, 'presidential_birthplaces.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2)
print(f"\nFinished: saved {len(results)} records to {out_path}")
```