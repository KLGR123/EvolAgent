### Development Step 25: Extract Survivor US Seasons 1–44 Winners from Wikipedia into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated aggregation of “Survivor” winners for a fan-made dashboard tracking gender and age trends across seasons
- Data ingestion pipeline for a reality TV analytics startup to correlate winner profiles with viewer ratings
- Backend data source for a trivia mobile app generating season-specific questions about “Survivor” champions
- Content feed for a pop-culture news site publishing “On this day” winner anniversaries and career highlights
- Archival metadata creation in a media library system cataloging reality show champions for research purposes
- Machine-learning dataset preparation to train a name-recognition model on celebrity and contestant names
- Social media automation tool scheduling daily “Survivor” winner spotlights with images and fun facts

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable Survivor page
page = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={page}&printable=yes"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers=headers)
resp.raise_for_status()
print(f"Page fetched successfully (status {resp.status_code})\n")

# 3) Parse HTML
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Find the correct wikitable
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} wikitable(s). Looking for Season & Winner columns...\n")
target = None
for i, tbl in enumerate(tables, 1):
    hdr = tbl.find('tr')
    cols = [c.get_text(strip=True).lower() for c in hdr.find_all(['th','td'], recursive=False)]
    print(f" Table {i} headers: {cols}")
    if 'season' in cols and 'winner' in cols:
        target = tbl
        print(f"→ Selected table {i}.\n")
        break

if target is None:
    print("❌ No table found with both 'Season' and 'Winner'. Exiting.")
    sys.exit(1)

# 5) Identify column indices
hdr_cells = target.find('tr').find_all(['th','td'], recursive=False)
col_texts = [c.get_text(strip=True).lower() for c in hdr_cells]
season_idx = col_texts.index('season')
winner_idx = col_texts.index('winner')
print(f"Column indices -> Season: {season_idx}, Winner: {winner_idx}\n")

# 6) Helper to extract winner name

def extract_winner(cell):
    # remove any footnotes
    for s in cell.find_all('sup'):
        s.decompose()
    # a) first pick <a> tags with letters
    for a in cell.find_all('a'):
        txt = a.get_text(strip=True)
        if re.search('[A-Za-z]', txt):
            return txt
    # b) then pick <b> tags with letters
    for b in cell.find_all('b'):
        txt = b.get_text(strip=True)
        if re.search('[A-Za-z]', txt):
            return txt
    # c) fallback: strip trailing vote counts
    full = cell.get_text(' ', strip=True)
    return re.sub(r'\s*\d+(?:[–-]\d+)*$', '', full).strip()

# 7) Iterate rows and collect winners for seasons 1–44
winners = []
for row in target.find_all('tr')[1:]:  # skip header
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # parse season number
    stxt = cells[season_idx].get_text(strip=True)
    try:
        num = int(stxt)
    except ValueError:
        continue
    if not (1 <= num <= 44):
        continue
    # extract winner name
    winner = extract_winner(cells[winner_idx])
    print(f"Parsed Season {num} → Winner: {winner}")
    winners.append({'season': num, 'winner': winner})

# 8) Sort and verify
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal seasons parsed: {len(winners_sorted)} (expect 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: Parsed count != 44. Please check extraction logic.")

# 9) Write output
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {out_file}")

```