### Development Step 3: Extract Survivor (US) Season 1–44 Winners from Wikipedia to JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Building an interactive web dashboard for pop culture analysts to visualize trends in Survivor winners by demographics across seasons 1–44
- Automating weekly newsletter content for entertainment news outlets with historical Survivor winner profiles and season summaries
- Integrating structured winner data into a fan mobile app to power push notifications and trivia quizzes about past Survivor champions
- Enriching a streaming platform’s recommendation engine with metadata on Survivor winners to personalize reality-TV suggestions for subscribers
- Supporting academic research on game theory and social dynamics by exporting Survivor winner information for statistical modeling of competition outcomes
- Feeding a TV production company’s competitor analysis tool with historical champion data to benchmark new reality show concepts against Survivor’s legacy
- Populating the backend database of a reality-TV podcast with quick-reference winner details for hosts to compare champions episode by episode
- Creating a machine learning dataset for NLP-based sentiment analysis by linking Survivor winner names with episode reviews and social media reactions over time

```
import os
import json
import requests
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# Fetch the Survivor page
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
print(f"Fetching page: {url}")
response = requests.get(url)
response.raise_for_status()
print("Page fetched successfully, parsing HTML...")

soup = BeautifulSoup(response.text, 'html.parser')

# Collect all tables with class 'wikitable'
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} wikitable(s) on the page.")

# Attempt 1: Locate by table caption 'Winners by season'
winners_table = None
for idx, table in enumerate(tables, start=1):
    caption_tag = table.find('caption')
    caption_text = caption_tag.get_text(strip=True) if caption_tag else ''
    print(f"Table {idx} caption: '{caption_text}'")
    if 'winners by season' in caption_text.lower():
        winners_table = table
        print(f"--> Selected table #{idx} by caption match")
        break

# Fallback: locate by scanning header cells for 'Season' and 'Winner'
if winners_table is None:
    print("Caption match failed, falling back to header scanning...")
    for idx, table in enumerate(tables, start=1):
        header = table.find('tr')
        if not header:
            continue
        th_texts = [th.get_text(strip=True).lower() for th in header.find_all('th')]
        print(f"Table {idx} headers: {th_texts}")
        if 'season' in th_texts and 'winner' in th_texts:
            winners_table = table
            print(f"--> Selected table #{idx} by header match")
            break

# If still not found, error out
if not winners_table:
    raise RuntimeError("Could not locate the 'Winners by season' table on the page.")

print("Parsing the winners table...")
rows = winners_table.find_all('tr')
print(f"Total rows (including header): {len(rows)}")
data_rows = rows[1:]  # skip header
winners = []

for row in data_rows:
    # Get direct child cells only to respect colspan/rowspan structure
    cells = row.find_all(['th', 'td'], recursive=False)
    if len(cells) < 2:
        print("Skipping row: fewer than 2 cells")
        continue
    # Extract season number from first cell
    season_cell = cells[0]
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    season_text = season_cell.get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"Skipping row: cannot parse season '{season_text}'")
        continue
    if not (1 <= season_num <= 44):
        print(f"Skipping season {season_num}: out of target range")
        continue
    # Extract winner name from second cell
    winner_cell = cells[1]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    winner_name = winner_cell.get_text(strip=True)
    print(f"Parsed Season {season_num}: Winner -> {winner_name}")
    winners.append({'season': season_num, 'winner': winner_name})

# Sort by season and save
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"Total winners extracted for seasons 1–44: {len(winners_sorted)}")

output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Winners list saved to: {output_path}")
```