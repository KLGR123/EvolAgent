### Development Step 7: Find Organization and Sobradinho Dam Advocate for S√£o Francisco Basin Environmental Education Plan

**Description**: Conduct a comprehensive web search to identify the organization that launched the 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco' covering 505 municipalities and collaborates with Minist√©rios P√∫blicos. Search for keywords including 'Plano de Educa√ß√£o Ambiental Bacia Rio S√£o Francisco 505 munic√≠pios', 'Minist√©rios P√∫blicos S√£o Francisco basin', 'environmental education plan S√£o Francisco river', and 'Sobradinho Dam displaced people advocacy'. Focus on identifying the specific organization and then finding which individual within that organization advocated for people displaced by the Sobradinho Dam construction.

**Use Cases**:
- Automated stakeholder mapping for an environmental NGO identifying organizations and key advocates behind river basin education programs
- Investigative journalism workflow to uncover responsible agencies and spokespersons for major dam displacement cases
- Academic research tool for compiling comprehensive data on regional environmental education initiatives and their leading institutions
- Government audit process to verify public agencies‚Äô implementation of multi-municipality sustainability plans and responsible officials
- Grant proposal preparation by non-profits gathering detailed background on similar projects and contact persons for partnership outreach
- Corporate ESG assessment system that profiles environmental governance plans across river basins and the ministries or foundations involved
- Legal due diligence for law firms researching organizations and individuals linked to infrastructure projects with social impact liabilities
- Human rights advocacy platform tracking historical displacement campaigns and the key figures who represented affected communities

```
from ddgs import DDGS
import json
import os
import time

print("Starting comprehensive search for S√£o Francisco environmental education plan organization...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Define search queries in Portuguese and English
search_queries = [
    "Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco 505 munic√≠pios",
    "PEABRIOSF Minist√©rios P√∫blicos",
    "S√£o Francisco basin environmental education plan 505 municipalities",
    "Sobradinho Dam displaced people advocacy",
    "Barragem Sobradinho deslocados advocacia",
    "CHESF environmental education S√£o Francisco",
    "CODEVASF educa√ß√£o ambiental",
    "IBAMA S√£o Francisco basin plan",
    "ANA Bacia S√£o Francisco educa√ß√£o ambiental"
]

print(f"Will conduct {len(search_queries)} targeted searches...")

# Initialize DDGS searcher
searcher = DDGS(timeout=10)
all_search_results = {}

# Conduct searches for each query
for i, query in enumerate(search_queries, 1):
    print(f"\n[{i}/{len(search_queries)}] Searching: {query[:60]}...")
    
    try:
        # Search with multiple backends for reliability
        results = searcher.text(
            query, 
            max_results=10, 
            page=1, 
            backend=["google", "duckduckgo", "bing", "yahoo"], 
            safesearch="off", 
            region="pt-br"
        )
        
        if results:
            all_search_results[f"query_{i}"] = {
                'query': query,
                'results_count': len(results),
                'results': results
            }
            print(f"‚úì Found {len(results)} results")
            
            # Display top results for immediate analysis
            for j, result in enumerate(results[:3], 1):
                print(f"  {j}. {result.get('title', 'No title')[:80]}...")
                print(f"     URL: {result.get('href', 'No URL')[:100]}...")
                print(f"     Snippet: {result.get('body', 'No snippet')[:150]}...")
        else:
            print(f"‚úó No results found for this query")
            all_search_results[f"query_{i}"] = {
                'query': query,
                'results_count': 0,
                'results': []
            }
            
    except Exception as e:
        print(f"‚úó Error searching '{query}': {str(e)}")
        all_search_results[f"query_{i}"] = {
            'query': query,
            'error': str(e),
            'results_count': 0,
            'results': []
        }
    
    # Add delay between searches to be respectful
    time.sleep(2)

print(f"\n{'='*80}")
print("SEARCH RESULTS ANALYSIS")
print(f"{'='*80}")

# Save all search results
search_results_file = "workspace/sao_francisco_search_results.json"
with open(search_results_file, 'w', encoding='utf-8') as f:
    json.dump(all_search_results, f, indent=2, ensure_ascii=False)
print(f"\nAll search results saved to {search_results_file}")

# Analyze results for key organizations and individuals
organizations_found = set()
individuals_found = set()
key_findings = []

org_keywords = ['chesf', 'codevasf', 'ibama', 'ana', 'funda√ß√£o', 'instituto', 'minist√©rio p√∫blico']
individual_indicators = ['dr.', 'professor', 'coordenador', 'diretor', 'presidente']

print(f"\nüìä ANALYZING {sum(len(q.get('results', [])) for q in all_search_results.values())} TOTAL RESULTS...")

for query_key, query_data in all_search_results.items():
    if query_data.get('results'):
        for result in query_data['results']:
            title = result.get('title', '').lower()
            body = result.get('body', '').lower()
            url = result.get('href', '')
            
            # Look for organizations
            for org in org_keywords:
                if org in title or org in body:
                    organizations_found.add(org.upper())
            
            # Look for key findings related to the plan
            if any(term in title or term in body for term in ['plano', 'educa√ß√£o ambiental', 'bacia', 's√£o francisco']):
                key_findings.append({
                    'title': result.get('title', ''),
                    'url': url,
                    'snippet': result.get('body', '')[:300],
                    'query': query_data['query']
                })
            
            # Look for individual names and roles
            if any(indicator in title or indicator in body for indicator in individual_indicators):
                # Extract potential names (simplified approach)
                words = (title + ' ' + body).split()
                for i, word in enumerate(words):
                    if word.lower() in individual_indicators and i < len(words) - 1:
                        potential_name = ' '.join(words[i:i+3])  # Get next few words
                        individuals_found.add(potential_name)

print(f"\nüè¢ ORGANIZATIONS IDENTIFIED: {len(organizations_found)}")
for org in sorted(organizations_found):
    print(f"   ‚Ä¢ {org}")

print(f"\nüë• POTENTIAL INDIVIDUALS: {len(individuals_found)}")
for individual in sorted(list(individuals_found)[:10]):  # Show top 10
    print(f"   ‚Ä¢ {individual}")

print(f"\nüìã KEY FINDINGS: {len(key_findings)}")
for i, finding in enumerate(key_findings[:5], 1):  # Show top 5
    print(f"\n{i}. {finding['title']}")
    print(f"   URL: {finding['url']}")
    print(f"   Snippet: {finding['snippet'][:200]}...")
    print(f"   From query: {finding['query'][:50]}...")

# Save analysis results
analysis_results = {
    'search_date': time.strftime('%Y-%m-%d %H:%M:%S'),
    'total_queries': len(search_queries),
    'total_results': sum(len(q.get('results', [])) for q in all_search_results.values()),
    'organizations_found': list(organizations_found),
    'individuals_found': list(individuals_found)[:20],  # Limit to top 20
    'key_findings': key_findings[:10],  # Limit to top 10
    'search_queries': search_queries
}

analysis_file = "workspace/sao_francisco_analysis.json"
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)
print(f"\nAnalysis results saved to {analysis_file}")

print(f"\n{'='*80}")
print("SEARCH PHASE 1 COMPLETED")
print(f"{'='*80}")
print(f"\nüìà SUMMARY:")
print(f"   ‚Ä¢ Queries executed: {len(search_queries)}")
print(f"   ‚Ä¢ Total results found: {sum(len(q.get('results', [])) for q in all_search_results.values())}")
print(f"   ‚Ä¢ Organizations identified: {len(organizations_found)}")
print(f"   ‚Ä¢ Key findings: {len(key_findings)}")

print(f"\nüéØ NEXT STEPS:")
print(f"   ‚Ä¢ Analyze specific results for the main organization")
print(f"   ‚Ä¢ Search for individuals involved in Sobradinho Dam advocacy")
print(f"   ‚Ä¢ Focus on the most promising leads from initial findings")

print(f"\n‚úÖ INITIAL SEARCH COMPLETED SUCCESSFULLY!")
```