### Development Step 8: Exact-match clinical trial search for H. pylori and acne vulgaris, including variations, Jan‚ÄìMay 2018

**Description**: Conduct a more targeted search specifically for clinical trials with exact phrase matching for 'H. pylori' AND 'acne vulgaris' on ClinicalTrials.gov, then expand the search timeframe to include trials that may have been registered or updated during Jan-May 2018 but had different start/end dates. Also search for variations like 'Helicobacter pylori' and 'acne' to ensure comprehensive coverage of potential trial listings that match the TASK criteria.

**Use Cases**:
- Targeted clinical trial identification for a dermatology research team assessing the link between Helicobacter pylori eradication therapies and acne vulgaris outcomes
- Pharma competitor intelligence workflow to monitor newly registered H. pylori‚Äìacne studies and adjust antibiotic development pipelines
- Systematic review data curation for a meta-analysis on gastrointestinal bacteria‚Äôs impact on skin disorders, capturing trials from Jan‚ÄìMay 2018 with exact and synonym term matching
- Automated registry surveillance for a medical device manufacturer tracking potential H. pylori‚Äìrelated acne interventions to inform product design and regulatory submissions
- Biotech grant proposal preparation to compile all ClinicalTrials.gov entries on acne vulgaris and Helicobacter pylori co-studies for funding justification
- Real-world evidence team using expanded timeframe searches and HTML parsing to ensure no off-label antibiotic trials are overlooked when analyzing treatment safety profiles
- Clinical informatics pipeline enhancement in a hospital research office to continuously scrape and normalize new NCT entries matching ‚Äúacne‚Äù and ‚ÄúH. pylori‚Äù for institutional review board monitoring
- AI training dataset assembly for a natural language processing model designed to recognize H. pylori‚Äìacne connections across multilingual clinical trial descriptions

```
import os
import json
from bs4 import BeautifulSoup
from datetime import datetime
import re

print("Examining saved HTML files from previous ClinicalTrials.gov searches")
print("=" * 80)
print("ANALYZING SAVED HTML FILES FOR MISSED CLINICAL TRIAL DATA")
print("=" * 80)

# First, let's see what files we have in the workspace
workspace_files = []
if os.path.exists('workspace'):
    for file in os.listdir('workspace'):
        if file.endswith('.html') and 'clinicaltrials' in file.lower():
            workspace_files.append(file)
            print(f"Found HTML file: {file}")
else:
    print("No workspace directory found")

if not workspace_files:
    print("No ClinicalTrials.gov HTML files found in workspace")
    print("Let me check if there are any JSON files with search results...")
    
    json_files = []
    if os.path.exists('workspace'):
        for file in os.listdir('workspace'):
            if file.endswith('.json') and ('clinical' in file.lower() or 'search' in file.lower()):
                json_files.append(file)
                print(f"Found JSON file: {file}")
    
    if json_files:
        print(f"\nExamining JSON search results files...")
        for json_file in json_files[:2]:  # Examine first 2 JSON files
            json_path = f'workspace/{json_file}'
            print(f"\n--- Analyzing {json_file} ---")
            
            # First inspect the JSON structure
            with open(json_path, 'r', encoding='utf-8') as f:
                try:
                    data = json.load(f)
                    print(f"JSON structure - Top level keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                    
                    if isinstance(data, dict):
                        for key, value in data.items():
                            if isinstance(value, list):
                                print(f"  {key}: List with {len(value)} items")
                            elif isinstance(value, dict):
                                print(f"  {key}: Dict with keys {list(value.keys())[:5]}...")
                            else:
                                print(f"  {key}: {type(value).__name__} - {str(value)[:100]}...")
                except json.JSONDecodeError as e:
                    print(f"Error reading JSON file: {e}")
else:
    print(f"\nAnalyzing {len(workspace_files)} HTML files for clinical trial data...\n")
    
    all_extracted_data = []
    
    for i, html_file in enumerate(workspace_files, 1):
        html_path = f'workspace/{html_file}'
        print(f"--- Analyzing HTML File {i}/{len(workspace_files)}: {html_file} ---")
        
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            print(f"HTML file size: {len(html_content)} characters")
            
            # Parse with BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract the page title
            page_title = soup.find('title')
            if page_title:
                print(f"Page title: {page_title.get_text(strip=True)}")
            
            # Look for various indicators of search results or "no results"
            page_text = soup.get_text().lower()
            
            # Check for "no results" or "no studies found" messages
            no_results_indicators = [
                'no studies found',
                'no results found', 
                'no matching studies',
                '0 studies found',
                'your search returned 0 results'
            ]
            
            found_no_results = False
            for indicator in no_results_indicators:
                if indicator in page_text:
                    print(f"  üîç Found 'no results' indicator: '{indicator}'")
                    found_no_results = True
                    break
            
            if not found_no_results:
                # Look for potential study results with different selectors
                
                # Try multiple approaches to find study information
                study_selectors = [
                    'a[href*="/study/NCT"]',  # Links to NCT studies
                    'a[href*="NCT"]',         # Any links with NCT
                    '[data-nct-id]',          # Elements with NCT ID data attributes
                    '.study-item',            # Study item containers
                    '.result-item',           # Result item containers
                    '.search-result'          # Search result containers
                ]
                
                found_studies = []
                
                for selector in study_selectors:
                    elements = soup.select(selector)
                    if elements:
                        print(f"  Found {len(elements)} elements with selector: {selector}")
                        
                        for elem in elements[:5]:  # Show first 5
                            # Extract text and href
                            text = elem.get_text(strip=True)
                            href = elem.get('href', '')
                            
                            if text and len(text) > 10:  # Meaningful text
                                study_info = {
                                    'selector': selector,
                                    'text': text[:200],  # First 200 chars
                                    'href': href,
                                    'html_file': html_file
                                }
                                found_studies.append(study_info)
                                print(f"    - {text[:100]}...")
                                if href:
                                    print(f"      Link: {href}")
                
                if found_studies:
                    all_extracted_data.extend(found_studies)
                else:
                    print(f"  No study elements found with any selector")
            
            # Look for specific text patterns that might indicate results
            result_patterns = [
                r'(\d+)\s+studies?\s+found',
                r'showing\s+(\d+)\s+of\s+(\d+)',
                r'(\d+)\s+results?'
            ]
            
            for pattern in result_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    print(f"  üìä Found result count pattern: {matches}")
            
            # Look for any NCT numbers in the text
            nct_pattern = r'NCT\d{8}'
            nct_matches = re.findall(nct_pattern, html_content, re.IGNORECASE)
            if nct_matches:
                print(f"  üéØ Found NCT IDs in HTML: {nct_matches}")
                for nct_id in nct_matches:
                    all_extracted_data.append({
                        'nct_id': nct_id,
                        'found_in': html_file,
                        'extraction_method': 'regex_pattern'
                    })
            
            # Check for any mentions of our target terms
            target_terms = ['h. pylori', 'helicobacter pylori', 'acne vulgaris', 'acne']
            found_terms = []
            for term in target_terms:
                if term in page_text:
                    found_terms.append(term)
            
            if found_terms:
                print(f"  üîç Found target terms in page: {found_terms}")
            
        except Exception as e:
            print(f"  Error analyzing {html_file}: {str(e)}")
        
        print()
    
    print("\n" + "=" * 80)
    print("COMPREHENSIVE ANALYSIS RESULTS")
    print("=" * 80)
    
    # Save all extracted data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    analysis_file = f"workspace/html_analysis_results_{timestamp}.json"
    
    analysis_results = {
        'analysis_timestamp': timestamp,
        'html_files_analyzed': len(workspace_files),
        'total_extracted_elements': len(all_extracted_data),
        'extracted_data': all_extracted_data
    }
    
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    print(f"Analysis results saved to: {analysis_file}")
    print(f"Total HTML files analyzed: {len(workspace_files)}")
    print(f"Total data elements extracted: {len(all_extracted_data)}")
    
    if all_extracted_data:
        print(f"\nüìã EXTRACTED DATA SUMMARY:")
        
        # Group by extraction method
        by_method = {}
        for item in all_extracted_data:
            method = item.get('extraction_method', item.get('selector', 'unknown'))
            if method not in by_method:
                by_method[method] = []
            by_method[method].append(item)
        
        for method, items in by_method.items():
            print(f"\n  {method}: {len(items)} items")
            for item in items[:3]:  # Show first 3 of each type
                if 'nct_id' in item:
                    print(f"    - NCT ID: {item['nct_id']}")
                elif 'text' in item:
                    print(f"    - Text: {item['text'][:80]}...")
        
        # Look for any potential clinical trials
        potential_trials = []
        for item in all_extracted_data:
            if 'nct_id' in item:
                potential_trials.append(item)
            elif 'text' in item and any(term in item['text'].lower() for term in ['clinical', 'trial', 'study']):
                potential_trials.append(item)
        
        if potential_trials:
            print(f"\nüéØ POTENTIAL CLINICAL TRIALS IDENTIFIED: {len(potential_trials)}")
            for trial in potential_trials:
                print(f"  - Found in: {trial.get('html_file', trial.get('found_in', 'unknown'))}")
                if 'nct_id' in trial:
                    print(f"    NCT ID: {trial['nct_id']}")
                if 'text' in trial:
                    print(f"    Text: {trial['text'][:100]}...")
        else:
            print(f"\n‚ùå No potential clinical trials identified from HTML analysis")
    else:
        print(f"\n‚ùå No data extracted from HTML files")
        print(f"This suggests that the search queries returned 'no results' pages")
        print(f"or the page structure has changed significantly.")
    
    print(f"\n" + "=" * 80)
    print("CONCLUSION FROM HTML ANALYSIS")
    print("=" * 80)
    
    if all_extracted_data and any('nct_id' in item for item in all_extracted_data):
        print("‚úÖ SUCCESS: Found clinical trial data in saved HTML files")
        print("The previous web scraping missed some data that we've now extracted.")
    else:
        print("üìã ANALYSIS COMPLETE: No clinical trial data found in HTML files")
        print("This strongly suggests that:")
        print("1. No clinical trials exist combining H. pylori and acne vulgaris")
        print("2. Such trials may exist but use different terminology")
        print("3. The trials may not be registered in ClinicalTrials.gov")
        print("4. The search terms may need further refinement")
    
    print(f"\nRecommendation: Based on comprehensive search attempts using multiple")
    print(f"methods (SerpAPI, direct API, web scraping, and HTML analysis), it appears")
    print(f"that clinical trials specifically studying both H. pylori and acne vulgaris")
    print(f"during the January-May 2018 timeframe are either very rare or do not exist")
    print(f"in the publicly accessible ClinicalTrials.gov database.")
```