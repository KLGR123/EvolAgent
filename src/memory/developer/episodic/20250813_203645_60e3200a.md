### Development Step 2: Title:  
Identify US Survivor Winners Born in May Using Official Sources and Winner Birth Date Data

**Description**: Search for comprehensive information about US Survivor winners and their birth dates. Focus on finding official sources such as CBS Survivor databases, Wikipedia pages, or entertainment databases that list all winners from Season 1 through the most recent season available as of August 2023. Extract winner names, seasons, and birth dates to identify any winners born in May.

**Use Cases**:
- Entertainment journalism research to quickly compile verified lists of Survivor winners and their birth dates for feature articles or anniversary retrospectives
- Automated database enrichment for TV trivia apps, ensuring up-to-date and accurate winner profiles including birth month for user quizzes
- Academic studies analyzing demographic trends among reality show winners, such as birth month correlations and age distributions across seasons
- Social media campaign planning for fan pages, identifying Survivor winners with May birthdays to create targeted celebratory posts
- Talent agency scouting and profiling, using extracted winner data to identify potential candidates for interviews or appearances based on birth dates
- Television network archival management, streamlining the process of updating official Survivor winner records with season and biographical details

```
import os
import sys
import json
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time

# 1) Use existing workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Load the previously saved HTML page
html_path = os.path.join(workspace_dir, 'survivor_main_page.html')
if not os.path.exists(html_path):
    print("❌ survivor_main_page.html not found. Need to fetch page first.")
    sys.exit(1)

print("Loading previously saved Survivor main page HTML...")
with open(html_path, 'r', encoding='utf-8') as f:
    html_content = f.read()

# 3) Parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')
print("HTML parsed successfully\n")

# 4) Find the winners table (we know from reconnaissance it's Table 1)
print("Locating the main winners table...")
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} wikitable(s)\n")

if len(tables) == 0:
    print("❌ No wikitables found")
    sys.exit(1)

# Get the first table which contains the winners
winners_table = tables[0]
print("Using first table (contains Season, Winner, etc.)")

# 5) Parse the table headers to find column indices
header_row = winners_table.find('tr')
headers = []
for cell in header_row.find_all(['th', 'td']):
    header_text = cell.get_text(strip=True)
    headers.append(header_text)

print(f"Table headers: {headers}")

# Find the indices for Season and Winner columns
try:
    season_idx = headers.index('Season')
    winner_idx = headers.index('Winner')
    print(f"Column indices -> Season: {season_idx}, Winner: {winner_idx}\n")
except ValueError as e:
    print(f"❌ Could not find required columns: {e}")
    sys.exit(1)

# 6) Extract all winners from the table
print("Extracting winners from the table...")
winners = []
rows = winners_table.find_all('tr')[1:]  # Skip header row

for i, row in enumerate(rows):
    cells = row.find_all(['td', 'th'])
    if len(cells) <= max(season_idx, winner_idx):
        print(f"  Skipping row {i+1}: insufficient columns ({len(cells)})")
        continue
    
    # Extract season number
    season_text = cells[season_idx].get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"  Skipping row {i+1}: invalid season '{season_text}'")
        continue
    
    # Extract winner name
    winner_cell = cells[winner_idx]
    # Look for a link first (more reliable)
    winner_link = winner_cell.find('a')
    if winner_link:
        winner_name = winner_link.get_text(strip=True)
        winner_wiki_link = winner_link.get('href', '')
    else:
        winner_name = winner_cell.get_text(strip=True)
        winner_wiki_link = ''
    
    # Clean up winner name (remove footnotes, etc.)
    winner_name = re.sub(r'\[.*?\]', '', winner_name).strip()
    
    if winner_name and season_num > 0:
        winner_info = {
            'season': season_num,
            'winner': winner_name,
            'wiki_link': winner_wiki_link
        }
        winners.append(winner_info)
        print(f"  Season {season_num:2d}: {winner_name} (link: {winner_wiki_link})")

print(f"\nExtracted {len(winners)} winners total\n")

# 7) Sort winners by season
winners_sorted = sorted(winners, key=lambda x: x['season'])

# 8) Save the winners list
winners_path = os.path.join(workspace_dir, 'survivor_winners_extracted.json')
with open(winners_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Saved complete winners list to: {winners_path}")

# 9) Display summary statistics
print(f"\n=== WINNERS EXTRACTION SUMMARY ===")
print(f"Total winners extracted: {len(winners_sorted)}")
print(f"Season range: {min(w['season'] for w in winners_sorted)} - {max(w['season'] for w in winners_sorted)}")
print(f"Winners with Wikipedia links: {sum(1 for w in winners_sorted if w['wiki_link'])}")
print(f"Winners without Wikipedia links: {sum(1 for w in winners_sorted if not w['wiki_link'])}")

# 10) Show first 10 and last 5 winners for verification
print(f"\nFirst 10 winners:")
for w in winners_sorted[:10]:
    print(f"  Season {w['season']:2d}: {w['winner']}")

print(f"\nLast 5 winners:")
for w in winners_sorted[-5:]:
    print(f"  Season {w['season']:2d}: {w['winner']}")

print(f"\nNext step: Fetch individual winner Wikipedia pages to extract birth dates")
```