### Development Step 2: Extract Axis Labels from a Three-Axis Figure in June 2022 arXiv AI Regulation Papers

**Description**: Search arXiv.org for AI regulation papers originally submitted in June 2022. Focus on identifying papers that discuss artificial intelligence regulation, policy, governance, or related topics. Look for papers with submission dates in June 2022 and examine their content to find one containing a figure with three axes where each axis has label words at both ends. Extract the specific words used as axis labels in this figure.

**Use Cases**:
- Academic literature review automation for policy researchers investigating the evolution of AI regulation frameworks in mid-2022
- Legal tech firms compiling evidence of regulatory trends for AI compliance consulting and client briefings
- Government agencies tracking the emergence of AI governance topics to inform national policy development
- Think tanks or NGOs mapping the landscape of AI ethics scholarship for strategic advocacy planning
- University instructors preparing course materials on AI policy by sourcing recent, relevant case studies and figures
- Data journalists visualizing the growth and focus areas of AI regulation research for news features
- Corporate compliance teams benchmarking their AI governance practices against the latest academic insights
- AI startup founders identifying gaps in regulatory research to guide responsible product development and investor communications

```
import os
import requests
import json
from datetime import datetime
import time
import xml.etree.ElementTree as ET

print('=== EXPANDED ARXIV SEARCH FOR AI REGULATION PAPERS FROM JUNE 2022 ===')
print('Objective: Find papers on AI regulation/policy/governance submitted in June 2022')
print('Strategy: Use broader search terms and different matching approaches\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# First, let's inspect the existing results to understand what we found
existing_file = os.path.join(workspace, 'arxiv_june_2022_ai_regulation_papers.json')
if os.path.exists(existing_file):
    print('=== INSPECTING EXISTING SEARCH RESULTS ===')
    with open(existing_file, 'r', encoding='utf-8') as f:
        existing_data = json.load(f)
    
    print(f'Previous search found {existing_data.get("june_2022_papers_count", 0)} papers from June 2022')
    print(f'Search queries used: {existing_data.get("search_queries_used", [])}\n')
    
    if 'june_2022_papers' in existing_data:
        for paper in existing_data['june_2022_papers']:
            print(f'Found paper: "{paper["title"]}"')
            print(f'Summary preview: {paper["summary"][:150]}...')
            print(f'This appears to be about: {"AI regulation" if any(term in paper["summary"].lower() for term in ["regulation", "policy", "governance", "ethics"]) else "Technical AI/ML methods"}\n')

# arXiv API base URL
arxiv_base_url = 'http://export.arxiv.org/api/query'

# Expanded search terms - using broader and more varied approaches
expanded_search_queries = [
    # Broader policy terms without exact phrase matching
    'artificial intelligence AND policy',
    'AI AND governance',
    'machine learning AND regulation',
    'algorithmic AND accountability',
    'AI AND ethics AND policy',
    'artificial intelligence AND law',
    'AI AND safety AND policy',
    'machine learning AND ethics',
    'algorithmic AND bias AND policy',
    'AI AND fairness AND regulation',
    'artificial intelligence AND society',
    'AI AND transparency',
    'machine learning AND fairness',
    'algorithmic AND governance',
    'AI AND responsible',
    # More specific regulatory terms
    'AI regulation',
    'algorithmic regulation',
    'AI policy',
    'machine learning policy',
    'AI governance',
    'algorithmic governance',
    'AI ethics',
    'responsible AI',
    'AI accountability',
    'algorithmic accountability'
]

print(f'=== STEP 1: EXPANDED SEARCH WITH {len(expanded_search_queries)} QUERY TERMS ===')

all_papers = []
search_results = {}

for i, query in enumerate(expanded_search_queries, 1):
    print(f'\nSearch {i}/{len(expanded_search_queries)}: "{query}"')
    
    # arXiv search parameters - using different search approaches
    if ' AND ' in query:
        # For AND queries, search in all fields
        search_query = f'all:{query}'
    else:
        # For single terms, search in title, abstract, and keywords
        search_query = f'ti:"{query}" OR abs:"{query}"'
    
    params = {
        'search_query': search_query,
        'start': 0,
        'max_results': 100,  # Increased to get more results
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    try:
        response = requests.get(arxiv_base_url, params=params, timeout=30)
        print(f'Status code: {response.status_code}')
        
        if response.status_code == 200:
            # Parse XML to extract paper information
            try:
                root = ET.fromstring(response.text)
                
                # Define XML namespaces
                namespaces = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                entries = root.findall('atom:entry', namespaces)
                print(f'Found {len(entries)} papers for query "{query}"')
                
                query_papers = []
                
                for entry in entries:
                    # Extract paper details
                    title_elem = entry.find('atom:title', namespaces)
                    title = title_elem.text.strip() if title_elem is not None else 'No title'
                    
                    summary_elem = entry.find('atom:summary', namespaces)
                    summary = summary_elem.text.strip() if summary_elem is not None else 'No summary'
                    
                    published_elem = entry.find('atom:published', namespaces)
                    published = published_elem.text.strip() if published_elem is not None else 'No date'
                    
                    # Get arXiv ID and PDF link
                    id_elem = entry.find('atom:id', namespaces)
                    arxiv_id = id_elem.text.strip() if id_elem is not None else 'No ID'
                    
                    # Extract arXiv ID from URL
                    if 'arxiv.org/abs/' in arxiv_id:
                        arxiv_id = arxiv_id.split('/')[-1]
                    
                    # Construct PDF URL
                    pdf_url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'
                    
                    # Get authors
                    authors = []
                    author_elems = entry.findall('atom:author', namespaces)
                    for author_elem in author_elems:
                        name_elem = author_elem.find('atom:name', namespaces)
                        if name_elem is not None:
                            authors.append(name_elem.text.strip())
                    
                    # Get categories
                    categories = []
                    category_elems = entry.findall('atom:category', namespaces)
                    for cat_elem in category_elems:
                        term = cat_elem.get('term')
                        if term:
                            categories.append(term)
                    
                    paper_info = {
                        'title': title,
                        'authors': authors,
                        'summary': summary,
                        'published': published,
                        'arxiv_id': arxiv_id,
                        'pdf_url': pdf_url,
                        'categories': categories,
                        'search_query': query
                    }
                    
                    query_papers.append(paper_info)
                    all_papers.append(paper_info)
                
                search_results[query] = query_papers
                print(f'âœ“ Extracted {len(query_papers)} papers from XML')
                
            except ET.ParseError as e:
                print(f'Error parsing XML for query "{query}": {str(e)}')
                continue
        
        else:
            print(f'Failed to search arXiv: {response.status_code}')
            continue
    
    except Exception as e:
        print(f'Error searching for "{query}": {str(e)}')
        continue
    
    # Small delay between requests to be respectful to arXiv
    time.sleep(0.5)

print(f'\n=== STEP 2: FILTERING PAPERS BY JUNE 2022 AND RELEVANCE ===') 
print(f'Total papers found across all queries: {len(all_papers)}')

# Remove duplicates based on arXiv ID
unique_papers = {}
for paper in all_papers:
    arxiv_id = paper['arxiv_id']
    if arxiv_id not in unique_papers:
        unique_papers[arxiv_id] = paper
    else:
        # Keep the one with more search queries (indicates higher relevance)
        existing_queries = unique_papers[arxiv_id].get('matching_queries', [unique_papers[arxiv_id]['search_query']])
        new_queries = existing_queries + [paper['search_query']]
        unique_papers[arxiv_id]['matching_queries'] = list(set(new_queries))

print(f'Unique papers after deduplication: {len(unique_papers)}')

# Filter papers by June 2022 submission date and relevance
june_2022_papers = []
relevant_keywords = [
    'regulation', 'policy', 'governance', 'ethics', 'law', 'legal', 'regulatory',
    'accountability', 'transparency', 'fairness', 'bias', 'responsible', 'safety',
    'society', 'social', 'impact', 'framework', 'guidelines', 'standards',
    'compliance', 'oversight', 'audit', 'explainable', 'interpretable'
]

for paper in unique_papers.values():
    published_date = paper['published']
    
    # Parse the date (arXiv format: 2022-06-15T17:59:59Z)
    try:
        if published_date and published_date != 'No date':
            # Extract year and month from date string
            if published_date.startswith('2022-06'):
                # Check if the paper is actually about AI regulation/policy
                title_lower = paper['title'].lower()
                summary_lower = paper['summary'].lower()
                combined_text = f'{title_lower} {summary_lower}'
                
                # Count relevant keywords
                relevance_score = sum(1 for keyword in relevant_keywords if keyword in combined_text)
                
                # Also check for AI/ML terms to ensure it's AI-related
                ai_terms = ['artificial intelligence', 'ai ', 'machine learning', 'ml ', 'deep learning', 'neural', 'algorithm']
                has_ai_terms = any(term in combined_text for term in ai_terms)
                
                paper['relevance_score'] = relevance_score
                paper['has_ai_terms'] = has_ai_terms
                
                # Include if it has both AI terms and policy relevance
                if has_ai_terms and relevance_score > 0:
                    june_2022_papers.append(paper)
                    print(f'âœ“ Relevant June 2022 paper found: "{paper["title"][:80]}..."')
                    print(f'  Published: {published_date}')
                    print(f'  arXiv ID: {paper["arxiv_id"]}')
                    print(f'  Relevance score: {relevance_score} (keywords: {[kw for kw in relevant_keywords if kw in combined_text][:5]})')
                    print(f'  Categories: {paper.get("categories", [])[:3]}')
                    print(f'  Authors: {", ".join(paper["authors"][:2])}{"..." if len(paper["authors"]) > 2 else ""}')
                    print()
                elif published_date.startswith('2022-06'):
                    print(f'âš  June 2022 paper found but low relevance: "{paper["title"][:60]}..."')
                    print(f'  Relevance score: {relevance_score}, Has AI terms: {has_ai_terms}')
                    print()
    
    except Exception as e:
        print(f'Error parsing date for paper: {str(e)}')
        continue

print(f'\n=== FILTERING RESULTS ===') 
print(f'Relevant AI regulation papers from June 2022: {len(june_2022_papers)}')

if june_2022_papers:
    # Sort by relevance score
    june_2022_papers.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    # Save June 2022 papers for detailed analysis
    june_papers_file = os.path.join(workspace, 'arxiv_june_2022_ai_regulation_papers_expanded.json')
    with open(june_papers_file, 'w', encoding='utf-8') as f:
        json.dump({
            'search_date': datetime.now().isoformat(),
            'total_papers_found': len(all_papers),
            'unique_papers_count': len(unique_papers),
            'june_2022_relevant_papers_count': len(june_2022_papers),
            'search_queries_used': expanded_search_queries,
            'relevance_keywords': relevant_keywords,
            'june_2022_papers': june_2022_papers
        }, f, indent=2, ensure_ascii=False)
    
    print(f'âœ“ June 2022 relevant papers saved to: {june_papers_file}')
    
    # Display summary of June 2022 papers
    print(f'\n=== JUNE 2022 AI REGULATION PAPERS SUMMARY (RANKED BY RELEVANCE) ===')
    for i, paper in enumerate(june_2022_papers, 1):
        print(f'\n{i}. Title: {paper["title"]}')
        print(f'   arXiv ID: {paper["arxiv_id"]}')
        print(f'   Published: {paper["published"]}')
        print(f'   Relevance Score: {paper.get("relevance_score", 0)}')
        print(f'   Categories: {", ".join(paper.get("categories", [])[:3])}')
        print(f'   Authors: {", ".join(paper["authors"][:2])}{"..." if len(paper["authors"]) > 2 else ""}')
        print(f'   PDF: {paper["pdf_url"]}')
        print(f'   Summary preview: {paper["summary"][:200]}...')
        
        # Show matching search queries if available
        if 'matching_queries' in paper:
            print(f'   Matched queries: {", ".join(paper["matching_queries"][:3])}{"..." if len(paper.get("matching_queries", [])) > 3 else ""}')
    
    print(f'\n=== NEXT STEPS ===')
    print(f'âœ“ Found {len(june_2022_papers)} relevant AI regulation papers from June 2022')
    print(f'âœ“ Papers ranked by policy relevance (regulation/governance keywords)')
    print(f'âœ“ Ready to download and analyze PDFs for figures with three axes')
    print(f'âœ“ Will search for figures with axis labels at both ends')
    
else:
    print('\nâš  No relevant AI regulation papers found from June 2022')
    print('Will save all search results for further analysis')
    
    # Save all search results for analysis
    all_results_file = os.path.join(workspace, 'arxiv_expanded_search_results.json')
    with open(all_results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'search_date': datetime.now().isoformat(),
            'total_papers_found': len(all_papers),
            'unique_papers_count': len(unique_papers),
            'search_queries_used': expanded_search_queries,
            'sample_papers': list(unique_papers.values())[:10]  # Save first 10 for inspection
        }, f, indent=2, ensure_ascii=False)
    
    print(f'All expanded search results saved to: {all_results_file}')
    print('Will analyze results and consider alternative approaches')

print('\n*** EXPANDED ARXIV SEARCH PHASE COMPLETE ***')
```