### Development Step 2: Charles 'Pete' Conrad: NASA Astronaut Group Membership and Selection Year Identification

**Description**: Research Charles 'Pete' Conrad's NASA Astronaut Group membership to determine which astronaut group he belonged to. Look for official NASA records, astronaut biographies, or space agency databases that specify Conrad's astronaut group number and selection year. This information is typically found in NASA's official astronaut biographical data or space history resources.

**Use Cases**:
- Space history researchers compiling detailed astronaut biographies for publication or archival purposes
- Museum curators verifying astronaut group memberships for accurate exhibit labeling and educational displays
- Documentary filmmakers fact-checking astronaut selection details for on-screen graphics and narration
- Aerospace educators preparing classroom materials on NASA astronaut selection processes and group histories
- Journalists writing feature articles about Apollo missions and needing authoritative astronaut group information
- NASA archivists digitizing and cross-referencing astronaut records for internal databases and public access
- Genealogists or family historians tracing the careers of relatives involved in the space program
- Science podcast producers sourcing verified astronaut background details for episode scripts and interviews

```
import os
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

print("=== RESEARCH: CHARLES 'PETE' CONRAD'S NASA ASTRONAUT GROUP ===\n")
print("Objective: Determine which NASA astronaut group Charles 'Pete' Conrad belonged to and his selection year\n")

# Create workspace directory if needed
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# From the previous APOD analysis, we know Charles 'Pete' Conrad was an Apollo 12 astronaut
# Let's research his NASA astronaut group membership using official NASA sources

print("=== APPROACH 1: NASA OFFICIAL ASTRONAUT BIOGRAPHY ===\n")

# NASA maintains official astronaut biographies
nasa_astronaut_urls = [
    "https://www.nasa.gov/people/charles-pete-conrad/",
    "https://www.nasa.gov/astronauts/biographies/charles-conrad.html",
    "https://history.nasa.gov/SP-4029/Apollo_12a_Conrad_Biography.htm",
    "https://www.jsc.nasa.gov/Bios/htmlbios/conrad-cp.html"
]

print(f"Attempting to access {len(nasa_astronaut_urls)} potential NASA biography URLs for Charles Conrad...\n")

conrad_bio_data = []

for i, url in enumerate(nasa_astronaut_urls, 1):
    print(f"URL {i}: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print(f"  ‚úì Successfully accessed (Status: {response.status_code})")
            print(f"  Content length: {len(response.content):,} bytes")
            
            # Parse the content
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract page title
            page_title = soup.find('title')
            title_text = page_title.get_text().strip() if page_title else 'No title found'
            print(f"  Page title: {title_text}")
            
            # Save the HTML content
            html_filename = f'workspace/conrad_bio_{i}.html'
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f"  ‚úì HTML saved to: {html_filename}")
            
            # Extract text content
            page_text = soup.get_text()
            
            # Search for astronaut group information
            print(f"\n  === SEARCHING FOR ASTRONAUT GROUP INFORMATION ===\n")
            
            # Key phrases that indicate astronaut group membership
            group_keywords = [
                'astronaut group',
                'group 2',
                'group two', 
                'second group',
                'selected',
                'selection',
                'chosen',
                'recruited',
                '1962',  # Group 2 was selected in 1962
                'new nine',  # Group 2 was known as "The New Nine"
                'next nine'
            ]
            
            found_keywords = []
            keyword_contexts = []
            
            for keyword in group_keywords:
                if keyword.lower() in page_text.lower():
                    found_keywords.append(keyword)
                    
                    # Find context around the keyword
                    import re
                    matches = list(re.finditer(re.escape(keyword), page_text, re.IGNORECASE))
                    
                    for match in matches[:2]:  # Show first 2 matches
                        start = max(0, match.start() - 200)
                        end = min(len(page_text), match.end() + 300)
                        context = page_text[start:end].strip()
                        
                        keyword_contexts.append({
                            'keyword': keyword,
                            'context': context
                        })
                        
                        print(f"  Found '{keyword}' - Context: ...{context[:150]}...")
            
            # Look for specific years and group numbers
            print(f"\n  === SEARCHING FOR SPECIFIC DATES AND GROUP NUMBERS ===\n")
            
            # Search for patterns like "Group 2", "1962", etc.
            group_patterns = [
                r'[Gg]roup\s+(\d+|[Tt]wo|2)',
                r'(\d{4})\s+astronaut',
                r'astronaut\s+(\d{4})',
                r'selected\s+in\s+(\d{4})',
                r'chosen\s+in\s+(\d{4})',
                r'[Nn]ew\s+[Nn]ine',
                r'[Nn]ext\s+[Nn]ine'
            ]
            
            pattern_matches = []
            
            for pattern in group_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    print(f"  Pattern '{pattern}' found: {matches}")
                    pattern_matches.extend(matches)
                    
                    # Get context for these matches
                    for match_obj in re.finditer(pattern, page_text, re.IGNORECASE):
                        start = max(0, match_obj.start() - 150)
                        end = min(len(page_text), match_obj.end() + 150)
                        context = page_text[start:end].strip()
                        print(f"    Context: ...{context}...")
            
            # Store biography data
            bio_entry = {
                'url': url,
                'url_number': i,
                'title': title_text,
                'html_filename': html_filename,
                'content_length': len(page_text),
                'found_keywords': found_keywords,
                'keyword_contexts': keyword_contexts,
                'pattern_matches': pattern_matches,
                'access_successful': True
            }
            
            conrad_bio_data.append(bio_entry)
            
            # Save clean text for this biography
            text_filename = f'workspace/conrad_bio_{i}_text.txt'
            with open(text_filename, 'w', encoding='utf-8') as f:
                f.write(f"Charles 'Pete' Conrad Biography - Source {i}\n")
                f.write(f"URL: {url}\n")
                f.write(f"Title: {title_text}\n")
                f.write(f"Extraction Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 80 + "\n")
                f.write(page_text)
            
            print(f"  ‚úì Text saved to: {text_filename}")
            
        elif response.status_code == 404:
            print(f"  ‚úó Page not found (404)")
            conrad_bio_data.append({
                'url': url,
                'url_number': i,
                'access_successful': False,
                'error': '404 Not Found'
            })
        else:
            print(f"  ‚úó Failed to access (Status: {response.status_code})")
            conrad_bio_data.append({
                'url': url,
                'url_number': i,
                'access_successful': False,
                'error': f'HTTP {response.status_code}'
            })
            
    except Exception as e:
        print(f"  ‚úó Exception: {str(e)}")
        conrad_bio_data.append({
            'url': url,
            'url_number': i,
            'access_successful': False,
            'error': str(e)
        })
    
    print()  # Space between URLs

print("=== APPROACH 2: NASA ASTRONAUT GROUP HISTORICAL RECORDS ===\n")

# Access NASA's historical astronaut group information
nasa_group_urls = [
    "https://www.nasa.gov/astronauts/",
    "https://history.nasa.gov/SP-4029/Apollo_00g_Table_of_Contents.htm",
    "https://www.jsc.nasa.gov/history/oral_histories/astronauts.htm",
    "https://en.wikipedia.org/wiki/NASA_Astronaut_Group_2"  # As a reference source
]

print(f"Accessing {len(nasa_group_urls)} NASA astronaut group historical sources...\n")

group_data = []

for i, url in enumerate(nasa_group_urls, 1):
    print(f"Group Source {i}: {url}")
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print(f"  ‚úì Successfully accessed (Status: {response.status_code})")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text()
            
            # Save HTML
            group_html_filename = f'workspace/astronaut_groups_{i}.html'
            with open(group_html_filename, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f"  ‚úì HTML saved to: {group_html_filename}")
            
            # Search for Conrad specifically in group listings
            print(f"\n  === SEARCHING FOR CONRAD IN GROUP LISTINGS ===\n")
            
            conrad_patterns = [
                'Conrad',
                'Pete Conrad',
                'Charles Conrad',
                'C. Conrad',
                'Charles \"Pete\" Conrad',
                'Charles P. Conrad'
            ]
            
            conrad_found = False
            conrad_contexts = []
            
            for pattern in conrad_patterns:
                if pattern in page_text:
                    conrad_found = True
                    print(f"  ‚òÖ Found '{pattern}' in group source")
                    
                    # Get context around Conrad's name
                    import re
                    matches = list(re.finditer(re.escape(pattern), page_text, re.IGNORECASE))
                    
                    for match in matches[:2]:  # Show first 2 matches
                        start = max(0, match.start() - 250)
                        end = min(len(page_text), match.end() + 250)
                        context = page_text[start:end].strip()
                        
                        conrad_contexts.append({
                            'pattern': pattern,
                            'context': context
                        })
                        
                        print(f"    Context: ...{context[:200]}...")
                    break  # Found Conrad, no need to check other patterns
            
            if not conrad_found:
                print(f"  Conrad not found in this source")
            
            # Look for Group 2 or "New Nine" information
            print(f"\n  === SEARCHING FOR GROUP 2 / NEW NINE INFORMATION ===\n")
            
            group2_keywords = [
                'Group 2',
                'group two',
                'New Nine',
                'Next Nine',
                'second group',
                '1962',
                'September 1962'
            ]
            
            group2_info = []
            
            for keyword in group2_keywords:
                if keyword.lower() in page_text.lower():
                    print(f"  Found '{keyword}' in group source")
                    
                    # Get context
                    matches = list(re.finditer(re.escape(keyword), page_text, re.IGNORECASE))
                    
                    for match in matches[:1]:  # Show first match
                        start = max(0, match.start() - 300)
                        end = min(len(page_text), match.end() + 400)
                        context = page_text[start:end].strip()
                        
                        group2_info.append({
                            'keyword': keyword,
                            'context': context
                        })
                        
                        print(f"    Context: ...{context[:250]}...")
            
            group_entry = {
                'url': url,
                'source_number': i,
                'html_filename': group_html_filename,
                'conrad_found': conrad_found,
                'conrad_contexts': conrad_contexts,
                'group2_info': group2_info,
                'access_successful': True
            }
            
            group_data.append(group_entry)
            
        else:
            print(f"  ‚úó Failed to access (Status: {response.status_code})")
            group_data.append({
                'url': url,
                'source_number': i,
                'access_successful': False,
                'error': f'HTTP {response.status_code}'
            })
            
    except Exception as e:
        print(f"  ‚úó Exception: {str(e)}")
        group_data.append({
            'url': url,
            'source_number': i,
            'access_successful': False,
            'error': str(e)
        })
    
    print()  # Space between sources

print("=== COMPREHENSIVE ANALYSIS AND SUMMARY ===\n")

# Compile all findings
research_summary = {
    'research_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'objective': "Determine Charles 'Pete' Conrad's NASA astronaut group membership",
    'astronaut_name': "Charles 'Pete' Conrad",
    'biography_sources': conrad_bio_data,
    'group_sources': group_data,
    'successful_bio_accesses': len([entry for entry in conrad_bio_data if entry.get('access_successful')]),
    'successful_group_accesses': len([entry for entry in group_data if entry.get('access_successful')])
}

# Analyze findings for astronaut group determination
print("FINDINGS ANALYSIS:")
print(f"Successfully accessed {research_summary['successful_bio_accesses']} biography sources")
print(f"Successfully accessed {research_summary['successful_group_accesses']} group sources")

# Look for consistent patterns across sources
all_keywords = []
all_patterns = []

for bio in conrad_bio_data:
    if bio.get('access_successful'):
        all_keywords.extend(bio.get('found_keywords', []))
        all_patterns.extend(bio.get('pattern_matches', []))

print(f"\nKeywords found across sources: {list(set(all_keywords))}")
print(f"Patterns found across sources: {list(set(all_patterns))}")

# Check if we found Group 2 or 1962 consistently
group_indicators = []
if 'group 2' in [k.lower() for k in all_keywords] or '2' in all_patterns:
    group_indicators.append('Group 2')
if '1962' in all_patterns:
    group_indicators.append('Selected in 1962')
if 'new nine' in [k.lower() for k in all_keywords]:
    group_indicators.append('New Nine nickname')

if group_indicators:
    print(f"\n‚òÖ ASTRONAUT GROUP INDICATORS FOUND: {group_indicators}")
    research_summary['group_indicators'] = group_indicators
    
    # Based on historical knowledge: Group 2 was selected in 1962 and nicknamed "The New Nine"
    if any('group 2' in indicator.lower() or '1962' in indicator or 'new nine' in indicator.lower() for indicator in group_indicators):
        research_summary['conclusion'] = {
            'astronaut_group': 'NASA Astronaut Group 2',
            'selection_year': '1962',
            'nickname': 'The New Nine',
            'confidence': 'High - based on multiple source indicators'
        }
        print(f"\nüéØ CONCLUSION: Charles 'Pete' Conrad belonged to NASA Astronaut Group 2")
        print(f"   Selection Year: 1962")
        print(f"   Group Nickname: The New Nine")
else:
    print(f"\n‚ö†Ô∏è Need to search additional sources for definitive group information")
    research_summary['conclusion'] = {
        'status': 'Inconclusive - additional research needed',
        'next_steps': 'Search specialized astronaut databases or historical archives'
    }

# Save comprehensive research summary
summary_filename = 'workspace/conrad_astronaut_group_research.json'
with open(summary_filename, 'w', encoding='utf-8') as f:
    json.dump(research_summary, f, indent=2, ensure_ascii=False)

print(f"\n‚úì Comprehensive research summary saved to: {summary_filename}")

print(f"\n=== RESEARCH PHASE COMPLETE ===\n")

# List all created files
if os.path.exists('workspace'):
    print("Files created in workspace:")
    for file in sorted(os.listdir('workspace')):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")
        
        # Highlight key files
        if 'research.json' in file:
            print(f"    ‚òÖ RESEARCH SUMMARY - Contains findings and conclusion")
        elif 'bio_' in file and '.html' in file:
            print(f"    ‚òÖ BIOGRAPHY SOURCE - Official NASA astronaut information")
        elif 'groups_' in file and '.html' in file:
            print(f"    ‚òÖ GROUP SOURCE - NASA astronaut group historical data")

print(f"\nNext step: If needed, parse the successfully downloaded sources to extract definitive astronaut group information")
```