### Development Step 6: Find organization behind São Francisco Basin Environmental Education Plan and Sobradinho Dam advocate

**Description**: Conduct a comprehensive web search to identify the organization that launched the 'Plano de Educação Ambiental da Bacia do Rio São Francisco' covering 505 municipalities and collaborates with Ministérios Públicos. Search for keywords including 'Plano de Educação Ambiental Bacia Rio São Francisco 505 municípios', 'Ministérios Públicos São Francisco basin', 'environmental education plan São Francisco river', and 'Sobradinho Dam displaced people advocacy'. Focus on identifying the specific organization and then finding which individual within that organization advocated for people displaced by the Sobradinho Dam construction.

**Use Cases**:
- Environmental NGO partner mapping for the São Francisco basin: automatically aggregate and analyze official environmental education plans, collaborating ministries, and committee structures across 505 municipalities to identify local NGOs and government partners.
- Public Prosecutor’s Office legal research on ministry collaborations: scrape and index mentions of “Ministério Público” alongside environmental education plans and inter-agency agreements to support case preparation and enforce environmental legislation.
- Human rights documentation of Sobradinho Dam resettlement: extract historical references to displaced populations, activist advocates, and official statements from multiple sources to build a comprehensive database for academic studies or reparations claims.
- Corporate ESG risk assessment for hydroelectric projects: collect detailed information on CHESF, CODEVASF, IBAMA, ANA and other stakeholders’ environmental plans and community impacts to feed into automated sustainability reporting and investment due diligence.
- Academic governance analysis of watershed management: mine interdisciplinary data on committee structures (“Comitê de Bacia Hidrográfica”), foundation partnerships, and federal agency roles in the São Francisco basin to map collaborative networks in environmental policy research.
- Government oversight dashboard for environmental compliance: schedule regular scrapes of official sites and Wikipedia entries to detect updates in environmental education plans, ministry-public collaborations, and dam displacement advocacy, enabling real-time monitoring by regulators.
- Investigative journalism on infrastructure social impacts: consolidate keywords and context around “Sobradinho Dam displaced” and “Ministérios Públicos” across dozens of web sources to uncover key individuals and hidden narratives behind resettlement controversies.

```
import requests
import json
import os
import time
from bs4 import BeautifulSoup
import datetime

print("Conducting comprehensive web search to identify the organization behind 'Plano de Educação Ambiental da Bacia do Rio São Francisco'...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Define search keywords in Portuguese and English
search_keywords = [
    'Plano de Educação Ambiental da Bacia do Rio São Francisco',
    'Plano Educação Ambiental São Francisco',
    'PEABRIOSF',
    'Bacia Rio São Francisco 505 municípios',
    'Ministérios Públicos São Francisco',
    'environmental education plan São Francisco river',
    'Sobradinho Dam displaced people',
    'Barragem Sobradinho deslocados',
    'São Francisco basin environmental education',
    'CHESF environmental education',
    'IBAMA São Francisco',
    'ANA São Francisco basin',
    'CODEVASF environmental',
    'Comitê Bacia Hidrográfica São Francisco'
]

print(f"Search will focus on {len(search_keywords)} key terms related to São Francisco basin environmental education")

# FIXED: Define headers at the top level BEFORE any functions
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'
}

print("Headers defined successfully")

# Initial URLs to search - focusing on Brazilian environmental and government sites
initial_urls = [
    "https://pt.wikipedia.org/wiki/Rio_S%C3%A3o_Francisco",
    "https://pt.wikipedia.org/wiki/Bacia_hidrogr%C3%A1fica_do_rio_S%C3%A3o_Francisco",
    "https://pt.wikipedia.org/wiki/Usina_Hidrel%C3%A9trica_de_Sobradinho",
    "https://pt.wikipedia.org/wiki/CHESF",
    "https://pt.wikipedia.org/wiki/CODEVASF",
    "https://en.wikipedia.org/wiki/S%C3%A3o_Francisco_River",
    "https://en.wikipedia.org/wiki/Sobradinho_Dam"
]

search_results = {}
analysis_results = {}

print(f"\nStarting web search of {len(initial_urls)} initial sources...")

for i, url in enumerate(initial_urls, 1):
    page_name = url.split('/')[-1].replace('%C3%A3', 'a').replace('%C3%A9', 'e').replace('%20', '_')
    print(f"\n[{i}/{len(initial_urls)}] Fetching: {page_name}")
    print(f"URL: {url}")
    
    try:
        # Headers should now be accessible
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        content = response.text
        
        # Parse with BeautifulSoup to extract clean text
        soup = BeautifulSoup(content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text content
        text_content = soup.get_text()
        lines = (line.strip() for line in text_content.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        clean_text = ' '.join(chunk for chunk in chunks if chunk)
        
        search_results[page_name] = {
            'url': url,
            'content_length': len(clean_text),
            'content': clean_text[:20000],  # Store first 20000 characters
            'fetch_time': datetime.datetime.now().isoformat(),
            'status': 'success'
        }
        
        print(f"✓ Successfully retrieved {len(clean_text):,} characters")
        
        # Analyze content for keywords immediately
        found_keywords = []
        relevant_sections = []
        
        content_lower = clean_text.lower()
        
        for keyword in search_keywords:
            if keyword.lower() in content_lower:
                found_keywords.append(keyword)
                
                # Find sections around keyword
                start_pos = 0
                keyword_lower = keyword.lower()
                
                while True:
                    pos = content_lower.find(keyword_lower, start_pos)
                    if pos == -1:
                        break
                    
                    # Extract context around keyword
                    section_start = max(0, pos - 800)
                    section_end = min(len(clean_text), pos + 800)
                    section = clean_text[section_start:section_end]
                    
                    relevant_sections.append({
                        'keyword': keyword,
                        'section': section,
                        'position': pos,
                        'occurrence': len([s for s in relevant_sections if s['keyword'] == keyword]) + 1
                    })
                    
                    start_pos = pos + 1
                    
                    # Limit to 3 occurrences per keyword per page
                    if len([s for s in relevant_sections if s['keyword'] == keyword]) >= 3:
                        break
        
        analysis_results[page_name] = {
            'url': url,
            'found_keywords': found_keywords,
            'relevant_sections': relevant_sections,
            'keyword_count': len(found_keywords),
            'section_count': len(relevant_sections)
        }
        
        print(f"✓ Found {len(found_keywords)} keywords, {len(relevant_sections)} relevant sections")
        if found_keywords:
            print(f"Keywords: {', '.join(found_keywords[:5])}{'...' if len(found_keywords) > 5 else ''}")
            
    except Exception as e:
        print(f"✗ Error fetching {url}: {str(e)}")
        search_results[page_name] = {
            'url': url,
            'error': str(e),
            'content_length': 0,
            'content': '',
            'fetch_time': datetime.datetime.now().isoformat(),
            'status': 'error'
        }
        analysis_results[page_name] = {
            'url': url,
            'found_keywords': [],
            'relevant_sections': [],
            'keyword_count': 0,
            'section_count': 0,
            'error': str(e)
        }
    
    # Add delay between requests to be respectful
    time.sleep(2)

print(f"\n{'='*80}")
print("INITIAL SEARCH COMPLETED")
print(f"{'='*80}")

# Save initial search results
initial_output_file = "workspace/sao_francisco_initial_search.json"
with open(initial_output_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)
print(f"\nInitial search results saved to {initial_output_file}")

# Generate search summary
search_summary = {
    'search_date': datetime.datetime.now().isoformat(),
    'urls_searched': len(search_results),
    'successful_fetches': len([r for r in search_results.values() if r.get('status') == 'success']),
    'failed_fetches': len([r for r in search_results.values() if r.get('status') == 'error']),
    'total_keywords_found': sum(r.get('keyword_count', 0) for r in analysis_results.values()),
    'total_sections_found': sum(r.get('section_count', 0) for r in analysis_results.values())
}

print(f"\nINITIAL SEARCH SUMMARY:")
print(f"URLs searched: {search_summary['urls_searched']}")
print(f"Successful fetches: {search_summary['successful_fetches']}")
print(f"Failed fetches: {search_summary['failed_fetches']}")
print(f"Total keywords found: {search_summary['total_keywords_found']}")
print(f"Total relevant sections: {search_summary['total_sections_found']}")

# Display results by page
print(f"\n{'='*80}")
print("INITIAL SEARCH RESULTS BY PAGE")
print(f"{'='*80}")

for page_name, results in analysis_results.items():
    if results.get('keyword_count', 0) > 0:
        print(f"\n📄 {page_name}")
        print(f"   URL: {results['url']}")
        print(f"   Keywords found ({results['keyword_count']}): {', '.join(results['found_keywords'])}")
        print(f"   Relevant sections: {results['section_count']}")
    elif 'error' in results:
        print(f"\n❌ {page_name} - Error: {results['error']}")
    else:
        print(f"\n⚪ {page_name} - No relevant keywords found")

print(f"\n{'='*80}")
print("ANALYZING CONTENT FOR SPECIFIC ORGANIZATIONS AND INDIVIDUALS")
print(f"{'='*80}")

# Look for specific patterns related to environmental education plans and organizations
organization_evidence = []
plan_details = []
sobradinho_advocacy = []
ministery_collaboration = []

for page_name, results in analysis_results.items():
    for section in results.get('relevant_sections', []):
        section_text = section['section'].lower()
        section_content = section['section']
        
        # Look for organization names and environmental education plans
        org_indicators = ['chesf', 'codevasf', 'ibama', 'ana', 'comitê', 'ministério público', 'fundação', 'instituto']
        if any(indicator in section_text for indicator in org_indicators) and ('educação ambiental' in section_text or 'environmental education' in section_text):
            organization_evidence.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for specific plan details
        if ('plano' in section_text or 'plan' in section_text) and ('educação ambiental' in section_text or 'environmental education' in section_text):
            plan_details.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for Sobradinho Dam and displaced people
        if 'sobradinho' in section_text and ('deslocad' in section_text or 'displaced' in section_text or 'reassent' in section_text):
            sobradinho_advocacy.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for Ministry collaboration
        if 'ministério' in section_text and ('público' in section_text or 'colabora' in section_text):
            ministery_collaboration.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })

# Display findings
print(f"\n🏢 ORGANIZATION EVIDENCE: {len(organization_evidence)}")
for i, evidence in enumerate(organization_evidence[:3], 1):
    print(f"\n{i}. From {evidence['source']}:")
    print(f"   {evidence['section'][:500]}...")

print(f"\n📋 PLAN DETAILS: {len(plan_details)}")
for i, detail in enumerate(plan_details[:3], 1):
    print(f"\n{i}. From {detail['source']}:")
    print(f"   {detail['section'][:500]}...")

print(f"\n🏗️ SOBRADINHO ADVOCACY: {len(sobradinho_advocacy)}")
for i, advocacy in enumerate(sobradinho_advocacy[:3], 1):
    print(f"\n{i}. From {advocacy['source']}:")
    print(f"   {advocacy['section'][:500]}...")

print(f"\n🤝 MINISTRY COLLABORATION: {len(ministery_collaboration)}")
for i, collab in enumerate(ministery_collaboration[:3], 1):
    print(f"\n{i}. From {collab['source']}:")
    print(f"   {collab['section'][:500]}...")

# Save comprehensive findings
comprehensive_findings = {
    'search_date': datetime.datetime.now().isoformat(),
    'search_summary': search_summary,
    'organization_evidence': organization_evidence,
    'plan_details': plan_details,
    'sobradinho_advocacy': sobradinho_advocacy,
    'ministry_collaboration': ministery_collaboration,
    'search_keywords': search_keywords,
    'pages_analyzed': list(analysis_results.keys())
}

findings_file = "workspace/sao_francisco_comprehensive_findings.json"
with open(findings_file, 'w', encoding='utf-8') as f:
    json.dump(comprehensive_findings, f, indent=2, ensure_ascii=False)
print(f"\nComprehensive findings saved to {findings_file}")

print(f"\n{'='*80}")
print("INITIAL SEARCH PHASE COMPLETED - ANALYZING RESULTS")
print(f"{'='*80}")
print(f"\n📊 Initial Search Statistics:")
print(f"   • Pages searched: {len(analysis_results)}")
print(f"   • Organization evidence found: {len(organization_evidence)}")
print(f"   • Plan details found: {len(plan_details)}")
print(f"   • Sobradinho advocacy references: {len(sobradinho_advocacy)}")
print(f"   • Ministry collaboration mentions: {len(ministery_collaboration)}")

print(f"\n🎯 NEXT STEPS:")
print(f"   • Conduct targeted search based on initial findings")
print(f"   • Focus on specific organizations identified (CHESF, CODEVASF, etc.)")
print(f"   • Search for specific individuals involved in Sobradinho Dam advocacy")
print(f"   • Look for official documents and reports")

print(f"\n✅ INITIAL SEARCH COMPLETED SUCCESSFULLY!")
```