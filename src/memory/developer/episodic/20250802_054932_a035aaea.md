### Development Step 55: Download University of Leicester’s “Can Hiccup Supply Enough Fish for Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Regulatory compliance officer automating retrieval of the latest environmental policy PDFs from government portals for quarterly audit preparation
- Patent analyst downloading newly published patent specification PDFs for technology scouting and competitive intelligence
- Medical researcher gathering clinical guideline PDFs on diabetes management from WHO and NIH websites for a comprehensive meta‐analysis
- Supply chain manager collecting vendor technical specification PDFs to validate component compatibility in new product designs
- Civil engineer retrieving municipal building code PDFs from city archives to ensure design plans meet local regulations
- Academic librarian automating acquisition of conference proceeding PDFs on AI ethics to update the university’s digital repository
- Financial auditor downloading public company quarterly report PDFs for detailed ratio analysis and risk assessment
- Investigative journalist scraping government budget and spending report PDFs to visualize public fund allocations in an interactive dashboard

```
import os
import re
import sys
import requests

# --- Configuration ---
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# Paths to use
html_path = os.path.join(workspace, 'ddg_search.html')
pdf_list_path = os.path.join(workspace, 'ddg_pdf_urls.txt')
pdf_out_path = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

# Ensure the DuckDuckGo HTML file exists
if not os.path.exists(html_path):
    print(f"[ERROR] Cannot find DuckDuckGo HTML file at {html_path}. Run the fetch step first.")
    sys.exit(1)

print(f"[INFO] Reading saved DuckDuckGo HTML from: {html_path}")
with open(html_path, 'r', encoding='utf-8') as f:
    html = f.read()

# Define regex to find PDF URLs
pattern = r'https?://[^\s"\'<>]+?\.pdf(?:\?[^\s"\'<>]*)?'
print(f"[INFO] Using regex to extract PDF URLs: {pattern}")

# Find all matches
matches = re.findall(pattern, html, flags=re.IGNORECASE)
unique_urls = []
for url in matches:
    if url not in unique_urls:
        unique_urls.append(url)

# Save extracted URLs
if unique_urls:
    print(f"[INFO] Found {len(unique_urls)} PDF candidate(s). Saving to {pdf_list_path}")
    with open(pdf_list_path, 'w', encoding='utf-8') as out:
        for u in unique_urls:
            out.write(u + '\n')
            print(f"  - {u}")
else:
    print("[WARN] No PDF URLs found in the HTML. Inspect ddg_search.html manually.")
    sys.exit(1)

# Function to download a PDF given a URL
headers = {'User-Agent': 'Mozilla/5.0'}
def download_pdf(url, dest_path):
    print(f"[INFO] Attempting download: {url}")
    try:
        r = requests.get(url, headers=headers, timeout=60)
        print(f"[INFO] HTTP status: {r.status_code}")
    except Exception as e:
        print(f"[ERROR] Download exception: {e}")
        return False
    if r.status_code == 200 and len(r.content) > 10000:
        with open(dest_path, 'wb') as pdffile:
            pdffile.write(r.content)
        size = os.path.getsize(dest_path)
        print(f"[SUCCESS] Saved PDF -> {dest_path} ({size} bytes)")
        return True
    else:
        print(f"[WARN] Bad response or too small ({len(r.content)} bytes). Skipping.")
        return False

# Try downloading each candidate until one succeeds
for url in unique_urls:
    if download_pdf(url, pdf_out_path):
        print("[INFO] Download succeeded. Exiting.")
        sys.exit(0)

print(f"[ERROR] All download attempts failed. Check {pdf_list_path} for alternatives.")
sys.exit(1)

```