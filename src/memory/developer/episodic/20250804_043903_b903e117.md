### Development Step 11: Extract Survivor US Season Winners (1–44) into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment analytics dashboard for a reality‐TV network: ingest the JSON of Survivor winners to visualize season‐by‐season trends, gender breakdowns, and airtime engagement metrics.
- Automated candidate for marketing campaigns: trigger personalized emails or push notifications on each season’s finale anniversary, highlighting the winner’s name to superfans.
- Trivia mobile and web game content loader: populate question banks with “Who won season X of Survivor?” to ensure up‐to‐date, accurate quiz rounds across 44 seasons.
- Academic media studies research: merge the winners list with demographic datasets to analyze diversity and representation patterns in reality‐TV winners over time.
- Data journalism fact‐checking pipeline: integrate the scraper into a newsroom workflow to instantly verify and update articles on Survivor’s history and milestone cases.
- CMS automation for broadcaster websites: auto‐sync the latest Survivor winners into the official show page, eliminating manual data entry and reducing update errors.
- Social media archival bot: schedule daily “On this day” posts that reference the exact season number and winner name, driving historical engagement on Twitter or Instagram.
- Business intelligence for streaming services: correlate extracted winner data with viewership and subscription spikes to inform content acquisition and promotional strategies.

```
import os
import json
import requests
from bs4 import BeautifulSoup

# 1) Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2) Fetch Survivor page with realistic User-Agent to avoid blocks
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/114.0.0.0 Safari/537.36'
}
print(f"Fetching page with headers: {url}")
response = requests.get(url, headers=headers)
response.raise_for_status()
print("Page fetched successfully (status code: {}).").format(response.status_code)

# 3) Save raw HTML for inspection
html_path = os.path.join(workspace_dir, 'survivor_page.html')
with open(html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"Saved full page HTML to: {html_path}")

# 4) Parse HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 5) Locate the "Winners by season" section headline robustly
print("Searching for section headline containing 'winners'")
headline_span = None
for span in soup.find_all('span', class_='mw-headline'):
    text = span.get_text(strip=True)
    if 'winners' in text.lower():
        print(f"  ✓ Found headline: '{text}' (id={span.get('id')})")
        headline_span = span
        break

if not headline_span:
    # fallback: write all headlines to file for manual inspection
    print("❌ No 'winners' headline found. Dumping all section headlines to workspace/section_headlines.txt...")
    with open(os.path.join(workspace_dir, 'section_headlines.txt'), 'w', encoding='utf-8') as f:
        for span in soup.find_all('span', class_='mw-headline'):
            f.write(f"id={span.get('id')}\ttext={span.get_text(strip=True)}\n")
    raise RuntimeError("Cannot locate a section headline containing 'winners'. See workspace/section_headlines.txt.")

# 6) From that headline, find its parent heading and the next table sibling
heading_tag = headline_span.find_parent(['h2', 'h3', 'h4'])
winners_table = None
for sib in heading_tag.next_siblings:
    if getattr(sib, 'name', None) == 'table':
        # ensure it's a wikitable
        classes = sib.get('class') or []
        if 'wikitable' in classes:
            winners_table = sib
            print("Found next <table class='wikitable'> after the 'Winners' heading.")
            break
        else:
            print("  Skipped a <table> without 'wikitable' class.")
    # stop if another heading appears
    if getattr(sib, 'name', None) in ['h2', 'h3', 'h4']:
        break

if not winners_table:
    raise RuntimeError("No wikitable found immediately after 'Winners' heading.")

# 7) Parse header row for column indices
header_row = winners_table.find('tr')
header_cells = header_row.find_all(['th', 'td'], recursive=False)
headers = []
for cell in header_cells:
    # remove any footnote markers
    for sup in cell.find_all('sup'):
        sup.decompose()
    headers.append(cell.get_text(strip=True).lower())
print(f"Table headers detected: {headers}")

if 'season' not in headers or 'winner' not in headers:
    raise RuntimeError(f"Unexpected table headers; expected 'Season' and 'Winner'. Got: {headers}")
season_idx = headers.index('season')
winner_idx = headers.index('winner')
print(f"Identified column indices → season: {season_idx}, winner: {winner_idx}")

# 8) Iterate data rows and extract season-winner pairs for seasons 1–44
rows = winners_table.find_all('tr')[1:]  # skip header
winners = []
print(f"Total rows to examine (excluding header): {len(rows)}")
for row in rows:
    cells = row.find_all(['th', 'td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        print(f"  Skipping row: only {len(cells)} cells")
        continue
    # parse season number
    season_cell = cells[season_idx]
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    season_text = season_cell.get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"  Skipping row: invalid season '{season_text}'")
        continue
    if not (1 <= season_num <= 44):
        print(f"  Skipping season {season_num}: out of range 1–44")
        continue
    # parse winner name
    winner_cell = cells[winner_idx]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    winner_name = winner_cell.get_text(strip=True)
    print(f"  Parsed Season {season_num} → Winner: {winner_name}")
    winners.append({'season': season_num, 'winner': winner_name})

# 9) Sort and save to JSON
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"Total winners extracted for seasons 1–44: {len(winners_sorted)}")
if len(winners_sorted) != 44:
    print(f"⚠️ Warning: expected 44 entries but found {len(winners_sorted)}. Verify parsing logic.")
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Winners list saved to: {out_path}")
```