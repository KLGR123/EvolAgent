### Development Step 6: Find South African-IFP Reform Party agreement details and identify subsequent South African ambassador to France

**Description**: Conduct a comprehensive web search to identify the agreement between a South African political figure and the leader of the Inkatha Freedom Party that led to the formation of the Reform Party and proposed a federal system and bill of rights. Focus on finding information about this specific political agreement, the parties involved, and identify who later served as South African ambassador to France. Search for keywords including 'Inkatha Freedom Party agreement Reform Party', 'South Africa federal system bill of rights agreement', 'South African ambassador France Reform Party', and 'IFP leader political agreement federal system'.

**Use Cases**:
- Political historian compiling a detailed timeline of South Africa’s constitutional negotiations by automatically extracting IFP–Reform Party agreement sections and signatories for archival research
- Diplomatic affairs analyst tracking career paths of South African envoys, using scripted web searches to identify who later served as ambassador to France and mapping their political backgrounds for briefing dossiers
- Academic researcher investigating federalism proposals, aggregating “federal system” and “bill of rights” content from multiple government and party sources to support a comparative constitutional law study
- NGO policy team building a database of negotiation accords, leveraging keyword‐driven extraction to capture details of the Inkatha Freedom Party’s Record of Understanding and allied Reform Party initiatives for advocacy reports
- Journalism fact‐checking unit verifying claims about the formation of the Reform Party and its leadership, deploying the automation to cross‐reference multiple Wikipedia entries and save verified excerpts for editorial review
- Data engineer constructing a political knowledge graph, automating the discovery of agreements like the IFP accord and tracking party alliances to populate nodes and relationships for semantic analysis
- Course developer creating e-learning modules on South Africa’s transition, using the tool to extract relevant sections on CODESA and federalism debates to generate interactive study materials
- Legal consultant researching decentralized governance precedents, scraping negotiation documents pertaining to the bill of rights and federal system proposals to inform client advisory memos

```
import requests
import json
import os
from datetime import datetime
import time

print("Conducting comprehensive web search to identify South African political agreement involving Inkatha Freedom Party...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Simple direct approach - define functions in order and execute immediately
print("\nSearching for South African political agreement information...")

# Search URLs for relevant information
search_urls = [
    "https://en.wikipedia.org/wiki/Inkatha_Freedom_Party",
    "https://en.wikipedia.org/wiki/Reform_Party_(South_Africa)",
    "https://en.wikipedia.org/wiki/Mangosuthu_Buthelezi",
    "https://en.wikipedia.org/wiki/South_African_transition_to_democracy",
    "https://en.wikipedia.org/wiki/Roelf_Meyer",
    "https://en.wikipedia.org/wiki/Convention_for_a_Democratic_South_Africa"
]

# Keywords to search for
search_keywords = [
    'Reform Party',
    'federal system',
    'bill of rights',
    'Inkatha Freedom Party',
    'IFP',
    'Mangosuthu Buthelezi',
    'ambassador to France',
    'political agreement',
    'Roelf Meyer',
    'CODESA',
    'National Party',
    'ANC',
    'Record of Understanding',
    'KwaZulu',
    'federalism',
    'constitutional negotiations'
]

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Storage for search results
search_results = {}
analysis_results = {}

# Conduct web search
for url in search_urls:
    print(f"\nFetching: {url}")
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        content = response.text
        
        page_name = url.split('/')[-1]
        search_results[page_name] = {
            'url': url,
            'content_length': len(content),
            'content': content[:20000]  # Store first 20000 characters
        }
        
        print(f"Successfully retrieved {len(content)} characters from {page_name}")
        
        # Analyze content for keywords immediately
        content_lower = content.lower()
        found_keywords = []
        relevant_sections = []
        
        for keyword in search_keywords:
            if keyword.lower() in content_lower:
                found_keywords.append(keyword)
                
                # Find section around keyword
                start_pos = content_lower.find(keyword.lower())
                if start_pos != -1:
                    section_start = max(0, start_pos - 600)
                    section_end = min(len(content), start_pos + 600)
                    section = content[section_start:section_end]
                    relevant_sections.append({
                        'keyword': keyword,
                        'section': section,
                        'position': start_pos
                    })
        
        analysis_results[page_name] = {
            'url': url,
            'found_keywords': found_keywords,
            'relevant_sections': relevant_sections,
            'keyword_count': len(found_keywords)
        }
        
        print(f"Found {len(found_keywords)} relevant keywords in {page_name}")
        if found_keywords:
            print(f"Keywords found: {', '.join(found_keywords)}")
        
    except Exception as e:
        print(f"Error fetching {url}: {str(e)}")
        search_results[url.split('/')[-1]] = {
            'url': url,
            'error': str(e),
            'content_length': 0,
            'content': ''
        }
    
    # Add delay between requests
    time.sleep(2)

# Save detailed analysis to file
output_file = "workspace/ifp_agreement_search_results.json"
with open(output_file, 'w') as f:
    json.dump(analysis_results, f, indent=2)

print(f"\nDetailed search results saved to {output_file}")

# Display search summary
print("\n" + "="*80)
print("SEARCH SUMMARY")
print("="*80)

for page_name, results in analysis_results.items():
    if results.get('keyword_count', 0) > 0:
        print(f"\n{page_name} ({results['url']})")
        print(f"Keywords found: {', '.join(results['found_keywords'])}")
        
        # Display most relevant sections
        for i, section in enumerate(results['relevant_sections'][:2], 1):
            print(f"\nRelevant section {i} for '{section['keyword']}':")
            print(f"{section['section'][:500]}...")

# Analyze for specific patterns
print("\n" + "="*80)
print("DETAILED ANALYSIS")
print("="*80)

potential_agreements = []
potential_ambassadors = []
reform_party_mentions = []
ifp_agreements = []

for page_name, results in analysis_results.items():
    for section in results.get('relevant_sections', []):
        section_text = section['section'].lower()
        
        # Look for Reform Party mentions
        if 'reform party' in section_text:
            reform_party_mentions.append({
                'source': page_name,
                'section': section['section'],
                'keyword': section['keyword']
            })
        
        # Look for IFP agreement patterns
        if 'inkatha' in section_text and ('agreement' in section_text or 'accord' in section_text):
            ifp_agreements.append({
                'source': page_name,
                'section': section['section'],
                'keyword': section['keyword']
            })
        
        # Look for federal system + bill of rights
        if ('federal' in section_text or 'federalism' in section_text) and 'bill of rights' in section_text:
            potential_agreements.append({
                'source': page_name,
                'section': section['section'],
                'keyword': section['keyword']
            })
        
        # Look for ambassador to France
        if 'ambassador' in section_text and 'france' in section_text:
            potential_ambassadors.append({
                'source': page_name,
                'section': section['section'],
                'keyword': section['keyword']
            })

# Save specific findings
findings = {
    'search_date': datetime.now().isoformat(),
    'potential_agreements': potential_agreements,
    'potential_ambassadors': potential_ambassadors,
    'reform_party_mentions': reform_party_mentions,
    'ifp_agreements': ifp_agreements,
    'search_keywords': search_keywords,
    'pages_searched': list(search_results.keys()),
    'total_keywords_found': sum(r.get('keyword_count', 0) for r in analysis_results.values())
}

findings_file = "workspace/ifp_agreement_findings.json"
with open(findings_file, 'w') as f:
    json.dump(findings, f, indent=2)

print(f"\nSpecific findings saved to {findings_file}")

# Display findings
if reform_party_mentions:
    print(f"\nFound {len(reform_party_mentions)} Reform Party mentions:")
    for i, mention in enumerate(reform_party_mentions[:3], 1):
        print(f"\n{i}. Reform Party mention (from {mention['source']}):")
        print(f"{mention['section'][:600]}...")

if ifp_agreements:
    print(f"\nFound {len(ifp_agreements)} IFP agreement references:")
    for i, agreement in enumerate(ifp_agreements[:3], 1):
        print(f"\n{i}. IFP agreement (from {agreement['source']}):")
        print(f"{agreement['section'][:600]}...")

if potential_agreements:
    print(f"\nFound {len(potential_agreements)} potential federal/bill of rights agreements:")
    for i, agreement in enumerate(potential_agreements[:3], 1):
        print(f"\n{i}. Agreement reference (from {agreement['source']}):")
        print(f"{agreement['section'][:600]}...")

if potential_ambassadors:
    print(f"\nFound {len(potential_ambassadors)} potential ambassador references:")
    for i, ambassador in enumerate(potential_ambassadors[:3], 1):
        print(f"\n{i}. Ambassador reference (from {ambassador['source']}):")
        print(f"{ambassador['section'][:600]}...")

print("\n" + "="*80)
print("SEARCH COMPLETED SUCCESSFULLY!")
print("="*80)
print(f"Total pages searched: {len(search_results)}")
print(f"Total keywords found: {findings['total_keywords_found']}")
print(f"Reform Party mentions: {len(reform_party_mentions)}")
print(f"IFP agreements: {len(ifp_agreements)}")
print(f"Federal/Bill of Rights agreements: {len(potential_agreements)}")
print(f"Ambassador references: {len(potential_ambassadors)}")
```