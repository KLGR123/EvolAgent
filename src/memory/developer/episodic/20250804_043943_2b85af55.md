### Development Step 13: Extract Survivor (US) Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment blog timeline creation and interactive visualization of Survivor winners for fan engagement
- Mobile trivia app database population with historical Survivor winner data for daily quiz challenges
- Media diversity research analyzing gender and age trends among Survivor winners for academic publications
- Social media marketing campaign targeting by correlating winner demographics with viewer engagement metrics
- Automated weekly digest generation of recent Survivor winners for television network subscriber newsletters
- Chatbot knowledge enhancement feeding a streaming platform assistant with historical winner information
- API development for entertainment data aggregators providing a JSON endpoint of Survivor season winners
- Advertising optimization in digital campaigns by analyzing season winner popularity trends across 44 seasons

```
import os
import sys

# 1) Locate the workspace directory (handles dynamic suffixes)
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Cannot inspect survivor_page.html.")
    sys.exit(1)
workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Define path to the saved HTML
html_path = os.path.join(workspace_dir, 'survivor_page.html')
if not os.path.exists(html_path):
    print(f"❌ File not found: {html_path}")
    sys.exit(1)

# 3) Read file content
with open(html_path, 'r', encoding='utf-8') as f:
    content = f.read()

# 4) Prepare debug output
debug_lines = []
debug_lines.append(f"File: {html_path}")
debug_lines.append(f"Size: {len(content)} characters\n")
debug_lines.append("=== Preview of first 2000 characters ===")
debug_lines.append(content[:2000])
debug_lines.append("\n=== Count key patterns ===")
debug_lines.append(f"Occurrences of <span class=\"mw-headline\": {content.count('class=\"mw-headline\"')}")
debug_lines.append(f"Occurrences of 'Winners by season': {content.lower().count('winners by season')}")
debug_lines.append(f"Occurrences of '<h2': {content.count('<h2')}")
debug_lines.append(f"Occurrences of '<h3': {content.count('<h3')}")
debug_lines.append("\n=== Sample lines containing 'mw-headline' or 'Winners' ===")

# 5) Extract sample lines for manual inspection
for line in content.splitlines():
    low = line.lower()
    if 'mw-headline' in low or 'winners' in low:
        debug_lines.append(line.strip())
        if len(debug_lines) > 50:  # limit output size
            break

# 6) Write debug output to file
debug_path = os.path.join(workspace_dir, 'debug_html_inspection.txt')
with open(debug_path, 'w', encoding='utf-8') as out:
    out.write('\n'.join(debug_lines))

# 7) Print summary to stdout
print(f"Debug inspection written to: {debug_path}\n")
print(f"Preview (first 20 lines) of debug file:")
with open(debug_path, 'r', encoding='utf-8') as out:
    for i, line in enumerate(out):
        print(line.rstrip())
        if i >= 19:
            break

```