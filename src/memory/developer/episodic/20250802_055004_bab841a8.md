### Development Step 57: Download University of Leicester paper PDF on Hiccup supplying fish for dragon diet

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated retrieval of government tax regulation PDFs from official portals for corporate tax compliance teams
- Systematic scraping of medical research papers (PDFs) on oncology drug trials for pharmaceutical R&D analysts
- Scheduled extraction and download of quarterly earnings report PDFs from competitor websites for financial analysts
- Bulk harvesting of patent specification PDFs in biotechnology using search engine result parsing for IP management departments
- Automated collection of architectural blueprint PDFs from city planning portals for engineering consulting firms
- Real‐time aggregation of legal case decision PDFs from court databases for law firms’ case preparation
- Continuous monitoring and downloading of grant opportunity PDFs from funding agency sites for university research offices
- Automated ingestion of marketing whitepaper PDFs from industry websites for strategic marketing and competitive intelligence teams

```
import os
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)
html_path       = os.path.join(workspace, 'ddg_search.html')
pdf_list_path   = os.path.join(workspace, 'ddg_pdf_urls.txt')
pdf_out_path    = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

# Verify the HTML file exists
if not os.path.exists(html_path):
    print(f"[ERROR] DuckDuckGo HTML not found at {html_path}. Please run the fetch step first.")
    sys.exit(1)

print(f"[INFO] Parsing DuckDuckGo HTML from: {html_path}")
with open(html_path, 'r', encoding='utf-8') as f:
    soup = BeautifulSoup(f, 'html.parser')

# Find all result links
anchors = soup.find_all('a', class_='result__a')
print(f"[INFO] Found {len(anchors)} <a class='result__a'> tags. Decoding uddg parameters...")

decoded_urls = []
for a in anchors:
    href = a.get('href', '')
    if not href:
        continue
    # Reconstruct full redirect URL
    if href.startswith('//'):
        redirect = 'https:' + href
    elif href.startswith('/'):
        redirect = 'https://duckduckgo.com' + href
    else:
        redirect = href

    # Parse the query string for uddg
    parsed = urlparse(redirect)
    qs = parse_qs(parsed.query)
    uddg_vals = qs.get('uddg')
    if not uddg_vals:
        continue
    decoded = unquote(uddg_vals[0])

    # Filter for PDF candidates (either .pdf in URL or 'download' path)
    if '.pdf' in decoded.lower() or '/download/' in decoded.lower():
        if decoded not in decoded_urls:
            decoded_urls.append(decoded)
            print(f"[FOUND] Candidate PDF URL: {decoded}")

# Save extracted URLs
if decoded_urls:
    with open(pdf_list_path, 'w', encoding='utf-8') as out:
        for u in decoded_urls:
            out.write(u + '\n')
    print(f"[SAVE] Extracted PDF URLs -> {pdf_list_path}")
else:
    print("[ERROR] No PDF candidates found. Please inspect ddg_search.html manually.")
    sys.exit(1)

# Attempt to download each candidate
headers = {'User-Agent': 'Mozilla/5.0'}
def download_pdf(url, dest_path):
    print(f"[INFO] Downloading: {url}")
    try:
        r = requests.get(url, headers=headers, timeout=60)
        print(f"[INFO] HTTP status: {r.status_code}, bytes received: {len(r.content)}")
    except Exception as e:
        print(f"[ERROR] Exception during download: {e}")
        return False

    # Check size and status
    if r.status_code == 200 and len(r.content) > 10000:
        with open(dest_path, 'wb') as pdffile:
            pdffile.write(r.content)
        size = os.path.getsize(dest_path)
        print(f"[SUCCESS] Saved PDF -> {dest_path} ({size} bytes)")
        return True
    else:
        print(f"[WARN] Response invalid or too small. Skipping.")
        return False

for url in decoded_urls:
    if download_pdf(url, pdf_out_path):
        print("[INFO] Download completed successfully. Exiting.")
        sys.exit(0)

print("[ERROR] All download attempts failed. Check workspace/ddg_pdf_urls.txt for alternatives.")
sys.exit(1)

```