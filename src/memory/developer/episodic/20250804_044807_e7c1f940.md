### Development Step 46: Extract Survivor Season 1–44 Winners to JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Media analytics for television networks: automatically extract Survivor winners by season to correlate winner demographics with Nielsen viewership spikes and advertising revenue trends
- Automation for fan-driven websites: programmatically update a Survivor wiki or fan portal’s “Winners” page after each new season airs, ensuring real-time accuracy without manual edits
- Academic research on cultural representation: compile winner names and seasons to analyze diversity trends in reality TV casting and success rates across four decades
- Podcast production for entertainment trivia: pull historical winner data to generate quiz questions and episode show notes for weekly “Survivor Flashback” segments
- Data science model training: integrate the winners list with in-game voting statistics and cast profiles to build a machine learning model predicting future Survivor champions
- Marketing intelligence for streaming platforms: extract winner information to tailor promotional campaigns, creating personalized viewer recommendations based on favorite past champions
- Mobile app development for fan engagement: populate an interactive “Season-by-Season Winners” feature in a Survivor companion app, allowing users to filter by year, gender, or tribe
- Journalism automation for fact-checking: script periodic checks of Wikipedia’s Survivor winners table to alert entertainment reporters of any discrepancies or vandalism before publication

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page for debug generation...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
})
try:
    resp.raise_for_status()
except Exception as e:
    print(f"❌ Error fetching page: {e}")
    sys.exit(1)

soup = BeautifulSoup(resp.text, 'html.parser')

# 3) Generate debug info: for each H2/H3/H4 heading with a .mw-headline span, record id/text and number of tables until next header
debug_lines = []
for header_tag in soup.find_all(['h2', 'h3', 'h4']):
    span = header_tag.find('span', class_='mw-headline')
    if not span or not span.get('id'):
        continue
    hid = span.get('id')
    text = span.get_text(strip=True)
    # Count <table> siblings under this section until next header
    tbl_count = 0
    for sib in header_tag.find_next_siblings():
        if sib.name in ['h2', 'h3', 'h4']:
            break
        if sib.name == 'table':
            tbl_count += 1
    debug_lines.append(f"{header_tag.name} id='{hid}' text='{text}' → {tbl_count} table(s)")

# 4) Write debug info to file
dbg_path = os.path.join(workspace_dir, 'survivor_debug_headings.txt')
with open(dbg_path, 'w', encoding='utf-8') as f:
    f.write("# Debug: H2/H3/H4 headings and number of tables until next heading\n")
    for line in debug_lines:
        f.write(line + "\n")

# 5) Output for tester
print(f"✅ Debug info generated: {dbg_path}")
print("=== Contents ===")
for line in debug_lines:
    print(line)
print("=== End of debug info ===")
```