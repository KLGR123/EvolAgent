### Development Step 62: Extract American Idol Season Numbers and Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/American_Idol and extract the season-by-season winners table into workspace/american_idol_winners_list.json, capturing each season number and winner name.

**Use Cases**:
- Media outlet automation and website content updates: schedule nightly extractions to update a broadcaster’s sidebar showing the latest American Idol winners.
- Music industry market research and talent scouting: compile winner data to analyze patterns in genre, background, and success rates for A&R strategy.
- Business intelligence dashboard integration: import season/winner JSON into Tableau or Power BI to correlate winner announcements with viewership ratings and advertising revenue.
- Data science educational assignments and workshops: use the scraping code to teach students real-world web scraping, HTML parsing, and JSON serialization techniques.
- Entertainment trivia game development and database seeding: auto-populate a quiz app’s questions with season-specific winner information to keep content up to date.
- Academic cultural studies research and demographic analysis: study diversity and representation trends in competition winners across multiple seasons.
- Chatbot knowledge base building and Q&A automation: feed the JSON file into a conversational AI to answer user questions like “Who won season 10 of American Idol?”
- Content management system automation for fan sites: automate nightly jobs to refresh the site’s American Idol archive page with the latest winner list.

```
import os
import requests
import json
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

url = 'https://en.wikipedia.org/wiki/American_Idol'
print(f"Fetching page: {url}")
response = requests.get(url)
print(f"  HTTP status code: {response.status_code}")
response.raise_for_status()

soup = BeautifulSoup(response.text, 'html.parser')

# Find all wikitable tables
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} 'wikitable' tables on the page.")

target_table = None
# Locate the table containing both 'Season' and 'Winner' in its headers
for idx, table in enumerate(tables, start=1):
    header_row = table.find('tr')
    headers = [th.get_text(strip=True) for th in header_row.find_all('th')]
    headers_lower = [h.lower() for h in headers]
    print(f"Table {idx} headers: {headers}")
    if 'season' in headers_lower and 'winner' in headers_lower:
        print(f"--> Selecting table {idx} (contains both 'Season' and 'Winner').")
        target_table = table
        break

if not target_table:
    raise RuntimeError("Could not find a table with both 'Season' and 'Winner' columns.")

# Extract the exact headers and their indices
header_cells = target_table.find('tr').find_all('th')
headers = [th.get_text(strip=True) for th in header_cells]
print(f"Using headers: {headers}")
season_idx = headers.index('Season')
winner_idx = headers.index('Winner')
print(f"Season column index: {season_idx}, Winner column index: {winner_idx}")

# Parse each data row
winners = []
rows = target_table.find_all('tr')[1:]
print(f"Parsing {len(rows)} rows from the selected table.")
for row_num, row in enumerate(rows, start=1):
    # Include both <th> (for season #) and <td> cells, but only direct children to keep columns aligned
    cells = row.find_all(['th', 'td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        print(f"  Skipping row {row_num}: only {len(cells)} cells found.")
        continue
    season = cells[season_idx].get_text(strip=True)
    winner = cells[winner_idx].get_text(strip=True)
    print(f"  Row {row_num}: Season = '{season}', Winner = '{winner}'")
    winners.append({'season': season, 'winner': winner})

print(f"Total season/winner pairs extracted: {len(winners)}")

# Save to JSON
output_path = os.path.join(workspace_dir, 'american_idol_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners, f, ensure_ascii=False, indent=2)
print(f"Winners list saved to: {output_path}")

```