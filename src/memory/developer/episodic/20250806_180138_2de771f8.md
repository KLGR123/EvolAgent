### Development Step 3: USGS Records of Clownfish Non-native U.S. Occurrences (Pre-2020) with Zip Codes

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Marine invasive species monitoring and hotspot mapping for state wildlife agencies using zip code–level occurrence data
- Environmental risk assessment and habitat suitability modeling for consulting firms advising coastal development projects
- Regulatory compliance audits for aquarium trade businesses tracking nonnative fish import locations by zip code
- Academic research on invasion biogeography, using USGS occurrence records to analyze spread patterns of Amphiprion ocellaris before 2020
- GIS-based public outreach dashboards for conservation NGOs visualizing nonnative clownfish sightings in U.S. communities
- Urban infrastructure planning support for municipal water management departments to prevent invasive species in stormwater systems
- Automated reporting and early‐warning alerts for insurance underwriters assessing ecological liabilities tied to nonnative aquatic species occurrences

```
print("=== DIRECT ACCESS TO USGS NONINDIGENOUS AQUATIC SPECIES DATABASE ===\n")

# Since the search results were not properly stored, let's directly access the USGS NAS database
# that was identified in the previous attempts: https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243

import requests
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse

print("Target Species: Amphiprion ocellaris (Clown anemonefish)")
print("Database: USGS Nonindigenous Aquatic Species (NAS) Database")
print("Objective: Find location data with zip codes for nonnative occurrences before 2020\n")

# Primary USGS NAS species profile URL identified from previous searches
species_profile_url = "https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243"

print(f"Accessing USGS NAS Species Profile: {species_profile_url}")

try:
    # Set headers to mimic browser request
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive'
    }
    
    # Access the species profile page
    response = requests.get(species_profile_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f"Successfully accessed USGS page (Status: {response.status_code})")
    print(f"Content length: {len(response.content):,} bytes\n")
    
    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract basic species information
    print("=== SPECIES PROFILE INFORMATION ===\n")
    
    # Look for species name and basic info
    title_element = soup.find('title')
    if title_element:
        page_title = title_element.get_text().strip()
        print(f"Page Title: {page_title}")
    
    # Look for species scientific name and common name in the content
    species_info = {}
    
    # Find all text content and look for key information
    page_text = soup.get_text()
    
    # Extract scientific name pattern
    scientific_name_match = re.search(r'(Amphiprion\s+\w+)', page_text, re.IGNORECASE)
    if scientific_name_match:
        species_info['scientific_name'] = scientific_name_match.group(1)
        print(f"Scientific Name: {species_info['scientific_name']}")
    
    # Look for common names
    common_name_patterns = [r'clown\s*anemonefish', r'clownfish', r'orange\s*clownfish']
    for pattern in common_name_patterns:
        match = re.search(pattern, page_text, re.IGNORECASE)
        if match:
            species_info['common_name'] = match.group(0)
            print(f"Common Name: {species_info['common_name']}")
            break
    
    print("\n=== SEARCHING FOR OCCURRENCE/LOCATION DATA LINKS ===\n")
    
    # Look for links to occurrence data, sightings, or location information
    occurrence_links = []
    location_keywords = ['occurrence', 'sighting', 'location', 'distribution', 'point map', 'specimen', 'collection']
    
    # Find all links on the page
    all_links = soup.find_all('a', href=True)
    print(f"Found {len(all_links)} total links on the species profile page")
    
    for link in all_links:
        href = link.get('href', '')
        link_text = link.get_text().strip().lower()
        
        # Check if link relates to occurrence/location data
        if any(keyword in link_text for keyword in location_keywords) or any(keyword in href.lower() for keyword in location_keywords):
            # Convert relative URLs to absolute URLs
            full_url = urljoin(species_profile_url, href)
            
            occurrence_links.append({
                'text': link.get_text().strip(),
                'url': full_url,
                'href': href
            })
            
            print(f"Occurrence Link Found:")
            print(f"  Text: {link.get_text().strip()}")
            print(f"  URL: {full_url}")
            print(f"  {"-"*60}")
    
    print(f"\nTotal occurrence-related links found: {len(occurrence_links)}")
    
    # Look specifically for point map or specimen data links
    print("\n=== SEARCHING FOR SPECIFIC LOCATION DATA SOURCES ===\n")
    
    point_map_links = []
    specimen_links = []
    
    for link in occurrence_links:
        link_text_lower = link['text'].lower()
        link_url_lower = link['url'].lower()
        
        # Look for point map links (these often contain specific coordinates)
        if 'point' in link_text_lower or 'map' in link_text_lower:
            point_map_links.append(link)
            print(f"POINT MAP LINK: {link['text']} -> {link['url']}")
        
        # Look for specimen or collection links
        if 'specimen' in link_text_lower or 'collection' in link_text_lower:
            specimen_links.append(link)
            print(f"SPECIMEN LINK: {link['text']} -> {link['url']}")
    
    # Also check for any forms or query interfaces
    print("\n=== SEARCHING FOR QUERY INTERFACES ===\n")
    
    forms = soup.find_all('form')
    query_interfaces = []
    
    for form in forms:
        form_action = form.get('action', '')
        if form_action:
            full_action_url = urljoin(species_profile_url, form_action)
            
            # Look for input fields that might relate to location queries
            inputs = form.find_all('input')
            selects = form.find_all('select')
            
            location_inputs = []
            for input_elem in inputs + selects:
                input_name = input_elem.get('name', '').lower()
                input_id = input_elem.get('id', '').lower()
                
                if any(keyword in input_name or keyword in input_id for keyword in ['state', 'zip', 'location', 'county', 'lat', 'lon', 'coord']):
                    location_inputs.append({
                        'type': input_elem.name,
                        'name': input_elem.get('name', ''),
                        'id': input_elem.get('id', '')
                    })
            
            if location_inputs:
                query_interfaces.append({
                    'form_action': full_action_url,
                    'location_inputs': location_inputs
                })
                
                print(f"QUERY INTERFACE FOUND:")
                print(f"  Form Action: {full_action_url}")
                print(f"  Location Inputs: {len(location_inputs)}")
                for inp in location_inputs:
                    print(f"    {inp['type']}: {inp['name']} (id: {inp['id']})")
                print(f"  {"-"*60}")
    
    # Save all extracted information
    usgs_data = {
        'species_profile': {
            'url': species_profile_url,
            'species_id': '3243',
            'scientific_name': species_info.get('scientific_name', 'Amphiprion ocellaris'),
            'common_name': species_info.get('common_name', 'Clown anemonefish'),
            'page_title': page_title if 'page_title' in locals() else 'Unknown',
            'access_date': '2024',
            'content_length': len(response.content)
        },
        'occurrence_links': occurrence_links,
        'point_map_links': point_map_links,
        'specimen_links': specimen_links,
        'query_interfaces': query_interfaces,
        'next_steps': [
            'Access point map or occurrence data links',
            'Query location-specific interfaces for zip code data',
            'Look for downloadable datasets with coordinates',
            'Filter results for pre-2020 records',
            'Extract specific US location data with zip codes'
        ]
    }
    
    # Save the extracted USGS data
    with open('workspace/usgs_nas_clownfish_data.json', 'w') as f:
        json.dump(usgs_data, f, indent=2)
    
    print(f"\n=== USGS DATABASE ACCESS SUMMARY ===\n")
    print(f"Species Profile Successfully Accessed: {species_profile_url}")
    print(f"Species: {species_info.get('scientific_name', 'Amphiprion ocellaris')} ({species_info.get('common_name', 'Clown anemonefish')})")
    print(f"Occurrence-related links found: {len(occurrence_links)}")
    print(f"Point map links found: {len(point_map_links)}")
    print(f"Specimen links found: {len(specimen_links)}")
    print(f"Query interfaces found: {len(query_interfaces)}")
    print(f"\nData saved to: workspace/usgs_nas_clownfish_data.json")
    
    # If we found specific occurrence or point map links, prioritize them for next access
    if point_map_links or occurrence_links:
        print(f"\n=== PRIORITY TARGETS FOR LOCATION DATA ===\n")
        
        priority_targets = point_map_links + occurrence_links[:3]  # Top 3 occurrence links
        
        for i, target in enumerate(priority_targets, 1):
            print(f"{i}. {target['text']}")
            print(f"   URL: {target['url']}")
            print(f"   Type: {'Point Map' if target in point_map_links else 'Occurrence Data'}")
        
        # Save priority targets for next step
        with open('workspace/priority_location_targets.json', 'w') as f:
            json.dump({
                'priority_targets': priority_targets,
                'objective': 'Extract specific location data with zip codes for nonnative Amphiprion ocellaris before 2020',
                'next_action': 'Access priority target URLs to find detailed occurrence records'
            }, f, indent=2)
        
        print(f"\nPriority targets saved to: workspace/priority_location_targets.json")
        print(f"Ready to access specific occurrence/location data sources.")
    
    else:
        print(f"\nNo specific occurrence or point map links found.")
        print(f"Will need to explore general database query interfaces.")
    
except requests.exceptions.RequestException as e:
    print(f"Error accessing USGS database: {e}")
    print("Will try alternative approaches to access USGS NAS data.")
    
except Exception as e:
    print(f"Unexpected error during USGS database access: {e}")
    print("Error details for debugging:")
    import traceback
    traceback.print_exc()
```