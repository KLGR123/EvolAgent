### Development Step 3: ClinicalTrials.gov H. pylori Treatment Trials in Acne Vulgaris (Jan‚ÄìMay 2018): Actual Enrollment Counts

**Description**: Search the NIH clinical trials database (ClinicalTrials.gov) for clinical trials involving H. pylori treatment in acne vulgaris patients that were conducted during the January-May 2018 timeframe. Extract the actual enrollment count from the trial listing, focusing on studies that specifically examine the relationship between H. pylori infection and acne vulgaris treatment during this exact time period.

**Use Cases**:
- Pharmaceutical market research and competitor analysis by scraping ClinicalTrials.gov to quantify enrollment of H. pylori‚Äìfocused acne trials in early 2018 for strategic pipeline decisions
- Academic meta-analysis workflow where dermatology researchers extract January‚ÄìMay 2018 trial enrollment data on H. pylori and acne vulgaris to feed into a systematic review and strength-of-evidence assessment
- Biostatistics team automating aggregation of patient enrollment figures from first-half-2018 H. pylori‚Äìacne studies to model sample size distributions and optimize power calculations for new trial designs
- Regulatory affairs department compiling a historical dossier of H. pylori acne treatment trials conducted between January and May 2018 to support an FDA briefing package for antibiotic repurposing
- Clinical trial consulting firm tracking enrollment trends in targeted H. pylori and acne vulgaris studies during the specified timeframe to advise sponsors on feasibility, recruitment timelines, and site selection
- Data journalism investigation into past clinical trial activity linking H. pylori infection and acne, using scraped trial counts and dates from early 2018 to highlight shifting research priorities and funding gaps
- Healthcare startup scouting for out-licensing or partnership opportunities by analyzing actual enrollment metrics of H. pylori acne vulgaris trials registered in the first half of 2018 to identify high-impact study results

```
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import time

# Direct web scraping approach to ClinicalTrials.gov since API endpoints are not working
print("=" * 80)
print("WEB SCRAPING CLINICALTRIALS.GOV FOR H. PYLORI ACNE VULGARIS TRIALS")
print("Target Period: January-May 2018")
print("=" * 80)

# ClinicalTrials.gov search URL
base_search_url = "https://clinicaltrials.gov/search"

# Headers to mimic a real browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# Search terms to try
search_queries = [
    "H. pylori acne",
    "Helicobacter pylori acne vulgaris", 
    "H pylori acne treatment",
    "helicobacter acne",
    "pylori acne vulgaris"
]

all_trial_links = set()
all_trials_data = []

print(f"Searching with {len(search_queries)} different terms...\n")

for i, query in enumerate(search_queries, 1):
    print(f"Search {i}/{len(search_queries)}: '{query}'")
    print("-" * 50)
    
    try:
        # Construct search URL with parameters
        params = {
            'term': query,
            'aggFilters': 'status:rec,not,unk,avail,com,sus,term,with',  # All statuses
            'page': '1'
        }
        
        response = requests.get(base_search_url, params=params, headers=headers, timeout=30)
        print(f"HTTP Status: {response.status_code}")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for study result cards or links
            study_links = []
            
            # Try different selectors for study links
            link_selectors = [
                'a[href*="/study/"]',
                'a[href*="NCT"]',
                '.study-card a',
                '.ct-results-item a',
                'a[data-nctid]'
            ]
            
            for selector in link_selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and ('NCT' in href or '/study/' in href):
                        if href.startswith('/'):
                            href = 'https://clinicaltrials.gov' + href
                        study_links.append(href)
                        all_trial_links.add(href)
            
            print(f"Found {len(study_links)} study links")
            
            # Also look for any text mentioning NCT numbers
            nct_pattern = r'NCT\d{8}'
            nct_matches = re.findall(nct_pattern, response.text)
            
            for nct_id in nct_matches:
                study_url = f"https://clinicaltrials.gov/study/{nct_id}"
                study_links.append(study_url)
                all_trial_links.add(study_url)
            
            print(f"Found {len(nct_matches)} NCT IDs in page content")
            
            # Look for results count or "no results" message
            results_text = soup.get_text().lower()
            if 'no studies found' in results_text or 'no results' in results_text:
                print("No studies found for this search term")
            elif 'studies found' in results_text:
                # Try to extract the number
                count_match = re.search(r'(\d+)\s+studies?\s+found', results_text)
                if count_match:
                    print(f"Page indicates {count_match.group(1)} studies found")
            
        else:
            print(f"Failed to retrieve search results: {response.status_code}")
            
    except Exception as e:
        print(f"Error during search: {str(e)}")
    
    print()
    time.sleep(2)  # Be respectful to the server

print(f"\nTotal unique study links found: {len(all_trial_links)}")

# Now scrape individual study pages to get detailed information
print("\n" + "=" * 80)
print("SCRAPING INDIVIDUAL STUDY PAGES")
print("=" * 80)

for i, study_url in enumerate(list(all_trial_links)[:10], 1):  # Limit to first 10 to avoid overloading
    print(f"\nScraping study {i}/{min(10, len(all_trial_links))}: {study_url}")
    print("-" * 60)
    
    try:
        response = requests.get(study_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract study information
            study_data = {
                'url': study_url,
                'nct_id': '',
                'title': '',
                'conditions': [],
                'interventions': [],
                'start_date': '',
                'enrollment': '',
                'status': '',
                'summary': '',
                'has_h_pylori': False,
                'has_acne': False,
                'scraped_at': datetime.now().isoformat()
            }
            
            # Extract NCT ID from URL
            nct_match = re.search(r'NCT\d{8}', study_url)
            if nct_match:
                study_data['nct_id'] = nct_match.group(0)
            
            # Get page text for analysis
            page_text = soup.get_text().lower()
            
            # Check for H. pylori and acne mentions
            h_pylori_terms = ['h. pylori', 'h pylori', 'helicobacter pylori', 'helicobacter']
            acne_terms = ['acne', 'acne vulgaris']
            
            study_data['has_h_pylori'] = any(term in page_text for term in h_pylori_terms)
            study_data['has_acne'] = any(term in page_text for term in acne_terms)
            
            # Extract title
            title_selectors = [
                'h1.ct-gov-title',
                'h1',
                '.study-title',
                '[data-testid="study-title"]'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    study_data['title'] = title_elem.get_text().strip()
                    break
            
            # Extract enrollment
            enrollment_patterns = [
                r'enrollment[:\s]*(\d+)',
                r'(\d+)\s+participants?',
                r'(\d+)\s+subjects?',
                r'estimated enrollment[:\s]*(\d+)',
                r'actual enrollment[:\s]*(\d+)'
            ]
            
            for pattern in enrollment_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    study_data['enrollment'] = matches[0]
                    break
            
            # Extract start date
            date_patterns = [
                r'study start date[:\s]*([^\n]+)',
                r'start date[:\s]*([^\n]+)',
                r'first posted[:\s]*([^\n]+)',
                r'(january|february|march|april|may)\s+\d{1,2},?\s+2018',
                r'2018-0[1-5]-\d{2}'
            ]
            
            for pattern in date_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    study_data['start_date'] = matches[0].strip()
                    break
            
            # Extract status
            status_patterns = [
                r'overall status[:\s]*([^\n]+)',
                r'status[:\s]*([^\n]+)',
                r'recruitment status[:\s]*([^\n]+)'
            ]
            
            for pattern in status_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    study_data['status'] = matches[0].strip()
                    break
            
            # Check if this study is from January-May 2018
            is_target_period = False
            if study_data['start_date']:
                date_str = study_data['start_date'].lower()
                jan_may_2018_patterns = [
                    r'january.*2018',
                    r'february.*2018', 
                    r'march.*2018',
                    r'april.*2018',
                    r'may.*2018',
                    r'2018-0[1-5]'
                ]
                
                is_target_period = any(re.search(pattern, date_str) for pattern in jan_may_2018_patterns)
            
            all_trials_data.append(study_data)
            
            print(f"NCT ID: {study_data['nct_id']}")
            print(f"Title: {study_data['title'][:100]}...")
            print(f"H. pylori mentioned: {'‚úì' if study_data['has_h_pylori'] else '‚úó'}")
            print(f"Acne mentioned: {'‚úì' if study_data['has_acne'] else '‚úó'}")
            print(f"Start date: {study_data['start_date']}")
            print(f"Enrollment: {study_data['enrollment']}")
            print(f"Status: {study_data['status']}")
            print(f"Target period (Jan-May 2018): {'‚úì' if is_target_period else '‚úó'}")
            
            if study_data['has_h_pylori'] and study_data['has_acne']:
                print("üéØ HIGH RELEVANCE: Contains both H. pylori and acne!")
            
        else:
            print(f"Failed to retrieve study page: {response.status_code}")
            
    except Exception as e:
        print(f"Error scraping study: {str(e)}")
    
    time.sleep(1)  # Be respectful to the server

# Analyze results
print("\n" + "=" * 80)
print("ANALYSIS OF SCRAPED CLINICAL TRIALS")
print("=" * 80)

# Filter for relevant trials
relevant_trials = []
jan_may_2018_trials = []

for trial in all_trials_data:
    # Check if relevant (has H. pylori or acne)
    if trial['has_h_pylori'] or trial['has_acne']:
        relevant_trials.append(trial)
        
        # Check if in target time period
        if trial['start_date']:
            date_str = trial['start_date'].lower()
            jan_may_patterns = [
                r'january.*2018',
                r'february.*2018', 
                r'march.*2018',
                r'april.*2018',
                r'may.*2018',
                r'2018-0[1-5]'
            ]
            
            if any(re.search(pattern, date_str) for pattern in jan_may_patterns):
                jan_may_2018_trials.append(trial)

print(f"Total trials scraped: {len(all_trials_data)}")
print(f"Relevant trials (H. pylori or acne): {len(relevant_trials)}")
print(f"Trials with both H. pylori and acne: {len([t for t in relevant_trials if t['has_h_pylori'] and t['has_acne']])}")
print(f"Trials in January-May 2018: {len(jan_may_2018_trials)}")

# Save results
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
results_file = f"workspace/clinicaltrials_scraping_results_{timestamp}.json"

with open(results_file, 'w', encoding='utf-8') as f:
    json.dump({
        'scraping_timestamp': timestamp,
        'search_queries_used': search_queries,
        'total_unique_links_found': len(all_trial_links),
        'total_trials_scraped': len(all_trials_data),
        'relevant_trials_count': len(relevant_trials),
        'jan_may_2018_trials_count': len(jan_may_2018_trials),
        'all_trials_data': all_trials_data,
        'relevant_trials': relevant_trials,
        'jan_may_2018_trials': jan_may_2018_trials,
        'search_criteria': {
            'conditions': ['H. pylori', 'Helicobacter pylori', 'acne', 'acne vulgaris'],
            'time_period': 'January-May 2018',
            'method': 'Web scraping ClinicalTrials.gov'
        }
    }, f, indent=2, ensure_ascii=False)

print(f"\nResults saved to: {results_file}")

# Final summary
print("\n" + "=" * 80)
print("FINAL RESULTS: H. PYLORI ACNE VULGARIS TRIALS (JAN-MAY 2018)")
print("=" * 80)

if jan_may_2018_trials:
    print(f"üéØ SUCCESS: Found {len(jan_may_2018_trials)} clinical trial(s) in target period")
    
    for i, trial in enumerate(jan_may_2018_trials, 1):
        print(f"\nTrial {i}:")
        print(f"  NCT ID: {trial['nct_id']}")
        print(f"  Title: {trial['title']}")
        print(f"  URL: {trial['url']}")
        print(f"  Start Date: {trial['start_date']}")
        print(f"  Enrollment Count: {trial['enrollment']}")
        print(f"  Status: {trial['status']}")
        print(f"  H. pylori mentioned: {'Yes' if trial['has_h_pylori'] else 'No'}")
        print(f"  Acne mentioned: {'Yes' if trial['has_acne'] else 'No'}")
else:
    print("‚ùå No clinical trials found for H. pylori treatment in acne vulgaris patients during January-May 2018")
    
    if relevant_trials:
        print(f"\nHowever, found {len(relevant_trials)} related trials in other time periods:")
        for trial in relevant_trials[:3]:  # Show top 3
            print(f"  - {trial['nct_id']}: {trial['title'][:80]}... (Start: {trial['start_date']})")

print(f"\nDetailed data available in: {results_file}")
```