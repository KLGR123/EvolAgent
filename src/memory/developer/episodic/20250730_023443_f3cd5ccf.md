### Development Step 12: Cross-Reference LOTR Outbounds and ‚ÄòA Song of Ice and Fire‚Äô Inbounds on July 3 Archive

**Description**: Access the archived Wikipedia page for 'A Song of Ice and Fire' (book series) as it appeared at the end of July 3, 2023. Use the same Wayback Machine approach to retrieve the specific version from that date. Extract all inbound links and cross-reference them with the previously extracted outbound links from The Lord of the Rings page to identify direct connections. If no direct connection exists, systematically trace potential multi-step paths by examining intermediate pages that were linked from LOTR, particularly focusing on the high-priority fantasy/literature connections like 'International Fantasy Award', 'High fantasy', and other genre-related pages that might contain links to George R.R. Martin's work.

**Use Cases**:
- Digital humanities research: mapping literary influences by extracting archived LOTR and ASOIAF Wikipedia links to analyze how fantasy tropes and author references propagated between series over time
- Streaming platform recommendation engine: leveraging cross-referenced fantasy series links from July 3, 2023 Wayback snapshots to improve personalized content suggestions based on direct and multi-step connections
- SEO optimization for fan wikis: identifying inbound/outbound link patterns between major fantasy pages to strategize high-value backlink outreach and boost search visibility of related franchise content
- Academic library metadata curation: using Wayback Machine‚Äìextracted link data to track historical references to fantasy awards (e.g., International Fantasy Award, Hugo Award) across series and enrich catalog records
- NLP dataset creation: building a ground-truth corpus of normalized entity relationships from medieval fantasy Wikipedia snapshots for training machine learning models in named-entity linking and relation extraction
- Video game lore integration: constructing an interconnected lore database for in-game encyclopedias by tracing multi-step Wikipedia connections between Tolkien‚Äôs legendarium and Martin‚Äôs world-building
- Journalism fact-checking and media analysis: verifying cross-series influence claims by comparing archived revisions of fantasy genre Wikipedia pages and auditing when and where references appeared
- Publishing editorial workflow: standardizing genre classifications and reference lists in fantasy book catalogs by extracting multi-step link paths to ensure consistent editorial guidelines across different series

```
import os
import json
from datetime import datetime
from collections import defaultdict
import re

print('=== CROSS-REFERENCING LOTR AND ASOIAF WIKIPEDIA LINKS ===')
print('Objective: Find direct connections between LOTR outbound links and ASOIAF inbound links')
print('Strategy: Compare the 379 ASOIAF links with 878 LOTR links to identify matches\n')

# First, let's understand the workspace structure and file contents
workspace_files = os.listdir('workspace')
print(f'Available workspace files: {workspace_files}\n')

# Load and inspect the ASOIAF data structure
asoiaf_file = 'workspace/asoiaf_wikipedia_archived_july_2023.json'
print('=== INSPECTING ASOIAF DATA STRUCTURE ===')

with open(asoiaf_file, 'r', encoding='utf-8') as f:
    asoiaf_data = json.load(f)

print(f'ASOIAF data top-level keys: {list(asoiaf_data.keys())}')
for key, value in asoiaf_data.items():
    if isinstance(value, list):
        print(f'  {key}: List with {len(value)} items')
    elif isinstance(value, dict):
        print(f'  {key}: Dictionary with keys: {list(value.keys())}')
    else:
        print(f'  {key}: {type(value).__name__} - {str(value)[:100]}...')

# Inspect the inbound links structure in detail
if 'inbound_links' in asoiaf_data:
    sample_links = asoiaf_data['inbound_links'][:3] if len(asoiaf_data['inbound_links']) > 0 else []
    print(f'\nSample ASOIAF inbound links structure:')
    for i, link in enumerate(sample_links, 1):
        print(f'  Link {i}: {link}')
        if isinstance(link, dict):
            print(f'    Keys: {list(link.keys())}')

# Load and inspect the LOTR data structure  
lotr_file = 'workspace/lotr_wikipedia_links_july_2023.json'
print('\n=== INSPECTING LOTR DATA STRUCTURE ===')

with open(lotr_file, 'r', encoding='utf-8') as f:
    lotr_data = json.load(f)

print(f'LOTR data top-level keys: {list(lotr_data.keys())}')
for key, value in lotr_data.items():
    if isinstance(value, list):
        print(f'  {key}: List with {len(value)} items')
    elif isinstance(value, dict):
        print(f'  {key}: Dictionary with keys: {list(value.keys())}')
        if key == 'categorized_links':
            for cat_key, cat_value in value.items():
                if isinstance(cat_value, list):
                    print(f'    {cat_key}: {len(cat_value)} items')
    else:
        print(f'  {key}: {type(value).__name__}')

# Inspect the wikipedia links structure in detail
if 'wikipedia_links' in lotr_data:
    sample_lotr_links = lotr_data['wikipedia_links'][:3] if len(lotr_data['wikipedia_links']) > 0 else []
    print(f'\nSample LOTR wikipedia links structure:')
    for i, link in enumerate(sample_lotr_links, 1):
        print(f'  Link {i}: {link}')
        if isinstance(link, dict):
            print(f'    Keys: {list(link.keys())}')

print('\n=== EXTRACTING CLEAN URLS FOR COMPARISON ===')

# Extract clean URLs from ASOIAF inbound links
asoiaf_links = asoiaf_data['inbound_links']
print(f'Total ASOIAF inbound links: {len(asoiaf_links)}')

# Clean ASOIAF URLs by removing Wayback Machine prefixes and extracting article titles
asoiaf_articles = set()
asoiaf_clean_urls = {}

print('\nProcessing ASOIAF links (first 5):')
for i, link in enumerate(asoiaf_links[:5]):
    print(f'  Processing link {i+1}: {link}')
    
    url = link['url']
    article_title = link['article_title']
    
    # Extract the actual Wikipedia URL from Wayback Machine URL
    if 'web.archive.org' in url:
        # Extract the original URL after the timestamp
        parts = url.split('https://en.wikipedia.org/wiki/')
        if len(parts) > 1:
            clean_article = parts[-1]
        else:
            clean_article = article_title.replace(' ', '_')
    else:
        clean_article = url.split('/wiki/')[-1]
    
    # Normalize the article title
    normalized_title = clean_article.replace('_', ' ').strip().lower()
    asoiaf_articles.add(normalized_title)
    asoiaf_clean_urls[normalized_title] = {
        'original_url': url,
        'article_title': article_title,
        'normalized_title': normalized_title
    }
    print(f'    Normalized: {normalized_title}')

# Process all ASOIAF links
for link in asoiaf_links:
    url = link['url']
    article_title = link['article_title']
    
    # Extract the actual Wikipedia URL from Wayback Machine URL
    if 'web.archive.org' in url:
        parts = url.split('https://en.wikipedia.org/wiki/')
        if len(parts) > 1:
            clean_article = parts[-1]
        else:
            clean_article = article_title.replace(' ', '_')
    else:
        clean_article = url.split('/wiki/')[-1]
    
    # Normalize the article title
    normalized_title = clean_article.replace('_', ' ').strip().lower()
    asoiaf_articles.add(normalized_title)
    asoiaf_clean_urls[normalized_title] = {
        'original_url': url,
        'article_title': article_title,
        'normalized_title': normalized_title
    }

print(f'\nUnique ASOIAF articles (normalized): {len(asoiaf_articles)}')
print('\nFirst 10 ASOIAF articles:')
for i, article in enumerate(sorted(list(asoiaf_articles))[:10], 1):
    print(f'  {i:2d}. {article}')

# Extract clean URLs from LOTR outbound links - FIX: Use 'href' instead of 'url'
lotr_links = lotr_data['wikipedia_links']
print(f'\nTotal LOTR outbound links: {len(lotr_links)}')

# Clean LOTR URLs - Updated to use correct keys
lotr_articles = set()
lotr_clean_urls = {}

print('\nProcessing LOTR links (first 5):')
for i, link in enumerate(lotr_links[:5]):
    print(f'  Processing link {i+1}: {link}')
    
    # Use 'href' key for LOTR links instead of 'url'
    href = link['href']
    article_name = link.get('article_name', '')
    text = link.get('text', '')
    
    # Extract clean article name from href
    if 'web.archive.org' in href:
        parts = href.split('https://en.wikipedia.org/wiki/')
        if len(parts) > 1:
            clean_article = parts[-1]
        else:
            clean_article = article_name.replace(' ', '_')
    else:
        clean_article = href.split('/wiki/')[-1]
    
    # URL decode the article name
    import urllib.parse
    clean_article = urllib.parse.unquote(clean_article)
    
    # Normalize the article title
    normalized_title = clean_article.replace('_', ' ').strip().lower()
    lotr_articles.add(normalized_title)
    lotr_clean_urls[normalized_title] = {
        'original_href': href,
        'article_name': article_name,
        'text': text,
        'normalized_title': normalized_title
    }
    print(f'    Normalized: {normalized_title}')

# Process all LOTR links
for link in lotr_links:
    href = link['href']
    article_name = link.get('article_name', '')
    text = link.get('text', '')
    
    # Extract clean article name from href
    if 'web.archive.org' in href:
        parts = href.split('https://en.wikipedia.org/wiki/')
        if len(parts) > 1:
            clean_article = parts[-1]
        else:
            clean_article = article_name.replace(' ', '_')
    else:
        clean_article = href.split('/wiki/')[-1]
    
    # URL decode the article name
    clean_article = urllib.parse.unquote(clean_article)
    
    # Normalize the article title
    normalized_title = clean_article.replace('_', ' ').strip().lower()
    lotr_articles.add(normalized_title)
    lotr_clean_urls[normalized_title] = {
        'original_href': href,
        'article_name': article_name,
        'text': text,
        'normalized_title': normalized_title
    }

print(f'\nUnique LOTR articles (normalized): {len(lotr_articles)}')
print('\nFirst 10 LOTR articles:')
for i, article in enumerate(sorted(list(lotr_articles))[:10], 1):
    print(f'  {i:2d}. {article}')

print('\n=== FINDING DIRECT CONNECTIONS ===')

# Find direct matches between LOTR outbound links and ASOIAF inbound links
direct_connections = lotr_articles.intersection(asoiaf_articles)

print(f'\nDirect connections found: {len(direct_connections)}')

if direct_connections:
    print('\n*** DIRECT CONNECTIONS DISCOVERED ***')
    for i, connection in enumerate(sorted(direct_connections), 1):
        print(f'{i:2d}. {connection}')
        
        # Show details from both sides
        lotr_info = lotr_clean_urls[connection]
        asoiaf_info = asoiaf_clean_urls[connection]
        
        print(f'    LOTR -> {lotr_info["text"]} ({lotr_info["original_href"]})')
        print(f'    ASOIAF -> {asoiaf_info["article_title"]} ({asoiaf_info["original_url"]})')
        print()
else:
    print('\n‚ùå NO DIRECT CONNECTIONS FOUND')
    print('Need to explore multi-step paths through intermediate pages')

# Let's also check high-priority fantasy/literature connections for potential stepping stones
print('\n=== ANALYZING HIGH-PRIORITY FANTASY/LITERATURE CONNECTIONS ===')

# Load the LOTR path-finding analysis
lotr_analysis_file = 'workspace/lotr_path_finding_analysis.json'
with open(lotr_analysis_file, 'r', encoding='utf-8') as f:
    lotr_analysis = json.load(f)

print(f'LOTR analysis structure: {list(lotr_analysis.keys())}')

if 'high_priority_links' in lotr_analysis:
    high_priority_links = lotr_analysis['high_priority_links']
    print(f'High-priority LOTR links: {len(high_priority_links)}')
    
    # Check if any high-priority links are also in ASOIAF inbound links
    high_priority_matches = []
    
    for hp_link in high_priority_links:
        if isinstance(hp_link, dict) and 'article_title' in hp_link:
            hp_title = hp_link['article_title'].lower()
            if hp_title in asoiaf_articles:
                high_priority_matches.append((hp_link, asoiaf_clean_urls[hp_title]))
    
    print(f'\nHigh-priority matches with ASOIAF: {len(high_priority_matches)}')
    
    if high_priority_matches:
        print('\n*** HIGH-PRIORITY CONNECTIONS FOUND ***')
        for i, (lotr_link, asoiaf_link) in enumerate(high_priority_matches, 1):
            print(f'{i}. {lotr_link["article_title"]} (Priority: {lotr_link.get("priority", "unknown")})')
            print(f'   Category: {lotr_link.get("category", "unknown")}')
            print(f'   LOTR URL: {lotr_link.get("href", lotr_link.get("url", "unknown"))}')
            print(f'   ASOIAF match: {asoiaf_link["normalized_title"]}')
            print()

# Identify potential stepping stones for multi-step paths
print('\n=== IDENTIFYING POTENTIAL STEPPING STONES ===')

# Look for fantasy/literature-related terms that might serve as intermediate connections
fantasy_keywords = [
    'fantasy', 'epic fantasy', 'high fantasy', 'dark fantasy', 'sword and sorcery',
    'literature', 'fiction', 'novel', 'author', 'writer', 'publishing',
    'award', 'hugo award', 'nebula award', 'world fantasy award', 'international fantasy award',
    'tolkien', 'martin', 'genre', 'medieval', 'magic', 'dragon', 'mythology'
]

stepping_stone_candidates = []

# Check LOTR links for fantasy-related terms
for link in lotr_links:
    text = link.get('text', '').lower()
    article_name = link.get('article_name', '').lower()
    search_text = f'{text} {article_name}'.lower()
    
    if any(keyword in search_text for keyword in fantasy_keywords):
        stepping_stone_candidates.append({
            'source': 'lotr',
            'text': link.get('text', ''),
            'article_name': link.get('article_name', ''),
            'href': link.get('href', ''),
            'matching_keywords': [kw for kw in fantasy_keywords if kw in search_text]
        })

print(f'Potential stepping stones from LOTR: {len(stepping_stone_candidates)}')

# Show top stepping stone candidates
print('\nTop 15 stepping stone candidates:')
for i, candidate in enumerate(stepping_stone_candidates[:15], 1):
    print(f'{i:2d}. {candidate["text"]} (Keywords: {", ".join(candidate["matching_keywords"])})')

# Save comprehensive analysis results
connection_analysis = {
    'analysis_metadata': {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'lotr_links_analyzed': len(lotr_articles),
        'asoiaf_links_analyzed': len(asoiaf_articles),
        'direct_connections_found': len(direct_connections),
        'high_priority_matches_found': len(high_priority_matches) if 'high_priority_matches' in locals() else 0,
        'stepping_stone_candidates': len(stepping_stone_candidates)
    },
    'direct_connections': list(direct_connections),
    'direct_connections_details': [
        {
            'article': connection,
            'lotr_info': lotr_clean_urls[connection],
            'asoiaf_info': asoiaf_clean_urls[connection]
        }
        for connection in direct_connections
    ],
    'high_priority_matches': high_priority_matches if 'high_priority_matches' in locals() else [],
    'stepping_stone_candidates': stepping_stone_candidates,
    'lotr_articles_sample': sorted(list(lotr_articles))[:50],  # First 50 for space
    'asoiaf_articles_sample': sorted(list(asoiaf_articles))[:50]  # First 50 for space
}

output_file = 'workspace/lotr_asoiaf_connection_analysis.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(connection_analysis, f, indent=2, ensure_ascii=False)

print(f'\n=== ANALYSIS COMPLETE ===')
print(f'‚úì Analyzed {len(lotr_articles)} LOTR outbound links vs {len(asoiaf_articles)} ASOIAF inbound links')
print(f'‚úì Found {len(direct_connections)} direct connections')
print(f'‚úì Identified {len(high_priority_matches) if "high_priority_matches" in locals() else 0} high-priority fantasy/literature matches')
print(f'‚úì Located {len(stepping_stone_candidates)} potential stepping stones for multi-step paths')
print(f'‚úì Analysis results saved to: {output_file}')

if direct_connections:
    print(f'\nüéâ SUCCESS: Direct Wikipedia connections found between LOTR and ASOIAF pages!')
    print(f'The connection exists through {len(direct_connections)} shared articles:')
    for connection in sorted(direct_connections):
        print(f'  - {connection}')
else:
    print(f'\n‚û°Ô∏è NEXT STEP: No direct connections found. Need to explore multi-step paths through the {len(stepping_stone_candidates)} identified stepping stones')
    print(f'Most promising candidates appear to be fantasy/literature-related pages that could link to both series')
```