### Development Step 5: Axis Labels in a Three-Axis Figure from a June 2022 arXiv AI Regulation Paper

**Description**: Search arXiv.org for AI regulation papers originally submitted in June 2022. Focus on identifying papers that discuss artificial intelligence regulation, policy, governance, or related topics. Look for papers with submission dates in June 2022 and examine their content to find one containing a figure with three axes where each axis has label words at both ends. Extract the specific words used as axis labels in this figure.

**Use Cases**:
- Academic literature review automation for policy researchers investigating trends in AI governance frameworks submitted to arXiv in mid-2022
- Regulatory compliance teams in technology firms scanning arXiv for visual models of AI risk assessment to inform internal guidelines
- Government advisory committees extracting axis label terminology from multi-dimensional policy diagrams in recent AI regulation papers
- Data science teams building knowledge graphs from figure metadata in scholarly articles about AI policy, focusing on June 2022 submissions
- University librarians curating collections of influential AI governance papers by identifying unique visual frameworks and their label semantics
- Think tanks conducting comparative analysis of global AI regulation strategies by mining arXiv for papers with three-axis conceptual models
- Automated citation tools enriching bibliographic databases with extracted figure label contexts from AI regulation research
- AI ethics educators sourcing illustrative diagrams with labeled axes from recent arXiv publications to use in classroom discussions

```
import os
import json

print('=== EXAMINING PDF ANALYSIS RESULTS FOR FIGURE DETECTION ===') 
print('Objective: Understand what content was found and refine search strategy\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# First, let's inspect the analysis results file structure
results_file = os.path.join(workspace, 'pdf_analysis_results.json')

if os.path.exists(results_file):
    print(f'Found analysis results file: {results_file}')
    print(f'File size: {os.path.getsize(results_file):,} bytes\n')
    
    # Load and inspect the structure
    with open(results_file, 'r', encoding='utf-8') as f:
        results_data = json.load(f)
    
    print('=== ANALYSIS RESULTS FILE STRUCTURE ===') 
    print(f'Main keys: {list(results_data.keys())}')
    print(f'Analysis date: {results_data.get("analysis_date", "Unknown")}')
    print(f'Objective: {results_data.get("objective", "Unknown")}')
    print(f'Papers analyzed: {results_data.get("papers_analyzed", 0)}\n')
    
    # Examine each paper's analysis results
    analysis_results = results_data.get('analysis_results', [])
    
    for i, result in enumerate(analysis_results, 1):
        print(f'=== PAPER {i} ANALYSIS DETAILS ===')
        
        paper_info = result.get('paper_info', {})
        print(f'Title: {paper_info.get("title", "Unknown")}')
        print(f'arXiv ID: {paper_info.get("arxiv_id", "Unknown")}')
        print(f'Pages: {paper_info.get("page_count", "Unknown")}')
        print(f'File size: {paper_info.get("file_size", "Unknown")} bytes')
        
        if 'error' in paper_info:
            print(f'❌ Error: {paper_info["error"]}')
            continue
        
        # Check what was actually found
        figure_matches = result.get('figure_matches', [])
        axis_matches = result.get('axis_matches', [])
        potential_labels = result.get('potential_labels', [])
        
        print(f'Figure matches found: {len(figure_matches)}')
        print(f'Axis matches found: {len(axis_matches)}')
        print(f'Potential label contexts: {len(potential_labels)}')
        
        # Show the potential label contexts that were found
        if potential_labels:
            print('\nPotential label contexts found:')
            for j, label_context in enumerate(potential_labels[:5], 1):
                # Clean up the context for display
                context = str(label_context).strip().replace('\n', ' ')[:150]
                print(f'  {j}. {context}...')
        
        print(f'\n{"-"*60}\n')
    
    # Now let's check what papers we haven't analyzed yet
    plan_file = os.path.join(workspace, 'pdf_download_plan.json')
    if os.path.exists(plan_file):
        with open(plan_file, 'r', encoding='utf-8') as f:
            plan_data = json.load(f)
        
        all_papers = plan_data.get('download_priority', [])
        analyzed_count = len(analysis_results)
        remaining_papers = all_papers[analyzed_count:]
        
        print(f'=== REMAINING PAPERS TO ANALYZE ===') 
        print(f'Total papers available: {len(all_papers)}')
        print(f'Papers already analyzed: {analyzed_count}')
        print(f'Papers remaining: {len(remaining_papers)}\n')
        
        if remaining_papers:
            print('Remaining papers:')
            for i, paper in enumerate(remaining_papers, 1):
                print(f'{i}. "{paper.get("title", "No title")[:60]}..."')
                print(f'   arXiv ID: {paper.get("arxiv_id", "Unknown")}')
                print(f'   Relevance Score: {paper.get("relevance_score", 0)}')
                print(f'   PDF: {paper.get("pdf_url", "No URL")}')
                print()
        
        print('=== ANALYSIS STRATEGY ASSESSMENT ===')
        print('Current findings:')
        print('✓ Successfully downloaded and parsed 3 PDF files')
        print('✓ Extracted text content from all pages')
        print('❌ No figure references with "axis" found using current regex patterns')
        print('❌ No three-axis figure descriptions detected')
        print('⚠ Only found basic potential label contexts (low/high, etc.)')
        print()
        print('Possible reasons for no matches:')
        print('1. The target figure might be in the remaining 2 papers')
        print('2. Figure descriptions might use different terminology')
        print('3. Figures might be described in captions or different sections')
        print('4. The search patterns might be too restrictive')
        print()
        print('RECOMMENDED NEXT STEPS:')
        print('1. Analyze the remaining 2 papers (scores 4 and 1)')
        print('2. Use broader search patterns for figure detection')
        print('3. Search for terms like "diagram", "chart", "visualization"')
        print('4. Look for figure captions and table descriptions')
        print('5. Search for dimensional analysis or framework descriptions')
        
        # Save assessment for next phase
        assessment = {
            'current_status': 'No three-axis figures found in first 3 papers',
            'papers_analyzed': analyzed_count,
            'papers_remaining': len(remaining_papers),
            'remaining_papers': remaining_papers,
            'next_actions': [
                'Analyze remaining 2 papers',
                'Use broader figure search patterns',
                'Search for diagram/chart/visualization terms',
                'Look for figure captions and descriptions',
                'Search for framework or dimensional analysis content'
            ],
            'assessment_timestamp': results_data.get('analysis_date')
        }
        
        assessment_file = os.path.join(workspace, 'figure_search_assessment.json')
        with open(assessment_file, 'w', encoding='utf-8') as f:
            json.dump(assessment, f, indent=2, ensure_ascii=False)
        
        print(f'\n✓ Assessment saved to: {assessment_file}')
        print('Ready to proceed with expanded analysis of remaining papers')
    
    else:
        print('❌ Download plan file not found - cannot determine remaining papers')

else:
    print(f'❌ Analysis results file not found: {results_file}')
    print('Need to run the PDF analysis phase first')

print('\n*** ANALYSIS RESULTS EXAMINATION COMPLETE ***')
```