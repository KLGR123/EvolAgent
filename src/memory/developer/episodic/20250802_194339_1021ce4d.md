### Development Step 19: Identify Hreidmarâ€™s Son-Named Journal and Locate Emily Midkiffâ€™s June 2014 Dragon Depiction Article

**Description**: Search for information about Emily Midkiff's June 2014 article, focusing on identifying the journal named after one of Hreidmar's sons from Norse mythology. Research Hreidmar's sons (Fafnir, Regin, and Otr) to determine which one has a journal named after them, then locate Midkiff's specific article from June 2014 that discusses dragon depictions and contains quotes from two different authors expressing distaste.

**Use Cases**:
- Automated academic literature retrieval and citation extraction for Norse mythology research using journal names derived from mythological figures as search filters
- Digital library curation tool for librarians to monitor and index new issues of â€œFafnir: Journal of Dragon Studiesâ€ and automatically extract article metadata
- Sentiment analysis pipeline for literary scholars to identify and extract critical quotes on dragon depictions within scholarly articles
- Workflow automation for folklore researchers to systematically scan HTML archives and extract contextual snippets around specific author mentions (e.g., Midkiff)
- Metadata verification system for publishing editors to validate references and link integrity in submitted mythological research papers
- Educational resource builder for mythology professors to compile annotated excerpts of key terms like â€œMidkiffâ€ in dragon studies literature
- Research reproducibility framework for digital humanities projects that validates web-scraped article content and tracks extraction success rates
- Automated content discovery and archiving service for conference organizers to pre-screen relevant papers by scanning search results and filtering by custom keywords

```
import os
import json
from bs4 import BeautifulSoup

print('=== FINAL PUSH: EXTRACTING EMILY MIDKIFF ARTICLE FROM SEARCH RESULTS ===')
print('Objective: Fix all variable scope issues and successfully extract Emily Midkiff article')
print('Strategy: Careful variable definition and systematic link extraction\n')

# Define workspace path
workspace = 'workspace'
if not os.path.exists(workspace):
    print(f'ERROR: Workspace directory {workspace} does not exist')
else:
    print(f'âœ“ Workspace directory found: {workspace}')

print('\n=== STEP 1: INSPECTING WORKSPACE STRUCTURE ===')
print()

# First, let's understand exactly what files we have
try:
    workspace_files = os.listdir(workspace)
    print(f'Total files in workspace: {len(workspace_files)}')
    
    # Categorize files
    search_files = [f for f in workspace_files if 'search_results_' in f and f.endswith('.html')]
    json_files = [f for f in workspace_files if f.endswith('.json')]
    archive_files = [f for f in workspace_files if 'archive_' in f and f.endswith('.html')]
    
    print(f'Search result files: {len(search_files)}')
    for filename in search_files:
        filepath = os.path.join(workspace, filename)
        file_size = os.path.getsize(filepath)
        print(f'  - {filename} ({file_size:,} bytes)')
    
    print(f'\nJSON analysis files: {len(json_files)}')
    for filename in json_files:
        print(f'  - {filename}')
    
    print(f'\nArchive files: {len(archive_files)}')
    for filename in archive_files:
        print(f'  - {filename}')
        
except Exception as e:
    print(f'Error listing workspace files: {str(e)}')

print('\n=== STEP 2: SYSTEMATIC ANALYSIS OF SEARCH RESULTS ===')
print()

# Process each search result file with careful variable management
search_results_analysis = {}

for search_file in search_files:
    search_path = os.path.join(workspace, search_file)
    print(f'Processing: {search_file}')
    
    try:
        # Read the search result file
        with open(search_path, 'r', encoding='utf-8') as f:
            search_content = f.read()
        
        # Parse with BeautifulSoup
        search_soup = BeautifulSoup(search_content, 'html.parser')
        search_text = search_soup.get_text()
        search_text_lower = search_text.lower()  # Define this variable properly
        
        print(f'  Content loaded: {len(search_content):,} characters')
        
        # Count key terms
        key_terms = {
            'midkiff': search_text_lower.count('midkiff'),
            'emily': search_text_lower.count('emily'),
            'dragon': search_text_lower.count('dragon'),
            'june': search_text_lower.count('june'),
            '2014': search_text_lower.count('2014'),
            'depiction': search_text_lower.count('depiction'),
            'distaste': search_text_lower.count('distaste')
        }
        
        print('  Key term analysis:')
        relevant_terms_found = []
        for term, count in key_terms.items():
            if count > 0:
                print(f'    {term}: {count} occurrences')
                relevant_terms_found.append(f'{term}({count})')
        
        # Extract all links with proper variable scoping
        all_links = search_soup.find_all('a', href=True)
        article_links = []
        
        print(f'  Processing {len(all_links)} links...')
        
        for link in all_links:
            # Define all variables at the start of the loop
            href = link.get('href', '')
            link_text = link.get_text().strip()  # Define this properly
            link_text_lower = link_text.lower()  # Define this properly
            href_lower = href.lower()  # Define this properly
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                full_url = f'https://fafnir.journal.fi{href}'
            elif not href.startswith('http'):
                full_url = f'https://fafnir.journal.fi/{href}'
            else:
                full_url = href
            
            # Check for article-like patterns
            is_article_url = '/article/' in href_lower or '/view/' in href_lower
            has_relevant_text = any(term in link_text_lower for term in ['midkiff', 'dragon', '2014', 'june', 'emily'])
            is_substantial = len(link_text.strip()) > 5
            
            # Collect promising links
            if (is_article_url or has_relevant_text) and is_substantial:
                article_links.append({
                    'url': full_url,
                    'text': link_text[:200],  # Longer text for better context
                    'href': href,
                    'is_article_url': is_article_url,
                    'has_relevant_text': has_relevant_text,
                    'link_length': len(link_text)
                })
        
        print(f'  Found {len(article_links)} potentially relevant links')
        
        # Show the most promising links
        if article_links:
            print('  Top promising links:')
            for i, link in enumerate(article_links[:3], 1):
                print(f'    {i}. "{link["text"][:80]}..."')
                print(f'       URL: {link["url"]}')
                print(f'       Article URL: {link["is_article_url"]}, Relevant text: {link["has_relevant_text"]}')
                print()
        
        # Store analysis results
        search_results_analysis[search_file] = {
            'file_size': len(search_content),
            'key_terms': key_terms,
            'relevant_terms_found': relevant_terms_found,
            'total_links': len(all_links),
            'article_links': article_links,
            'analysis_success': True
        }
        
    except Exception as e:
        print(f'  ERROR processing {search_file}: {str(e)}')
        search_results_analysis[search_file] = {
            'analysis_success': False,
            'error': str(e)
        }
    
    print()

print('=== STEP 3: CONSOLIDATING ALL PROMISING LINKS ===')
print()

# Collect all article links from successful analyses
all_promising_links = []
for search_file, analysis in search_results_analysis.items():
    if analysis.get('analysis_success', False) and 'article_links' in analysis:
        for link in analysis['article_links']:
            link['source_search'] = search_file
            all_promising_links.append(link)

# Remove duplicates based on URL
unique_links = []
seen_urls = set()
for link in all_promising_links:
    if link['url'] not in seen_urls:
        seen_urls.add(link['url'])
        unique_links.append(link)

print(f'Total unique promising links found: {len(unique_links)}')

# Sort by relevance (prioritize article URLs and relevant text)
unique_links.sort(key=lambda x: (x['is_article_url'], x['has_relevant_text'], x['link_length']), reverse=True)

print('\nTop 10 most promising links:')
for i, link in enumerate(unique_links[:10], 1):
    print(f'{i}. "{link["text"][:100]}..."')
    print(f'   URL: {link["url"]}')
    print(f'   Source: {link["source_search"]}')
    print(f'   Article URL: {link["is_article_url"]}, Relevant text: {link["has_relevant_text"]}')
    print()

print('=== STEP 4: ACCESSING THE MOST PROMISING ARTICLE ===')
print()

if unique_links:
    # Try to access the most promising article link
    target_link = unique_links[0]
    
    print(f'Attempting to access most promising article:')
    print(f'Title: "{target_link["text"][:100]}..."')
    print(f'URL: {target_link["url"]}')
    print()
    
    try:
        import requests
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive'
        }
        
        print('Making request to article...')
        article_response = requests.get(target_link['url'], headers=headers, timeout=30)
        print(f'Article response status: {article_response.status_code}')
        
        if article_response.status_code == 200:
            # Save the article
            article_filename = 'emily_midkiff_target_article.html'
            article_path = os.path.join(workspace, article_filename)
            
            with open(article_path, 'w', encoding='utf-8') as f:
                f.write(article_response.text)
            print(f'âœ“ Article saved to: {article_filename}')
            
            # Parse and analyze the article
            article_soup = BeautifulSoup(article_response.content, 'html.parser')
            article_text = article_soup.get_text()
            article_text_lower = article_text.lower()
            
            print(f'Article content length: {len(article_text):,} characters')
            
            # Analyze article content for our target terms
            article_analysis = {
                'emily midkiff': article_text_lower.count('emily midkiff'),
                'midkiff': article_text_lower.count('midkiff'),
                'dragon': article_text_lower.count('dragon'),
                'depiction': article_text_lower.count('depiction'),
                'distaste': article_text_lower.count('distaste'),
                'june': article_text_lower.count('june'),
                '2014': article_text_lower.count('2014')
            }
            
            print('\nArticle content analysis:')
            target_found = False
            for term, count in article_analysis.items():
                if count > 0:
                    print(f'  {term}: {count} occurrences')
                    if term in ['emily midkiff', 'midkiff'] and count > 0:
                        target_found = True
            
            if target_found:
                print('\nðŸŽ¯ *** EMILY MIDKIFF ARTICLE SUCCESSFULLY FOUND! ***')
                
                # Extract contexts around Midkiff mentions
                midkiff_contexts = []
                start_pos = 0
                while True:
                    pos = article_text_lower.find('midkiff', start_pos)
                    if pos == -1:
                        break
                    
                    context_start = max(0, pos - 300)
                    context_end = min(len(article_text), pos + 400)
                    context = article_text[context_start:context_end].strip()
                    midkiff_contexts.append(context)
                    start_pos = pos + 1
                
                print(f'\nFound {len(midkiff_contexts)} Midkiff contexts:')
                for i, context in enumerate(midkiff_contexts[:2], 1):
                    print(f'{i}. Context around Midkiff:')
                    print(f'   ...{context[:200]}...')
                    print()
                
                # Look for quotes in the article
                quote_patterns = []
                # Split by various quote marks
                for quote_char in ['"', '
```