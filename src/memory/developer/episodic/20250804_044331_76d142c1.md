### Development Step 28: Extract Survivor Seasons 1–44 Winners into workspace JSON file

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Reality TV fan portal automation for an entertainment website: scrape and update an interactive Season vs. Winner timeline widget whenever a new Survivor season concludes.
- Data journalism analysis on reality show diversity: extract winner names and cross‐reference with public demographic data to chart changes in gender and ethnicity representation across 44 seasons.
- Trivia chatbot development for Slack communities: feed the season–winner JSON into a bot’s knowledge base so team members can query “Who won season 12?” in real time.
- Academic media‐studies research on outcome patterns: automate extraction of winners for statistical modelling of social dynamics and alliance strategies in reality television.
- E-commerce merchandising personalization: pull the latest winner list to dynamically recommend champion‐themed apparel or memorabilia tied to each Survivor season in an online store.
- Pub quiz and event planning tool for themed bars: generate question banks and answer sheets by importing season–winner pairs to create engaging Survivor trivia nights.
- DevOps monitoring and alert system: schedule a daily job to scrape the winners table, compare against existing JSON, and trigger notifications if a new ‘season’ entry appears (e.g., when Season 45 is added).

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace}\n")

# 2) Fetch the printable Wiki page
page = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={page}&printable=yes"
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers=headers)
resp.raise_for_status()
print(f"Page fetched (status {resp.status_code})\n")

# 3) Parse with BeautifulSoup
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Find the winners-only table if it exists (header ['Season','Winner']), else fallback
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} wikitable(s). Scanning headers...\n")

target = None
for idx, tbl in enumerate(tables, 1):
    # only look at <th> cells in first row
    first = tbl.find('tr')
    hdr_cells = first.find_all('th', recursive=False)
    hdr = [c.get_text(strip=True).lower() for c in hdr_cells]
    print(f"Table {idx} headers: {hdr}")
    # pick the simple winners-only table
    if hdr == ['season', 'winner']:
        target = tbl
        print(f"→ Selected winners-only table {idx}.\n")
        break

# fallback: any table containing both
if not target:
    for idx, tbl in enumerate(tables, 1):
        hdr_cells = tbl.find('tr').find_all(['th','td'], recursive=False)
        hdr = [c.get_text(strip=True).lower() for c in hdr_cells]
        if 'season' in hdr and 'winner' in hdr:
            target = tbl
            print(f"→ Fallback selected table {idx} (contains 'season' & 'winner').\n")
            break

if not target:
    print("❌ Could not find a table with Season & Winner. Exiting.")
    sys.exit(1)

# 5) Determine indices (for simple winners-only table these will be 0 and 1)
hdr_cells = target.find('tr').find_all(['th','td'], recursive=False)
cols = [c.get_text(strip=True).lower() for c in hdr_cells]
season_idx = cols.index('season')
winner_idx = cols.index('winner')
print(f"Column indices -> season: {season_idx}, winner: {winner_idx}\n")

# 6) Helper to clean winner name

def clean_winner(cell):
    # remove footnotes
    for sup in cell.find_all('sup'):
        sup.decompose()
    # get link text if any
    a = cell.find('a')
    if a and re.search(r'[A-Za-z]', a.get_text()):
        return a.get_text(strip=True)
    # else return stripped text
    text = cell.get_text(' ', strip=True)
    return text

# 7) Iterate rows and build list
winners = []
for row in target.find_all('tr')[1:]:
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # parse season number
    s = cells[season_idx].get_text(strip=True)
    if not s.isdigit():
        continue
    num = int(s)
    if not (1 <= num <= 44):
        continue
    name = clean_winner(cells[winner_idx])
    print(f"Parsed Season {num} → Winner: '{name}'")
    winners.append({'season': num, 'winner': name})

# 8) Sort & verify
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expect 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: count != 44. Check page structure.")

# 9) Save JSON
out = os.path.join(workspace, 'survivor_winners_list.json')
with open(out, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Saved winners list to: {out}")",
  
```