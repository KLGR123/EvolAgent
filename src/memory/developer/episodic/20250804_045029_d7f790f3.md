### Development Step 56: Extract Survivor (US) Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- TV network analytics automation for correlating Survivor season winners with viewership ratings and marketing spend
- Mobile quiz app development to dynamically populate trivia questions and ensure up-to-date Survivor winner data
- Sports betting platform trend analysis to scrape historical Survivor winners and adjust odds for upcoming seasons
- Academic research in media studies for compiling gender and diversity patterns among Survivor champions over 44 seasons
- Pop-culture blog content generation to auto-produce “All Survivor Winners” interactive lists and social media posts
- Streaming service metadata enrichment by syncing each season’s winner name into episode and season descriptions
- Digital archive curation for tagging and verifying video highlights of Survivor champions in an online television museum
- Fan community dashboard creation to visualize winner demographics and poll predictions for future Survivor seasons

```
import os
import sys
import json
import re
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP Status: {resp.status_code} OK\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML with BeautifulSoup
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Find the "Series overview" table by caption or fallback on header row containing 'Season' and 'Winner'
wikitables = soup.find_all('table', class_='wikitable')
print(f"Found {len(wikitables)} wikitables on the page. Searching for 'Series overview'...\n")
series_table = None
for idx, tbl in enumerate(wikitables, start=1):
    cap = tbl.find('caption')
    cap_text = cap.get_text(strip=True).lower() if cap else ''
    print(f"Table {idx}: caption='{cap_text}'")
    if 'series overview' in cap_text:
        series_table = tbl
        print(f"→ Matched 'Series overview' table at index {idx}\n")
        break

if not series_table:
    print("No caption match found, trying header‐based fallback...\n")
    for idx, tbl in enumerate(wikitables, start=1):
        first_row = tbl.find('tr')
        if not first_row:
            continue
        headers = [th.get_text(strip=True).lower() for th in first_row.find_all(['th','td'])]
        print(f"Fallback check Table {idx}: headers={headers}")
        if 'season' in headers and any('winner' in h for h in headers):
            series_table = tbl
            print(f"→ Fallback matched table at index {idx}\n")
            break

if not series_table:
    print("❌ Could not locate the Series overview table. Exiting.")
    sys.exit(1)

# 5) Determine column indices dynamically
header_cells = series_table.find('tr').find_all(['th','td'])
col_texts = [c.get_text(strip=True) for c in header_cells]
col_lower = [t.lower() for t in col_texts]
print(f"Series overview columns detected: {col_texts}\n")
try:
    season_idx = col_lower.index('season')
except ValueError:
    season_idx = next((i for i,t in enumerate(col_lower) if 'season' in t), None)
winner_idx = next((i for i,t in enumerate(col_lower) if 'winner' in t), None)
if season_idx is None or winner_idx is None:
    print("❌ Could not find 'Season' or 'Winner' columns. Exiting.")
    sys.exit(1)
print(f"Detected season column at index {season_idx}, winner column at index {winner_idx}\n")

# 6) Extract season–winner pairs with fallback for misaligned cells
data = []
numeric_pattern = re.compile(r'^[\d–\-\s]+$')
rows = series_table.find_all('tr')[1:]  # skip header
for i, row in enumerate(rows, start=1):
    cells = row.find_all(['td','th'])
    # Only consider rows with enough cells for season
    if len(cells) <= season_idx:
        continue
    # Parse season number
    season_text = cells[season_idx].get_text(' ', strip=True).split()[0]
    try:
        season_num = int(season_text)
    except ValueError:
        continue
    if not (1 <= season_num <= 44):
        continue
    # Extract primary winner text
    primary_txt = cells[winner_idx].get_text(' ', strip=True)
    # Fallback if primary is blank or numeric (vote count)
    if not primary_txt or numeric_pattern.fullmatch(primary_txt):
        # search other cells for name
        fallback = None
        for j, cell in enumerate(cells):
            if j == season_idx:
                continue
            link = cell.find('a')
            candidate = link.get_text(strip=True) if link else cell.get_text(' ', strip=True)
            if any(ch.isalpha() for ch in candidate):
                fallback = candidate
                print(f"  Season {season_num} fallback: picked cell {j} -> '{fallback}' (primary was '{primary_txt}')")
                break
        winner_name = fallback or primary_txt
    else:
        winner_name = primary_txt
    data.append({'season': season_num, 'winner': winner_name})
    print(f"Parsed Season {season_num}: {winner_name}")

# 7) Validate and save
if len(data) != 44:
    print(f"⚠️ Warning: Parsed {len(data)} seasons (expected 44). Please verify missing entries.")
# Sort entries by season
data_sorted = sorted(data, key=lambda x: x['season'])
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(data_sorted, f, indent=2, ensure_ascii=False)
print(f"\n✅ Successfully wrote {len(data_sorted)} entries to: {out_file}")
```