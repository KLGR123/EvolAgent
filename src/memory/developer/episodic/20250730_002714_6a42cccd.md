### Development Step 1: NASA APOD Aug 1â€“7, 2015: Find Horizon City Lights and Identify the City

**Description**: Search for NASA Astronomy Pictures of the Day from the first week of August 2015 (August 1-7, 2015). Look for entries that show city lights on the horizon and identify which city is featured. Use search approaches including: (1) Direct access to NASA APOD archives for August 2015, (2) Search 'NASA APOD August 2015 city lights horizon', (3) Browse the official NASA APOD website archive section for that specific date range. Extract the complete APOD entries including titles, descriptions, and image details to identify which city's lights are shown.

**Use Cases**:
- Urban environmental monitoring for light pollution: extract and analyze NASA APOD city lights horizon images from August 1â€“7, 2015 to assess nighttime illumination trends in major metropolitan areas.
- Tourism destination highlight curation: automatically scrape APOD entries featuring iconic city skylines at night during early August 2015 for inclusion in travel agency brochures and social media campaigns.
- Astronomy outreach and education modules: compile and present APOD photos showing city lights on the horizon from August 2015 to teach high school students about Earth observation and astrophotography techniques.
- Historical energy consumption analysis: correlate brightness metrics from NASA APODâ€™s August 2015 night-city images with municipal power grid usage data to support utility research reports.
- Computer vision dataset creation: gather labeled horizon city-light photographs from the APOD archive (August 1â€“7, 2015) as training data for machine learning models aimed at nighttime urban footprint detection.
- Science communication and SEO content strategy: automate retrieval of APOD metadata and imagery details for blog posts and social media articles on NASAâ€™s city-lights horizon pictures to improve search visibility.
- Multimedia digital archiving for historical research: index and store APOD pages featuring city lights horizons from early August 2015 in a museum or library collection management system.
- Emergency management scenario planning: analyze urban lighting patterns from NASA APODâ€™s city-horizon images in August 2015 to inform disaster preparedness mapping and resource allocation in densely populated regions.

```
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date
import json

print("=== NASA ASTRONOMY PICTURES OF THE DAY - AUGUST 1-7, 2015 SEARCH ===\n")
print("Objective: Find APOD entries from first week of August 2015 showing city lights on horizon\n")

# Create workspace directory if needed
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Get SerpAPI key from environment
api_key = os.getenv("SERPAPI_API_KEY")

if not api_key:
    print("Error: SERPAPI_API_KEY not found in environment variables")
else:
    print(f"API key found: {api_key[:8]}...")
    
    # Multiple targeted search queries for NASA APOD August 2015
    search_queries = [
        'NASA APOD August 2015 city lights horizon',
        'site:apod.nasa.gov August 2015 city lights',
        '"NASA Astronomy Picture of the Day" "August 2015" city horizon',
        'NASA APOD "August 1 2015" OR "August 2 2015" OR "August 3 2015" OR "August 4 2015" OR "August 5 2015" OR "August 6 2015" OR "August 7 2015"',
        'site:apod.nasa.gov "August 2015" urban lights cityscape',
        'NASA APOD archive August 2015 metropolitan lights',
        '"apod.nasa.gov" "2015 August" city illuminated horizon'
    ]
    
    print(f"Executing {len(search_queries)} targeted search queries...\n")
    
    all_search_results = []
    
    for i, query in enumerate(search_queries, 1):
        print(f"Query {i}: {query}")
        
        # Prepare API request parameters
        params = {
            "q": query,
            "api_key": api_key,
            "engine": "google",
            "google_domain": "google.com",
            "safe": "off",
            "num": 15  # Get more results for comprehensive coverage
        }
        
        try:
            response = requests.get("https://serpapi.com/search.json", params=params)
            
            if response.status_code == 200:
                results = response.json()
                
                if results.get("organic_results"):
                    print(f"  Found {len(results['organic_results'])} results")
                    
                    # Process and analyze each result
                    for j, result in enumerate(results['organic_results'], 1):
                        result_info = {
                            'query_number': i,
                            'result_number': j,
                            'title': result.get('title', 'No title'),
                            'link': result.get('link', 'No link'),
                            'snippet': result.get('snippet', 'No snippet'),
                            'is_nasa_apod': 'apod.nasa.gov' in result.get('link', '').lower(),
                            'has_august_2015': any(term in result.get('snippet', '').lower() + result.get('title', '').lower() 
                                                 for term in ['august 2015', 'aug 2015', '2015 august']),
                            'has_city_reference': any(term in result.get('snippet', '').lower() + result.get('title', '').lower() 
                                                    for term in ['city', 'urban', 'lights', 'horizon', 'metropolitan', 'skyline'])
                        }
                        
                        all_search_results.append(result_info)
                        
                        # Print relevant results immediately
                        if result_info['is_nasa_apod']:
                            print(f"  â˜… NASA APOD RESULT {j}:")
                            print(f"    Title: {result_info['title']}")
                            print(f"    Link: {result_info['link']}")
                            print(f"    Snippet: {result_info['snippet'][:200]}...")
                            print(f"    August 2015: {result_info['has_august_2015']}")
                            print(f"    City reference: {result_info['has_city_reference']}")
                        elif result_info['has_august_2015'] and result_info['has_city_reference']:
                            print(f"  â—† RELEVANT RESULT {j}:")
                            print(f"    Title: {result_info['title'][:60]}...")
                            print(f"    Link: {result_info['link'][:50]}...")
                        else:
                            print(f"  Result {j}: {result_info['title'][:60]}... | {result_info['link'][:50]}...")
                            
                else:
                    print(f"  No organic results found")
            else:
                print(f"  API Error: {response.status_code} - {response.text[:100]}...")
                
        except Exception as e:
            print(f"  Exception during search: {str(e)}")
        
        print()  # Space between queries
    
    print(f"\n=== SEARCH RESULTS SUMMARY ===\n")
    print(f"Total results collected: {len(all_search_results)}")
    
    # Filter and prioritize NASA APOD results
    apod_results = [r for r in all_search_results if r['is_nasa_apod']]
    print(f"NASA APOD official results: {len(apod_results)}")
    
    # Filter results with August 2015 and city references
    relevant_results = [r for r in all_search_results if r['has_august_2015'] and r['has_city_reference']]
    print(f"Results with August 2015 + city references: {len(relevant_results)}")
    
    if apod_results:
        print("\n=== NASA APOD OFFICIAL RESULTS ===\n")
        for i, result in enumerate(apod_results, 1):
            print(f"{i}. {result['title']}")
            print(f"   Link: {result['link']}")
            print(f"   Snippet: {result['snippet']}")
            print(f"   August 2015: {result['has_august_2015']}")
            print(f"   City reference: {result['has_city_reference']}")
            print(f"   From Query: {result['query_number']}")
            print()
        
        # Try to access the most promising NASA APOD links
        promising_links = [r for r in apod_results if r['has_august_2015'] or r['has_city_reference']]
        if not promising_links:
            promising_links = apod_results[:3]  # Take first 3 if no specific matches
        
        print(f"\n=== ACCESSING NASA APOD PAGES ===\n")
        print(f"Attempting to access {len(promising_links)} most promising links...")
        
        accessed_pages = []
        
        for i, result in enumerate(promising_links, 1):
            target_url = result['link']
            print(f"\n{i}. Accessing: {target_url}")
            
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                page_response = requests.get(target_url, headers=headers, timeout=30)
                
                if page_response.status_code == 200:
                    print(f"   âœ“ Successfully accessed (Status: {page_response.status_code})")
                    print(f"   Content length: {len(page_response.content):,} bytes")
                    
                    # Parse the HTML content
                    soup = BeautifulSoup(page_response.content, 'html.parser')
                    
                    # Extract key information from the page
                    page_title = soup.find('title')
                    if page_title:
                        print(f"   Page title: {page_title.get_text().strip()}")
                    
                    # Save the HTML content for detailed analysis
                    safe_filename = target_url.replace('https://', '').replace('http://', '').replace('/', '_').replace(':', '_')
                    html_filename = f'workspace/nasa_apod_{safe_filename}.html'
                    
                    with open(html_filename, 'w', encoding='utf-8') as f:
                        f.write(page_response.text)
                    
                    print(f"   âœ“ Page content saved to: {html_filename}")
                    
                    # Quick content analysis for August 2015 and city references
                    page_text = soup.get_text().lower()
                    
                    # Look for August 2015 date indicators
                    august_2015_indicators = ['august 1, 2015', 'august 2, 2015', 'august 3, 2015', 
                                            'august 4, 2015', 'august 5, 2015', 'august 6, 2015', 
                                            'august 7, 2015', 'aug 1, 2015', 'aug 2, 2015',
                                            '2015 august 01', '2015 august 02', '2015 august 03',
                                            '2015 august 04', '2015 august 05', '2015 august 06', '2015 august 07']
                    august_found = any(indicator in page_text for indicator in august_2015_indicators)
                    
                    # Look for city/lights indicators
                    city_indicators = ['city lights', 'urban lights', 'metropolitan', 'skyline', 
                                     'horizon', 'cityscape', 'illuminated', 'night lights', 
                                     'artificial lights', 'light pollution']
                    city_found = any(indicator in page_text for indicator in city_indicators)
                    
                    # Store page information
                    page_info = {
                        'url': target_url,
                        'title': page_title.get_text().strip() if page_title else 'No title',
                        'filename': html_filename,
                        'content_length': len(page_response.content),
                        'august_2015_found': august_found,
                        'city_lights_found': city_found,
                        'search_result': result
                    }
                    
                    accessed_pages.append(page_info)
                    
                    print(f"   August 2015 reference: {august_found}")
                    print(f"   City lights reference: {city_found}")
                    
                else:
                    print(f"   âœ— Failed to access (Status: {page_response.status_code})")
                    
            except Exception as e:
                print(f"   âœ— Exception accessing page: {str(e)}")
    
    else:
        print("\nâš ï¸ No official NASA APOD results found")
        print("Checking other relevant results...")
        
        if relevant_results:
            print(f"\nFound {len(relevant_results)} other relevant results:")
            for i, result in enumerate(relevant_results[:5], 1):  # Show first 5
                print(f"{i}. {result['title']}")
                print(f"   Link: {result['link']}")
                print(f"   Snippet: {result['snippet'][:150]}...")
                print()
    
    # Now let's try direct access to NASA APOD archive for August 2015
    print("\n=== DIRECT ACCESS TO NASA APOD ARCHIVE ===\n")
    print("Attempting direct access to NASA APOD archive for August 2015...")
    
    # Generate URLs for each day in the first week of August 2015
    target_dates = [
        ('2015', '08', '01'),  # August 1, 2015
        ('2015', '08', '02'),  # August 2, 2015
        ('2015', '08', '03'),  # August 3, 2015
        ('2015', '08', '04'),  # August 4, 2015
        ('2015', '08', '05'),  # August 5, 2015
        ('2015', '08', '06'),  # August 6, 2015
        ('2015', '08', '07'),  # August 7, 2015
    ]
    
    apod_pages = []
    
    for year, month, day in target_dates:
        # NASA APOD URL format: https://apod.nasa.gov/apod/apYYMMDD.html
        apod_url = f"https://apod.nasa.gov/apod/ap{year[2:]}{month}{day}.html"
        print(f"\nAccessing: {apod_url} (August {int(day)}, 2015)")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(apod_url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print(f"   âœ“ Successfully accessed APOD for August {int(day)}, 2015")
                
                # Parse content
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract title
                title_element = soup.find('b')
                title = title_element.get_text().strip() if title_element else 'No title found'
                print(f"   Title: {title}")
                
                # Extract description
                content_text = soup.get_text()
                
                # Look for city/lights references
                city_keywords = ['city', 'urban', 'lights', 'horizon', 'metropolitan', 'skyline', 
                               'cityscape', 'illuminated', 'night lights', 'artificial lights']
                
                found_keywords = [keyword for keyword in city_keywords if keyword in content_text.lower()]
                
                if found_keywords:
                    print(f"   â˜… CITY LIGHTS KEYWORDS FOUND: {found_keywords}")
                    
                    # Save this promising page
                    filename = f'workspace/nasa_apod_august_{day}_2015.html'
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    print(f"   âœ“ Saved to: {filename}")
                    
                    apod_pages.append({
                        'date': f'August {int(day)}, 2015',
                        'url': apod_url,
                        'title': title,
                        'filename': filename,
                        'city_keywords_found': found_keywords,
                        'content_preview': content_text[:300] + '...' if len(content_text) > 300 else content_text
                    })
                else:
                    print(f"   No city lights keywords found")
                    print(f"   Title preview: {title}")
            
            elif response.status_code == 404:
                print(f"   âœ— Page not found (404) - may not exist for this date")
            else:
                print(f"   âœ— Failed to access (Status: {response.status_code})")
                
        except Exception as e:
            print(f"   âœ— Exception: {str(e)}")
    
    # Compile comprehensive search results
    search_summary = {
        'search_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'target_period': 'August 1-7, 2015',
        'objective': 'Find NASA APOD entries showing city lights on horizon',
        'search_queries': search_queries,
        'total_search_results': len(all_search_results),
        'nasa_apod_results': len(apod_results),
        'relevant_results': len(relevant_results),
        'direct_apod_pages_found': len(apod_pages),
        'apod_pages_with_city_keywords': apod_pages,
        'accessed_pages_summary': accessed_pages if 'accessed_pages' in locals() else [],
        'all_search_results': all_search_results
    }
    
    # Save comprehensive results
    summary_filename = 'workspace/nasa_apod_august_2015_search_results.json'
    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(search_summary, f, indent=2)
    
    print(f"\n=== SEARCH PHASE COMPLETE ===\n")
    print(f"Search results summary:")
    print(f"- Total search results: {len(all_search_results)}")
    print(f"- NASA APOD official results: {len(apod_results)}")
    print(f"- Direct APOD pages accessed: 7")
    print(f"- Pages with city lights keywords: {len(apod_pages)}")
    print(f"\nFiles created:")
    
    # List all created files
    created_files = []
    for file in os.listdir('workspace'):
        if file.startswith('nasa_apod'):
            created_files.append(file)
    
    for file in created_files:
        print(f"- workspace/{file}")
    
    print(f"\nSummary saved to: {summary_filename}")
    
    if apod_pages:
        print(f"\nðŸŽ¯ PROMISING CANDIDATES FOUND:")
        for page in apod_pages:
            print(f"- {page['date']}: {page['title']}")
            print(f"  Keywords: {page['city_keywords_found']}")
            print(f"  File: {page['filename']}")
    
    print(f"\nNext step: Parse the saved HTML files to extract detailed APOD information and identify the specific city")
```