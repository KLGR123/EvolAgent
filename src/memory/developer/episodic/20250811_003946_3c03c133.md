### Development Step 1: AI Regulation Papers from arXiv (June 2022): Extract Axis Label Words from Three-Axis Figure

**Description**: Search arXiv.org for AI regulation papers originally submitted in June 2022. Focus on identifying papers that discuss artificial intelligence regulation, policy, governance, or related topics. Look for papers with submission dates in June 2022 and examine their content to find one containing a figure with three axes where each axis has label words at both ends. Extract the specific words used as axis labels in this figure.

**Use Cases**:
- Academic literature review for a university research team studying the evolution of AI regulation frameworks in mid-2022
- Automated policy landscape mapping for a government agency evaluating global trends in AI governance
- Corporate compliance team tracking recent scholarly discussions on AI ethics and regulatory challenges for risk assessment
- Legal analysts identifying precedent-setting academic papers on AI policy to inform technology law cases
- Think tank researchers extracting multidimensional frameworks from figures in AI regulation papers to visualize governance models
- Data scientists building a curated dataset of AI regulation literature with annotated axis labels for machine learning model training
- Journal editors screening June 2022 submissions for innovative visualizations of AI governance concepts
- Educational content creators sourcing authoritative diagrams from academic papers to illustrate AI policy concepts in online courses

```
import os
import requests
import json
from datetime import datetime
import time

print('=== SEARCHING ARXIV FOR AI REGULATION PAPERS FROM JUNE 2022 ===')
print('Objective: Find papers on AI regulation/policy/governance submitted in June 2022')
print('Target: Locate paper with figure having three axes with label words at both ends\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# arXiv API base URL
arxiv_base_url = 'http://export.arxiv.org/api/query'

# Search terms for AI regulation papers
search_queries = [
    'artificial intelligence regulation',
    'AI policy governance', 
    'artificial intelligence policy',
    'AI regulation framework',
    'machine learning governance',
    'AI ethics policy',
    'artificial intelligence governance'
]

print('=== STEP 1: SEARCHING ARXIV WITH MULTIPLE QUERY TERMS ===')

all_papers = []
search_results = {}

for i, query in enumerate(search_queries, 1):
    print(f'\nSearch {i}/{len(search_queries)}: "{query}"')
    
    # arXiv search parameters
    params = {
        'search_query': f'all:"{query}"',
        'start': 0,
        'max_results': 50,  # Get more results to filter by date
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    try:
        response = requests.get(arxiv_base_url, params=params, timeout=30)
        print(f'Status code: {response.status_code}')
        
        if response.status_code == 200:
            # Save raw XML response
            xml_file = os.path.join(workspace, f'arxiv_search_{i}_{query.replace(" ", "_")}.xml')
            with open(xml_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f'✓ Raw XML saved to: {xml_file}')
            
            # Parse XML to extract paper information
            import xml.etree.ElementTree as ET
            
            try:
                root = ET.fromstring(response.text)
                
                # Define XML namespaces
                namespaces = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                entries = root.findall('atom:entry', namespaces)
                print(f'Found {len(entries)} papers for query "{query}"')
                
                query_papers = []
                
                for entry in entries:
                    # Extract paper details
                    title_elem = entry.find('atom:title', namespaces)
                    title = title_elem.text.strip() if title_elem is not None else 'No title'
                    
                    summary_elem = entry.find('atom:summary', namespaces)
                    summary = summary_elem.text.strip() if summary_elem is not None else 'No summary'
                    
                    published_elem = entry.find('atom:published', namespaces)
                    published = published_elem.text.strip() if published_elem is not None else 'No date'
                    
                    # Get arXiv ID and PDF link
                    id_elem = entry.find('atom:id', namespaces)
                    arxiv_id = id_elem.text.strip() if id_elem is not None else 'No ID'
                    
                    # Extract arXiv ID from URL
                    if 'arxiv.org/abs/' in arxiv_id:
                        arxiv_id = arxiv_id.split('/')[-1]
                    
                    # Construct PDF URL
                    pdf_url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'
                    
                    # Get authors
                    authors = []
                    author_elems = entry.findall('atom:author', namespaces)
                    for author_elem in author_elems:
                        name_elem = author_elem.find('atom:name', namespaces)
                        if name_elem is not None:
                            authors.append(name_elem.text.strip())
                    
                    paper_info = {
                        'title': title,
                        'authors': authors,
                        'summary': summary,
                        'published': published,
                        'arxiv_id': arxiv_id,
                        'pdf_url': pdf_url,
                        'search_query': query
                    }
                    
                    query_papers.append(paper_info)
                    all_papers.append(paper_info)
                
                search_results[query] = query_papers
                print(f'✓ Extracted {len(query_papers)} papers from XML')
                
            except ET.ParseError as e:
                print(f'Error parsing XML for query "{query}": {str(e)}')
                continue
        
        else:
            print(f'Failed to search arXiv: {response.status_code}')
            continue
    
    except Exception as e:
        print(f'Error searching for "{query}": {str(e)}')
        continue
    
    # Small delay between requests to be respectful to arXiv
    time.sleep(1)

print(f'\n=== STEP 2: FILTERING PAPERS BY JUNE 2022 SUBMISSION DATE ===')
print(f'Total papers found across all queries: {len(all_papers)}')

# Filter papers by June 2022 submission date
june_2022_papers = []

for paper in all_papers:
    published_date = paper['published']
    
    # Parse the date (arXiv format: 2022-06-15T17:59:59Z)
    try:
        if published_date and published_date != 'No date':
            # Extract year and month from date string
            if published_date.startswith('2022-06'):
                june_2022_papers.append(paper)
                print(f'✓ June 2022 paper found: "{paper["title"][:80]}..."')
                print(f'  Published: {published_date}')
                print(f'  arXiv ID: {paper["arxiv_id"]}')
                print(f'  Authors: {", ".join(paper["authors"][:3])}{", ..." if len(paper["authors"]) > 3 else ""}')
                print()
    
    except Exception as e:
        print(f'Error parsing date for paper: {str(e)}')
        continue

print(f'\n=== FILTERING RESULTS ===')
print(f'Papers from June 2022: {len(june_2022_papers)}')

if june_2022_papers:
    # Save June 2022 papers for detailed analysis
    june_papers_file = os.path.join(workspace, 'arxiv_june_2022_ai_regulation_papers.json')
    with open(june_papers_file, 'w', encoding='utf-8') as f:
        json.dump({
            'search_date': datetime.now().isoformat(),
            'total_papers_found': len(all_papers),
            'june_2022_papers_count': len(june_2022_papers),
            'search_queries_used': search_queries,
            'june_2022_papers': june_2022_papers
        }, f, indent=2, ensure_ascii=False)
    
    print(f'✓ June 2022 papers saved to: {june_papers_file}')
    
    # Display summary of June 2022 papers
    print(f'\n=== JUNE 2022 AI REGULATION PAPERS SUMMARY ===')
    for i, paper in enumerate(june_2022_papers, 1):
        print(f'\n{i}. Title: {paper["title"]}')
        print(f'   arXiv ID: {paper["arxiv_id"]}')
        print(f'   Published: {paper["published"]}')
        print(f'   Authors: {", ".join(paper["authors"][:2])}{", ..." if len(paper["authors"]) > 2 else ""}')
        print(f'   PDF: {paper["pdf_url"]}')
        print(f'   Summary preview: {paper["summary"][:200]}...')
    
    print(f'\n=== NEXT STEPS ===')
    print(f'✓ Found {len(june_2022_papers)} AI regulation papers from June 2022')
    print(f'✓ Ready to download and analyze PDFs for figures with three axes')
    print(f'✓ Will search for figures with axis labels at both ends')
    
else:
    print('\n⚠ No papers found from June 2022 with current search terms')
    print('May need to try broader search terms or different date filtering approach')
    
    # Save all search results for analysis
    all_results_file = os.path.join(workspace, 'arxiv_all_search_results.json')
    with open(all_results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'search_date': datetime.now().isoformat(),
            'total_papers_found': len(all_papers),
            'search_queries_used': search_queries,
            'all_papers': all_papers[:20]  # Save first 20 for inspection
        }, f, indent=2, ensure_ascii=False)
    
    print(f'All search results saved to: {all_results_file}')
    print('Will analyze date patterns and try alternative approaches')

print('\n*** ARXIV SEARCH PHASE COMPLETE ***')
```