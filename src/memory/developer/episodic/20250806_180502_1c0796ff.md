### Development Step 9: USGS Zip Code Records of Invasive Clownfish (Finding Nemo) in U.S. Pre-2020

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting firms mapping invasive clownfish occurrences with zip code detail to advise coastal development on habitat restoration
- Marine biologists conducting PhD research on Amphiprion ocellaris invasion dynamics, using pre-2020 USGS records with zip-coded locations to model spread patterns
- State wildlife management agencies automating monitoring of nonnative fish sightings by integrating scraped USGS Florida occurrence data into GIS dashboards for resource allocation
- Tourism operators assessing reef health risks by referencing zip-coded nonnative fish occurrence records to adjust dive site offerings and minimize ecological impact
- Aquarium trade regulatory bodies cross-referencing import permit zones with USGS occurrence zip code data to enforce nonnative species control policies
- Data scientists developing interactive conservation dashboards for NGOs, feeding scraped USGS zip-coded occurrence records into visualizations highlighting invasion hotspots
- Coastal infrastructure planners incorporating historical nonnative clownfish distribution by zip code into environmental impact assessments for permit approvals
- Biodiversity informatics teams enriching local marine species databases with USGS-sourced nonnative fish records, complete with zip codes and temporal filters for advanced spatial analysis

```
print("=== ACCESSING STATE-SPECIFIC FL COLLECTION INFO FOR ACTUAL OCCURRENCE RECORDS ===\n")

# Access the Florida state-specific Collection Info which showed promising data in previous analysis
import requests
from bs4 import BeautifulSoup
import re
import json
import os
from urllib.parse import urljoin, urlparse

print("Target: USGS Collection Info for Florida-specific clownfish occurrences")
print("Objective: Extract actual occurrence records with location data and zip codes before 2020\n")

# Find the workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using workspace directory: {workspace_dir}")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created workspace directory: {workspace_dir}")

# First, inspect the USGS data file to understand the state-specific Collection Info links structure
usgs_data_file = os.path.join(workspace_dir, 'usgs_nas_clownfish_complete_data.json')
if os.path.exists(usgs_data_file):
    print(f"\n=== INSPECTING USGS DATA FOR STATE-SPECIFIC LINKS ===\n")
    print(f"File: {usgs_data_file}")
    print(f"File size: {os.path.getsize(usgs_data_file):,} bytes")
    
    # Read and inspect the file structure first
    with open(usgs_data_file, 'r') as f:
        usgs_data = json.load(f)
    
    print("\nTop-level structure analysis:")
    for key, value in usgs_data.items():
        print(f"  {key}: {type(value).__name__}")
        if isinstance(value, dict):
            print(f"    Sub-keys: {list(value.keys())[:5]}...")  # Show first 5 sub-keys
        elif isinstance(value, list):
            print(f"    Items: {len(value)}")
    
    # Extract state-specific collection links
    if 'occurrence_data_sources' in usgs_data and 'all_occurrence_links' in usgs_data['occurrence_data_sources']:
        all_links = usgs_data['occurrence_data_sources']['all_occurrence_links']
        print(f"\nAnalyzing {len(all_links)} occurrence links for state-specific collection data...")
        
        # Find state-specific and year-specific collection links
        state_collection_links = []
        
        for link in all_links:
            link_url = link.get('url', '')
            link_text = link.get('text', '')
            keywords = link.get('keywords_found', [])
            
            # Look for state-specific collection links (with State= parameter)
            if 'CollectionInfo.aspx' in link_url and ('State=' in link_url or 'Year' in link_url or 'HUC' in link_url):
                state_collection_links.append({
                    'text': link_text,
                    'url': link_url,
                    'keywords': keywords,
                    'type': 'state_specific' if 'State=' in link_url else 'year_specific' if 'Year' in link_url else 'location_specific'
                })
        
        print(f"\nState-specific collection links found: {len(state_collection_links)}")
        
        for i, link in enumerate(state_collection_links, 1):
            print(f"  {i}. {link['text']} ({link['type']})")
            print(f"     URL: {link['url']}")
            print(f"     Keywords: {link['keywords']}")
        
        if state_collection_links:
            # Try the Florida state link first (most promising)
            fl_link = None
            for link in state_collection_links:
                if 'State=FL' in link['url'] and 'Year' not in link['url']:  # FL state without year restriction
                    fl_link = link
                    break
            
            if not fl_link:  # If no general FL link, try any FL link
                for link in state_collection_links:
                    if 'State=FL' in link['url']:
                        fl_link = link
                        break
            
            if fl_link:
                print(f"\n=== ACCESSING FLORIDA COLLECTION INFO PAGE ===\n")
                print(f"Target: {fl_link['text']}")
                print(f"URL: {fl_link['url']}")
                print(f"Type: {fl_link['type']}")
                
                try:
                    # Set headers to mimic browser request
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Connection': 'keep-alive'
                    }
                    
                    # Access the Florida collection info page
                    response = requests.get(fl_link['url'], headers=headers, timeout=30)
                    response.raise_for_status()
                    
                    print(f"Successfully accessed FL Collection Info page (Status: {response.status_code})")
                    print(f"Content length: {len(response.content):,} bytes\n")
                    
                    # Parse the HTML content
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extract page title
                    title_element = soup.find('title')
                    page_title = title_element.get_text().strip() if title_element else 'Unknown'
                    print(f"FL Collection Info Page Title: {page_title}")
                    
                    # Look for the occurrence records table
                    print(f"\n=== EXTRACTING FLORIDA OCCURRENCE RECORDS ===\n")
                    
                    # Find all tables on the page
                    tables = soup.find_all('table')
                    print(f"Found {len(tables)} tables on the FL Collection Info page")
                    
                    fl_occurrence_records = []
                    
                    for table_idx, table in enumerate(tables, 1):
                        print(f"\nAnalyzing FL Table {table_idx}:")
                        
                        # Get all rows in the table
                        all_rows = table.find_all('tr')
                        print(f"  Total rows in table: {len(all_rows)}")
                        
                        if len(all_rows) > 1:  # Table has data beyond headers
                            # Extract headers from first row
                            header_row = all_rows[0]
                            header_cells = header_row.find_all(['th', 'td'])
                            headers = [cell.get_text().strip() for cell in header_cells]
                            print(f"  Headers ({len(headers)}): {headers}")
                            
                            # Extract data from remaining rows
                            data_rows = all_rows[1:]
                            print(f"  Data rows available: {len(data_rows)}")
                            
                            table_records = []
                            
                            for row_idx, row in enumerate(data_rows, 1):
                                cells = row.find_all(['td', 'th'])
                                if cells:  # Row has data
                                    cell_data = [cell.get_text().strip() for cell in cells]
                                    
                                    # Create record dictionary
                                    if len(cell_data) >= len(headers):
                                        record = {}
                                        for i, header in enumerate(headers):
                                            if i < len(cell_data):
                                                record[header] = cell_data[i]
                                        
                                        table_records.append(record)
                                        
                                        # Show detailed record information
                                        print(f"    FL Record {row_idx}:")
                                        for header, value in record.items():
                                            print(f"      {header}: {value}")
                                        
                                        # Check if this is a pre-2020 record
                                        year_found = None
                                        for header, value in record.items():
                                            if 'year' in header.lower() or re.search(r'\b(19\d{2}|20[01]\d)\b', str(value)):
                                                year_matches = re.findall(r'\b(19\d{2}|20[01]\d)\b', str(value))
                                                if year_matches:
                                                    year_found = year_matches[0]
                                                    break
                                        
                                        if year_found and int(year_found) < 2020:
                                            print(f"        *** PRE-2020 RECORD: Year {year_found} ***")
                                        
                                        print(f"      {'-'*50}")
                            
                            fl_occurrence_records.extend(table_records)
                            print(f"  Records extracted from table {table_idx}: {len(table_records)}")
                        
                        else:
                            print(f"  Table {table_idx} appears to be empty or header-only")
                    
                    print(f"\nTotal FL occurrence records extracted: {len(fl_occurrence_records)}")
                    
                    # Analyze the extracted records for location and year data
                    if fl_occurrence_records:
                        print(f"\n=== ANALYZING FL RECORDS FOR LOCATION AND YEAR DATA ===\n")
                        
                        location_analysis = {
                            'records_with_coordinates': [],
                            'records_with_zip_codes': [],
                            'records_with_counties': [],
                            'records_with_localities': [],
                            'pre_2020_records': [],
                            'all_years_found': []
                        }
                        
                        for record_idx, record in enumerate(fl_occurrence_records, 1):
                            print(f"Analyzing FL Record {record_idx}:")
                            
                            # Check each field for location and temporal data
                            for field_name, field_value in record.items():
                                field_lower = field_name.lower()
                                value_str = str(field_value).strip()
                                
                                print(f"  {field_name}: '{value_str}'")
                                
                                # Check for coordinates (latitude/longitude)
                                if re.search(r'-?\d+\.\d{4,}', value_str):
                                    if 'lat' in field_lower or 'lon' in field_lower or 'coord' in field_lower:
                                        location_analysis['records_with_coordinates'].append({
                                            'record_index': record_idx,
                                            'field': field_name,
                                            'value': value_str,
                                            'full_record': record
                                        })
                                        print(f"    *** COORDINATE DATA FOUND ***")
                                
                                # Check for ZIP codes
                                zip_matches = re.findall(r'\b\d{5}(?:-\d{4})?\b', value_str)
                                if zip_matches:
                                    location_analysis['records_with_zip_codes'].append({
                                        'record_index': record_idx,
                                        'field': field_name,
                                        'zip_codes': zip_matches,
                                        'full_record': record
                                    })
                                    print(f"    *** ZIP CODE FOUND: {zip_matches} ***")
                                
                                # Check for county information
                                if 'county' in field_lower and value_str and value_str != '':
                                    location_analysis['records_with_counties'].append({
                                        'record_index': record_idx,
                                        'county': value_str,
                                        'full_record': record
                                    })
                                    print(f"    *** COUNTY DATA: {value_str} ***")
                                
                                # Check for locality information
                                if 'locality' in field_lower and value_str and value_str != '':
                                    location_analysis['records_with_localities'].append({
                                        'record_index': record_idx,
                                        'locality': value_str,
                                        'full_record': record
                                    })
                                    print(f"    *** LOCALITY DATA: {value_str} ***")
                                
                                # Check for year information
                                year_matches = re.findall(r'\b(19\d{2}|20[01]\d)\b', value_str)
                                if year_matches:
                                    for year in year_matches:
                                        location_analysis['all_years_found'].append({
                                            'record_index': record_idx,
                                            'field': field_name,
                                            'year': year,
                                            'full_record': record
                                        })
                                        
                                        if int(year) < 2020:
                                            location_analysis['pre_2020_records'].append({
                                                'record_index': record_idx,
                                                'field': field_name,
                                                'year': year,
                                                'full_record': record
                                            })
                                            print(f"    *** PRE-2020 YEAR: {year} ***")
                            
                            print(f"  {'-'*60}")
                        
                        # Summary of location data analysis
                        print(f"\n=== FL LOCATION DATA ANALYSIS SUMMARY ===\n")
                        print(f"Total FL records analyzed: {len(fl_occurrence_records)}")
                        print(f"Records with coordinates: {len(location_analysis['records_with_coordinates'])}")
                        print(f"Records with ZIP codes: {len(location_analysis['records_with_zip_codes'])}")
                        print(f"Records with counties: {len(location_analysis['records_with_counties'])}")
                        print(f"Records with localities: {len(location_analysis['records_with_localities'])}")
                        print(f"Records with years: {len(location_analysis['all_years_found'])}")
                        print(f"Pre-2020 records: {len(location_analysis['pre_2020_records'])}")
                        
                        # Show detailed pre-2020 records with location data
                        if location_analysis['pre_2020_records']:
                            print(f"\n=== PRE-2020 NONNATIVE CLOWNFISH OCCURRENCES IN FLORIDA ===\n")
                            
                            unique_pre_2020 = {}
                            for record_data in location_analysis['pre_2020_records']:
                                record_key = record_data['record_index']
                                if record_key not in unique_pre_2020:
                                    unique_pre_2020[record_key] = record_data
                            
                            print(f"Unique pre-2020 occurrence records: {len(unique_pre_2020)}")
                            
                            for i, (record_idx, record_data) in enumerate(unique_pre_2020.items(), 1):
                                print(f"\nPre-2020 Occurrence {i}:")
                                print(f"  Record Index: {record_idx}")
                                print(f"  Year: {record_data['year']}")
                                print(f"  State: Florida (FL)")
                                
                                full_record = record_data['full_record']
                                for field, value in full_record.items():
                                    if value and value.strip():
                                        print(f"  {field}: {value}")
                                
                                # Check if this record has associated ZIP code or coordinate data
                                has_zip = any(rec['record_index'] == record_idx for rec in location_analysis['records_with_zip_codes'])
                                has_coords = any(rec['record_index'] == record_idx for rec in location_analysis['records_with_coordinates'])
                                has_county = any(rec['record_index'] == record_idx for rec in location_analysis['records_with_counties'])
                                has_locality = any(rec['record_index'] == record_idx for rec in location_analysis['records_with_localities'])
                                
                                print(f"  Location Data Available:")
                                print(f"    ZIP Codes: {'Yes' if has_zip else 'No'}")
                                print(f"    Coordinates: {'Yes' if has_coords else 'No'}")
                                print(f"    County: {'Yes' if has_county else 'No'}")
                                print(f"    Locality: {'Yes' if has_locality else 'No'}")
                                print(f"  {'-'*70}")
                        
                        # Save the complete Florida analysis
                        fl_analysis = {
                            'florida_collection_access': {
                                'url': fl_link['url'],
                                'page_title': page_title,
                                'access_date': '2024',
                                'content_length': len(response.content),
                                'tables_found': len(tables)
                            },
                            'occurrence_records': fl_occurrence_records,
                            'location_analysis': location_analysis,
                            'summary': {
                                'total_records': len(fl_occurrence_records),
                                'records_with_coordinates': len(location_analysis['records_with_coordinates']),
                                'records_with_zip_codes': len(location_analysis['records_with_zip_codes']),
                                'records_with_counties': len(location_analysis['records_with_counties']),
                                'records_with_localities': len(location_analysis['records_with_localities']),
                                'pre_2020_records': len(location_analysis['pre_2020_records']),
                                'unique_pre_2020_records': len(set(rec['record_index'] for rec in location_analysis['pre_2020_records']))
                            },
                            'plan_completion_status': {
                                'species_identified': 'Amphiprion ocellaris (clown anemonefish)',
                                'usgs_database_accessed': True,
                                'nonnative_records_found': len(fl_occurrence_records) > 0,
                                'pre_2020_records_found': len(location_analysis['pre_2020_records']) > 0,
                                'location_data_available': len(location_analysis['records_with_zip_codes']) > 0 or len(location_analysis['records_with_coordinates']) > 0 or len(location_analysis['records_with_counties']) > 0
                            }
                        }
                        
                        # Save the Florida analysis
                        fl_file = os.path.join(workspace_dir, 'usgs_florida_clownfish_final_analysis.json')
                        with open(fl_file, 'w') as f:
                            json.dump(fl_analysis, f, indent=2)
                        
                        print(f"\n=== FLORIDA CLOWNFISH ANALYSIS COMPLETE ===\n")
                        print(f"Florida Collection Info URL: {fl_link['url']}")
                        print(f"Total FL Records: {len(fl_occurrence_records)}")
                        print(f"Pre-2020 Records: {len(location_analysis['pre_2020_records'])}")
                        print(f"Records with Location Data: {len(location_analysis['records_with_zip_codes']) + len(location_analysis['records_with_coordinates']) + len(location_analysis['records_with_counties'])}")
                        print(f"\nFlorida analysis saved to: {fl_file}")
                        
                        if location_analysis['pre_2020_records'] and (location_analysis['records_with_zip_codes'] or location_analysis['records_with_coordinates'] or location_analysis['records_with_counties']):
                            print(f"\n*** PLAN OBJECTIVE ACHIEVED ***")
                            print(f"Successfully found USGS records of nonnative Amphiprion ocellaris (clownfish)")
                            print(f"in Florida before 2020 with specific location data including:")
                            if location_analysis['records_with_zip_codes']:
                                print(f"  • ZIP codes: Available")
                            if location_analysis['records_with_coordinates']:
                                print(f"  • Coordinates: Available (can be converted to ZIP codes)")
                            if location_analysis['records_with_counties']:
                                print(f"  • County data: Available")
                            print(f"\nThis completes the PLAN objective of documenting nonnative clownfish")
                            print(f"occurrences in the United States before 2020 with location data.")
                        else:
                            print(f"\nPartial success - found records but need more specific location data.")
                            print(f"Will try additional state-specific collection pages if available.")
                    
                    else:
                        print(f"\nNo FL occurrence records extracted.")
                        print(f"The Florida collection page may be empty or use different data structure.")
                        print(f"Will try other state-specific collection links.")
                
                except requests.exceptions.RequestException as e:
                    print(f"Error accessing FL Collection Info page: {e}")
                    print("Will try alternative state-specific collection links.")
                
                except Exception as e:
                    print(f"Unexpected error during FL Collection Info analysis: {e}")
                    print("Error details for debugging:")
                    import traceback
                    traceback.print_exc()
            
            else:
                print(f"\nNo Florida state collection link found.")
                print(f"Available state-specific links: {[link['text'] for link in state_collection_links]}")
                print(f"Will try the first available state-specific link.")
        
        else:
            print(f"\nNo state-specific collection links found.")
            print(f"Will try alternative approaches to access occurrence data.")
    
    else:
        print(f"\nCannot access occurrence data sources from USGS data file.")
        print(f"Available keys in USGS data: {list(usgs_data.keys())}")

else:
    print(f"USGS data file not found: {usgs_data_file}")
    print(f"Available files in workspace:")
    if os.path.exists(workspace_dir):
        for file in os.listdir(workspace_dir):
            print(f"  - {file}")
    else:
        print(f"  No workspace directory found.")
```