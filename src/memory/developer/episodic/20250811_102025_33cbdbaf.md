### Development Step 3: Counting Twitter/X Citations in Archived August Wikipedia Pages from June 2023 Versions

**Description**: Search for and access the archived Wikipedia pages for each day of August (August 1st through August 31st) as they appeared in the last versions from June 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific versions of these 31 daily pages from late June 2023. Extract all reference citations from each page and identify those that cite Twitter/X posts (looking for twitter.com, x.com, or @username patterns in the reference URLs). Count the total number of Twitter/X citations for each individual August day page and compile the results.

**Use Cases**:
- Media studies research analyzing the prevalence of Twitter/X citations in Wikipedia's daily event pages to assess social media's influence on historical documentation
- Digital preservation audits by librarians tracking the use of ephemeral social media sources in Wikipedia references for long-term archiving strategies
- Automated compliance checks for fact-checking organizations to identify Wikipedia content reliant on Twitter/X posts, flagging potential reliability issues
- Academic studies in information science examining citation patterns and the integration of social media into encyclopedic knowledge bases
- Journalism retrospectives quantifying the role of Twitter/X as a primary source in Wikipedia's coverage of August 2023 news events
- Data-driven content review for Wikipedia editors to evaluate and potentially replace Twitter/X references with more stable sources
- Legal investigations into the sourcing of public information, using citation counts to map the spread of specific tweets in Wikipedia documentation
- Social media impact analysis for communications researchers, measuring how often Twitter/X posts were cited in Wikipedia's daily summaries during a given period

```
import os
import requests
import json
from datetime import datetime, timedelta
import time
from bs4 import BeautifulSoup
import re

print("=== PIVOTING TO WIKIPEDIA REVISION HISTORY API ===\n")
print("Issue: Wayback Machine consistently returns July/August 2023 versions")
print("Solution: Use Wikipedia's revision history API to find June 2023 versions directly\n")

# First, let's inspect existing workspace files to understand what we have
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if workspace_dirs:
    workspace_dir = workspace_dirs[0]
    print(f"Using existing workspace: {workspace_dir}\n")
else:
    workspace_dir = 'workspace'
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created new workspace: {workspace_dir}\n")

# Inspect existing files to understand structure
print("=== INSPECTING EXISTING WORKSPACE FILES ===\n")
existing_files = [f for f in os.listdir(workspace_dir) if f.endswith('.json')]
print(f"Found {len(existing_files)} JSON files in workspace:")

for file in existing_files:
    file_path = os.path.join(workspace_dir, file)
    file_size = os.path.getsize(file_path)
    print(f"  - {file} ({file_size:,} bytes)")

# Load the August pages list from previous analysis
august_pages = []
if existing_files:
    # Try to find the August pages list from any existing file
    for file in existing_files:
        file_path = os.path.join(workspace_dir, file)
        try:
            print(f"\nInspecting structure of {file}:")
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"  Root keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
            
            # Look for August pages list
            if isinstance(data, dict):
                if 'august_pages_list' in data:
                    august_pages = data['august_pages_list']
                    print(f"  Found august_pages_list with {len(august_pages)} items")
                    break
                elif 'august_pages' in data:
                    august_pages = data['august_pages']
                    print(f"  Found august_pages with {len(august_pages)} items")
                    break
        except Exception as e:
            print(f"  Error reading {file}: {str(e)}")

# Generate August pages if not found
if not august_pages:
    august_pages = [f"August {day}" for day in range(1, 32)]
    print(f"\nGenerated {len(august_pages)} August pages for analysis")

print(f"\nAugust pages to analyze: {august_pages[:5]}... (showing first 5 of {len(august_pages)})\n")

print("=== STEP 1: WIKIPEDIA REVISION HISTORY API APPROACH ===\n")
print("Strategy: Use Wikipedia API to get revision history and find June 2023 versions")
print("This bypasses Wayback Machine limitations and gets actual Wikipedia data\n")

# Wikipedia API endpoint
api_url = "https://en.wikipedia.org/w/api.php"

def get_june_2023_revision(page_title):
    """Get the latest revision of a Wikipedia page from June 2023"""
    print(f"  Searching revisions for: {page_title}")
    
    # Parameters for getting revision history
    params = {
        'action': 'query',
        'format': 'json',
        'prop': 'revisions',
        'titles': page_title,
        'rvlimit': 50,  # Get last 50 revisions
        'rvprop': 'timestamp|ids|user|comment|size',
        'rvdir': 'older',  # Start from newest and go backwards
        'rvstart': '2023-07-01T00:00:00Z',  # Start from July 1, 2023
        'rvend': '2023-06-01T00:00:00Z'     # End at June 1, 2023
    }
    
    try:
        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if 'query' in data and 'pages' in data['query']:
            pages = data['query']['pages']
            page_id = list(pages.keys())[0]
            
            if page_id == '-1':
                print(f"    ‚ùå Page '{page_title}' not found")
                return None
            
            if 'revisions' in pages[page_id]:
                revisions = pages[page_id]['revisions']
                print(f"    Found {len(revisions)} revisions in June 2023 timeframe")
                
                if revisions:
                    # Get the latest revision from June 2023 (first in the list since we're going backwards)
                    latest_june_rev = revisions[0]
                    timestamp = latest_june_rev['timestamp']
                    revid = latest_june_rev['revid']
                    
                    # Parse timestamp to verify it's from June 2023
                    rev_date = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    
                    if rev_date.year == 2023 and rev_date.month == 6:
                        print(f"    ‚úì Found June 2023 revision: {timestamp} (ID: {revid})")
                        return {
                            'available': True,
                            'page_title': page_title,
                            'page_id': page_id,
                            'revision_id': revid,
                            'timestamp': timestamp,
                            'formatted_date': rev_date.strftime('%Y-%m-%d'),
                            'user': latest_june_rev.get('user', 'Unknown'),
                            'comment': latest_june_rev.get('comment', ''),
                            'size': latest_june_rev.get('size', 0)
                        }
                    else:
                        print(f"    ‚ö†Ô∏è Latest revision is from {rev_date.strftime('%Y-%m')}, not June 2023")
                        return None
                else:
                    print(f"    ‚ùå No revisions found in June 2023 timeframe")
                    return None
            else:
                print(f"    ‚ùå No revision data found")
                return None
        else:
            print(f"    ‚ùå No page data in API response")
            return None
            
    except Exception as e:
        print(f"    ‚ùå Error fetching revisions: {str(e)}")
        return None

print("=== STEP 2: TESTING WIKIPEDIA API APPROACH ===\n")
print("Testing on first 3 August pages to validate the approach...\n")

# Test the Wikipedia API approach on first 3 pages
test_pages = august_pages[:3]
api_results = {}

for page_title in test_pages:
    print(f"\n--- Testing: {page_title} ---")
    
    result = get_june_2023_revision(page_title)
    api_results[page_title] = result
    
    # Add delay to be respectful to Wikipedia's servers
    time.sleep(1)

print(f"\n=== WIKIPEDIA API TEST RESULTS ===\n")

success_count = 0
for page_title, result in api_results.items():
    if result and result.get('available', False):
        success_count += 1
        print(f"‚úì {page_title}: Found June 2023 revision")
        print(f"  Date: {result['formatted_date']} ({result['timestamp']})")
        print(f"  Revision ID: {result['revision_id']}")
        print(f"  Size: {result['size']:,} bytes")
        print(f"  User: {result['user']}")
        if result['comment']:
            comment_preview = result['comment'][:100] + ('...' if len(result['comment']) > 100 else '')
            print(f"  Comment: {comment_preview}")
    else:
        print(f"‚ùå {page_title}: No June 2023 revision found")

print(f"\n=== API APPROACH RESULTS ===\n")
print(f"Pages with June 2023 revisions: {success_count}/{len(test_pages)}")
print(f"Success rate: {(success_count/len(test_pages)*100):.1f}%")

if success_count > 0:
    print(f"\nüéØ SUCCESS: Wikipedia API found {success_count} June 2023 revisions!")
    print("This approach works - ready to scale to all 31 pages")
    print("Next step: Extract content and analyze Twitter citations")
else:
    print(f"\n‚ö†Ô∏è NO SUCCESS: Wikipedia API found no June 2023 revisions")
    print("May need to expand date range or reconsider approach")

# Save API test results
api_analysis = {
    'analysis_metadata': {
        'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'approach': 'wikipedia_revision_history_api',
        'date_range_searched': '2023-06-01 to 2023-07-01',
        'test_pages_count': len(test_pages),
        'total_august_pages': len(august_pages)
    },
    'api_parameters': {
        'rvstart': '2023-07-01T00:00:00Z',
        'rvend': '2023-06-01T00:00:00Z',
        'rvlimit': 50
    },
    'test_results': api_results,
    'summary': {
        'pages_with_june_revisions': success_count,
        'success_rate_percent': round((success_count/len(test_pages)*100), 1)
    }
}

api_file = os.path.join(workspace_dir, 'august_pages_wikipedia_api_test.json')
with open(api_file, 'w', encoding='utf-8') as f:
    json.dump(api_analysis, f, indent=2, ensure_ascii=False)

print(f"\n‚úÖ Wikipedia API test results saved to: {os.path.basename(api_file)}")

print(f"\n=== DECISION POINT ===\n")
if success_count >= 1:
    print("‚úì Wikipedia API approach is viable")
    print("üîÑ Next: Scale to all 31 August pages")
    print("üîÑ Then: Extract content and analyze Twitter citations")
else:
    print("‚ùå Wikipedia API approach not finding June 2023 versions")
    print("ü§î Consider: Accept closest available dates from Wayback Machine")
    print("ü§î Or: Expand search to May 2023 or earlier")
    
print(f"\nüìä Current status: Testing phase complete, ready for next decision")
```