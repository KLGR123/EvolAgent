### Development Step 1: Parse Excel to List 2022 Books, Calculate Words-Per-Day, Find Slowest Read

**Description**: Parse and analyze the attached Excel file data/gaia/2023/validation/da52d699-e8d2-4dc5-9191-a2199e0b6a9b.xlsx to extract the complete list of books read in 2022. Identify the data structure including columns for book titles, word counts, reading start dates, reading end dates, or any other relevant metrics. Calculate the reading rate in words per day for each book by determining the total words divided by the number of days taken to read each book. Identify which book has the lowest words-per-day rate to determine the book that was read the slowest.

**Use Cases**:
- Publishing house editorial planning and beta reader performance analysis by calculating words-per-day rates to streamline book release timelines
- Corporate training program evaluation and employee engagement tracking using module word counts and reading durations to optimize e-learning content
- Academic library student reading behavior analysis and curriculum adjustment by identifying books with lowest reading rates to target supplemental support
- Book club coordinator reading pace monitoring and discussion scheduling by determining individual word-per-day metrics to enhance member participation
- Personal reading coach app performance assessment and goal setting by extracting year-long reading logs and highlighting slowest-read titles for targeted improvement
- Educational technology UX design feedback and content density optimization by analyzing digital textbook reading speeds across chapters to improve learner experience
- Online bookstore recommendation algorithm refinement and inventory curation by leveraging customer reading rate data to surface more engaging titles
- Content marketing team editorial workflow optimization and publication timeline management by measuring authors’ reading rates to schedule article reviews and launches

```
import os
import pandas as pd
from openpyxl import load_workbook
import json
from datetime import datetime

# The path to the Excel file to be analyzed
xlsx_path = "data/gaia/2023/validation/da52d699-e8d2-4dc5-9191-a2199e0b6a9b.xlsx"

print("=== ANALYZING 2022 READING DATA FROM EXCEL FILE ===")
print(f"Analyzing file: {xlsx_path}")
print("=" * 60)

# Check if file exists
if not os.path.exists(xlsx_path):
    print(f"Error: Excel file '{xlsx_path}' does not exist.")
    exit()

print(f"✓ File exists: {xlsx_path}")
file_size = os.path.getsize(xlsx_path)
print(f"File size: {file_size} bytes")

# Load workbook with openpyxl to examine structure first
print("\nLoading workbook with openpyxl to examine structure...")
wb = load_workbook(xlsx_path, data_only=True)

print(f"Number of worksheets: {len(wb.worksheets)}")
print(f"Worksheet names: {[sheet.title for sheet in wb.worksheets]}")

# Examine each worksheet structure
for sheet_idx, sheet in enumerate(wb.worksheets):
    print(f"\n=== ANALYZING WORKSHEET: {sheet.title} ===")
    
    max_row = sheet.max_row
    max_col = sheet.max_column
    print(f"Sheet dimensions: {max_row} rows x {max_col} columns")
    
    # Get the range of actual data
    min_row = sheet.min_row
    min_col = sheet.min_column
    print(f"Data range: rows {min_row}-{max_row}, columns {min_col}-{max_col}")
    
    print("\n=== FIRST 10 ROWS PREVIEW ===")
    # Display first 10 rows to understand structure
    for row in range(min_row, min(max_row + 1, min_row + 10)):
        row_data = []
        for col in range(min_col, max_col + 1):
            cell = sheet.cell(row=row, column=col)
            cell_value = cell.value if cell.value is not None else ""
            row_data.append(str(cell_value))
        print(f"Row {row}: {row_data}")
    
    print("\n=== COLUMN HEADERS ANALYSIS ===")
    # Examine the first row as potential headers
    headers = []
    for col in range(min_col, max_col + 1):
        cell = sheet.cell(row=min_row, column=col)
        header_value = cell.value if cell.value is not None else f"Col_{col}"
        headers.append(str(header_value))
        print(f"Column {col}: '{header_value}'")
    
    print(f"\nIdentified headers: {headers}")
    
    # Sample some data rows to understand content
    print("\n=== DATA SAMPLE (Rows 2-6) ===")
    for row in range(min_row + 1, min(max_row + 1, min_row + 6)):
        row_data = {}
        print(f"Row {row}:")
        for col_idx, col in enumerate(range(min_col, max_col + 1)):
            cell = sheet.cell(row=row, column=col)
            cell_value = cell.value if cell.value is not None else ""
            header = headers[col_idx] if col_idx < len(headers) else f"Col_{col}"
            row_data[header] = cell_value
            print(f"  {header}: '{cell_value}' (type: {type(cell_value).__name__})")
    
    # Look for reading-related keywords
    print("\n=== SEARCHING FOR READING-RELATED KEYWORDS ===")
    reading_keywords = ['book', 'title', 'word', 'count', 'start', 'end', 'date', 'read', 'finish', 'page', 'author', 'genre', 'rating', 'days', 'time']
    
    found_keywords = []
    for row in range(min_row, min(max_row + 1, min_row + 20)):  # Check first 20 rows
        for col in range(min_col, max_col + 1):
            cell = sheet.cell(row=row, column=col)
            if cell.value:
                cell_text = str(cell.value).lower()
                for keyword in reading_keywords:
                    if keyword in cell_text:
                        found_keywords.append({
                            'row': row,
                            'col': col,
                            'value': cell.value,
                            'keyword': keyword
                        })
                        print(f"Found keyword '{keyword}' in cell ({row}, {col}): '{cell.value}'")
    
    print(f"\nTotal reading-related keywords found: {len(found_keywords)}")

# Load with pandas for easier data manipulation
print("\n" + "="*60)
print("PANDAS DATAFRAME ANALYSIS")
print("="*60)

try:
    # Try to read the Excel file with pandas
    df_dict = pd.read_excel(xlsx_path, sheet_name=None)  # Read all sheets
    
    print(f"Pandas successfully loaded {len(df_dict)} sheet(s)")
    
    for sheet_name, sheet_df in df_dict.items():
        print(f"\n=== PANDAS ANALYSIS: {sheet_name} ===")
        print(f"DataFrame shape: {sheet_df.shape}")
        print(f"Column names: {list(sheet_df.columns)}")
        print(f"Data types:\n{sheet_df.dtypes}")
        
        print("\nFirst 5 rows:")
        print(sheet_df.head())
        
        print("\nLast 5 rows:")
        print(sheet_df.tail())
        
        print("\nBasic statistics for numeric columns:")
        numeric_cols = sheet_df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            print(sheet_df[numeric_cols].describe())
        else:
            print("No numeric columns found")
        
        print("\nMissing values:")
        print(sheet_df.isnull().sum())
        
        # Look for date columns
        print("\nDate-like columns analysis:")
        for col in sheet_df.columns:
            if any(keyword in col.lower() for keyword in ['date', 'start', 'end', 'finish', 'begin']):
                print(f"  {col}: {sheet_df[col].dtype}")
                print(f"    Sample values: {sheet_df[col].dropna().head(3).tolist()}")
        
        # Look for word count or numeric data
        print("\nNumeric columns that might be word counts:")
        for col in numeric_cols:
            if any(keyword in col.lower() for keyword in ['word', 'count', 'page', 'length']):
                print(f"  {col}: min={sheet_df[col].min()}, max={sheet_df[col].max()}, mean={sheet_df[col].mean():.2f}")
                print(f"    Sample values: {sheet_df[col].dropna().head(3).tolist()}")
        
except Exception as e:
    print(f"Error reading with pandas: {e}")
    print("Will proceed with openpyxl analysis only")

# Save initial analysis
analysis_data = {
    'file_path': xlsx_path,
    'file_size': file_size,
    'worksheets': [sheet.title for sheet in wb.worksheets],
    'analysis_timestamp': datetime.now().isoformat()
}

with open('workspace/reading_data_initial_analysis.json', 'w') as f:
    json.dump(analysis_data, f, indent=2)

print(f"\n✓ Initial analysis saved to: workspace/reading_data_initial_analysis.json")
print("\nNext step: Extract book reading data and calculate reading rates")
```