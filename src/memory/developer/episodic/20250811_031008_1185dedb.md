### Development Step 6: **Title:**  
Extract Wikipedia Access Date from Endnote on Page 11 of 'The Responsibility of Intellectuals' (2019)

**Description**: Access and download the full text of 'The Responsibility of Intellectuals' (DOI 10.2307/j.ctv9b2xdv) from JSTOR or UCL Press. Since this is a 2019 UCL Press publication available through JSTOR, retrieve the complete book content and save it to the workspace. Focus on locating page 11, identifying the second-to-last paragraph on that page, and extracting the specific endnote referenced in that paragraph. The endnote should contain a Wikipedia article citation with a November access date - extract the exact day of the month when the Wikipedia article was accessed.

**Use Cases**:
- Academic citation verification for university librarians ensuring the accuracy of Wikipedia references in scholarly books
- Automated compliance audits for publishers to confirm open access books properly cite and date online sources
- Digital humanities research extracting and mapping Wikipedia usage trends in recent historical monographs
- Legal due diligence for copyright lawyers reviewing source attribution and access dates in published works
- Automated syllabus preparation for professors needing to extract and summarize referenced online materials from course texts
- Quality assurance in academic publishing, checking that endnote formatting and citation dates meet editorial standards
- Data mining for bibliometric analysts studying the prevalence and recency of Wikipedia citations in social science literature
- Accessibility support for visually impaired students by extracting and presenting key references and citation details from academic PDFs

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time

print('=== DOWNLOADING UCL PRESS OPEN ACCESS PDF ===') 
print('DOI: 10.2307/j.ctv9b2xdv')
print('Target: Download full PDF and locate page 11 Wikipedia endnote with November access date')
print('\n' + '='*100 + '\n')

# The UCL Press PDF URL discovered in the previous attempt
ucl_pdf_url = 'http://discovery.ucl.ac.uk/10080589/1/The-Responsibility-of-Intellectuals.pdf'
print(f'UCL Press PDF URL: {ucl_pdf_url}')

# Set up headers for the download
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': 'https://www.uclpress.co.uk/'
}

print('=== DOWNLOADING FULL BOOK PDF FROM UCL PRESS ===\n')

try:
    print(f'Requesting PDF from: {ucl_pdf_url}')
    response = requests.get(ucl_pdf_url, headers=headers, timeout=60, stream=True)
    
    print(f'Response status: {response.status_code}')
    print(f'Content-Type: {response.headers.get("content-type", "unknown")}')
    print(f'Content-Length: {response.headers.get("content-length", "unknown")} bytes')
    
    if response.status_code == 200:
        content_type = response.headers.get('content-type', '').lower()
        
        if 'pdf' in content_type:
            print('\n*** PDF CONTENT CONFIRMED ***')
            
            # Download the PDF
            pdf_path = 'workspace/responsibility_intellectuals_full_book.pdf'
            
            print(f'Downloading PDF to: {pdf_path}')
            with open(pdf_path, 'wb') as pdf_file:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        pdf_file.write(chunk)
            
            # Verify download
            file_size = os.path.getsize(pdf_path)
            print(f'‚úì PDF downloaded successfully')
            print(f'File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
            
            # Extract text from the PDF
            print('\n=== EXTRACTING TEXT FROM FULL BOOK PDF ===\n')
            
            try:
                from langchain_community.document_loaders import PyPDFLoader
                
                print('Loading PDF with PyPDFLoader...')
                loader = PyPDFLoader(pdf_path)
                pages = loader.load_and_split()
                
                print(f'‚úì PDF loaded successfully with {len(pages)} pages')
                
                # Verify we have enough pages
                if len(pages) >= 11:
                    print(f'\n=== ANALYZING PAGE 11 CONTENT ===\n')
                    
                    # Get page 11 (index 10)
                    page_11 = pages[10]
                    page_11_content = page_11.page_content
                    
                    print(f'Page 11 content length: {len(page_11_content):,} characters')
                    print(f'\nFirst 300 characters of page 11:')
                    print('='*80)
                    print(page_11_content[:300] + '...')
                    print('='*80)
                    
                    # Save page 11 content for reference
                    with open('workspace/page_11_full_content.txt', 'w', encoding='utf-8') as f:
                        f.write('PAGE 11 - THE RESPONSIBILITY OF INTELLECTUALS\n')
                        f.write('Source: UCL Press Open Access PDF\n')
                        f.write('='*80 + '\n\n')
                        f.write(page_11_content)
                    
                    print('\n‚úì Page 11 content saved to workspace/page_11_full_content.txt')
                    
                    # Split page 11 into paragraphs
                    print('\n=== IDENTIFYING PARAGRAPHS ON PAGE 11 ===\n')
                    
                    # Try multiple paragraph splitting approaches
                    paragraphs = []
                    
                    # Method 1: Split by double newlines
                    if '\n\n' in page_11_content:
                        paragraphs = [p.strip() for p in page_11_content.split('\n\n') if p.strip()]
                        print(f'Method 1 (double newlines): Found {len(paragraphs)} paragraphs')
                    
                    # Method 2: If few paragraphs, try single newlines with grouping
                    if len(paragraphs) < 3:
                        lines = [line.strip() for line in page_11_content.split('\n') if line.strip()]
                        current_para = []
                        paragraphs = []
                        
                        for line in lines:
                            if len(line) > 50:  # Likely a paragraph line
                                current_para.append(line)
                            else:
                                if current_para:
                                    paragraphs.append(' '.join(current_para))
                                    current_para = []
                                if len(line) > 10:  # Short but meaningful line
                                    paragraphs.append(line)
                        
                        if current_para:
                            paragraphs.append(' '.join(current_para))
                        
                        print(f'Method 2 (line grouping): Found {len(paragraphs)} paragraphs')
                    
                    # Show all paragraphs for inspection
                    print(f'\nAll paragraphs on page 11:')
                    for i, para in enumerate(paragraphs, 1):
                        print(f'\nParagraph {i} ({len(para)} chars):')
                        print('-'*60)
                        print(para[:200] + ('...' if len(para) > 200 else ''))
                        print('-'*60)
                    
                    # Identify the second-to-last paragraph
                    if len(paragraphs) >= 2:
                        second_to_last_para = paragraphs[-2]
                        print(f'\n=== SECOND-TO-LAST PARAGRAPH ON PAGE 11 ===\n')
                        print('='*80)
                        print(second_to_last_para)
                        print('='*80)
                        
                        # Save the specific paragraph
                        with open('workspace/page_11_second_to_last_paragraph.txt', 'w', encoding='utf-8') as f:
                            f.write('SECOND-TO-LAST PARAGRAPH FROM PAGE 11\n')
                            f.write('Source: The Responsibility of Intellectuals (UCL Press, 2019)\n')
                            f.write('='*80 + '\n\n')
                            f.write(second_to_last_para)
                        
                        print('\n‚úì Second-to-last paragraph saved to workspace/page_11_second_to_last_paragraph.txt')
                        
                        # Search for endnote references in this paragraph
                        print('\n=== SEARCHING FOR ENDNOTE REFERENCES ===\n')
                        
                        import re
                        
                        # Comprehensive endnote patterns
                        endnote_patterns = [
                            r'\b(\d+)\b',  # Simple numbers
                            r'\[(\d+)\]',  # Numbers in brackets
                            r'\((\d+)\)',  # Numbers in parentheses
                            r'\b(\d+)\.',  # Numbers with periods
                            r'see note (\d+)',  # "see note X" format
                            r'note (\d+)',  # "note X" format
                            r'footnote (\d+)',  # "footnote X" format
                            r'endnote (\d+)',  # "endnote X" format
                            r'\^(\d+)',  # Superscript-style
                        ]
                        
                        found_endnotes = []
                        for pattern in endnote_patterns:
                            matches = re.findall(pattern, second_to_last_para, re.IGNORECASE)
                            if matches:
                                for match in matches:
                                    if match.isdigit() and 1 <= int(match) <= 300:  # Reasonable endnote range
                                        found_endnotes.append(int(match))
                        
                        # Remove duplicates and sort
                        found_endnotes = sorted(list(set(found_endnotes)))
                        
                        if found_endnotes:
                            print(f'*** FOUND ENDNOTE REFERENCES: {found_endnotes} ***')
                            
                            # Search the entire book for endnotes section
                            print('\n=== SEARCHING ENTIRE BOOK FOR ENDNOTES/REFERENCES SECTION ===\n')
                            
                            # Combine all pages
                            full_book_text = '\n\n'.join([page.page_content for page in pages])
                            print(f'Total book text: {len(full_book_text):,} characters')
                            
                            # Look for endnotes/references section
                            endnotes_section_indicators = [
                                'notes', 'endnotes', 'references', 'footnotes',
                                'bibliography', 'works cited', 'sources'
                            ]
                            
                            endnotes_sections_found = []
                            for indicator in endnotes_section_indicators:
                                # Look for section headers
                                pattern = rf'\b{indicator}\b'
                                matches = list(re.finditer(pattern, full_book_text, re.IGNORECASE))
                                if matches:
                                    print(f'Found "{indicator}" section at {len(matches)} locations')
                                    endnotes_sections_found.extend(matches)
                            
                            # Search for Wikipedia citations with November access dates
                            print('\n=== SEARCHING FOR WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATE ===\n')
                            
                            # Comprehensive Wikipedia citation patterns
                            wikipedia_patterns = [
                                r'wikipedia[^\n]{0,300}november[^\n]{0,100}\d{1,2}[^\n]{0,100}',
                                r'en\.wikipedia\.org[^\n]{0,300}november[^\n]{0,100}\d{1,2}[^\n]{0,100}',
                                r'accessed[^\n]{0,150}november[^\n]{0,50}\d{1,2}[^\n]{0,150}wikipedia[^\n]{0,200}',
                                r'november[^\n]{0,50}\d{1,2}[^\n]{0,150}wikipedia[^\n]{0,300}',
                                r'\d{1,2}[^\n]{0,30}november[^\n]{0,150}wikipedia[^\n]{0,300}',
                                r'wikipedia[^\n]{0,400}accessed[^\n]{0,150}november[^\n]{0,50}\d{1,2}[^\n]{0,100}',
                                r'\bwikipedia\b[^\n]{0,500}\bnovember\b[^\n]{0,100}\b\d{1,2}\b[^\n]{0,100}',
                                r'\bnovember\b[^\n]{0,100}\b\d{1,2}\b[^\n]{0,200}\bwikipedia\b[^\n]{0,300}'
                            ]
                            
                            wikipedia_citations = []
                            for pattern in wikipedia_patterns:
                                matches = re.finditer(pattern, full_book_text, re.IGNORECASE | re.DOTALL)
                                for match in matches:
                                    citation_text = match.group(0)
                                    
                                    # Extract the day from November date
                                    day_patterns = [
                                        r'november\s+(\d{1,2})',
                                        r'(\d{1,2})\s+november',
                                        r'november\s+(\d{1,2})(?:st|nd|rd|th)?',
                                        r'(\d{1,2})(?:st|nd|rd|th)?\s+november',
                                        r'november\s*,?\s*(\d{1,2})',
                                        r'(\d{1,2})\s*,?\s*november'
                                    ]
                                    
                                    day_found = None
                                    for day_pattern in day_patterns:
                                        day_match = re.search(day_pattern, citation_text, re.IGNORECASE)
                                        if day_match:
                                            day_found = day_match.group(1)
                                            break
                                    
                                    if day_found:
                                        # Get broader context around the citation
                                        context_start = max(0, match.start() - 800)
                                        context_end = min(len(full_book_text), match.end() + 800)
                                        citation_context = full_book_text[context_start:context_end]
                                        
                                        # Check if this citation is related to our endnote numbers
                                        related_endnotes = []
                                        for endnote_num in found_endnotes:
                                            if str(endnote_num) in citation_context:
                                                related_endnotes.append(endnote_num)
                                        
                                        wikipedia_citations.append({
                                            'citation': citation_text,
                                            'november_day': day_found,
                                            'position': match.start(),
                                            'context': citation_context,
                                            'related_endnotes': related_endnotes
                                        })
                            
                            # Remove duplicates based on citation text and day
                            unique_citations = []
                            seen_citations = set()
                            for citation in wikipedia_citations:
                                citation_key = (citation['citation'].strip().lower(), citation['november_day'])
                                if citation_key not in seen_citations:
                                    seen_citations.add(citation_key)
                                    unique_citations.append(citation)
                            
                            if unique_citations:
                                print(f'üéØ FOUND {len(unique_citations)} UNIQUE WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATES:')
                                
                                for i, citation in enumerate(unique_citations, 1):
                                    print(f'\nCitation {i}:')
                                    print(f'November day: {citation["november_day"]}')
                                    print(f'Position in book: {citation["position"]:,}')
                                    if citation['related_endnotes']:
                                        print(f'Related to endnotes: {citation["related_endnotes"]}')
                                    print('Citation text:')
                                    print('='*80)
                                    print(citation['citation'])
                                    print('='*80)
                                    
                                    # Show relevant context
                                    context_preview = citation['context'][:400] + '...' if len(citation['context']) > 400 else citation['context']
                                    print(f'Context: {context_preview}')
                                    print('-'*80)
                                
                                # Save the complete analysis
                                final_analysis = {
                                    'source_pdf': pdf_path,
                                    'book_title': 'The Responsibility of Intellectuals',
                                    'publisher': 'UCL Press',
                                    'year': 2019,
                                    'total_pages': len(pages),
                                    'page_11_analysis': {
                                        'paragraph_count': len(paragraphs),
                                        'second_to_last_paragraph': second_to_last_para,
                                        'endnote_references_found': found_endnotes
                                    },
                                    'wikipedia_citations_with_november_dates': unique_citations,
                                    'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                                }
                                
                                with open('workspace/final_wikipedia_endnote_analysis.json', 'w', encoding='utf-8') as f:
                                    json.dump(final_analysis, f, indent=2, ensure_ascii=False)
                                
                                print('\n‚úì Complete analysis saved to workspace/final_wikipedia_endnote_analysis.json')
                                
                                # Determine the final answer
                                if len(unique_citations) == 1:
                                    answer_day = unique_citations[0]['november_day']
                                    print(f'\n*** FINAL ANSWER: The Wikipedia article was accessed on November {answer_day} ***')
                                elif len(unique_citations) > 1:
                                    # Find the citation most closely related to page 11 endnotes
                                    best_citation = None
                                    max_related_endnotes = 0
                                    
                                    for citation in unique_citations:
                                        if len(citation['related_endnotes']) > max_related_endnotes:
                                            max_related_endnotes = len(citation['related_endnotes'])
                                            best_citation = citation
                                    
                                    if best_citation and best_citation['related_endnotes']:
                                        answer_day = best_citation['november_day']
                                        print(f'\n*** MOST LIKELY ANSWER: November {answer_day} ***')
                                        print(f'(This citation relates to endnotes: {best_citation["related_endnotes"]} from page 11)')
                                    else:
                                        print(f'\n*** MULTIPLE CANDIDATES FOUND ***')
                                        print('All November access dates found:')
                                        for i, citation in enumerate(unique_citations, 1):
                                            print(f'{i}. November {citation["november_day"]} (endnotes: {citation["related_endnotes"]})')
                                        
                                        # Default to first citation if no clear winner
                                        answer_day = unique_citations[0]['november_day']
                                        print(f'\nDefaulting to first citation: November {answer_day}')
                            else:
                                print('\n‚ö† No Wikipedia citations with November access dates found')
                                
                                # Broader search for any Wikipedia references
                                print('\nSearching for any Wikipedia references...')
                                wiki_matches = re.finditer(r'wikipedia[^\n]{0,200}', full_book_text, re.IGNORECASE)
                                wiki_refs = [match.group(0) for match in wiki_matches]
                                
                                if wiki_refs:
                                    print(f'Found {len(wiki_refs)} general Wikipedia references:')
                                    for i, ref in enumerate(wiki_refs[:5], 1):
                                        print(f'{i}. {ref[:100]}...')
                                else:
                                    print('No Wikipedia references found in the entire book')
                        else:
                            print('\n‚ö† No endnote references found in second-to-last paragraph')
                            print('Paragraph content for manual inspection:')
                            print(second_to_last_para)
                    else:
                        print(f'\n‚ö† Page 11 has fewer than 2 paragraphs ({len(paragraphs)} found)')
                        if paragraphs:
                            print('Available paragraphs:')
                            for i, para in enumerate(paragraphs, 1):
                                print(f'{i}. {para[:200]}...')
                else:
                    print(f'\n‚ö† PDF has only {len(pages)} pages, page 11 not available')
                    print('Available pages:')
                    for i in range(min(10, len(pages))):
                        preview = pages[i].page_content[:100].replace('\n', ' ')
                        print(f'Page {i+1}: {preview}...')
            
            except ImportError:
                print('‚ùå PyPDFLoader not available - cannot extract text from PDF')
                print('PDF has been downloaded but text extraction is not possible')
            except Exception as extraction_error:
                print(f'‚ùå Error during PDF text extraction: {str(extraction_error)}')
        else:
            print(f'\n‚ö† Content does not appear to be PDF: {content_type}')
            print('Saving response for analysis')
            with open('workspace/ucl_response_content.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
    else:
        print(f'\n‚ùå Download failed with status: {response.status_code}')
        print(f'Response content: {response.text[:500]}...')

except Exception as e:
    print(f'‚ùå Error downloading PDF: {str(e)}')

print('\n' + '='*100)
print('UCL PRESS PDF DOWNLOAD AND ANALYSIS COMPLETE')
print('='*100)
print('Objective: Download full book PDF, locate page 11, find second-to-last paragraph,')
print('          and extract Wikipedia endnote with November access date')
print('\nFiles created in workspace:')
if os.path.exists('workspace'):
    for file in sorted(os.listdir('workspace')):
        if file.endswith(('.pdf', '.txt', '.json')):
            file_path = os.path.join('workspace', file)
            file_size = os.path.getsize(file_path)
            print(f'- {file} ({file_size:,} bytes)')
```