### Development Step 39: Download UoL PDF: 'Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?' to workspace/dragon_diet_fish_bag.pdf

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic researchers automating the retrieval of discipline-specific PDF papers from university repositories (e.g., le.ac.uk) to build comprehensive literature reviews without manual downloads
- Patent analysts batch-downloading PDF patents from official patent office domains (e.g., uspto.gov) to feed into an internal patent prior-art database
- Financial analysts extracting quarterly earnings and annual report PDFs from SEC EDGAR (.gov) for automated parsing and valuation modeling
- Legal teams fetching court decision PDFs from government judiciary websites to keep case-law databases up to date and support brief writing
- Market research firms collecting product whitepaper PDFs from vendor domains for competitive landscape mapping and feature comparison
- Healthcare data managers downloading clinical guideline and trial result PDFs from hospital or university subdomains to standardize treatment protocol repositories
- Environmental scientists aggregating technical report PDFs from government environmental agency sites to analyze long-term ecological impact trends
- Education technologists harvesting course syllabus PDFs from university department pages to automate curriculum mapping and prerequisite checks

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
bing_html = os.path.join(workspace, 'bing_search.html')
candidates_out = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure workspace exists ---
if not os.path.isdir(workspace):
    print(f"[INFO] Creating workspace directory: {workspace}")
    os.makedirs(workspace, exist_ok=True)

# --- Step 1: Perform a broader Bing search for any PDF of the paper ---
query = "filetype:pdf \"Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?\""
bing_url = 'https://www.bing.com/search'
params = {'q': query, 'count': '50'}  # fetch more results
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:104.0) Gecko/20100101 Firefox/104.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"[INFO] Sending Bing search request with query:\n      {query}\n")
response = requests.get(bing_url, params=params, headers=headers, timeout=30)
print(f"[INFO] Bing returned status: {response.status_code}")
if response.status_code != 200:
    print(f"ERROR: Bing search failed with status {response.status_code}. Exiting.")
    sys.exit(1)

# Save raw HTML for inspection
with open(bing_html, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"[SAVE] Bing search HTML → {bing_html}\n")

# --- Step 2: Parse HTML and extract candidate PDF URLs ---
print("[INFO] Parsing HTML and extracting PDF URLs...")
with open(bing_html, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()

# 2a) Extract main result anchors (li.b_algo h2 a)
anchors = soup.select('li.b_algo h2 a')
print(f"[STEP] Found {len(anchors)} main result anchors. Scanning for .pdf links...")
for a in anchors:
    href = a.get('href') or ''
    if '.pdf' in href.lower():
        pdf_urls.add(href)
        print(f"    [FOUND] PDF in result anchor: {href}")

# 2b) Decode Bing redirect links (/url?q=...)
print(f"[STEP] Parsing Bing redirect links for PDFs...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q','u','url'):
            if key in qs:
                real = unquote(qs[key][0])
                if '.pdf' in real.lower():
                    pdf_urls.add(real)
                    print(f"    [FOUND] PDF in redirect URL: {real}")
                break

# 2c) Regex fallback over raw HTML
print("[STEP] Running regex fallback for http(s)://...pdf patterns...")
pattern = r'''https?://[^\s"']+?\.pdf(?:\?[^\s"']*)?'''
matches = re.findall(pattern, html, flags=re.IGNORECASE)
for m in matches:
    pdf_urls.add(m)
print(f"    [INFO] Total after regex fallback: {len(pdf_urls)} candidates")

# 2d) Filter for University of Leicester domains
print("[STEP] Filtering for University of Leicester domains...")
leicester_domains = ('le.ac.uk','lra.le.ac.uk')
le_urls = [u for u in pdf_urls if any(d in u.lower() for d in leicester_domains)]
if le_urls:
    print(f"    [SUCCESS] Found {len(le_urls)} Leicester PDF URL(s):")
    for u in le_urls:
        print(f"      - {u}")
else:
    print("    [WARN] No Leicester-specific PDFs found. Listing all candidates:")
    for u in sorted(pdf_urls):
        print(f"      - {u}")

# Save all candidates
with open(candidates_out, 'w', encoding='utf-8') as f:
    for u in sorted(pdf_urls):
        f.write(u + '\n')
print(f"\n[SAVE] All PDF candidates written to: {candidates_out}")
print("[COMPLETE] Extraction phase finished. Inspect the file to choose the correct PDF link.")

```