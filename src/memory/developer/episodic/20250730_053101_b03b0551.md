### Development Step 3: Download A Dark Trace and Extract Chapter 2 to Identify Freud’s ‘Endopsychic Myths’ Source

**Description**: Access and download the full text of 'A Dark Trace: Sigmund Freud on the Sense of Guilt' by H. Westerink from Project MUSE using DOI 10.1353/book.24372. Since the book was confirmed to be open access, retrieve the complete text and save it to workspace/dark_trace_freud_book.pdf or appropriate format. Focus on locating and extracting Chapter 2 content to identify the author who influenced Freud's belief in 'endopsychic myths'. If the full book is not directly downloadable, extract Chapter 2 specifically or access the book's table of contents to determine the exact chapter title and content structure.

**Use Cases**:
- Literary scholarship automation for digital humanities projects, retrieving and parsing Chapter 2 of Freud’s “A Dark Trace” to analyze the intellectual origins of endopsychic myths
- Academic library workflows for bulk ingestion of open‐access monographs from Project MUSE into institutional repositories with metadata and full‐text archival
- Educational content platforms extracting specific chapters into LMS modules for graduate courses on psychoanalytic theory and guilt studies
- NLP research pipelines assembling targeted corpora of psychological theory texts by DOI‐driven downloads and chapter‐level extraction for topic modeling
- Digital preservation operations scheduling automated downloads and integrity checks of OA scholarly books to ensure long-term archival compliance
- Comparative religion studies automating retrieval of historical myth analyses from open-access monographs to cross-reference influences on modern belief systems
- Scholarly publishing QA processes validating open-access status and extracting previewable chapter content to populate online book previews and catalogs
- Legal consulting firms gathering expert literature on guilt and psychoanalysis by programmatic DOI access and chapter extraction for forensic case research

```
import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

print('=== ANALYZING PROJECT MUSE ACCESS RESULTS ===')
print('Objective: Examine the CrossRef chooser redirect and find direct Project MUSE access\n')

# First, let's inspect the analysis file structure
analysis_file = 'workspace/project_muse_page_analysis.json'

if os.path.exists(analysis_file):
    print('=== INSPECTING SAVED ANALYSIS FILE ===')
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    print(f'Analysis file keys: {list(analysis_data.keys())}')
    
    for key, value in analysis_data.items():
        if isinstance(value, (str, bool, int)):
            print(f'{key}: {value}')
        elif isinstance(value, list):
            print(f'{key}: List with {len(value)} items')
            if len(value) > 0:
                print(f'  Sample item: {value[0]}')
        elif isinstance(value, dict):
            print(f'{key}: Dictionary with keys {list(value.keys())}')
        else:
            print(f'{key}: {type(value).__name__}')
    
    print(f'\nDetailed analysis:')
    print(f'DOI URL: {analysis_data.get("doi_url", "Unknown")}')
    print(f'Final redirect URL: {analysis_data.get("final_url", "Unknown")}')
    print(f'Page title: {analysis_data.get("page_title", "Unknown")}')
    print(f'Book title found: {analysis_data.get("book_title_found", "Unknown")}')
    print(f'Download links found: {len(analysis_data.get("download_links", []))}')
    print(f'Open access status: {analysis_data.get("is_open_access", False)}')
else:
    print(f'Analysis file not found: {analysis_file}')
    print('Available files in workspace:')
    if os.path.exists('workspace'):
        for file in os.listdir('workspace'):
            print(f'  - {file}')

print('\n=== ACCESSING CROSSREF CHOOSER PAGE FOR DIRECT LINKS ===')

# The CrossRef chooser often contains direct links to the actual publisher page
crossref_url = 'https://chooser.crossref.org/?doi=10.1353%2Fbook.24372'
print(f'CrossRef chooser URL: {crossref_url}')

# Headers to mimic a real browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

try:
    print('Accessing CrossRef chooser page...')
    crossref_response = requests.get(crossref_url, headers=headers, timeout=30)
    print(f'Status code: {crossref_response.status_code}')
    print(f'Content length: {len(crossref_response.content):,} bytes')
    
    if crossref_response.status_code == 200:
        soup = BeautifulSoup(crossref_response.content, 'html.parser')
        
        # Look for Project MUSE links or publisher links
        print('\n=== SEARCHING FOR PUBLISHER LINKS ===')
        
        all_links = soup.find_all('a', href=True)
        print(f'Total links found: {len(all_links)}')
        
        project_muse_links = []
        publisher_links = []
        
        for link in all_links:
            href = link.get('href')
            text = link.get_text().strip()
            
            if 'muse.jhu.edu' in href:
                project_muse_links.append({
                    'url': href,
                    'text': text,
                    'type': 'Project MUSE'
                })
            elif any(publisher in href.lower() for publisher in ['publisher', 'book', 'doi']):
                publisher_links.append({
                    'url': href,
                    'text': text,
                    'type': 'Publisher'
                })
        
        print(f'\nProject MUSE links found: {len(project_muse_links)}')
        for i, link in enumerate(project_muse_links, 1):
            print(f'{i}. {link["text"]} -> {link["url"]}')
        
        print(f'\nOther publisher links: {len(publisher_links)}')
        for i, link in enumerate(publisher_links[:5], 1):  # Show first 5
            print(f'{i}. {link["text"]} -> {link["url"]}')
        
        # Try to access Project MUSE link if found
        if project_muse_links:
            target_link = project_muse_links[0]['url']
            print(f'\n=== ACCESSING PROJECT MUSE DIRECTLY ===')
            print(f'Target URL: {target_link}')
            
            try:
                muse_response = requests.get(target_link, headers=headers, timeout=30)
                print(f'Project MUSE response status: {muse_response.status_code}')
                print(f'Final URL: {muse_response.url}')
                
                if muse_response.status_code == 200:
                    muse_soup = BeautifulSoup(muse_response.content, 'html.parser')
                    
                    # Get page title
                    page_title = muse_soup.find('title')
                    if page_title:
                        print(f'Page title: {page_title.get_text().strip()}')
                    
                    # Look for book information
                    book_title_elem = muse_soup.find('h1') or muse_soup.find('h2')
                    if book_title_elem:
                        print(f'Book title on page: {book_title_elem.get_text().strip()}')
                    
                    # Search for download/access links
                    print('\n=== SEARCHING FOR BOOK ACCESS OPTIONS ===')
                    
                    # Look for PDF, download, or full text links
                    access_selectors = [
                        'a[href*=".pdf"]',
                        'a[href*="download"]',
                        'a[href*="fulltext"]',
                        'a[href*="read"]',
                        '.pdf-link',
                        '.download-link',
                        '.access-link',
                        '.full-text'
                    ]
                    
                    access_links = []
                    for selector in access_selectors:
                        links = muse_soup.select(selector)
                        for link in links:
                            href = link.get('href')
                            if href:
                                if href.startswith('/'):
                                    href = urljoin(muse_response.url, href)
                                access_links.append({
                                    'url': href,
                                    'text': link.get_text().strip(),
                                    'selector': selector
                                })
                    
                    # Remove duplicates
                    unique_access = []
                    seen_urls = set()
                    for link in access_links:
                        if link['url'] not in seen_urls:
                            seen_urls.add(link['url'])
                            unique_access.append(link)
                    
                    print(f'Access options found: {len(unique_access)}')
                    for i, link in enumerate(unique_access, 1):
                        print(f'{i}. {link["text"]} -> {link["url"]}')
                    
                    # Look for table of contents or chapter information
                    print('\n=== SEARCHING FOR TABLE OF CONTENTS ===')
                    
                    toc_indicators = ['table of contents', 'contents', 'chapter', 'toc']
                    page_text = muse_soup.get_text().lower()
                    
                    chapter_2_found = False
                    for indicator in ['chapter 2', 'chapter two', 'ch. 2']:
                        if indicator in page_text:
                            print(f'Found Chapter 2 reference: "{indicator}"')
                            chapter_2_found = True
                            
                            # Extract context
                            index = page_text.find(indicator)
                            context_start = max(0, index - 150)
                            context_end = min(len(page_text), index + 200)
                            context = page_text[context_start:context_end]
                            print(f'Context: ...{context}...')
                            break
                    
                    if not chapter_2_found:
                        print('No explicit Chapter 2 references found in main page text')
                    
                    # Look for "Look Inside" or preview functionality
                    preview_selectors = [
                        'a:contains("Look Inside")',
                        'a:contains("Preview")',
                        'a:contains("Browse")',
                        '.preview-link',
                        '.look-inside'
                    ]
                    
                    preview_links = []
                    for selector in preview_selectors:
                        try:
                            links = muse_soup.select(selector)
                            for link in links:
                                href = link.get('href')
                                if href:
                                    if href.startswith('/'):
                                        href = urljoin(muse_response.url, href)
                                    preview_links.append({
                                        'url': href,
                                        'text': link.get_text().strip()
                                    })
                        except:
                            pass  # Skip selector if it causes issues
                    
                    print(f'\nPreview options found: {len(preview_links)}')
                    for i, link in enumerate(preview_links, 1):
                        print(f'{i}. {link["text"]} -> {link["url"]}')
                    
                    # Check for open access indicators
                    open_access_indicators = ['open access', 'freely available', 'free', 'oa']
                    is_open_access = any(indicator in page_text for indicator in open_access_indicators)
                    print(f'\nOpen access indicators on Project MUSE page: {is_open_access}')
                    
                    # Save the Project MUSE page content for analysis
                    muse_content = {
                        'url': muse_response.url,
                        'title': page_title.get_text().strip() if page_title else None,
                        'book_title': book_title_elem.get_text().strip() if book_title_elem else None,
                        'access_links': unique_access,
                        'preview_links': preview_links,
                        'chapter_2_found': chapter_2_found,
                        'is_open_access': is_open_access,
                        'content_length': len(muse_response.content),
                        'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    with open('workspace/project_muse_book_page.json', 'w', encoding='utf-8') as f:
                        json.dump(muse_content, f, indent=2, ensure_ascii=False)
                    
                    print(f'\nProject MUSE page analysis saved to: workspace/project_muse_book_page.json')
                    
                    # Try to download if access links are available
                    if unique_access:
                        pdf_links = [link for link in unique_access if '.pdf' in link['url'].lower()]
                        if pdf_links:
                            print(f'\n=== ATTEMPTING PDF DOWNLOAD ===')
                            pdf_url = pdf_links[0]['url']
                            print(f'PDF URL: {pdf_url}')
                            
                            try:
                                pdf_response = requests.get(pdf_url, headers=headers, timeout=60)
                                if pdf_response.status_code == 200:
                                    content_type = pdf_response.headers.get('content-type', '').lower()
                                    if 'pdf' in content_type and len(pdf_response.content) > 10000:
                                        pdf_path = 'workspace/dark_trace_freud_book.pdf'
                                        with open(pdf_path, 'wb') as pdf_file:
                                            pdf_file.write(pdf_response.content)
                                        
                                        file_size = os.path.getsize(pdf_path)
                                        print(f'\n*** PDF SUCCESSFULLY DOWNLOADED ***')
                                        print(f'Saved to: {pdf_path}')
                                        print(f'File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                                    else:
                                        print(f'PDF download failed - Content type: {content_type}, Size: {len(pdf_response.content)}')
                                else:
                                    print(f'PDF download failed - Status: {pdf_response.status_code}')
                            except Exception as pdf_error:
                                print(f'PDF download error: {str(pdf_error)}')
                
                else:
                    print(f'Failed to access Project MUSE page: {muse_response.status_code}')
            
            except Exception as muse_error:
                print(f'Error accessing Project MUSE: {str(muse_error)}')
        
        else:
            print('No Project MUSE links found in CrossRef chooser')
            
            # Try constructing Project MUSE URL pattern
            print('\n=== TRYING PROJECT MUSE URL PATTERNS ===')
            
            # Extract book ID from DOI
            doi = '10.1353/book.24372'
            book_id = doi.split('.')[-1]  # Extract '24372'
            
            possible_urls = [
                f'https://muse.jhu.edu/book/{book_id}',
                f'https://muse.jhu.edu/book/{book_id}/summary',
                f'https://www.muse.jhu.edu/book/{book_id}',
                f'https://muse.jhu.edu/chapter/{book_id}'
            ]
            
            print(f'Book ID extracted: {book_id}')
            print('Trying possible Project MUSE URL patterns:')
            
            for url_pattern in possible_urls:
                print(f'\nTrying: {url_pattern}')
                try:
                    pattern_response = requests.get(url_pattern, headers=headers, timeout=20)
                    print(f'Status: {pattern_response.status_code}')
                    
                    if pattern_response.status_code == 200:
                        print(f'*** SUCCESS - Found working URL: {url_pattern} ***')
                        print(f'Final URL: {pattern_response.url}')
                        
                        # Save this successful URL for further processing
                        success_info = {
                            'working_url': url_pattern,
                            'final_url': pattern_response.url,
                            'status_code': pattern_response.status_code,
                            'content_length': len(pattern_response.content),
                            'found_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                        
                        with open('workspace/successful_muse_url.json', 'w') as f:
                            json.dump(success_info, f, indent=2)
                        
                        print('Success info saved to: workspace/successful_muse_url.json')
                        break
                    
                except Exception as pattern_error:
                    print(f'Error: {str(pattern_error)}')
    
    else:
        print(f'Failed to access CrossRef chooser: {crossref_response.status_code}')

except Exception as e:
    print(f'Error accessing CrossRef chooser: {str(e)}')

print('\n=== PROJECT MUSE ACCESS ATTEMPT COMPLETE ===')
print('Summary:')
print('- Analyzed CrossRef chooser redirect behavior')
print('- Attempted direct Project MUSE access')
print('- Searched for download and preview options')
print('- Tried multiple URL patterns to locate the book')
print('\nNext steps based on results:')
print('1. If PDF found: Extract Chapter 2 content')
print('2. If preview access: Navigate to Chapter 2')
print('3. If no direct access: Search for alternative sources')
```