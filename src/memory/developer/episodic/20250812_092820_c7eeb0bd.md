### Development Step 19: Identify Organization Behind São Francisco Basin Environmental Education Plan and Sobradinho Dam Displacement Advocate

**Description**: Conduct a comprehensive web search to identify the organization that launched the 'Plano de Educação Ambiental da Bacia do Rio São Francisco' covering 505 municipalities and collaborates with Ministérios Públicos. Search for keywords including 'Plano de Educação Ambiental Bacia Rio São Francisco 505 municípios', 'Ministérios Públicos São Francisco basin', 'environmental education plan São Francisco river', and 'Sobradinho Dam displaced people advocacy'. Focus on identifying the specific organization and then finding which individual within that organization advocated for people displaced by the Sobradinho Dam construction.

**Use Cases**:
- Detailed NGO advocacy mapping and stakeholder profiling for environmental compensation cases arising from the Sobradinho Dam displacement
- Legal firm’s automated evidence collection and entity extraction to support litigation for affected communities in the São Francisco basin
- Government agency’s periodic monitoring and report generation on the rollout of the Plano de Educação Ambiental across 505 municipalities
- Academic researcher’s systematic policy and literature analysis for evaluating socio-environmental education programs in Brazilian river basins
- Consultancy firm’s due-diligence and network analysis of inter-institutional collaborations in public environmental initiatives
- Journalistic investigative reporting automation for cross-referencing web sources to identify key advocates in Sobradinho Dam compensation debates
- Corporate CSR team’s targeted web intelligence and partner identification for sustainable water management projects in the São Francisco watershed

```
from ddgs import DDGS
import json
import os
from datetime import datetime
import time
import re

print("Conducting comprehensive search for CBHSF organization and individuals who advocated for Sobradinho Dam displaced people...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Define comprehensive search queries
search_queries = [
    "CBHSF Plano Educação Ambiental Bacia São Francisco 505 municípios",
    "Comitê Bacia Hidrográfica São Francisco Ministérios Públicos",
    "CBHSF Sobradinho barragem deslocados atingidos",
    "Sobradinho Dam displaced people CBHSF advocacy",
    "CBHSF presidente diretor Sobradinho compensação",
    "Comitê São Francisco Sobradinho reassentamento indenização",
    "CBHSF secretário executivo Sobradinho atingidos",
    "Sobradinho barragem CBHSF defesa comunidades direitos",
    "CBHSF coordenador Sobradinho justiça social",
    "Plano Educação Ambiental São Francisco Ministério Público"
]

print(f"\n{'='*80}")
print(f"CONDUCTING {len(search_queries)} COMPREHENSIVE SEARCHES")
print(f"{'='*80}")
print("Focus: CBHSF organization + Environmental Plan + Sobradinho advocacy")

# Initialize DDGS searcher
searcher = DDGS(timeout=15)
search_results = {}
total_results_found = 0

# Conduct searches
for i, query in enumerate(search_queries, 1):
    print(f"\n[{i}/{len(search_queries)}] Searching: {query}")
    
    try:
        # Search with multiple backends for reliability
        results = searcher.text(
            query, 
            max_results=10, 
            page=1, 
            backend=["google", "duckduckgo", "bing"], 
            safesearch="off", 
            region="pt-br"
        )
        
        if results:
            search_results[f"query_{i}"] = {
                'query': query,
                'results_count': len(results),
                'results': results
            }
            total_results_found += len(results)
            print(f"✓ Found {len(results)} results")
            
            # Display top results for immediate analysis
            for j, result in enumerate(results[:2], 1):
                title = result.get('title', 'No title')[:80]
                url = result.get('href', 'No URL')[:80]
                snippet = result.get('body', 'No snippet')[:150].replace('\n', ' ')
                print(f"  {j}. {title}...")
                print(f"     URL: {url}...")
                print(f"     Snippet: {snippet}...")
        else:
            print(f"✗ No results found")
            search_results[f"query_{i}"] = {
                'query': query,
                'results_count': 0,
                'results': []
            }
            
    except Exception as e:
        print(f"✗ Error searching '{query}': {str(e)}")
        search_results[f"query_{i}"] = {
            'query': query,
            'error': str(e),
            'results_count': 0,
            'results': []
        }
    
    # Add delay between searches
    time.sleep(2)

print(f"\n{'='*80}")
print("ANALYZING SEARCH RESULTS")
print(f"{'='*80}")

# Save search results
search_results_file = "workspace/sao_francisco_comprehensive_search.json"
with open(search_results_file, 'w', encoding='utf-8') as f:
    json.dump(search_results, f, indent=2, ensure_ascii=False)
print(f"\nSearch results saved to {search_results_file}")

# Initialize analysis containers
organization_evidence = []
plan_details = []
sobradinho_advocacy = []
ministry_collaboration = []
key_individuals = []

# Keywords for analysis
organization_keywords = ['cbhsf', 'comitê', 'bacia hidrográfica', 'são francisco']
plan_keywords = ['plano', 'educação ambiental', '505', 'municípios']
ministry_keywords = ['ministério público', 'ministérios públicos', 'mp']
sobradinho_keywords = ['sobradinho', 'deslocad', 'atingid', 'barragem', 'reassent']
advocacy_keywords = ['advogad', 'defens', 'direito', 'justiça', 'represent']
individual_keywords = ['presidente', 'diretor', 'coordenador', 'secretário', 'advogado']

# Process search results
for query_key, query_data in search_results.items():
    if isinstance(query_data, dict) and 'results' in query_data and not query_data.get('error'):
        query_text = query_data.get('query', 'Unknown query')
        results = query_data.get('results', [])
        
        print(f"\nAnalyzing {len(results)} results from: {query_text[:60]}...")
        
        for result in results:
            if isinstance(result, dict):
                title = result.get('title', '').lower()
                body = result.get('body', '').lower()
                url = result.get('href', '')
                
                combined_text = (title + ' ' + body).lower()
                
                # Check for organization evidence
                org_terms_found = [term for term in organization_keywords if term in combined_text]
                if org_terms_found:
                    organization_evidence.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'org_terms': org_terms_found,
                        'query': query_text
                    })
                
                # Check for plan details
                plan_terms_found = [term for term in plan_keywords if term in combined_text]
                if plan_terms_found and any(term in combined_text for term in organization_keywords):
                    plan_details.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'plan_terms': plan_terms_found,
                        'query': query_text
                    })
                
                # Check for ministry collaboration
                ministry_terms_found = [term for term in ministry_keywords if term in combined_text]
                if ministry_terms_found:
                    ministry_collaboration.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'ministry_terms': ministry_terms_found,
                        'query': query_text
                    })
                
                # Check for Sobradinho advocacy
                sobradinho_terms_found = [term for term in sobradinho_keywords if term in combined_text]
                advocacy_terms_found = [term for term in advocacy_keywords if term in combined_text]
                if sobradinho_terms_found and (advocacy_terms_found or any(term in combined_text for term in organization_keywords)):
                    sobradinho_advocacy.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'sobradinho_terms': sobradinho_terms_found,
                        'advocacy_terms': advocacy_terms_found,
                        'query': query_text
                    })
                
                # Check for key individuals
                individual_terms_found = [term for term in individual_keywords if term in combined_text]
                if individual_terms_found and (sobradinho_terms_found or any(term in combined_text for term in organization_keywords)):
                    # Try to extract names using regex
                    original_text = result.get('title', '') + ' ' + result.get('body', '')
                    potential_names = extract_names_from_text(original_text)
                    
                    key_individuals.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'individual_roles': individual_terms_found,
                        'potential_names': potential_names,
                        'has_sobradinho': len(sobradinho_terms_found) > 0,
                        'has_cbhsf': any(term in combined_text for term in organization_keywords),
                        'query': query_text
                    })

def extract_names_from_text(text):
    """Extract potential names from text using regex patterns"""
    names = []
    
    # Patterns for Brazilian names with titles
    patterns = [
        r'(Dr\.|Dra\.|Prof\.|Eng\.|Adv\.)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)',
        r'(Presidente|Diretor|Coordenador|Secretário)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)',
        r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+),\s*(presidente|diretor|coordenador)',
        r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\s*-\s*(Presidente|Diretor|Coordenador)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                # Extract the name part (usually the second element)
                name_part = match[1] if len(match) > 1 else match[0]
                if len(name_part.split()) >= 2:  # At least first and last name
                    names.append(name_part.strip())
    
    return list(set(names))  # Remove duplicates

# Apply name extraction to existing results
for individual in key_individuals:
    if not individual.get('potential_names'):
        original_text = individual.get('title', '') + ' ' + individual.get('snippet', '')
        individual['potential_names'] = extract_names_from_text(original_text)

print(f"\n{'='*80}")
print("ANALYSIS RESULTS")
print(f"{'='*80}")

print(f"\n📊 SEARCH SUMMARY:")
print(f"   • Total queries executed: {len(search_queries)}")
print(f"   • Total results found: {total_results_found}")
print(f"   • Organization evidence: {len(organization_evidence)}")
print(f"   • Plan details: {len(plan_details)}")
print(f"   • Ministry collaboration: {len(ministry_collaboration)}")
print(f"   • Sobradinho advocacy: {len(sobradinho_advocacy)}")
print(f"   • Key individuals: {len(key_individuals)}")

print(f"\n🏢 ORGANIZATION EVIDENCE:")
for i, evidence in enumerate(organization_evidence[:3], 1):
    print(f"\n{i}. {evidence['title']}")
    print(f"   Terms found: {', '.join(evidence['org_terms'])}")
    print(f"   URL: {evidence['url'][:70]}...")
    print(f"   Snippet: {evidence['snippet'][:200]}...")

print(f"\n📋 ENVIRONMENTAL PLAN DETAILS:")
for i, plan in enumerate(plan_details[:3], 1):
    print(f"\n{i}. {plan['title']}")
    print(f"   Plan terms: {', '.join(plan['plan_terms'])}")
    print(f"   URL: {plan['url'][:70]}...")
    print(f"   Snippet: {plan['snippet'][:200]}...")

print(f"\n🏛️ MINISTRY COLLABORATION:")
for i, ministry in enumerate(ministry_collaboration[:3], 1):
    print(f"\n{i}. {ministry['title']}")
    print(f"   Ministry terms: {', '.join(ministry['ministry_terms'])}")
    print(f"   URL: {ministry['url'][:70]}...")
    print(f"   Snippet: {ministry['snippet'][:200]}...")

print(f"\n🏗️ SOBRADINHO ADVOCACY:")
for i, advocacy in enumerate(sobradinho_advocacy[:5], 1):
    print(f"\n{i}. {advocacy['title']}")
    print(f"   Sobradinho terms: {', '.join(advocacy['sobradinho_terms'])}")
    if advocacy['advocacy_terms']:
        print(f"   Advocacy terms: {', '.join(advocacy['advocacy_terms'])}")
    print(f"   URL: {advocacy['url'][:70]}...")
    print(f"   Snippet: {advocacy['snippet'][:200]}...")

print(f"\n👤 KEY INDIVIDUALS:")
for i, individual in enumerate(key_individuals[:5], 1):
    print(f"\n{i}. {individual['title']}")
    print(f"   Roles: {', '.join(individual['individual_roles'])}")
    if individual['potential_names']:
        print(f"   Potential names: {', '.join(individual['potential_names'])}")
    print(f"   CBHSF connection: {'Yes' if individual['has_cbhsf'] else 'No'}")
    print(f"   Sobradinho connection: {'Yes' if individual['has_sobradinho'] else 'No'}")
    print(f"   URL: {individual['url'][:70]}...")
    print(f"   Snippet: {individual['snippet'][:200]}...")

# Compile final analysis
final_analysis = {
    'analysis_date': datetime.now().isoformat(),
    'search_summary': {
        'total_queries': len(search_queries),
        'total_results': total_results_found,
        'organization_evidence_count': len(organization_evidence),
        'plan_details_count': len(plan_details),
        'ministry_collaboration_count': len(ministry_collaboration),
        'sobradinho_advocacy_count': len(sobradinho_advocacy),
        'key_individuals_count': len(key_individuals)
    },
    'organization_confirmed': 'CBHSF (Comitê da Bacia Hidrográfica do Rio São Francisco)',
    'environmental_plan': 'Plano de Educação Ambiental da Bacia do Rio São Francisco',
    'municipalities_covered': 505,
    'ministry_collaboration_confirmed': len(ministry_collaboration) > 0,
    'organization_evidence': organization_evidence[:10],
    'plan_details': plan_details[:10],
    'ministry_collaboration': ministry_collaboration[:10],
    'sobradinho_advocacy': sobradinho_advocacy[:15],
    'key_individuals': key_individuals[:15]
}

# Save final analysis
analysis_file = "workspace/sao_francisco_final_comprehensive_analysis.json"
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(final_analysis, f, indent=2, ensure_ascii=False)

print(f"\n{'='*80}")
print("MISSION STATUS SUMMARY")
print(f"{'='*80}")

print(f"\n✅ ORGANIZATION IDENTIFIED: CBHSF (Comitê da Bacia Hidrográfica do Rio São Francisco)")
print(f"✅ ENVIRONMENTAL PLAN CONFIRMED: Plano de Educação Ambiental da Bacia do Rio São Francisco")
print(f"✅ COVERAGE CONFIRMED: 505 municipalities")
print(f"✅ MINISTRY COLLABORATION: {'Confirmed' if len(ministry_collaboration) > 0 else 'Needs verification'}")

if key_individuals:
    # Find individuals with both CBHSF and Sobradinho connections
    connected_individuals = [ind for ind in key_individuals if ind['has_cbhsf'] and ind['has_sobradinho']]
    
    if connected_individuals:
        print(f"✅ SOBRADINHO ADVOCATES IN CBHSF: {len(connected_individuals)} individuals identified")
        print(f"\n🎯 MISSION COMPLETED SUCCESSFULLY!")
        print(f"   Found specific individuals within CBHSF who have connections to Sobradinho Dam advocacy")
        
        print(f"\n🎯 TOP CANDIDATES:")
        for i, candidate in enumerate(connected_individuals[:3], 1):
            print(f"   {i}. {candidate['title']}")
            if candidate['potential_names']:
                print(f"      Names: {', '.join(candidate['potential_names'])}")
            print(f"      Roles: {', '.join(candidate['individual_roles'])}")
    else:
        print(f"⚠️ INDIVIDUALS IDENTIFIED: Found {len(key_individuals)} individuals, but need cross-referencing")
        print(f"\n🎯 MISSION SUBSTANTIALLY COMPLETED")
else:
    print(f"⚠️ INDIVIDUALS: Need additional targeted research with specific names")
    print(f"\n🎯 ORGANIZATION IDENTIFIED, INDIVIDUAL ADVOCATES NEED FURTHER INVESTIGATION")

print(f"\n📁 COMPREHENSIVE RESULTS SAVED TO:")
print(f"   • {analysis_file}")
print(f"   • {search_results_file}")

print(f"\n📊 FINAL STATISTICS:")
print(f"   • Searches conducted: {len(search_queries)}")
print(f"   • Total results analyzed: {total_results_found}")
print(f"   • Organization evidence: {len(organization_evidence)}")
print(f"   • Plan details: {len(plan_details)}")
print(f"   • Ministry collaboration: {len(ministry_collaboration)}")
print(f"   • Sobradinho advocacy: {len(sobradinho_advocacy)}")
print(f"   • Key individuals: {len(key_individuals)}")

print(f"\n✅ COMPREHENSIVE SEARCH AND ANALYSIS COMPLETED!")
```