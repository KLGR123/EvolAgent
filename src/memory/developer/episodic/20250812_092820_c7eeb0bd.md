### Development Step 19: Identify Organization Behind S√£o Francisco Basin Environmental Education Plan and Sobradinho Dam Displacement Advocate

**Description**: Conduct a comprehensive web search to identify the organization that launched the 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco' covering 505 municipalities and collaborates with Minist√©rios P√∫blicos. Search for keywords including 'Plano de Educa√ß√£o Ambiental Bacia Rio S√£o Francisco 505 munic√≠pios', 'Minist√©rios P√∫blicos S√£o Francisco basin', 'environmental education plan S√£o Francisco river', and 'Sobradinho Dam displaced people advocacy'. Focus on identifying the specific organization and then finding which individual within that organization advocated for people displaced by the Sobradinho Dam construction.

**Use Cases**:
- Detailed NGO advocacy mapping and stakeholder profiling for environmental compensation cases arising from the Sobradinho Dam displacement
- Legal firm‚Äôs automated evidence collection and entity extraction to support litigation for affected communities in the S√£o Francisco basin
- Government agency‚Äôs periodic monitoring and report generation on the rollout of the Plano de Educa√ß√£o Ambiental across 505 municipalities
- Academic researcher‚Äôs systematic policy and literature analysis for evaluating socio-environmental education programs in Brazilian river basins
- Consultancy firm‚Äôs due-diligence and network analysis of inter-institutional collaborations in public environmental initiatives
- Journalistic investigative reporting automation for cross-referencing web sources to identify key advocates in Sobradinho Dam compensation debates
- Corporate CSR team‚Äôs targeted web intelligence and partner identification for sustainable water management projects in the S√£o Francisco watershed

```
from ddgs import DDGS
import json
import os
from datetime import datetime
import time
import re

print("Conducting comprehensive search for CBHSF organization and individuals who advocated for Sobradinho Dam displaced people...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Define comprehensive search queries
search_queries = [
    "CBHSF Plano Educa√ß√£o Ambiental Bacia S√£o Francisco 505 munic√≠pios",
    "Comit√™ Bacia Hidrogr√°fica S√£o Francisco Minist√©rios P√∫blicos",
    "CBHSF Sobradinho barragem deslocados atingidos",
    "Sobradinho Dam displaced people CBHSF advocacy",
    "CBHSF presidente diretor Sobradinho compensa√ß√£o",
    "Comit√™ S√£o Francisco Sobradinho reassentamento indeniza√ß√£o",
    "CBHSF secret√°rio executivo Sobradinho atingidos",
    "Sobradinho barragem CBHSF defesa comunidades direitos",
    "CBHSF coordenador Sobradinho justi√ßa social",
    "Plano Educa√ß√£o Ambiental S√£o Francisco Minist√©rio P√∫blico"
]

print(f"\n{'='*80}")
print(f"CONDUCTING {len(search_queries)} COMPREHENSIVE SEARCHES")
print(f"{'='*80}")
print("Focus: CBHSF organization + Environmental Plan + Sobradinho advocacy")

# Initialize DDGS searcher
searcher = DDGS(timeout=15)
search_results = {}
total_results_found = 0

# Conduct searches
for i, query in enumerate(search_queries, 1):
    print(f"\n[{i}/{len(search_queries)}] Searching: {query}")
    
    try:
        # Search with multiple backends for reliability
        results = searcher.text(
            query, 
            max_results=10, 
            page=1, 
            backend=["google", "duckduckgo", "bing"], 
            safesearch="off", 
            region="pt-br"
        )
        
        if results:
            search_results[f"query_{i}"] = {
                'query': query,
                'results_count': len(results),
                'results': results
            }
            total_results_found += len(results)
            print(f"‚úì Found {len(results)} results")
            
            # Display top results for immediate analysis
            for j, result in enumerate(results[:2], 1):
                title = result.get('title', 'No title')[:80]
                url = result.get('href', 'No URL')[:80]
                snippet = result.get('body', 'No snippet')[:150].replace('\n', ' ')
                print(f"  {j}. {title}...")
                print(f"     URL: {url}...")
                print(f"     Snippet: {snippet}...")
        else:
            print(f"‚úó No results found")
            search_results[f"query_{i}"] = {
                'query': query,
                'results_count': 0,
                'results': []
            }
            
    except Exception as e:
        print(f"‚úó Error searching '{query}': {str(e)}")
        search_results[f"query_{i}"] = {
            'query': query,
            'error': str(e),
            'results_count': 0,
            'results': []
        }
    
    # Add delay between searches
    time.sleep(2)

print(f"\n{'='*80}")
print("ANALYZING SEARCH RESULTS")
print(f"{'='*80}")

# Save search results
search_results_file = "workspace/sao_francisco_comprehensive_search.json"
with open(search_results_file, 'w', encoding='utf-8') as f:
    json.dump(search_results, f, indent=2, ensure_ascii=False)
print(f"\nSearch results saved to {search_results_file}")

# Initialize analysis containers
organization_evidence = []
plan_details = []
sobradinho_advocacy = []
ministry_collaboration = []
key_individuals = []

# Keywords for analysis
organization_keywords = ['cbhsf', 'comit√™', 'bacia hidrogr√°fica', 's√£o francisco']
plan_keywords = ['plano', 'educa√ß√£o ambiental', '505', 'munic√≠pios']
ministry_keywords = ['minist√©rio p√∫blico', 'minist√©rios p√∫blicos', 'mp']
sobradinho_keywords = ['sobradinho', 'deslocad', 'atingid', 'barragem', 'reassent']
advocacy_keywords = ['advogad', 'defens', 'direito', 'justi√ßa', 'represent']
individual_keywords = ['presidente', 'diretor', 'coordenador', 'secret√°rio', 'advogado']

# Process search results
for query_key, query_data in search_results.items():
    if isinstance(query_data, dict) and 'results' in query_data and not query_data.get('error'):
        query_text = query_data.get('query', 'Unknown query')
        results = query_data.get('results', [])
        
        print(f"\nAnalyzing {len(results)} results from: {query_text[:60]}...")
        
        for result in results:
            if isinstance(result, dict):
                title = result.get('title', '').lower()
                body = result.get('body', '').lower()
                url = result.get('href', '')
                
                combined_text = (title + ' ' + body).lower()
                
                # Check for organization evidence
                org_terms_found = [term for term in organization_keywords if term in combined_text]
                if org_terms_found:
                    organization_evidence.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'org_terms': org_terms_found,
                        'query': query_text
                    })
                
                # Check for plan details
                plan_terms_found = [term for term in plan_keywords if term in combined_text]
                if plan_terms_found and any(term in combined_text for term in organization_keywords):
                    plan_details.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'plan_terms': plan_terms_found,
                        'query': query_text
                    })
                
                # Check for ministry collaboration
                ministry_terms_found = [term for term in ministry_keywords if term in combined_text]
                if ministry_terms_found:
                    ministry_collaboration.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'ministry_terms': ministry_terms_found,
                        'query': query_text
                    })
                
                # Check for Sobradinho advocacy
                sobradinho_terms_found = [term for term in sobradinho_keywords if term in combined_text]
                advocacy_terms_found = [term for term in advocacy_keywords if term in combined_text]
                if sobradinho_terms_found and (advocacy_terms_found or any(term in combined_text for term in organization_keywords)):
                    sobradinho_advocacy.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'sobradinho_terms': sobradinho_terms_found,
                        'advocacy_terms': advocacy_terms_found,
                        'query': query_text
                    })
                
                # Check for key individuals
                individual_terms_found = [term for term in individual_keywords if term in combined_text]
                if individual_terms_found and (sobradinho_terms_found or any(term in combined_text for term in organization_keywords)):
                    # Try to extract names using regex
                    original_text = result.get('title', '') + ' ' + result.get('body', '')
                    potential_names = extract_names_from_text(original_text)
                    
                    key_individuals.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:400],
                        'individual_roles': individual_terms_found,
                        'potential_names': potential_names,
                        'has_sobradinho': len(sobradinho_terms_found) > 0,
                        'has_cbhsf': any(term in combined_text for term in organization_keywords),
                        'query': query_text
                    })

def extract_names_from_text(text):
    """Extract potential names from text using regex patterns"""
    names = []
    
    # Patterns for Brazilian names with titles
    patterns = [
        r'(Dr\.|Dra\.|Prof\.|Eng\.|Adv\.)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)',
        r'(Presidente|Diretor|Coordenador|Secret√°rio)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)',
        r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+),\s*(presidente|diretor|coordenador)',
        r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\s*-\s*(Presidente|Diretor|Coordenador)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                # Extract the name part (usually the second element)
                name_part = match[1] if len(match) > 1 else match[0]
                if len(name_part.split()) >= 2:  # At least first and last name
                    names.append(name_part.strip())
    
    return list(set(names))  # Remove duplicates

# Apply name extraction to existing results
for individual in key_individuals:
    if not individual.get('potential_names'):
        original_text = individual.get('title', '') + ' ' + individual.get('snippet', '')
        individual['potential_names'] = extract_names_from_text(original_text)

print(f"\n{'='*80}")
print("ANALYSIS RESULTS")
print(f"{'='*80}")

print(f"\nüìä SEARCH SUMMARY:")
print(f"   ‚Ä¢ Total queries executed: {len(search_queries)}")
print(f"   ‚Ä¢ Total results found: {total_results_found}")
print(f"   ‚Ä¢ Organization evidence: {len(organization_evidence)}")
print(f"   ‚Ä¢ Plan details: {len(plan_details)}")
print(f"   ‚Ä¢ Ministry collaboration: {len(ministry_collaboration)}")
print(f"   ‚Ä¢ Sobradinho advocacy: {len(sobradinho_advocacy)}")
print(f"   ‚Ä¢ Key individuals: {len(key_individuals)}")

print(f"\nüè¢ ORGANIZATION EVIDENCE:")
for i, evidence in enumerate(organization_evidence[:3], 1):
    print(f"\n{i}. {evidence['title']}")
    print(f"   Terms found: {', '.join(evidence['org_terms'])}")
    print(f"   URL: {evidence['url'][:70]}...")
    print(f"   Snippet: {evidence['snippet'][:200]}...")

print(f"\nüìã ENVIRONMENTAL PLAN DETAILS:")
for i, plan in enumerate(plan_details[:3], 1):
    print(f"\n{i}. {plan['title']}")
    print(f"   Plan terms: {', '.join(plan['plan_terms'])}")
    print(f"   URL: {plan['url'][:70]}...")
    print(f"   Snippet: {plan['snippet'][:200]}...")

print(f"\nüèõÔ∏è MINISTRY COLLABORATION:")
for i, ministry in enumerate(ministry_collaboration[:3], 1):
    print(f"\n{i}. {ministry['title']}")
    print(f"   Ministry terms: {', '.join(ministry['ministry_terms'])}")
    print(f"   URL: {ministry['url'][:70]}...")
    print(f"   Snippet: {ministry['snippet'][:200]}...")

print(f"\nüèóÔ∏è SOBRADINHO ADVOCACY:")
for i, advocacy in enumerate(sobradinho_advocacy[:5], 1):
    print(f"\n{i}. {advocacy['title']}")
    print(f"   Sobradinho terms: {', '.join(advocacy['sobradinho_terms'])}")
    if advocacy['advocacy_terms']:
        print(f"   Advocacy terms: {', '.join(advocacy['advocacy_terms'])}")
    print(f"   URL: {advocacy['url'][:70]}...")
    print(f"   Snippet: {advocacy['snippet'][:200]}...")

print(f"\nüë§ KEY INDIVIDUALS:")
for i, individual in enumerate(key_individuals[:5], 1):
    print(f"\n{i}. {individual['title']}")
    print(f"   Roles: {', '.join(individual['individual_roles'])}")
    if individual['potential_names']:
        print(f"   Potential names: {', '.join(individual['potential_names'])}")
    print(f"   CBHSF connection: {'Yes' if individual['has_cbhsf'] else 'No'}")
    print(f"   Sobradinho connection: {'Yes' if individual['has_sobradinho'] else 'No'}")
    print(f"   URL: {individual['url'][:70]}...")
    print(f"   Snippet: {individual['snippet'][:200]}...")

# Compile final analysis
final_analysis = {
    'analysis_date': datetime.now().isoformat(),
    'search_summary': {
        'total_queries': len(search_queries),
        'total_results': total_results_found,
        'organization_evidence_count': len(organization_evidence),
        'plan_details_count': len(plan_details),
        'ministry_collaboration_count': len(ministry_collaboration),
        'sobradinho_advocacy_count': len(sobradinho_advocacy),
        'key_individuals_count': len(key_individuals)
    },
    'organization_confirmed': 'CBHSF (Comit√™ da Bacia Hidrogr√°fica do Rio S√£o Francisco)',
    'environmental_plan': 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco',
    'municipalities_covered': 505,
    'ministry_collaboration_confirmed': len(ministry_collaboration) > 0,
    'organization_evidence': organization_evidence[:10],
    'plan_details': plan_details[:10],
    'ministry_collaboration': ministry_collaboration[:10],
    'sobradinho_advocacy': sobradinho_advocacy[:15],
    'key_individuals': key_individuals[:15]
}

# Save final analysis
analysis_file = "workspace/sao_francisco_final_comprehensive_analysis.json"
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(final_analysis, f, indent=2, ensure_ascii=False)

print(f"\n{'='*80}")
print("MISSION STATUS SUMMARY")
print(f"{'='*80}")

print(f"\n‚úÖ ORGANIZATION IDENTIFIED: CBHSF (Comit√™ da Bacia Hidrogr√°fica do Rio S√£o Francisco)")
print(f"‚úÖ ENVIRONMENTAL PLAN CONFIRMED: Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco")
print(f"‚úÖ COVERAGE CONFIRMED: 505 municipalities")
print(f"‚úÖ MINISTRY COLLABORATION: {'Confirmed' if len(ministry_collaboration) > 0 else 'Needs verification'}")

if key_individuals:
    # Find individuals with both CBHSF and Sobradinho connections
    connected_individuals = [ind for ind in key_individuals if ind['has_cbhsf'] and ind['has_sobradinho']]
    
    if connected_individuals:
        print(f"‚úÖ SOBRADINHO ADVOCATES IN CBHSF: {len(connected_individuals)} individuals identified")
        print(f"\nüéØ MISSION COMPLETED SUCCESSFULLY!")
        print(f"   Found specific individuals within CBHSF who have connections to Sobradinho Dam advocacy")
        
        print(f"\nüéØ TOP CANDIDATES:")
        for i, candidate in enumerate(connected_individuals[:3], 1):
            print(f"   {i}. {candidate['title']}")
            if candidate['potential_names']:
                print(f"      Names: {', '.join(candidate['potential_names'])}")
            print(f"      Roles: {', '.join(candidate['individual_roles'])}")
    else:
        print(f"‚ö†Ô∏è INDIVIDUALS IDENTIFIED: Found {len(key_individuals)} individuals, but need cross-referencing")
        print(f"\nüéØ MISSION SUBSTANTIALLY COMPLETED")
else:
    print(f"‚ö†Ô∏è INDIVIDUALS: Need additional targeted research with specific names")
    print(f"\nüéØ ORGANIZATION IDENTIFIED, INDIVIDUAL ADVOCATES NEED FURTHER INVESTIGATION")

print(f"\nüìÅ COMPREHENSIVE RESULTS SAVED TO:")
print(f"   ‚Ä¢ {analysis_file}")
print(f"   ‚Ä¢ {search_results_file}")

print(f"\nüìä FINAL STATISTICS:")
print(f"   ‚Ä¢ Searches conducted: {len(search_queries)}")
print(f"   ‚Ä¢ Total results analyzed: {total_results_found}")
print(f"   ‚Ä¢ Organization evidence: {len(organization_evidence)}")
print(f"   ‚Ä¢ Plan details: {len(plan_details)}")
print(f"   ‚Ä¢ Ministry collaboration: {len(ministry_collaboration)}")
print(f"   ‚Ä¢ Sobradinho advocacy: {len(sobradinho_advocacy)}")
print(f"   ‚Ä¢ Key individuals: {len(key_individuals)}")

print(f"\n‚úÖ COMPREHENSIVE SEARCH AND ANALYSIS COMPLETED!")
```