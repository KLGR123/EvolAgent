### Development Step 18: Search and Download Leicester Paper on Hiccup’s Fish Supply for Dragon’s Diet

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic literature retrieval for systematic reviews by extracting direct PDF links from saved DuckDuckGo and Bing result HTML to university-hosted papers
- Competitive intelligence gathering by scanning search-result pages to collect product brochure URLs on rival corporate domains for market benchmarking
- Legal e-discovery automation by parsing archived search results to locate and download court opinions and government filings from official judiciary websites
- SEO backlink auditing by inspecting search engine HTML dumps to identify all indexed pages on a client’s domain and generate a comprehensive sitemap report
- Cyber threat intelligence collection by detecting redirect patterns in search-result HTML that point to phishing or malware-hosting domains for security analysis
- Data journalism research by extracting press release and report URLs from non-profit and governmental sites found in search-result snapshots for investigative stories
- Corporate compliance monitoring by automating the discovery of internal policy documents on an organization’s intranet domain through parsed search-result HTML

```
import os
import sys
import re
from urllib.parse import urlparse, parse_qs, unquote

# Step 1: Define workspace and verify its existence
workspace = 'workspace'
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    print("Please ensure that search result HTML files are saved in the workspace directory.")
    sys.exit(1)

# Step 2: Define the expected search-result HTML file paths
ddg_html = os.path.join(workspace, 'site_le_duckduckgo_search.html')
bing_html = os.path.join(workspace, 'site_le_bing_search.html')
for html_file in (ddg_html, bing_html):
    if not os.path.isfile(html_file):
        print(f"ERROR: Required file not found: {html_file}")
        print("Make sure you've saved the DuckDuckGo and Bing search-result HTML into workspace/ before running this tool.")
        sys.exit(1)

print(f"[INFO] Inspecting for 'le.ac.uk' links in:\n - {ddg_html}\n - {bing_html}\n")

# Container for all detected University of Leicester URLs
leicester_urls = []

# Function to inspect one HTML file for le.ac.uk URLs using regex parsing
def inspect_html(path, label):
    print(f"[INSPECT] {label}")
    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
        raw = f.read()

    # 1) Regex to find all href attributes
    href_pattern = re.compile(r'<a\s+[^>]*href=["\']([^"\']+)["\']', re.IGNORECASE)
    for match in href_pattern.finditer(raw):
        href = match.group(1)
        # Handle DuckDuckGo redirect (/l/?uddg=...)
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                decoded = unquote(q[0])
                host = urlparse(decoded).netloc.lower()
                if 'le.ac.uk' in host:
                    print(f"  [REDIRECT] decoded uddg -> {decoded}")
                    leicester_urls.append(decoded)
        # Direct link with le.ac.uk in netloc
        try:
            netloc = urlparse(href).netloc.lower()
        except Exception:
            netloc = ''
        if 'le.ac.uk' in netloc:
            print(f"  [DIRECT] {href}")
            leicester_urls.append(href)

    # 2) Raw text scan for le.ac.uk occurrences outside <a> tags
    lines = raw.splitlines()
    for idx, line in enumerate(lines):
        if 'le.ac.uk' in line and '<a ' not in line.lower():
            start = max(0, idx - 2)
            end = min(len(lines), idx + 3)
            print(f"  [TEXT] Context around line {idx+1}:")
            for i in range(start, end):
                print(f"    {i+1:4d}: {lines[i].strip()}")
            print("")

# Inspect both search-result pages
inspect_html(ddg_html, 'DuckDuckGo results')
print('-' * 60)
inspect_html(bing_html, 'Bing results')

# Deduplicate found URLs
unique_urls = []
for url in leicester_urls:
    if url not in unique_urls:
        unique_urls.append(url)

# Step 4: Write the inspection report
report_path = os.path.join(workspace, 'leicester_link_inspection.txt')
with open(report_path, 'w', encoding='utf-8') as out_f:
    out_f.write('Detected University of Leicester (le.ac.uk) landing URLs and contexts:\n\n')
    for url in unique_urls:
        out_f.write(url + '\n')

print(f"[DONE] Found {len(unique_urls)} unique Leicester-related URLs.")
print(f"[INFO] Detailed list written to: {report_path}")
print("Next: Open and review 'leicester_link_inspection.txt' to identify the correct landing-page URL(s) on le.ac.uk for the target paper.")
```