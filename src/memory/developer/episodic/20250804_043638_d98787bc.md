### Development Step 2: Extract Survivor US Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Development of an entertainment mobile app for Survivor fans to browse and filter previous champions by season, with profile pages linking to winner bios and photos
- Building a trivia quiz platform that dynamically generates questions about Survivor winners (e.g., “Who won season 23?”) using the JSON list for question and answer sets
- Academic analysis of gender and geographic diversity trends among Survivor winners from seasons 1–44, feeding the data into statistical models or visualizations
- Data journalism project investigating patterns in Survivor winners’ ages and strategic playstyles over 44 seasons, creating infographics for an online news article
- Automated social media scheduler that posts anniversary highlights of past Survivor champions to Facebook and Twitter, pulling names and seasons from the JSON list
- Fantasy sports–style Survivor prediction engine training on historical winners data to forecast likely champions of upcoming seasons using machine learning
- Podcast production tool for a Survivor recap show that references champions by season when scripting episode segments, automatically fetching winner details
- TV network programming planner generating broadcast schedules and promotional banners for Survivor marathons, listing all champions in chronological order

```
import os
import json
import requests
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
print(f"Fetching page: {url}")
response = requests.get(url)
response.raise_for_status()
print("Page fetched successfully, parsing HTML...")

soup = BeautifulSoup(response.text, 'html.parser')

# Locate the Winners by season section (unique section id), then the next wikitable
winners_span = soup.find('span', id='Winners_by_season')
if not winners_span:
    raise RuntimeError("Could not find the 'Winners by season' section on the page.")

# The winners table should be the next 'wikitable' after this span
winners_table = winners_span.find_next('table', class_='wikitable')
if not winners_table:
    raise RuntimeError("Could not find the wikitable following the 'Winners by season' heading.")

print("Found 'Winners by season' table, parsing rows...")
rows = winners_table.find_all('tr')
print(f"Total rows (including header): {len(rows)}")

# Skip the header row
data_rows = rows[1:]
winners = []

for idx, row in enumerate(data_rows, start=1):
    # Extract all header/data cells in order
    cells = row.find_all(['th', 'td'], recursive=False)
    if len(cells) < 2:
        print(f"Skipping row {idx}: not enough cells ({len(cells)})")
        continue

    # Season number is in the first cell
    season_cell = cells[0]
    # Remove any sup/footnote tags
    for sup in season_cell.find_all('sup'):
        sup.decompose()
    season_text = season_cell.get_text(strip=True)
    try:
        season_num = int(season_text)
    except ValueError:
        print(f"Skipping row {idx}: cannot parse season '{season_text}'")
        continue
    if season_num < 1 or season_num > 44:
        print(f"Skipping season {season_num}: out of target range 1–44.")
        continue

    # Winner name is in the second cell
    winner_cell = cells[1]
    for sup in winner_cell.find_all('sup'):
        sup.decompose()
    winner_name = winner_cell.get_text(strip=True)

    print(f"Parsed Season {season_num}: Winner -> {winner_name}")
    winners.append({'season': season_num, 'winner': winner_name})

# Sort by season number
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"Total winners extracted for seasons 1-44: {len(winners_sorted)}")

# Save to JSON
output_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"Winners list saved to: {output_path}")

```