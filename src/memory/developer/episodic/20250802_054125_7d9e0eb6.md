### Development Step 28: Find and download ‘Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?’ PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Legal research team automating the download of PDF court opinions from online legal database search result pages for case law analysis
- Academic consortium rapidly collecting PDF neuroscience preprints from Google Scholar HTML search outputs for a large‐scale meta-analysis
- Regulatory compliance department scraping PDF versions of newly published environmental regulations from government portal search listings for audit reporting
- Corporate knowledge management system harvesting PDF employee guidelines and policy documents from intranet search results to centralize organizational resources
- Patent research analysts extracting PDF patent specification files from patent office web search result pages for detailed freedom-to-operate studies
- Financial analysts auto-downloading PDF investor presentations and annual reports from company IR search pages to feed quantitative modeling pipelines
- Marketing intelligence team aggregating PDF press releases and media kits from competitor websites by parsing search result HTML for trend monitoring
- Nonprofit environmental group collecting PDF climate policy reports from UN and WHO search-result listings for impact assessment and advocacy research

```
import os
import sys
import re
from bs4 import BeautifulSoup

# --- Configuration ---
workspace = 'workspace'
html_file = os.path.join(workspace, 'bing_search.html')
out_file = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure prerequisites ---
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' not found. Please run the search script first.")
    sys.exit(1)
if not os.path.isfile(html_file):
    print(f"ERROR: HTML file not found at {html_file}. Please generate it with the debug script.")
    sys.exit(1)
print(f"[INFO] Reading HTML from: {html_file}\n")

# --- Load and parse HTML ---
with open(html_file, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

# --- Extract PDF URLs from <a> tags ---
pdf_links = set()
for a in soup.find_all('a', href=True):
    href = a['href']
    if '.pdf' in href.lower():
        pdf_links.add(href)

# --- Extract PDF URLs via regex as fallback ---
regex_pattern = r"(https?://[^\s'\"]+?\.pdf(?:\?[^\s'\"]*)?)"
for match in re.findall(regex_pattern, html, flags=re.IGNORECASE):
    pdf_links.add(match)

# --- Report and save ---
print(f"[PARSE] Found {len(pdf_links)} unique PDF link candidates:\n")
for idx, link in enumerate(sorted(pdf_links), 1):
    print(f" {idx}. {link}")

# Save to file for manual inspection
with open(out_file, 'w', encoding='utf-8') as f:
    for link in sorted(pdf_links):
        f.write(link + "\n")
print(f"\n[SAVE] PDF URL candidates written to: {out_file}")
```