### Development Step 23: Christgau’s letter grades for pre-1999 Fiona Apple and Paula Cole albums

**Description**: Research Robert Christgau's reviews and letter grades for all the pre-1999 albums by Fiona Apple and Paula Cole that were identified in the previous research. Search Christgau's Consumer Guide database, his official website, and archived reviews to determine which albums received letter grades (A+, A, A-, B+, B, B-, C+, C, C-, D+, D, etc.) and which albums were not reviewed or did not receive letter grades. Focus on finding his specific reviews for albums like Fiona Apple's 'Tidal' (1996) and Paula Cole's 'Harbinger' (1994) and 'This Fire' (1996), among others from the compiled list.

**Use Cases**:
- Musicology research into Fiona Apple and Paula Cole’s pre-1999 albums, extracting Christgau’s letter grades to analyze gendered patterns in 1990s album criticism
- Metadata enrichment for a streaming service, automatically scraping Christgau’s grades to tag and recommend vintage singer‐songwriter records
- Digital humanities archiving project, systematically collecting archived Consumer Guide pages to preserve critical reception histories of 1990s albums
- Journalistic comparison report generation, programmatically gathering Christgau’s reviews for side‐by‐side analysis in music blogs and industry articles
- Machine learning dataset creation for sentiment analysis, using scraped letter grades as labeled ground truth to train models on music review tone
- Library catalog enhancement, integrating Christgau’s letter‐grade evaluations into university library records for improved music collection discovery
- Fan‐driven website automation, auto‐updating artist pages with historical Christgau grades whenever new archival content is found
- Academic coursework support, providing students with a curated dataset of 1990s music criticism grades for cultural studies and media analysis projects

```
import os
import json
from bs4 import BeautifulSoup
import requests
import time
import re

print('=== ROBERT CHRISTGAU REVIEWS SEARCH: PHASE 5 ===')
print('Objective: Analyze saved search results and find correct Christgau search method')
print('Strategy: Inspect saved HTML files to understand what was returned, then find correct URLs\n')

# Step 1: Analyze what we actually got from the previous searches
workspace_dir = 'workspace'

print('=== STEP 1: ANALYZING SAVED SEARCH RESULT FILES ===')
print()

# Find all saved search result files
search_files = [f for f in os.listdir(workspace_dir) if f.startswith('christgau_search_')]
print(f'Found {len(search_files)} search result files to analyze')

# Analyze the first search file to understand what we're getting
if search_files:
    sample_file = search_files[0]
    sample_path = os.path.join(workspace_dir, sample_file)
    
    print(f'\nAnalyzing sample file: {sample_file}')
    print(f'File size: {os.path.getsize(sample_path):,} bytes')
    
    with open(sample_path, 'r', encoding='utf-8') as f:
        sample_content = f.read()
    
    print(f'Content length: {len(sample_content):,} characters')
    
    # Show first 1000 characters to understand what we're getting
    print('\nFirst 1000 characters of content:')
    print('-' * 60)
    print(sample_content[:1000])
    print('-' * 60)
    
    # Parse with BeautifulSoup to understand structure
    soup = BeautifulSoup(sample_content, 'html.parser')
    title = soup.find('title')
    title_text = title.get_text().strip() if title else 'No title found'
    
    print(f'\nPage title: "{title_text}"')
    
    # Look for error messages or redirects
    body_text = soup.get_text().lower()
    error_indicators = ['error', '404', 'not found', 'page not found', 'invalid', 'redirect']
    found_errors = [indicator for indicator in error_indicators if indicator in body_text]
    
    if found_errors:
        print(f'Error indicators found: {found_errors}')
        print('*** This suggests our search URLs are incorrect ***')
    
    # Look for forms or navigation that might show correct search methods
    forms = soup.find_all('form')
    links = soup.find_all('a', href=True)
    
    print(f'\nPage structure analysis:')
    print(f'  Forms found: {len(forms)}')
    print(f'  Links found: {len(links)}')
    
    # Show relevant links that might lead to search functionality
    relevant_links = []
    for link in links:
        href = link.get('href', '')
        text = link.get_text().strip()
        
        if any(keyword in text.lower() for keyword in ['search', 'consumer guide', 'artist', 'album', 'database']):
            relevant_links.append({
                'text': text,
                'href': href,
                'full_url': href if href.startswith('http') else f'https://www.robertchristgau.com{href}'
            })
    
    if relevant_links:
        print(f'\nRelevant links found in the page:')
        for i, link in enumerate(relevant_links[:10], 1):
            print(f'  {i}. "{link["text"]}" -> {link["full_url"]}')
    
    print('\n=== STEP 2: ANALYZING MAIN CHRISTGAU PAGE ===')
    print()
    
    # Check if we saved the main page successfully
    main_page_file = 'christgau_main_page.html'
    main_page_path = os.path.join(workspace_dir, main_page_file)
    
    if os.path.exists(main_page_path):
        print(f'✓ Found main page file: {main_page_file}')
        
        with open(main_page_path, 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        main_soup = BeautifulSoup(main_content, 'html.parser')
        print(f'Main page content length: {len(main_content):,} characters')
        
        # Look for actual search functionality on the main page
        main_forms = main_soup.find_all('form')
        print(f'Forms on main page: {len(main_forms)}')
        
        for i, form in enumerate(main_forms, 1):
            print(f'\n  Form {i}:')
            action = form.get('action', 'No action')
            method = form.get('method', 'GET')
            print(f'    Action: {action}')
            print(f'    Method: {method}')
            
            # Show input fields
            inputs = form.find_all('input')
            for input_field in inputs:
                input_type = input_field.get('type', 'text')
                input_name = input_field.get('name', 'no name')
                input_placeholder = input_field.get('placeholder', '')
                print(f'    Input: {input_type} name="{input_name}" placeholder="{input_placeholder}"')
        
        # Look for navigation links to Consumer Guide
        main_links = main_soup.find_all('a', href=True)
        consumer_guide_links = []
        
        for link in main_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            if 'consumer guide' in text.lower() or 'cg' in href.lower() or 'guide' in text.lower():
                consumer_guide_links.append({
                    'text': text,
                    'href': href,
                    'full_url': href if href.startswith('http') else f'https://www.robertchristgau.com{href}'
                })
        
        print(f'\nConsumer Guide related links found: {len(consumer_guide_links)}')
        for i, link in enumerate(consumer_guide_links, 1):
            print(f'  {i}. "{link["text"]}" -> {link["full_url"]}')
        
        print('\n=== STEP 3: TRYING ALTERNATIVE SEARCH APPROACHES ===')
        print()
        
        # Try to find the correct Consumer Guide URLs from the main page
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Test some of the Consumer Guide links we found
        tested_urls = []
        
        for link in consumer_guide_links[:3]:  # Test first 3 CG links
            test_url = link['full_url']
            print(f'Testing Consumer Guide URL: {test_url}')
            
            try:
                response = requests.get(test_url, headers=headers, timeout=15)
                print(f'  Response: {response.status_code}')
                
                if response.status_code == 200:
                    # Save this page for analysis
                    filename = f'christgau_cg_test_{len(tested_urls)+1}.html'
                    filepath = os.path.join(workspace_dir, filename)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    # Quick analysis
                    test_soup = BeautifulSoup(response.content, 'html.parser')
                    test_title = test_soup.find('title')
                    test_title_text = test_title.get_text().strip() if test_title else 'No title'
                    
                    print(f'  Page title: "{test_title_text}"')
                    print(f'  Content length: {len(response.text):,} characters')
                    print(f'  Saved as: {filename}')
                    
                    # Look for artist mentions or search functionality
                    page_text = response.text.lower()
                    if 'fiona apple' in page_text or 'paula cole' in page_text:
                        print(f'  *** FOUND ARTIST MENTIONS - This might be the right place ***')
                    
                    # Look for letter grades
                    grade_pattern = r'\b[A-E][+-]?\b'
                    grades_found = re.findall(grade_pattern, response.text)
                    if grades_found:
                        print(f'  Letter grades found: {grades_found[:10]}')
                    
                    tested_urls.append({
                        'url': test_url,
                        'status': response.status_code,
                        'title': test_title_text,
                        'filename': filename,
                        'has_artists': 'fiona apple' in page_text or 'paula cole' in page_text,
                        'has_grades': len(grades_found) > 0
                    })
                
                time.sleep(2)  # Be respectful
                
            except Exception as e:
                print(f'  Error: {str(e)}')
        
        print('\n=== STEP 4: TRYING DIRECT ARTIST SEARCH APPROACH ===')
        print()
        
        # Try some common Christgau URL patterns for artist searches
        base_url = 'https://www.robertchristgau.com'
        artist_search_patterns = [
            f'{base_url}/get_artist.php?name=fiona+apple',
            f'{base_url}/get_artist.php?artist=fiona+apple', 
            f'{base_url}/xg/cg/cgv7-apple.php',
            f'{base_url}/xg/cg/cgv7-cole.php',
            f'{base_url}/get_chap.php?k=A&bk=70',  # Try alphabetical listing
            f'{base_url}/xg/bk-cg70/grades-90s.php'  # Try decade grades
        ]
        
        for test_url in artist_search_patterns:
            print(f'Trying URL pattern: {test_url}')
            
            try:
                response = requests.get(test_url, headers=headers, timeout=15)
                print(f'  Response: {response.status_code}')
                
                if response.status_code == 200:
                    # Quick check for relevant content
                    content_text = response.text.lower()
                    has_fiona = 'fiona apple' in content_text
                    has_paula = 'paula cole' in content_text
                    has_tidal = 'tidal' in content_text
                    has_harbinger = 'harbinger' in content_text
                    
                    print(f'  Contains Fiona Apple: {has_fiona}')
                    print(f'  Contains Paula Cole: {has_paula}')
                    print(f'  Contains "Tidal": {has_tidal}')
                    print(f'  Contains "Harbinger": {has_harbinger}')
                    
                    if any([has_fiona, has_paula, has_tidal, has_harbinger]):
                        print(f'  *** PROMISING RESULT - Saving for analysis ***')
                        
                        # Save this promising result
                        filename = f'christgau_promising_{test_url.split("/")[-1].replace("?", "_").replace("=", "_")}.html'
                        filepath = os.path.join(workspace_dir, filename)
                        
                        with open(filepath, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        
                        print(f'  Saved as: {filename}')
                        
                        # Look for letter grades in this promising content
                        grade_pattern = r'\b[A-E][+-]?\b'
                        grades_found = re.findall(grade_pattern, response.text)
                        if grades_found:
                            print(f'  Letter grades found: {set(grades_found)}')
                
                time.sleep(2)
                
            except Exception as e:
                print(f'  Error: {str(e)}')
        
        print('\n=== STEP 5: SUMMARY OF FINDINGS ===')
        print()
        
        # Summarize what we've learned
        all_files = [f for f in os.listdir(workspace_dir) if f.endswith('.html')]
        print(f'Total HTML files saved: {len(all_files)}')
        
        promising_files = []
        for filename in all_files:
            if 'promising' in filename or 'cg_test' in filename:
                filepath = os.path.join(workspace_dir, filename)
                file_size = os.path.getsize(filepath)
                promising_files.append({'filename': filename, 'size': file_size})
        
        if promising_files:
            print(f'\nPromising files for detailed analysis:')
            for file_info in promising_files:
                print(f'  - {file_info["filename"]} ({file_info["size"]:,} bytes)')
        
        # Create analysis summary
        analysis_summary = {
            'analysis_phase': 'Christgau search method debugging and URL discovery',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'original_search_files': len(search_files),
            'original_search_file_size': os.path.getsize(sample_path) if search_files else 0,
            'search_url_issues': 'All original searches returned identical 5,016 byte files',
            'consumer_guide_links_found': len(consumer_guide_links),
            'alternative_urls_tested': len(artist_search_patterns),
            'promising_results': len(promising_files),
            'next_steps': [
                'Analyze promising HTML files for actual reviews and grades',
                'Parse letter grades from successful search results',
                'Identify correct search URLs for remaining albums',
                'Compile final grade summary for all pre-1999 albums'
            ]
        }
        
        summary_file = 'christgau_search_debugging_summary.json'
        summary_path = os.path.join(workspace_dir, summary_file)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_summary, f, indent=2)
        
        print(f'\nAnalysis summary saved: {summary_file}')
        
    else:
        print('✗ Main page file not found - cannot analyze site structure')

else:
    print('No search result files found to analyze')

print('\n=== PHASE 5 COMPLETE ===')
print('Debugging analysis complete - identified search URL issues and tested alternatives')
print('Next: Parse promising results to extract actual Christgau reviews and letter grades')
```