### Development Step 2: Identify Nemo’s Clownfish and Locate USGS Pre-2020 Nonnative Fish Occurrence Records with Zip Codes

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting for coastal development through automated retrieval of zip‐coded USGS invasive species records to assess project impact and mitigation needs
- Marine biology graduate thesis mapping nonnative clownfish (Amphiprion ocellaris) occurrences by Florida zip code using USGS NAS data for habitat fragmentation analysis
- Aquarium trade compliance audit cross‐referencing reported escape incidents of orange clownfish by zip code against USGS nonnative species records before 2020
- GIS dashboard automation for state wildlife agencies visualizing zip‐level hotspots of invasive clownfish populations to inform targeted eradication efforts
- Ecotourism itinerary planning leveraging zip‐coded USGS occurrence data of nonnative clownfish to design educational snorkeling tours along U.S. coastlines
- Federal invasive species reporting automation populating management reports with ZIP code–specific USGS occurrence records of clown anemonefish prior to 2020
- Mobile citizen science app integration pulling USGS zip‐coded nonnative clownfish records to validate and geotag user‐submitted observations in local communities
- Environmental liability insurance modeling assessing aquaculture site exposure to nonnative fish incursions by analyzing historical zip‐level USGS occurrence data

```
print("=== DEBUGGING AND FIXING SEARCH RESULT PROCESSING ===\n")

# First, let's inspect the workspace files to understand what data we have
import os
import json

print("Available workspace files:")
workspace_files = [f for f in os.listdir('.') if f.startswith('workspace')]
if workspace_files:
    workspace_dir = workspace_files[0]
    print(f"Using workspace directory: {workspace_dir}")
    
    for file in os.listdir(workspace_dir):
        file_path = os.path.join(workspace_dir, file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")
else:
    print("  No workspace directory found")

# Inspect the search results file structure before processing
search_results_file = os.path.join(workspace_dir, 'usgs_clownfish_search_results.json')
if os.path.exists(search_results_file):
    print(f"\n=== INSPECTING SEARCH RESULTS FILE STRUCTURE ===\n")
    
    with open(search_results_file, 'r') as f:
        search_data = json.load(f)
    
    print("Top-level keys in search data:")
    for key, value in search_data.items():
        if isinstance(value, dict):
            print(f"  {key}: Dictionary with {len(value)} keys")
        elif isinstance(value, list):
            print(f"  {key}: List with {len(value)} items")
        else:
            print(f"  {key}: {type(value).__name__} - {str(value)[:100]}..." if len(str(value)) > 100 else f"  {key}: {value}")
    
    # Check if we have search results data
    if 'search_results_by_query' in search_data:
        print(f"\nFound {len(search_data['search_results_by_query'])} query results")
        
        # Inspect structure of first query result
        if search_data['search_results_by_query']:
            first_query = search_data['search_results_by_query'][0]
            print(f"\nFirst query result keys: {list(first_query.keys())}")
            
            if 'results' in first_query and first_query['results']:
                first_result = first_query['results'][0]
                print(f"Sample result structure: {list(first_result.keys())}")
                print(f"Sample title: {first_result.get('title', 'No title')}")
                print(f"Sample link: {first_result.get('link', 'No link')}")

print(f"\n=== FIXED SEARCH PROCESSING AND USGS DATA EXTRACTION ===\n")

# Now let's properly process the search results and extract USGS data
if os.path.exists(search_results_file):
    with open(search_results_file, 'r') as f:
        search_data = json.load(f)
    
    print("Target Species Information:")
    target_species = search_data.get('target_species', {})
    print(f"Character: {target_species.get('character_name', 'Unknown')}")
    print(f"Common Name: {target_species.get('species_common_name', 'Unknown')}")
    print(f"Scientific Name: {target_species.get('species_scientific_name', 'Unknown')}")
    
    print(f"\n=== ANALYZING SEARCH RESULTS FOR USGS DATABASES ===\n")
    
    all_usgs_sources = []
    all_database_sources = []
    clownfish_specific_sources = []
    
    total_results_processed = 0
    
    # Process each search query's results
    for query_idx, query_result in enumerate(search_data.get('search_results_by_query', []), 1):
        query_text = query_result.get('query', f'Query {query_idx}')
        results = query_result.get('results', [])
        
        print(f"Processing Query {query_idx}: {query_text}")
        print(f"Results found: {len(results)}")
        
        total_results_processed += len(results)
        
        for result_idx, result in enumerate(results, 1):
            # Safely extract result data
            result_title = result.get('title', 'No title')
            result_link = result.get('link', 'No link')
            result_snippet = result.get('snippet', 'No snippet')
            
            print(f"\n  Result {result_idx}:")
            print(f"  Title: {result_title}")
            print(f"  Link: {result_link}")
            print(f"  Snippet: {result_snippet[:150]}..." if len(result_snippet) > 150 else f"  Snippet: {result_snippet}")
            
            # Identify USGS official sources
            if 'usgs.gov' in result_link.lower() or 'usgs' in result_title.lower():
                usgs_source = {
                    'title': result_title,
                    'link': result_link,
                    'snippet': result_snippet,
                    'from_query': query_text
                }
                all_usgs_sources.append(usgs_source)
                print(f"  *** USGS OFFICIAL SOURCE IDENTIFIED ***")
            
            # Identify database sources
            database_keywords = ['database', 'species profile', 'nonindigenous', 'aquatic species']
            if any(keyword in (result_title + result_snippet).lower() for keyword in database_keywords):
                database_source = {
                    'title': result_title,
                    'link': result_link,
                    'snippet': result_snippet,
                    'from_query': query_text
                }
                all_database_sources.append(database_source)
                print(f"  *** DATABASE SOURCE IDENTIFIED ***")
            
            # Identify clownfish-specific sources
            clownfish_keywords = ['clownfish', 'clown anemonefish', 'amphiprion ocellaris', 'amphiprion percula']
            if any(keyword in (result_title + result_snippet).lower() for keyword in clownfish_keywords):
                clownfish_source = {
                    'title': result_title,
                    'link': result_link,
                    'snippet': result_snippet,
                    'from_query': query_text
                }
                clownfish_specific_sources.append(clownfish_source)
                print(f"  *** CLOWNFISH-SPECIFIC SOURCE IDENTIFIED ***")
            
            print(f"  {'-'*60}")
    
    print(f"\n=== PROCESSING SUMMARY ===\n")
    print(f"Total Results Processed: {total_results_processed}")
    print(f"USGS Official Sources Found: {len(all_usgs_sources)}")
    print(f"Database Sources Found: {len(all_database_sources)}")
    print(f"Clownfish-Specific Sources Found: {len(clownfish_specific_sources)}")
    
    # Focus on the most important USGS clownfish database
    print(f"\n=== KEY USGS CLOWNFISH DATABASE IDENTIFIED ===\n")
    
    primary_usgs_source = None
    for source in all_usgs_sources:
        if 'nas.er.usgs.gov' in source['link'] and 'clown' in source['title'].lower():
            primary_usgs_source = source
            break
    
    if primary_usgs_source:
        print("PRIMARY TARGET IDENTIFIED:")
        print(f"Title: {primary_usgs_source['title']}")
        print(f"Link: {primary_usgs_source['link']}")
        print(f"Description: {primary_usgs_source['snippet']}")
        print(f"Source: USGS Nonindigenous Aquatic Species (NAS) Database")
        
        # Extract species ID from the URL for direct access
        import re
        species_id_match = re.search(r'speciesID=(\d+)', primary_usgs_source['link'])
        if species_id_match:
            species_id = species_id_match.group(1)
            print(f"Species ID: {species_id}")
            print(f"Direct Species Profile: {primary_usgs_source['link']}")
    
    # Create corrected analysis data
    corrected_analysis = {
        'species_identification': {
            'character_name': 'Nemo and Marlin',
            'species_common_name': 'Clownfish (Orange Clownfish)',
            'species_scientific_name': 'Amphiprion ocellaris',
            'movie_source': 'Finding Nemo (2003)',
            'description': 'Small orange fish with white stripes and black borders, living in sea anemones'
        },
        'search_results_summary': {
            'total_queries_conducted': len(search_data.get('search_results_by_query', [])),
            'total_results_found': total_results_processed,
            'usgs_sources_identified': len(all_usgs_sources),
            'database_sources_found': len(all_database_sources),
            'clownfish_specific_sources': len(clownfish_specific_sources)
        },
        'primary_usgs_database': {
            'database_name': 'USGS Nonindigenous Aquatic Species (NAS) Database',
            'species_profile_url': primary_usgs_source['link'] if primary_usgs_source else None,
            'species_id': species_id if primary_usgs_source and 'species_id_match' in locals() and species_id_match else None,
            'species_scientific_name': 'Amphiprion ocellaris',
            'species_common_name': 'Clown anemonefish'
        },
        'all_usgs_sources': all_usgs_sources,
        'all_database_sources': all_database_sources,
        'next_steps': [
            'Access USGS NAS Species Profile for Amphiprion ocellaris',
            'Look for occurrence/location data within the species profile',
            'Search for zip code or geographic coordinate information',
            'Filter for records before 2020',
            'Extract specific nonnative population locations in the United States'
        ]
    }
    
    # Save corrected analysis
    with open(os.path.join(workspace_dir, 'corrected_usgs_analysis.json'), 'w') as f:
        json.dump(corrected_analysis, f, indent=2)
    
    print(f"\n=== CORRECTED ANALYSIS COMPLETE ===\n")
    print(f"Corrected analysis saved to: {os.path.join(workspace_dir, 'corrected_usgs_analysis.json')}")
    
    print(f"\n=== NEXT STEP: ACCESS USGS DATABASE ===\n")
    if primary_usgs_source:
        print(f"Target Database: USGS Nonindigenous Aquatic Species Database")
        print(f"Species Profile URL: {primary_usgs_source['link']}")
        print(f"Species: Amphiprion ocellaris (Clown anemonefish)")
        print(f"Objective: Find location data with zip codes for nonnative occurrences before 2020")
        print(f"\nReady to access the USGS database and extract specific location records.")
    else:
        print("Primary USGS source not clearly identified - will need to investigate database sources manually.")

else:
    print(f"Search results file not found: {search_results_file}")
```