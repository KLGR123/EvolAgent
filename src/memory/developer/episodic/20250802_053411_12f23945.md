### Development Step 4: Search and Download PDF of "Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?"

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Legal due diligence teams automating the retrieval of PDF copies of cited case law and statutes for comprehensive compliance reviews
- Pharmaceutical R&D departments fetching the latest clinical trial protocols published by universities into their document management systems
- Market research analysts downloading competitor whitepapers and industry reports in PDF format to feed into trend-analysis dashboards
- Environmental scientists collecting government-issued conservation and impact assessment reports as PDFs for GIS-based modeling
- Supply chain managers automating the download of global shipping regulation PDFs (e.g., IATA, IMO) for real-time compliance checks
- Academic library staff syncing departmental publications by pulling faculty-authored PDF papers from university repositories
- Cybersecurity teams retrieving NIST and ISO standard PDFs to integrate into automated vulnerability assessment pipelines
- E-learning content developers gathering partner-supplied technical training manuals in PDF form to embed into online course modules

```
import os
import sys
import time
import requests

# Purpose: Search and download the University of Leicester paper
# "Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?"
# Saves PDF to workspace/dragon_diet_fish_bag.pdf

# 1. Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2. Prepare search query
query = '"Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?" University of Leicester PDF'
print("Starting search/download for the dragon diet paper PDF...")
print(f"Search query: {query}")

# 3. Check for manual override via environment variable
override_link = os.getenv('DRAGON_PDF_URL')
if override_link:
    pdf_link = override_link
    print(f"üöÄ Using override PDF URL from DRAGON_PDF_URL: {pdf_link}")
else:
    pdf_link = None

# 4. Primary: SerpAPI search (if API key provided)
api_key = os.getenv('SERPAPI_API_KEY')
if not pdf_link and api_key:
    params = {
        'engine': 'google',
        'q': query,
        'api_key': api_key,
        'num': 10,
        'google_domain': 'google.com',
        'safe': 'off'
    }
    max_retries = 3
    for attempt in range(1, max_retries + 1):
        print(f"[SerpAPI] Attempt {attempt}/{max_retries}...")
        try:
            resp = requests.get('https://serpapi.com/search.json', params=params, timeout=30)
            resp.raise_for_status()
            results = resp.json()
            print("‚úÖ SerpAPI response received.")
            # inspect organic_results
            for idx, item in enumerate(results.get('organic_results', []), start=1):
                link = item.get('link','')
                title = item.get('title','')
                snippet = item.get('snippet','')
                print(f"  [#${idx}] {title}\n    URL: {link}\n    Snippet: {snippet[:100]}{'...' if len(snippet)>100 else ''}")
                low = link.lower()
                if low.endswith('.pdf') or '.pdf?' in low:
                    pdf_link = link
                    print(f"üéØ Selected PDF link from SerpAPI: {pdf_link}")
                    break
            if pdf_link:
                break
        except requests.exceptions.HTTPError as e:
            code = e.response.status_code if e.response else None
            if code == 429:
                backoff = 2 ** (attempt - 1)
                print(f"‚ö†Ô∏è 429 Too Many Requests. Backing off for {backoff}s...")
                time.sleep(backoff)
                continue
            else:
                print(f"‚ùå SerpAPI HTTPError {code}: {e}")
                break
        except Exception as e:
            print(f"‚ùå Error querying SerpAPI: {e}")
            break
    else:
        print(f"‚ùå All {max_retries} SerpAPI attempts failed.")

# 5. Fallback #1: DuckDuckGo HTML search
if not pdf_link:
    print("\n---\nNo PDF link found via SerpAPI/override. Trying DuckDuckGo HTML fallback...\n---")
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("ERROR: BeautifulSoup4 is required for DuckDuckGo fallback. Install via 'pip install beautifulsoup4'.")
        sys.exit(1)

    ddg_url = 'https://duckduckgo.com/html/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
        'Accept-Language': 'en-US,en;q=0.5'
    }
    resp = requests.get(ddg_url, params={'q': query}, headers=headers, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')

    candidates = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '.pdf' in href.lower() and not href.startswith('/l/'):
            candidates.append(href)
    # unique
    candidates = list(dict.fromkeys(candidates))
    if candidates:
        print(f"Found {len(candidates)} candidate PDF URLs via DuckDuckGo:")
        for i, link in enumerate(candidates, start=1): print(f"  {i}. {link}")
        pdf_link = candidates[0]
        print(f"üéØ Selected first DuckDuckGo PDF link: {pdf_link}")

# 6. Fallback #2: Google HTML scraping
if not pdf_link:
    print("\n---\nStill no PDF link. Trying Google search HTML scraping...\n---")
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("ERROR: BeautifulSoup4 is required for Google fallback. Install with 'pip install beautifulsoup4'.")
        sys.exit(1)

    google_url = 'https://www.google.com/search'
    gheaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)', 'Accept-Language': 'en-US,en;q=0.5'}
    params = {'q': query + ' filetype:pdf', 'num': '10', 'hl': 'en'}
    resp = requests.get(google_url, params=params, headers=gheaders, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    links = []
    for a in soup.find_all('a', href=True):
        h = a['href']
        if h.startswith('/url?q='):
            url_full = h.split('/url?q=')[1].split('&')[0]
            if '.pdf' in url_full.lower(): links.append(url_full)
    links = list(dict.fromkeys(links))
    if links:
        print(f"Found {len(links)} candidate PDF URLs via Google HTML:")
        for i, lnk in enumerate(links, start=1): print(f"  {i}. {lnk}")
        pdf_link = links[0]
        print(f"üéØ Selected first Google fallback PDF link: {pdf_link}")

# 7. Abort if still no link
if not pdf_link:
    print("‚ùå ERROR: Unable to locate any .pdf URL. Please set the DRAGON_PDF_URL environment variable with a direct link.")
    sys.exit(1)

# 8. Download the PDF
pdf_path = os.path.join(workspace_dir, 'dragon_diet_fish_bag.pdf')
print(f"\nDownloading PDF from: {pdf_link}")
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)', 'Accept': 'application/pdf'}
total_bytes = 0
with requests.get(pdf_link, headers=headers, stream=True, timeout=60) as dl:
    dl.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in dl.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                total_bytes += len(chunk)

print(f"Download complete: {pdf_path}")
print(f"File size: {total_bytes} bytes ({total_bytes/1024/1024:.2f} MB)")
if total_bytes < 10000:
    print("‚ö†Ô∏è WARNING: Download is very small (<10 KB). May not be full PDF.")
else:
    print("‚úÖ PDF appears to have been downloaded successfully.")

print("Script finished.")
```