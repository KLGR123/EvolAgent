### Development Step 5: Identify Author of Third-Volume ‘Francia’s Reign of Terror’ Letters Documenting Four-Year Stay in Paraguay

**Description**: Search for information about a collection of letters titled 'Francia's Reign of Terror' that documents a four-year stay under Dictator Francia's rule in Paraguay. Focus on identifying the author of this work, which is described as the third volume that criticizes local laziness and government policies and was written while relying on local hospitality. Search using keywords including 'Francia's Reign of Terror letters Paraguay dictator', 'four year stay Francia Paraguay author', and 'third volume Francia Paraguay criticism government policies'.

**Use Cases**:
- Digital humanities research for systematic author attribution of 19th-century Latin American letter collections by scraping encyclopedia entries and book APIs
- Library metadata enrichment to automatically extract author names, publication dates, and thematic keywords for rare historical manuscripts in a university catalog
- Small-press publishing project planning to programmatically gather content summaries, critique themes, and author information on Paraguay under Francia’s rule
- Academic coursework support enabling history professors to compile primary source descriptions and author attributions on Paraguay’s dictatorship for syllabus creation
- Heritage digitization workflow that integrates web scraping and API data to generate metadata records for a public archive of translated 19th-century correspondence
- AI training dataset creation for building a labeled corpus of political memoirs and letters from authoritarian regimes, complete with author labels and topical tags
- Investigative journalism fact-checking to automatically cross-reference obscure historical figures, publication details, and critical excerpts on Francia’s Reign of Terror
- Educational mobile app content generation producing concise historical text summaries, author biographies, and thematic keywords for Latin American history learners

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Searching for information about 'Francia's Reign of Terror' letters collection...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Initialize results storage
all_results = []
search_errors = []

print("\n=== DIRECT WIKIPEDIA SEARCH FOR FRANCIA INFORMATION ===")

# Search Wikipedia pages directly
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Jos%C3%A9_Gaspar_Rodr%C3%ADguez_de_Francia",
    "https://en.wikipedia.org/wiki/Paraguay", 
    "https://en.wikipedia.org/wiki/History_of_Paraguay"
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

for url in wikipedia_urls:
    try:
        print(f"\nFetching: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        print(f"Page title: {title}")
        
        # Extract main content
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove scripts and styles
            for elem in content_div.find_all(['script', 'style']):
                elem.decompose()
            
            content = content_div.get_text(separator=' ', strip=True)
            print(f"Content length: {len(content)} characters")
            
            # Create lowercase version for keyword searching
            content_text = content.lower()
            
            # Target keywords from the plan
            target_keywords = [
                'francia', 'dictator', 'paraguay', 'reign', 'terror', 'letters', 
                'correspondence', 'memoir', 'account', 'four year', 'four-year',
                'third volume', 'criticism', 'government policies', 'local hospitality', 
                'laziness', 'stay', 'collection'
            ]
            
            # Find matching keywords
            found_keywords = []
            for keyword in target_keywords:
                if keyword in content_text:
                    found_keywords.append(keyword)
            
            print(f"Keywords found: {', '.join(found_keywords)}")
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content[:2000],  # First 2000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                all_results.append(result)
                print(f"Added relevant result with {len(found_keywords)} keyword matches")
            
        time.sleep(2)  # Be respectful
        
    except Exception as e:
        error_msg = f"Error fetching {url}: {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

print(f"\n=== GOOGLE BOOKS API SEARCH ===")

# Search Google Books API
book_queries = [
    "Francia Paraguay dictator letters",
    "Paraguay Francia reign of terror", 
    "Francia Paraguay memoir correspondence",
    "Francia's Reign of Terror Paraguay",
    "four year stay Francia Paraguay",
    "third volume Francia Paraguay"
]

for query in book_queries:
    try:
        print(f"\nSearching Google Books for: {query}")
        api_url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': query,
            'maxResults': 8,
            'printType': 'books',
            'langRestrict': 'en'
        }
        
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            print(f"Found {len(data['items'])} books")
            
            for item in data['items']:
                volume_info = item.get('volumeInfo', {})
                title = volume_info.get('title', 'Unknown Title')
                authors = volume_info.get('authors', ['Unknown Author'])
                description = volume_info.get('description', '')
                published_date = volume_info.get('publishedDate', 'Unknown Date')
                
                # Create text for keyword analysis
                book_text = (title + ' ' + ' '.join(authors) + ' ' + description).lower()
                
                # Check for relevant keywords
                target_keywords = [
                    'francia', 'paraguay', 'dictator', 'letters', 'reign of terror',
                    'four year', 'four-year', 'third volume', 'criticism', 
                    'government policies', 'local hospitality', 'laziness', 
                    'memoir', 'correspondence', 'account'
                ]
                
                # Find matching keywords
                book_keywords = []
                for keyword in target_keywords:
                    if keyword in book_text:
                        book_keywords.append(keyword)
                
                # Only include if relevant (at least 2 keywords)
                if len(book_keywords) >= 2:
                    result = {
                        'title': title,
                        'authors': authors,
                        'description': description,
                        'published_date': published_date,
                        'keywords_found': book_keywords,
                        'source': 'Google Books',
                        'search_query': query,
                        'relevance_score': len(book_keywords)
                    }
                    all_results.append(result)
                    print(f"Added book: {title}")
                    print(f"Authors: {', '.join(authors)}")
                    print(f"Keywords: {', '.join(book_keywords)}")
        else:
            print(f"No books found for: {query}")
        
        time.sleep(1)  # Be respectful to API
        
    except Exception as e:
        error_msg = f"Error searching Google Books for '{query}': {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

print(f"\n=== ANALYZING RESULTS ===")

# Sort results by relevance score
all_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print(f"Total results collected: {len(all_results)}")
print(f"Total errors: {len(search_errors)}")

# Analyze for specific details from the plan
specific_analysis = {
    'four_year_stay': [],
    'third_volume': [],
    'criticism_laziness': [],
    'government_policies': [],
    'local_hospitality': [],
    'reign_of_terror': [],
    'potential_authors': set()
}

for result in all_results:
    # Get text content for analysis
    if 'content' in result:
        text_content = result['content'].lower()
    elif 'description' in result:
        text_content = result['description'].lower()
    else:
        text_content = result.get('title', '').lower()
    
    # Look for specific details mentioned in the plan
    if 'four year' in text_content or 'four-year' in text_content:
        specific_analysis['four_year_stay'].append({
            'source': result['title'],
            'text_sample': text_content[:400]
        })
    
    if 'third volume' in text_content or 'volume 3' in text_content:
        specific_analysis['third_volume'].append({
            'source': result['title'],
            'text_sample': text_content[:400]
        })
    
    if 'laziness' in text_content or 'lazy' in text_content:
        specific_analysis['criticism_laziness'].append({
            'source': result['title'],
            'text_sample': text_content[:400]
        })
    
    if 'government policies' in text_content or 'policy' in text_content:
        specific_analysis['government_policies'].append({
            'source': result['title'],
            'text_sample': text_content[:400]
        })
    
    if 'hospitality' in text_content:
        specific_analysis['local_hospitality'].append({
            'source': result['title'],
            'text_sample': text_content[:400]
        })
    
    if 'reign of terror' in text_content:
        specific_analysis['reign_of_terror'].append({
            'source': result['title'],
            'text_sample': text_content[:400]
        })
    
    # Collect potential authors
    if 'authors' in result:
        for author in result['authors']:
            specific_analysis['potential_authors'].add(author)

# Convert set to list for JSON serialization
specific_analysis['potential_authors'] = list(specific_analysis['potential_authors'])

# Save comprehensive results
final_data = {
    'search_date': datetime.now().isoformat(),
    'search_summary': {
        'total_results': len(all_results),
        'total_errors': len(search_errors),
        'wikipedia_pages_searched': len(wikipedia_urls),
        'google_books_queries': len(book_queries)
    },
    'search_focus': "Francia's Reign of Terror letters - four year stay under Dictator Francia's rule in Paraguay, third volume criticizing local laziness and government policies",
    'all_results': all_results,
    'specific_analysis': specific_analysis,
    'search_errors': search_errors
}

output_file = 'workspace/francia_reign_terror_search_results.json'
with open(output_file, 'w') as f:
    json.dump(final_data, f, indent=2)

print(f"\nResults saved to: {output_file}")

# Display top results
print("\n=== TOP RELEVANT RESULTS ===")
for i, result in enumerate(all_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result['title']}")
    print(f"Source: {result['source']}")
    if 'authors' in result:
        print(f"Authors: {', '.join(result['authors'])}")
    print(f"Keywords found: {', '.join(result['keywords_found'])}")
    
    # Display content preview
    if 'content' in result:
        print(f"Content preview: {result['content'][:250]}...")
    elif 'description' in result:
        print(f"Description: {result['description'][:250]}...")

# Display specific findings
print("\n=== SPECIFIC FINDINGS FROM THE PLAN ===")
for finding_type, findings in specific_analysis.items():
    if findings and finding_type != 'potential_authors':
        print(f"\n{finding_type.replace('_', ' ').title()}: {len(findings)} mentions")
        for finding in findings[:2]:  # Show first 2
            print(f"  Source: {finding['source']}")
            print(f"  Text: {finding['text_sample'][:150]}...")

if specific_analysis['potential_authors']:
    print(f"\nPotential Authors Identified: {len(specific_analysis['potential_authors'])}")
    for author in specific_analysis['potential_authors'][:8]:  # Show first 8
        print(f"  - {author}")

print(f"\n=== SEARCH SUMMARY ===")
print(f"Wikipedia pages searched: {len(wikipedia_urls)}")
print(f"Google Books queries: {len(book_queries)}")
print(f"Total relevant results: {len(all_results)}")
print(f"Search errors: {len(search_errors)}")

if search_errors:
    print("\nErrors encountered:")
    for error in search_errors[:3]:  # Show first 3 errors
        print(f"  - {error}")

print("\nSearch for Francia's Reign of Terror letters completed!")
```