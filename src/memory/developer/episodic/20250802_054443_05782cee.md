### Development Step 40: Download University of Leicester PDF: Hiccup’s Fish Supply for Dragon Diet Paper

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- University library automation for batch downloading new faculty papers from the institutional DSpace repository into the local digital archive
- Graduate researcher gathering full-text PDFs of targeted ecology and animal-nutrition studies for a systematic literature review
- Corporate R&D team automatically retrieving technical standards and protocol documents hosted on their DSpace server for internal compliance checks
- Journal editorial office verifying and archiving accepted manuscripts by fetching final-version PDFs directly from the publisher’s repository
- Digital preservation specialist scanning the university’s repository for broken or missing bitstream links and re-downloading complete PDF assets for archival integrity
- AI/NLP engineer building a domain-specific corpus by programmatically collecting scholarly PDFs from multiple DSpace installations for model training
- Government policy analyst consolidating public-sector research reports in PDF form from academic repositories to support evidence-based decision-making
- Metadata librarian validating and harvesting newly uploaded thesis PDFs from the institutional repository to synchronize metadata records and local storage

```
import os
import sys
import requests
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

# --- Configuration ---
workspace = 'workspace'
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# Base for Leicester Research Archive (DSpace)
base_url = 'https://lra.le.ac.uk'
# Paper title (using ASCII apostrophe)
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
# Build search URL (DSpace simple-search)
encoded = quote(f'"{title}"')
search_url = f'{base_url}/simple-search?query={encoded}'

# Paths for saving intermediate files
search_html = os.path.join(workspace, 'lra_search.html')
candidates_txt = os.path.join(workspace, 'lra_pdf_urls.txt')
pdf_out = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

print(f"[INFO] Searching DSpace for paper: {title}")
print(f"[INFO] GET {search_url}")
resp = requests.get(search_url, timeout=30)
print(f"[INFO] Search page response: {resp.status_code}")
if resp.status_code != 200:
    print("ERROR: Failed to fetch search page.")
    sys.exit(1)

# Save search HTML for inspection
with open(search_html, 'w', encoding='utf-8') as f:
    f.write(resp.text)
print(f"[SAVE] Search HTML → {search_html}")

# Parse HTML and extract PDF links
soup = BeautifulSoup(resp.text, 'html.parser')
pdf_links = set()

# Strategy: find anchors that either end in .pdf or whose text mentions PDF
for a in soup.find_all('a', href=True):
    href = a['href']
    txt = a.get_text(strip=True).lower()
    # detect direct PDF files or DSpace bitstream links
    if href.lower().endswith('.pdf') or ('.pdf' in href.lower() and 'bitstream' in href.lower()):
        pdf_links.add(href)
    # also check link text for 'pdf'
    elif 'pdf' in txt:
        pdf_links.add(href)

# Normalize and filter full URLs
full_links = []
for link in pdf_links:
    if link.startswith('http'):
        full = link
    else:
        full = urljoin(base_url, link)
    full_links.append(full)

# Write candidates to file
with open(candidates_txt, 'w', encoding='utf-8') as f:
    for u in full_links:
        f.write(u + '\n')
print(f"[SAVE] Candidate PDF URLs → {candidates_txt}")

# If found, download the first candidate
if full_links:
    first = full_links[0]
    print(f"[INFO] Downloading paper PDF from: {first}")
    pdf_resp = requests.get(first, timeout=60)
    print(f"[INFO] PDF download status: {pdf_resp.status_code}")
    if pdf_resp.status_code == 200 and len(pdf_resp.content) > 10000:
        with open(pdf_out, 'wb') as f:
            f.write(pdf_resp.content)
        size = os.path.getsize(pdf_out)
        print(f"[SUCCESS] Saved PDF → {pdf_out} ({size} bytes)")
    else:
        print("ERROR: Download failed or file too small. Content-Type/size may be incorrect.")
        print(f"Content length: {len(pdf_resp.content)} bytes")
else:
    print("WARN: No PDF links found on the repository search page.")
    print("Please inspect 'lra_search.html' and adjust search or selectors accordingly.")
```