### Development Step 15: Pre-April 2019 US DHS Secretaries’ Bachelor's Degree Institutions

**Description**: Search for the complete list of United States Secretaries of Homeland Security who served prior to April 2019 (excluding acting secretaries) and identify their educational backgrounds, specifically focusing on where each secretary obtained their bachelor's degree.

**Use Cases**:
- Government compliance auditing and historical verification of DHS leadership educational credentials
- Political science academic research analyzing the correlation between secretaries’ alma maters and policy initiatives
- Data journalism pipeline for creating interactive visualizations of DHS secretaries’ educational backgrounds
- Nonprofit diversity and inclusion reports examining the range of bachelor’s degree institutions among DHS leadership
- University career services highlighting case studies of alumni who became U.S. Secretaries of Homeland Security
- Civic-tech open data integration to provide searchable profiles of past DHS secretaries by bachelor’s institution
- Historical exhibit development for museums showcasing DHS leadership through timelines of educational achievements
- Alumni association targeted outreach campaigns to engage graduates from universities that produced DHS secretaries

```
import requests
import os
import re
import json
from bs4 import BeautifulSoup
import time

# Define workspace directory
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

print("Starting search for US Secretaries of Homeland Security and their education...")

# Function to perform web requests with exponential backoff
def fetch_with_backoff(url, max_retries=5):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(max_retries):
        try:
            print(f"Attempt {attempt + 1} to fetch URL: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            wait_time = 2 ** attempt
            if attempt < max_retries - 1:
                print(f"Error: {e}. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Failed after {max_retries} attempts: {e}")
                return None

# Fetch list of US Secretaries of Homeland Security from Wikipedia
print("Fetching list of Secretaries from Wikipedia...")
wiki_url = "https://en.wikipedia.org/wiki/United_States_Secretary_of_Homeland_Security"
wiki_response = fetch_with_backoff(wiki_url)

if not wiki_response:
    print("Failed to fetch Wikipedia page. Exiting.")
    exit(1)

# Parse the Wikipedia page to extract secretaries and their tenures
wiki_soup = BeautifulSoup(wiki_response.content, 'html.parser')

# Find the table with secretaries
secretaries_table = None

print("Locating the table of Secretaries...")
for table in wiki_soup.find_all('table', class_='wikitable'):
    # Look for a table with specific column headers
    headers = [th.get_text().strip() for th in table.find_all('th')]
    print(f"Found table with headers: {headers}")
    if "No." in headers and "Portrait" in headers and "Name" in headers:
        secretaries_table = table
        print("Found the correct secretaries table!")
        break

if not secretaries_table:
    print("Could not find the secretaries table on the Wikipedia page. Trying alternative approach.")
    # Try finding the table by looking for specific text
    for table in wiki_soup.find_all('table'):
        if 'secretary of homeland security' in table.get_text().lower():
            secretaries_table = table
            print("Found secretaries table using alternative method.")
            break

if not secretaries_table:
    print("Could not find the secretaries table. Exiting.")
    exit(1)

# Extract secretaries' information
secretaries = []

# Debug: Print the number of rows in the table
rows = secretaries_table.find_all('tr')
print(f"Found {len(rows)} rows in the secretaries table")

# Skip the header row
for i, row in enumerate(rows[1:], 1):
    print(f"Processing row {i}...")
    cells = row.find_all(['th', 'td'])
    
    # Debug: Print the number of cells in this row
    print(f"Row {i} has {len(cells)} cells")
    
    if len(cells) < 3:
        print(f"Skipping row {i} - not enough cells")
        continue
        
    # Extract name - typically in the 3rd column (index 2)
    # but let's verify by looking at header cells
    name_cell_index = None
    for idx, header in enumerate(rows[0].find_all(['th', 'td'])):
        if 'name' in header.get_text().lower():
            name_cell_index = idx
            break
    
    if name_cell_index is None:
        name_cell_index = 2  # Default to the typical position
        
    if len(cells) <= name_cell_index:
        print(f"Skipping row {i} - no name cell at index {name_cell_index}")
        continue
        
    name_cell = cells[name_cell_index]
    name_text = name_cell.get_text().strip()
    
    # Print the raw name text for debugging
    print(f"Raw name text: '{name_text}'")
    
    # Skip if it contains "Acting"
    if "acting" in name_text.lower():
        print(f"Skipping row {i} - Acting Secretary")
        continue
        
    # Clean up the name
    name = re.sub(r'\[.*?\]', '', name_text).strip()  # Remove reference tags
    
    # Extract term of office - typically the next column after name
    term_cell_index = name_cell_index + 1
    if len(cells) <= term_cell_index:
        print(f"No term cell found for {name}")
        term_text = "Term information not available"
    else:
        term_cell = cells[term_cell_index]
        term_text = term_cell.get_text().strip()
    
    print(f"Term text: '{term_text}'")
    
    # Extract end date to check if before April 2019
    end_date_match = re.search(r'(\w+ \d+, \d{4})\s*[–—-]\s*(\w+ \d+, \d{4}|Incumbent|present)', term_text, re.IGNORECASE)
    
    # Extract all links from the name cell to find the person's Wikipedia page
    wiki_link = None
    if name_cell:
        links = name_cell.find_all('a')
        for link in links:
            if link.has_attr('href'):
                href = link['href']
                # Make sure we're getting the person's page, not an image or file
                if href.startswith('/wiki/') and not href.startswith('/wiki/File:'):
                    wiki_link = "https://en.wikipedia.org" + href
                    print(f"Found wiki link for {name}: {wiki_link}")
                    break
        
        # If no proper link was found
        if wiki_link is None:
            print(f"No valid Wikipedia link found for {name}")
                
    # Determine if the secretary served before April 2019
    served_before_april_2019 = True  # Default to True and check conditions to exclude
    
    if end_date_match:
        end_date = end_date_match.group(2).lower()
        start_date = end_date_match.group(1)
        
        # If they're still serving, check when they started
        if "incumbent" in end_date or "present" in end_date:
            start_year_match = re.search(r'\d{4}', start_date)
            if start_year_match:
                start_year = int(start_year_match.group(0))
                if start_year > 2019:  # Started after 2019
                    served_before_april_2019 = False
                elif start_year == 2019:  # Started in 2019
                    start_month_match = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)', start_date, re.IGNORECASE)
                    if start_month_match:
                        start_month = start_month_match.group(1).title()
                        months = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
                        if months.index(start_month) > 3:  # Started after April
                            served_before_april_2019 = False
        else:  # Has an end date
            end_year_match = re.search(r'\d{4}', end_date)
            if end_year_match:
                end_year = int(end_year_match.group(0))
                # Include only those who served until at least January 2019
                if end_year < 2019:
                    served_before_april_2019 = True  # Definitely served before April 2019
                elif end_year == 2019:  # Ended in 2019
                    # Check if they ended after April 2019
                    end_month_match = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)', end_date, re.IGNORECASE)
                    if end_month_match:
                        end_month = end_month_match.group(1).title()
                        months = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
                        if months.index(end_month) < 4:  # Ended before May (i.e., before or during April)
                            served_before_april_2019 = True
                        else:
                            served_before_april_2019 = True  # Still served before April even if they ended after April
                    else:
                        served_before_april_2019 = True  # Assume they served before April if we can't determine month
                else:  # Ended after 2019
                    served_before_april_2019 = True  # Definitely served before April 2019
    
    if not served_before_april_2019:
        print(f"Skipping {name} - did not serve before April 2019")
        continue
    
    secretary_info = {
        'name': name,
        'term': term_text,
        'wiki_link': wiki_link
    }
    
    print(f"Adding secretary: {name}")
    secretaries.append(secretary_info)

print(f"Found {len(secretaries)} Secretaries of Homeland Security who served before April 2019 (excluding acting secretaries)")

# Function to extract educational background from a secretary's Wikipedia page
def get_education_background(wiki_link):
    if not wiki_link:
        return "Wikipedia link not available"
    
    print(f"Fetching education details from: {wiki_link}")
    response = fetch_with_backoff(wiki_link)
    if not response:
        return "Education information not available"
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Look for education information in the infobox
    education = []
    infobox = soup.find('table', class_='infobox')
    if infobox:
        for row in infobox.find_all('tr'):
            header = row.find('th')
            if header and ('education' in header.get_text().lower() or 'alma mater' in header.get_text().lower()):
                value = row.find('td')
                if value:
                    education.append(value.get_text().strip())
    
    # If not found in infobox, look in the content
    if not education:
        print("Education not found in infobox, searching in content...")
        content = soup.find('div', class_='mw-parser-output')
        if content:
            paragraphs = content.find_all('p')
            education_keywords = ['graduate', 'graduated', 'degree', 'university', 'college', 'b.a.', 'b.s.', 'bachelor', 'education']
            
            for paragraph in paragraphs:
                text = paragraph.get_text().lower()
                if any(keyword in text for keyword in education_keywords):
                    education.append(paragraph.get_text().strip())
    
    if education:
        return "\n".join(education)
    else:
        return "Education information not found"

# Function to extract bachelor's degree from education text
def extract_bachelors_degree(education_text):
    if not education_text or education_text in ["Education information not available", "Education information not found", "Wikipedia link not available"]:
        return "Unknown"
    
    # List of patterns to try in order of specificity
    patterns = [
        r"(?:bachelor[']?s? (?:of|degree|in)|B\.?A\.?|B\.?S\.?)[^.]*?(?:from|at)\s+([^.,;()]+)",
        r"(?:earned|received|completed|obtained)\s+(?:a|an|his|her)\s+(?:bachelor[']?s?|undergraduate\s+degree|B\.?A\.?|B\.?S\.?)[^.]*?(?:from|at)\s+([^.,;()]+)",
        r"(?:attended|enrolled\s+(?:at|in))\s+([^.,;()]+)\s+(?:where|and)\s+(?:earned|received|graduated|obtained)\s+(?:a|an|his|her)\s+(?:bachelor[']?s?|B\.?A\.?|B\.?S\.?)",
        r"graduated\s+(?:from|in)\s+([^.,;()]+)\s+(?:with|earning)\s+(?:a|an)\s+(?:bachelor[']?s?|B\.?A\.?|B\.?S\.?)",
        r"([^.,;()]+?)\s+(?:University|College|Institute)",
        r"(University|College|Institute)\s+of\s+[^.,;()]+",
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, education_text, re.IGNORECASE)
        if matches:
            # Clean up any remaining references or annotations
            degree = re.sub(r'\[\d+\]', '', matches[0]).strip()
            return degree
    
    # If no match found with the patterns, try to find any university or college mention
    university_pattern = r'(?:[A-Z][a-z]+\s+)+(?:University|College|Institute)|(?:University|College|Institute)\s+of\s+(?:[A-Z][a-z]+\s*)+'  
    university_matches = re.findall(university_pattern, education_text)
    
    if university_matches:
        return university_matches[0].strip()
    
    return "Not specifically identified"

# Enhance secretaries data with education information
print("\nRetrieving educational background for each Secretary...")
for secretary in secretaries:
    if secretary['wiki_link']:
        education_text = get_education_background(secretary['wiki_link'])
        secretary['education'] = education_text
        
        # Extract bachelor's degree
        bachelors_degree = extract_bachelors_degree(education_text)
        secretary['bachelors_degree'] = bachelors_degree
        
        print(f"Secretary: {secretary['name']}")
        print(f"Bachelor's Degree: {bachelors_degree}")
        print("-" * 40)
    else:
        secretary['education'] = "Wikipedia link not available"
        secretary['bachelors_degree'] = "Unknown"
        print(f"Secretary: {secretary['name']} - No Wikipedia link available")

# Save the results
results_file = os.path.join(workspace_dir, 'homeland_security_secretaries.json')
with open(results_file, 'w') as f:
    json.dump(secretaries, f, indent=4)

print(f"\nResults saved to {results_file}")

# Print a summary of the findings
print("\nSummary of US Secretaries of Homeland Security who served prior to April 2019:")
print("-" * 80)
for secretary in secretaries:
    print(f"Name: {secretary['name']}")
    print(f"Term: {secretary['term']}")
    print(f"Bachelor's Degree: {secretary.get('bachelors_degree', 'Unknown')}")
    print("-" * 80)
```