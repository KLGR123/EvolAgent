### Development Step 61: Extract American Idol Season Winners Table from Wikipedia to JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/American_Idol and extract the season-by-season winners table into workspace/american_idol_winners_list.json, capturing each season number and winner name.

**Use Cases**:
- Automated update of a fan trivia mobile app’s question bank by extracting each American Idol season winner into JSON for real-time quiz refresh
- Integration into a music analytics pipeline that correlates show winners with streaming numbers to model post-show career success
- Feeding a chatbot’s knowledge base for a television trivia assistant, enabling instant answers about which contestant won each season
- Populating an entertainment marketing dashboard that tracks winner announcements and triggers tailored email campaigns to reality TV subscribers
- Building a labeled dataset of names and seasons for training an NLP model to recognize proper nouns and dates in televised competition transcripts
- Powering a research study on reality competition outcomes by aggregating winner data alongside Nielsen ratings and social media engagement metrics
- Streamlining content curation for a streaming service by linking each winner’s discography and spotlight episodes to improve recommendation algorithms

```
import os
import requests
import json
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

url = 'https://en.wikipedia.org/wiki/American_Idol'
print(f"Fetching page: {url}")
response = requests.get(url)
print(f"  HTTP status code: {response.status_code}")
response.raise_for_status()

soup = BeautifulSoup(response.text, 'html.parser')

# Find all wikitable tables on the page
tables = soup.find_all('table', class_='wikitable')
print(f"Found {len(tables)} 'wikitable' tables on the page.")

target_table = None
for idx, table in enumerate(tables, start=1):
    # Extract header texts
    first_row = table.find('tr')
    headers = [th.get_text(strip=True) for th in first_row.find_all('th')]
    headers_lower = [h.lower() for h in headers]
    print(f"Table {idx} headers: {headers}")
    # Relaxed selection: pick first table containing both 'season' and 'winner'
    if 'season' in headers_lower and 'winner' in headers_lower:
        print(f"--> Selecting table {idx} (contains both 'Season' and 'Winner').")
        target_table = table
        break

if not target_table:
    raise RuntimeError("Could not find a table containing both 'Season' and 'Winner' columns.")

# Re-extract and report headers of the selected table
header_cells = target_table.find('tr').find_all('th')
headers = [th.get_text(strip=True) for th in header_cells]
print(f"Using headers from selected table: {headers}")

# Compute indices for Season and Winner
season_idx = headers.index('Season')
winner_idx = headers.index('Winner')
print(f"Season column index: {season_idx}, Winner column index: {winner_idx}")

# Parse data rows using only <td> for alignment
winners = []
rows = target_table.find_all('tr')[1:]  # skip header
print(f"Parsing {len(rows)} rows from the selected table.")
for row_num, row in enumerate(rows, start=1):
    cols = row.find_all('td')
    if len(cols) <= max(season_idx, winner_idx):
        print(f"  Skipping row {row_num}: found only {len(cols)} <td> cells.")
        continue
    season = cols[season_idx].get_text(strip=True)
    winner = cols[winner_idx].get_text(strip=True)
    print(f"  Row {row_num}: Season = '{season}', Winner = '{winner}'")
    winners.append({'season': season, 'winner': winner})

print(f"Total seasons extracted: {len(winners)}")

# Save JSON output
output_file = os.path.join(workspace_dir, 'american_idol_winners_list.json')
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(winners, f, ensure_ascii=False, indent=2)
print(f"Winners list successfully saved to: {output_file}")
```