### Development Step 32: Search and Download Leicester Paper “Dragon Diet Fish Supply” PDF into workspace/dragon_diet_fish_bag.pdf

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated academic literature review for a PhD student: crawl university domains (e.g., le.ac.uk) to fetch all relevant PDF papers on “dragon diet” hypotheses for systematic annotation.
- Legal due diligence in a law firm: search government and university research sites to download court filings and regulatory PDFs for building a compliance repository.
- Competitive intelligence for a product manager: scrape competitor’s technical whitepapers (PDF) from their official site to analyze feature roadmaps and R&D focus areas.
- Pharmaceutical R&D document collection: regularly harvest clinical trial reports (PDF) from NIH or university domains to update a drug safety monitoring database.
- Patent analysis for IP consultants: extract patent specification PDFs from WIPO or national patent office domains to feed into automated claim-mapping software.
- Investor relations data aggregation: collect annual and quarterly report PDFs from public company domains to drive financial modeling and trend analysis in spreadsheets.
- E-learning content curation for educators: pull lecture slides and course materials (PDF) from multiple university sites to integrate into a centralized learning management system.

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
bing_html_path = os.path.join(workspace, 'bing_search.html')
out_pdf_candidates = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure workspace exists ---
if not os.path.isdir(workspace):
    print(f"[INFO] Creating workspace directory: {workspace}")
    os.makedirs(workspace, exist_ok=True)

# --- Step 1: Perform Bing search for the target paper ---
query = "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet? filetype:pdf site:le.ac.uk"
bing_url = 'https://www.bing.com/search'
params = {'q': query}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:104.0) Gecko/20100101 Firefox/104.0',
    'Accept-Language': 'en-US,en;q=0.9'
}

print(f"[INFO] Sending Bing search request with query:\n      {query}\n")
response = requests.get(bing_url, params=params, headers=headers, timeout=30)
print(f"[INFO] Bing response status: {response.status_code}")

if response.status_code != 200:
    print(f"ERROR: Bing search failed with status {response.status_code}\nResponse body:\n{response.text[:500]}")
    sys.exit(1)

# Save search HTML
with open(bing_html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"[SAVE] Search page saved to: {bing_html_path}\n")

# --- Step 2: Load HTML and extract PDF candidates ---
print(f"[INFO] Parsing HTML for PDF URLs...")
with open(bing_html_path, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()

# 2a) Direct <a href> that endswith .pdf or contains .pdf?
print("[STEP] Scanning <a> tags for literal .pdf links...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.lower().endswith('.pdf') or '.pdf?' in href.lower():
        pdf_urls.add(href)
print(f"    Found {len(pdf_urls)} literal PDF href(s)")

# 2b) Handle Bing redirect links (/url?q=...)
print("[STEP] Parsing Bing redirect links (/url?q=...pdf)...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q', 'u', 'url'):
            if key in qs:
                real_url = unquote(qs[key][0])
                if real_url.lower().endswith('.pdf') or '.pdf?' in real_url.lower():
                    pdf_urls.add(real_url)
                break
print(f"    Total after redirect parsing: {len(pdf_urls)} candidate(s)")

# 2c) Regex fallback on raw HTML
print("[STEP] Running regex fallback for any PDF URLs in page text...")
regex = r'https?://[^\s"']+?\.pdf(?:\?[^\s"']*)?'
for match in re.findall(regex, html, flags=re.IGNORECASE):
    pdf_urls.add(match)
print(f"    Total after regex fallback: {len(pdf_urls)} candidate(s)")

# 2d) Filter for Leicester domains
print("[STEP] Filtering candidates for Leicester domains (le.ac.uk, lra.le.ac.uk)...")
le_domains = ('le.ac.uk', 'lra.le.ac.uk', 'core.ac.uk', 'hdl.handle.net')
leicester_pdfs = [u for u in pdf_urls if any(d in u.lower() for d in le_domains)]

if leicester_pdfs:
    print(f"    Found {len(leicester_pdfs)} Leicester-specific PDF(s):")
    for url in leicester_pdfs:
        print(f"      - {url}")
else:
    print("    No PDF URLs from Leicester domains found; listing all candidates below.")
    for url in sorted(pdf_urls):
        print(f"      - {url}")

# Save all candidates
with open(out_pdf_candidates, 'w', encoding='utf-8') as f:
    for url in sorted(pdf_urls):
        f.write(url + '\n')
print(f"\n[SAVE] All PDF URL candidates written to: {out_pdf_candidates}")
print("[COMPLETE] Extraction complete. Inspect the candidate URLs to choose the correct PDF link.")
```