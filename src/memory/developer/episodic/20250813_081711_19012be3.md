### Development Step 4: Abel Hugo’s Napoleonic Service, Madrid Tenure, Cartographic Publications and Reign Context

**Description**: Research Abel Hugo, Victor Hugo's brother, focusing on his military service during the Napoleonic Wars and his time in Madrid. Search for information about his authorship of works containing maps of Napoleon's camps in Spain, his connection to the Hospital de Saint Louis in Madrid (converted to military hospital in 1809), and identify during which ruler's reign these events occurred. Look for biographical details about Abel Hugo's military career, his publications, and the specific timeframe when he was in Madrid with Victor Hugo.

**Use Cases**:
- Academic historian automating extraction of Abel Hugo’s military service details and publications from Wikipedia to build a Napoleonic Wars research database
- Digital humanities researchers comparing English and French Wikipedia entries for Victor Hugo and Abel Hugo to identify language-based discrepancies in biographical content
- Museum curator compiling map references and authorship notes on Napoleon’s Spanish camps for a Peninsular War exhibition catalog
- Genealogy specialist scraping and summarizing family relationships and biographical snippets of the Hugo brothers to enrich an online family-tree portal
- Publishing editor verifying dates and contexts of 19th-century military memoirs by automatically parsing Wikipedia for author and publication information
- Data journalist monitoring real-time Wikipedia updates on Madrid’s Hospital de Saint Louis and Peninsular War events to detect emerging historical narratives
- EdTech developer ingesting structured biographical timelines and ruler context from Wikipedia into an interactive educational app on Napoleonic Europe

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Researching Abel Hugo with completely fixed Wikipedia parsing...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Initialize results storage
all_results = []
search_errors = []

print("\n=== COMPLETELY FIXED WIKIPEDIA SEARCH FOR ABEL HUGO ===")

# Wikipedia URLs to search
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Abel_Hugo",
    "https://fr.wikipedia.org/wiki/Abel_Hugo",  # French Wikipedia often has more details
    "https://en.wikipedia.org/wiki/Victor_Hugo",
    "https://en.wikipedia.org/wiki/Napoleonic_Wars",
    "https://en.wikipedia.org/wiki/Peninsular_War",
    "https://en.wikipedia.org/wiki/Madrid"
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

for url in wikipedia_urls:
    try:
        print(f"\nFetching: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        print(f"Page title: {title}")
        
        # Extract main content - COMPLETELY FIXED VERSION
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove scripts and styles first
            for script_elem in content_div.find_all('script'):
                script_elem.decompose()
            for style_elem in content_div.find_all('style'):
                style_elem.decompose()
            
            # Remove navigation and info boxes - PROPERLY SCOPED VARIABLES
            divs_to_remove = []
            for div_elem in content_div.find_all('div'):
                div_classes = div_elem.get('class')
                if div_classes:  # Only process if classes exist
                    # Create class string within proper scope
                    div_class_str = ' '.join(div_classes)
                    if any(unwanted in div_class_str for unwanted in ['navbox', 'infobox', 'sidebar', 'hatnote', 'ambox']):
                        divs_to_remove.append(div_elem)
            
            # Remove identified divs
            for div_to_remove in divs_to_remove:
                div_to_remove.decompose()
            
            # Remove problematic tables
            tables_to_remove = []
            for table_elem in content_div.find_all('table'):
                table_classes = table_elem.get('class')
                if table_classes:
                    # Create table class string within proper scope
                    table_class_str = ' '.join(table_classes)
                    if any(unwanted in table_class_str for unwanted in ['infobox', 'navbox', 'sidebar']):
                        tables_to_remove.append(table_elem)
            
            # Remove identified tables
            for table_to_remove in tables_to_remove:
                table_to_remove.decompose()
            
            content = content_div.get_text(separator=' ', strip=True)
            print(f"Content length: {len(content)} characters")
            
            # Target keywords for Abel Hugo research
            target_keywords = [
                'abel hugo', 'victor hugo', 'napoleonic wars', 'madrid', 'spain', 
                'military service', 'military hospital', 'hospital',
                'napoleon', 'camps', 'maps', 'publications', 'author', 'brother',
                '1809', 'peninsular war', 'french army', 'officer', 'career',
                'joseph bonaparte', 'ferdinand vii', 'saint louis', 'saint-louis'
            ]
            
            # Find matching keywords
            content_lower = content.lower()
            found_keywords = []
            for keyword in target_keywords:
                if keyword in content_lower:
                    found_keywords.append(keyword)
            
            print(f"Keywords found: {', '.join(found_keywords)}")
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content[:5000],  # First 5000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                all_results.append(result)
                print(f"Added relevant result with {len(found_keywords)} keyword matches")
                
                # Special handling for Abel Hugo pages
                if 'abel hugo' in content_lower:
                    print("\n*** ABEL HUGO BIOGRAPHICAL CONTENT FOUND ***")
                    # Extract sentences containing Abel Hugo
                    sentences = content.split('.')
                    abel_sentences = []
                    for sentence in sentences:
                        sentence_lower = sentence.lower()
                        if 'abel hugo' in sentence_lower or ('abel' in sentence_lower and 'hugo' in sentence_lower and len(sentence) < 500):
                            clean_sentence = sentence.strip()
                            if len(clean_sentence) > 10:  # Avoid very short fragments
                                abel_sentences.append(clean_sentence)
                    
                    print(f"Found {len(abel_sentences)} sentences about Abel Hugo:")
                    for i, sentence in enumerate(abel_sentences[:5], 1):
                        print(f"  {i}. {sentence[:300]}...")
                    
                    # Look for specific military and Madrid information
                    military_sentences = []
                    madrid_sentences = []
                    publication_sentences = []
                    
                    for sentence in abel_sentences:
                        sentence_lower = sentence.lower()
                        if any(term in sentence_lower for term in ['military', 'army', 'officer', 'service', 'soldier', 'war', 'campaign']):
                            military_sentences.append(sentence)
                        if 'madrid' in sentence_lower:
                            madrid_sentences.append(sentence)
                        if any(term in sentence_lower for term in ['author', 'wrote', 'published', 'publication', 'work', 'book']):
                            publication_sentences.append(sentence)
                    
                    if military_sentences:
                        print(f"\n  Military information found in {len(military_sentences)} sentences:")
                        for info in military_sentences[:3]:
                            print(f"    - {info[:250]}...")
                    
                    if madrid_sentences:
                        print(f"\n  Madrid information found in {len(madrid_sentences)} sentences:")
                        for info in madrid_sentences[:3]:
                            print(f"    - {info[:250]}...")
                    
                    if publication_sentences:
                        print(f"\n  Publication information found in {len(publication_sentences)} sentences:")
                        for info in publication_sentences[:3]:
                            print(f"    - {info[:250]}...")
            
        time.sleep(2)  # Be respectful to Wikipedia
        
    except Exception as e:
        error_msg = f"Error fetching {url}: {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

# Check if we have any existing results from previous attempts
print(f"\n=== CHECKING FOR PREVIOUS RESEARCH DATA ===")
existing_results_file = 'workspace/abel_hugo_final_research.json'
if os.path.exists(existing_results_file):
    print(f"Found existing research file: {existing_results_file}")
    try:
        with open(existing_results_file, 'r') as f:
            existing_data = json.load(f)
        
        existing_results = existing_data.get('all_results', [])
        print(f"Found {len(existing_results)} existing results")
        
        # Add existing Google Books results to our collection
        google_books_results = [r for r in existing_results if r.get('source') == 'Google Books']
        print(f"Adding {len(google_books_results)} Google Books results from previous search")
        all_results.extend(google_books_results)
        
    except Exception as e:
        print(f"Error loading existing results: {str(e)}")

print(f"\n=== ANALYZING COMBINED RESULTS FOR ABEL HUGO RESEARCH ===")

# Sort results by relevance score
all_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print(f"Total results collected: {len(all_results)}")
print(f"Total errors: {len(search_errors)}")

# Detailed analysis for Abel Hugo specific information
abel_hugo_analysis = {
    'military_service_details': [],
    'madrid_connections': [],
    'napoleon_maps_camps': [],
    'hospital_saint_louis': [],
    'publications_authorship': [],
    'timeframe_1809': [],
    'ruler_context': [],
    'victor_hugo_relationship': [],
    'biographical_details': []
}

for result in all_results:
    # Get text content for analysis
    if 'content' in result:
        text_content = result['content'].lower()
        full_text = result['content']  # Keep original case for extraction
    elif 'description' in result:
        text_content = result['description'].lower()
        full_text = result['description']
    else:
        text_content = result.get('title', '').lower()
        full_text = result.get('title', '')
    
    # Look for specific Abel Hugo details
    if 'abel hugo' in text_content or ('abel' in text_content and 'hugo' in text_content):
        print(f"\nAnalyzing '{result['title']}' for Abel Hugo information...")
        
        # Military service analysis
        if any(term in text_content for term in ['military', 'army', 'officer', 'service', 'soldier', 'war', 'campaign']):
            abel_hugo_analysis['military_service_details'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Military service information found")
        
        # Madrid connections
        if 'madrid' in text_content:
            abel_hugo_analysis['madrid_connections'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Madrid connection found")
        
        # Napoleon maps/camps
        if 'napoleon' in text_content and ('maps' in text_content or 'camps' in text_content):
            abel_hugo_analysis['napoleon_maps_camps'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Napoleon maps/camps reference found")
        
        # Hospital Saint Louis
        if 'hospital' in text_content and ('saint louis' in text_content or 'saint-louis' in text_content):
            abel_hugo_analysis['hospital_saint_louis'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Hospital Saint Louis reference found")
        
        # Publications and authorship
        if any(term in text_content for term in ['author', 'wrote', 'published', 'publication', 'work', 'book']):
            abel_hugo_analysis['publications_authorship'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Publication/authorship information found")
        
        # 1809 timeframe
        if '1809' in text_content:
            abel_hugo_analysis['timeframe_1809'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ 1809 timeframe reference found")
        
        # Ruler context
        if any(ruler in text_content for ruler in ['joseph bonaparte', 'napoleon', 'ferdinand vii', 'charles iv']):
            abel_hugo_analysis['ruler_context'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Ruler context found")
        
        # Victor Hugo relationship
        if 'victor hugo' in text_content and 'brother' in text_content:
            abel_hugo_analysis['victor_hugo_relationship'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': full_text[:1000],
                'keywords': result['keywords_found']
            })
            print("  ✓ Victor Hugo brother relationship found")
        
        # General biographical details
        abel_hugo_analysis['biographical_details'].append({
            'source': result['title'],
            'url': result.get('url', ''),
            'text_sample': full_text[:1200],
            'keywords': result['keywords_found'],
            'relevance_score': result['relevance_score']
        })

# Save comprehensive Abel Hugo research
final_data = {
    'research_date': datetime.now().isoformat(),
    'research_focus': 'Abel Hugo - Victor Hugo\'s brother: military service during Napoleonic Wars, Madrid connections, Napoleon camp maps, Hospital de Saint Louis, publications, ruler context',
    'search_summary': {
        'total_results': len(all_results),
        'wikipedia_pages_searched': len(wikipedia_urls),
        'wikipedia_pages_successful': len([r for r in all_results if r['source'] == 'Wikipedia']),
        'google_books_results': len([r for r in all_results if r['source'] == 'Google Books']),
        'search_errors': len(search_errors)
    },
    'all_results': all_results,
    'abel_hugo_detailed_analysis': abel_hugo_analysis,
    'search_errors': search_errors
}

output_file = 'workspace/abel_hugo_complete_research.json'
with open(output_file, 'w') as f:
    json.dump(final_data, f, indent=2)

print(f"\n{'='*80}")
print("ABEL HUGO RESEARCH SUMMARY")
print(f"{'='*80}")
print(f"Results saved to: {output_file}")
print(f"Total results collected: {len(all_results)}")
print(f"Wikipedia pages processed successfully: {len([r for r in all_results if r['source'] == 'Wikipedia'])}")
print(f"Google Books results: {len([r for r in all_results if r['source'] == 'Google Books'])}")

# Display key findings about Abel Hugo
print(f"\n=== ABEL HUGO KEY FINDINGS ===")
for category, findings in abel_hugo_analysis.items():
    if findings:
        print(f"\n{category.replace('_', ' ').title()}: {len(findings)} sources")
        # Show the most relevant finding
        best_finding = max(findings, key=lambda x: x.get('relevance_score', len(x.get('keywords', []))))
        print(f"  Best source: {best_finding['source']}")
        print(f"  Keywords: {', '.join(best_finding.get('keywords', []))}")
        print(f"  Sample: {best_finding['text_sample'][:400]}...")

# Display top overall results
print(f"\n=== TOP OVERALL RESULTS ===")
for i, result in enumerate(all_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result['title']}")
    print(f"Source: {result['source']}")
    if 'authors' in result:
        print(f"Authors: {', '.join(result['authors'])}")
    print(f"Keywords found: {', '.join(result['keywords_found'])}")
    
    # Display content preview
    if 'content' in result:
        print(f"Content preview: {result['content'][:300]}...")
    elif 'description' in result:
        print(f"Description: {result['description'][:300]}...")

print(f"\n=== SEARCH ERRORS ===")
if search_errors:
    for error in search_errors:
        print(f"  - {error}")
else:
    print("No search errors - all sources processed successfully!")

print(f"\nAbel Hugo research completed successfully!")
```