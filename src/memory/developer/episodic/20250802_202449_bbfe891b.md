### Development Step 14: 2011 Bulgarian Census: Tertiary Education Completion by Gender

**Description**: Search for the 2011 Bulgarian census data on tertiary education completion by gender. Look for official census results from Bulgaria's National Statistical Institute or other authoritative sources that provide the specific breakdown of men and women who completed tertiary education in 2011.

**Use Cases**:
- Ministry of Education data-driven gender gap assessment in Bulgarian regions for targeted scholarship and resource allocation
- NGO monitoring gender parity progress in tertiary education across EU member states to support UN Sustainable Development Goal 4 reporting
- Academic comparative study integrating Bulgarian 2011 census data with Eurostat and UNESCO datasets for peer‐reviewed demographic research
- University admissions office automating extraction of regional male‐female graduation rates to refine recruitment strategies and diversity initiatives
- EdTech company conducting market segmentation analysis on postgraduate course demand by gender in Bulgaria to tailor new online program offerings
- National Statistical Institute auditing and validating published tertiary education statistics via automated HTML parsing and regex pattern matching
- Data journalism team extracting and visualizing gender‐disaggregated higher education trends from Bulgarian census reports for interactive news articles

```
import os
import json
from bs4 import BeautifulSoup
import re
import time

print("=== BULGARIAN 2011 CENSUS TERTIARY EDUCATION BY GENDER - SYSTEMATIC APPROACH ===")
print("Strategy: First inspect existing files, then use completely different variable handling\n")

# Check workspace contents
workspace_path = 'workspace'
if not os.path.exists(workspace_path):
    print("No workspace directory found")
    exit()

all_files = os.listdir(workspace_path)
html_files = [f for f in all_files if f.endswith('.html')]
json_files = [f for f in all_files if f.endswith('.json')]

print(f"Total files: {len(all_files)}")
print(f"HTML files: {len(html_files)}")
print(f"JSON files: {len(json_files)}")

# First, let's inspect any existing JSON analysis files to understand structure
print("\n=== INSPECTING EXISTING JSON FILES ===\n")
for json_file in json_files[:3]:  # Check first 3 JSON files
    filepath = os.path.join(workspace_path, json_file)
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"{json_file}:")
        print(f"  Keys: {list(data.keys())[:8]}")
        if 'tertiary_education_findings' in data:
            print(f"  Tertiary education findings: {len(data['tertiary_education_findings'])}")
        print()
    except Exception as e:
        print(f"Error reading {json_file}: {e}\n")

# Now let's identify the most promising HTML files
print("=== IDENTIFYING PRIORITY HTML FILES ===\n")
priority_files = []

for filename in html_files:
    filepath = os.path.join(workspace_path, filename)
    file_size = os.path.getsize(filepath)
    
    # Skip very small files (likely error pages)
    if file_size < 30000:  # Less than 30KB
        continue
    
    filename_lower = filename.lower()
    score = 0
    
    # Score based on filename relevance
    if 'demographics' in filename_lower:
        score += 5
    if 'education' in filename_lower:
        score += 5
    if 'census' in filename_lower:
        score += 4
    if 'bulgaria' in filename_lower:
        score += 3
    if 'nsi' in filename_lower:
        score += 3
    if 'eurostat' in filename_lower:
        score += 2
    
    if score > 0:
        priority_files.append({
            'filename': filename,
            'score': score,
            'size': file_size
        })

priority_files.sort(key=lambda x: x['score'], reverse=True)

print(f"Priority files identified: {len(priority_files)}")
for i, pf in enumerate(priority_files[:5], 1):
    print(f"{i}. {pf['filename']} (Score: {pf['score']}, Size: {pf['size']:,} bytes)")

print("\n=== ANALYZING FILES WITH COMPLETELY NEW APPROACH ===\n")

# Results storage
analysis_results = []
tertiary_education_findings = []

# Analyze top 3 files using a completely different variable approach
for file_info in priority_files[:3]:
    filename = file_info['filename']
    print(f"Analyzing: {filename}")
    
    filepath = os.path.join(workspace_path, filename)
    
    try:
        # Read HTML file
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # Parse HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Get page title
        title_element = soup.find('title')
        page_title = title_element.get_text().strip() if title_element else 'No title'
        
        # Extract all text content
        full_page_text = soup.get_text()
        
        print(f"  Title: {page_title}")
        print(f"  Content length: {len(full_page_text):,} characters")
        
        # Use direct string operations instead of variables
        # Check for key indicators by calling .lower() directly on the text
        bulgaria_found = 'bulgaria' in full_page_text.lower() or 'bulgarian' in full_page_text.lower()
        year_2011_found = '2011' in full_page_text.lower()
        census_found = 'census' in full_page_text.lower()
        tertiary_found = any(term in full_page_text.lower() for term in ['tertiary', 'tertiary education', 'higher education'])
        gender_found = any(term in full_page_text.lower() for term in ['men', 'women', 'male', 'female', 'gender', 'sex'])
        education_found = 'education' in full_page_text.lower()
        
        relevance_score = sum([bulgaria_found, year_2011_found, census_found, tertiary_found, gender_found, education_found])
        
        print(f"  Bulgaria: {bulgaria_found} | 2011: {year_2011_found} | Census: {census_found}")
        print(f"  Tertiary: {tertiary_found} | Gender: {gender_found} | Education: {education_found}")
        print(f"  Relevance score: {relevance_score}/6")
        
        if relevance_score >= 3:  # Analyze relevant files
            print(f"  *** RELEVANT SOURCE - EXTRACTING DATA ***")
            
            # Search for tertiary education statistics by gender using regex patterns
            education_patterns = [
                r'tertiary education[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                r'(?:men|women|male|female)[^.]*?tertiary education[^.]*?(\d+[.,]?\d*\s*%?)',
                r'higher education[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                r'(?:men|women|male|female)[^.]*?higher education[^.]*?(\d+[.,]?\d*\s*%?)',
                r'university[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                r'completed[^.]*?tertiary[^.]*?(\d+[.,]?\d*\s*%?)',
                r'2011[^.]*?education[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                r'census[^.]*?2011[^.]*?education[^.]*?(\d+[.,]?\d*\s*%?)'
            ]
            
            pattern_matches = []
            for pattern in education_patterns:
                matches = re.finditer(pattern, full_page_text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    # Get context around the match
                    start = max(0, match.start() - 250)
                    end = min(len(full_page_text), match.end() + 250)
                    context = full_page_text[start:end].strip()
                    
                    # Ensure context mentions Bulgaria by checking directly
                    if 'bulgaria' in context.lower() or 'bulgarian' in context.lower():
                        pattern_matches.append({
                            'match_text': match.group(),
                            'context': context,
                            'pattern_used': pattern,
                            'source_file': filename
                        })
            
            print(f"  Pattern matches found: {len(pattern_matches)}")
            
            # Look for sentences with education and gender data
            relevant_sentences = []
            sentences = full_page_text.split('.')
            
            for sentence in sentences:
                sentence_clean = sentence.strip()
                if len(sentence_clean) < 30:  # Skip very short sentences
                    continue
                
                # Check sentence content by calling .lower() directly
                sentence_lower_text = sentence_clean.lower()
                
                # Check for Bulgaria + education + numbers + gender
                has_bulgaria_ref = 'bulgaria' in sentence_lower_text or 'bulgarian' in sentence_lower_text
                has_education_ref = any(term in sentence_lower_text for term in 
                                      ['education', 'tertiary', 'university', 'higher', 'degree', 'graduate'])
                has_numbers = bool(re.search(r'\d+[.,]?\d*\s*%?', sentence_clean))
                has_gender_ref = any(term in sentence_lower_text for term in ['men', 'women', 'male', 'female'])
                has_year_ref = '2011' in sentence_lower_text
                
                if has_bulgaria_ref and has_education_ref and has_numbers and (has_gender_ref or has_year_ref):
                    relevant_sentences.append(sentence_clean)
            
            print(f"  Relevant sentences: {len(relevant_sentences)}")
            
            # Look for tables with education data
            tables = soup.find_all('table')
            education_tables = []
            
            for table_idx, table in enumerate(tables):
                table_text = table.get_text()
                table_text_lower = table_text.lower()
                
                # Check if table has education and gender information
                has_education_content = any(term in table_text_lower for term in 
                                          ['education', 'tertiary', 'university', 'degree'])
                has_gender_content = any(term in table_text_lower for term in 
                                       ['men', 'women', 'male', 'female', 'gender'])
                
                if has_education_content and has_gender_content:
                    # Extract table headers
                    headers = [th.get_text().strip() for th in table.find_all('th')]
                    
                    # Extract sample rows
                    rows = table.find_all('tr')
                    sample_rows = []
                    
                    for row in rows[1:4]:  # Skip header, take next 3 rows
                        cells = [cell.get_text().strip() for cell in row.find_all(['td', 'th'])]
                        if cells and any(cell for cell in cells if cell.strip()):
                            sample_rows.append(cells)
                    
                    education_tables.append({
                        'table_index': table_idx,
                        'headers': headers,
                        'sample_rows': sample_rows,
                        'total_rows': len(rows)
                    })
            
            print(f"  Education tables found: {len(education_tables)}")
            
            # Store analysis results
            analysis_result = {
                'filename': filename,
                'title': page_title,
                'content_length': len(full_page_text),
                'relevance_score': relevance_score,
                'pattern_matches': pattern_matches[:3],  # Top 3 matches
                'relevant_sentences': relevant_sentences[:3],  # Top 3 sentences
                'education_tables': education_tables[:2],  # Top 2 tables
                'indicators': {
                    'bulgaria': bulgaria_found,
                    '2011': year_2011_found,
                    'census': census_found,
                    'tertiary': tertiary_found,
                    'gender': gender_found,
                    'education': education_found
                }
            }
            
            analysis_results.append(analysis_result)
            
            # Add findings to main list
            for match in pattern_matches:
                tertiary_education_findings.append({
                    'type': 'statistical_pattern',
                    'source_file': filename,
                    'match': match['match_text'],
                    'context': match['context'],
                    'pattern': match['pattern_used']
                })
            
            for sentence in relevant_sentences:
                tertiary_education_findings.append({
                    'type': 'relevant_sentence',
                    'source_file': filename,
                    'content': sentence
                })
            
            # Display key findings
            if pattern_matches:
                print(f"  Key match: {pattern_matches[0]['match_text']}")
                print(f"  Context: {pattern_matches[0]['context'][:150]}...")
            
            if relevant_sentences:
                print(f"  Key sentence: {relevant_sentences[0][:200]}...")
            
            if education_tables:
                print(f"  Table headers: {education_tables[0]['headers'][:5]}")
        
        else:
            print(f"  Lower relevance (score {relevance_score}) - basic analysis only")
            analysis_results.append({
                'filename': filename,
                'title': page_title,
                'content_length': len(full_page_text),
                'relevance_score': relevance_score,
                'basic_analysis': True
            })
        
        print()
        
    except Exception as e:
        print(f"  ERROR analyzing {filename}: {str(e)}")
        print()

# Save comprehensive results
print("=== SAVING COMPREHENSIVE RESULTS ===\n")

final_results = {
    'search_objective': 'Bulgarian 2011 census tertiary education completion by gender',
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'html_files_total': len(html_files),
    'priority_files_identified': len(priority_files),
    'files_successfully_analyzed': len(analysis_results),
    'tertiary_education_findings_count': len(tertiary_education_findings),
    'detailed_analysis_results': analysis_results,
    'tertiary_education_findings': tertiary_education_findings
}

results_file = os.path.join(workspace_path, 'bulgarian_2011_census_tertiary_education_comprehensive_results.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False)

print(f"Comprehensive results saved to: {results_file}")
print(f"Total HTML files: {len(html_files)}")
print(f"Files analyzed: {len(analysis_results)}")
print(f"Tertiary education findings: {len(tertiary_education_findings)}")

# Display summary of findings
if tertiary_education_findings:
    print("\n=== TERTIARY EDUCATION FINDINGS SUMMARY ===\n")
    
    statistical_patterns = [f for f in tertiary_education_findings if f['type'] == 'statistical_pattern']
    relevant_sentences = [f for f in tertiary_education_findings if f['type'] == 'relevant_sentence']
    
    print(f"Statistical patterns found: {len(statistical_patterns)}")
    print(f"Relevant sentences found: {len(relevant_sentences)}")
    
    # Show top findings
    print("\nTop findings:")
    for i, finding in enumerate(tertiary_education_findings[:8], 1):
        print(f"{i}. Type: {finding['type']}")
        print(f"   Source: {finding['source_file']}")
        
        if finding['type'] == 'statistical_pattern':
            print(f"   Match: {finding['match']}")
            print(f"   Context: {finding['context'][:150]}...")
        else:
            print(f"   Content: {finding['content'][:180]}...")
        print()

else:
    print("\n=== NO SPECIFIC FINDINGS ===\n")
    print("The analyzed files did not contain specific 2011 Bulgarian census")
    print("tertiary education completion statistics by gender.")
    print("This suggests the data may be:")
    print("1. In a different format or section")
    print("2. Using different terminology")
    print("3. Requiring more specific official sources")

# Save file inventory for reference
inventory_data = {
    'total_files': len(all_files),
    'html_files': len(html_files),
    'json_files': len(json_files),
    'html_file_details': [],
    'priority_files': priority_files
}

for html_file in html_files:
    filepath = os.path.join(workspace_path, html_file)
    file_size = os.path.getsize(filepath)
    inventory_data['html_file_details'].append({
        'filename': html_file,
        'size': file_size
    })

inventory_file = os.path.join(workspace_path, 'file_inventory_and_analysis_attempts.json')
with open(inventory_file, 'w', encoding='utf-8') as f:
    json.dump(inventory_data, f, indent=2, ensure_ascii=False)

print(f"\nFile inventory saved to: {inventory_file}")
print("\n=== ANALYSIS COMPLETE ===\n")
print("Bulgarian 2011 census tertiary education analysis completed with new approach")
print("All findings systematically extracted and saved for review")
```