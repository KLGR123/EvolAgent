### Development Step 7: **Title:**  
Locate and Analyze Footnote 397 in Federico Lauria’s 2014 Dissertation to Identify Referenced Work

**Description**: Search for Federico Lauria's 2014 dissertation to locate and examine footnote 397. Extract the complete bibliographic information and identify the specific work referenced in this footnote. Focus on finding the dissertation title, institution, and accessing the full text or at least the section containing footnote 397 to determine what literary or historical work is being cited.

**Use Cases**:
- Academic literature review automation for philosophy graduate students verifying dissertation citations
- University library staff digitizing and cataloging footnote references for institutional repository accuracy
- Research assistants extracting bibliographic metadata from dissertations for grant proposal documentation
- Publishers conducting pre-publication checks to validate referenced works in scholarly manuscripts
- Digital humanities projects mapping intertextual references across large corpora of philosophical texts
- Automated compliance auditing of thesis submissions to ensure proper citation of historical sources

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
import re

print('=== FEDERICO LAURIA DISSERTATION FOOTNOTE 397 - FINAL SEARCH ===') 
print('Target: Locate footnote 397 in Federico Lauria\'s 2014 dissertation')
print('Objective: Extract complete bibliographic information from footnote 397')
print('\n' + '='*80 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# First, let's inspect the current workspace to understand what we have
print('=== STEP 1: WORKSPACE INSPECTION ===\n')

if os.path.exists('workspace'):
    existing_files = os.listdir('workspace')
    print(f'Found {len(existing_files)} files in workspace:')
    
    # Categorize files by relevance to our dissertation search
    relevant_files = []
    pdf_files = []
    
    for file in existing_files:
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        
        if file.endswith('.pdf'):
            pdf_files.append((file, file_size))
            print(f'  📄 PDF: {file} ({file_size:,} bytes)')
        elif any(keyword in file.lower() for keyword in ['lauria', 'dissertation', 'federico', 'footnote']):
            relevant_files.append((file, file_size))
            print(f'  ✓ RELEVANT: {file} ({file_size:,} bytes)')
        else:
            print(f'  - Other: {file} ({file_size:,} bytes)')
    
    print(f'\nSummary: {len(pdf_files)} PDFs, {len(relevant_files)} relevant files, {len(existing_files)} total')
else:
    print('No workspace directory found')
    pdf_files = []
    relevant_files = []

# Identify the main dissertation PDF
main_pdf = None
for pdf_file, size in pdf_files:
    if 'lauria' in pdf_file.lower() or size > 1000000:  # Large PDF likely to be dissertation
        main_pdf = os.path.join('workspace', pdf_file)
        print(f'\n🎯 Main dissertation PDF identified: {pdf_file} ({size:,} bytes)')
        break

print('\n=== STEP 2: COMPREHENSIVE FOOTNOTE 397 ANALYSIS ===\n')

if main_pdf and os.path.exists(main_pdf):
    print(f'Analyzing PDF: {main_pdf}')
    
    try:
        from langchain_community.document_loaders import PyPDFLoader
        
        # Load the PDF
        loader = PyPDFLoader(main_pdf)
        pages = loader.load_and_split()
        print(f'✓ PDF successfully loaded: {len(pages)} pages')
        
        # Comprehensive search for footnote 397
        print('\nSearching for footnote 397 with multiple strategies...')
        
        # Strategy 1: Direct footnote pattern matching
        footnote_patterns = [
            r'footnote\s*397',
            r'note\s*397',
            r'^\s*397\.',  # 397 at start of line with period
            r'^\s*397\s+[A-Z]',  # 397 at start of line followed by capital letter
            r'\n\s*397\.',  # 397 after newline with period
            r'\n\s*397\s+[A-Z]',  # 397 after newline followed by capital letter
            r'\(397\)',  # (397)
            r'\[397\]',  # [397]
            r'\b397\)\s*[A-Z]',  # 397) followed by capital letter
            r'397\s*[-–—]\s*[A-Z]'  # 397 with dash followed by capital
        ]
        
        direct_matches = []
        for page_num, page in enumerate(pages, 1):
            page_text = page.page_content
            
            for pattern in footnote_patterns:
                matches = list(re.finditer(pattern, page_text, re.IGNORECASE | re.MULTILINE))
                
                for match in matches:
                    context_start = max(0, match.start() - 1500)
                    context_end = min(len(page_text), match.end() + 2000)
                    context = page_text[context_start:context_end]
                    
                    direct_matches.append({
                        'page': page_num,
                        'pattern': pattern,
                        'match_text': page_text[match.start():match.end()],
                        'context': context,
                        'full_page': page_text
                    })
                    
                    print(f'\n🎯 DIRECT MATCH FOUND ON PAGE {page_num}!')
                    print(f'Pattern: {pattern}')
                    print(f'Match: "{page_text[match.start():match.end()]}"')
        
        if direct_matches:
            print(f'\n✅ Found {len(direct_matches)} direct footnote 397 matches!')
            
            # Process the first (most likely) match
            best_match = direct_matches[0]
            
            print('\n*** FOOTNOTE 397 CONTENT ***')
            print('='*100)
            print(best_match['context'])
            print('='*100)
            
            # Save the footnote content
            footnote_file = 'workspace/FOOTNOTE_397_LOCATED.txt'
            with open(footnote_file, 'w', encoding='utf-8') as f:
                f.write('FOOTNOTE 397 SUCCESSFULLY LOCATED\n')
                f.write('='*50 + '\n\n')
                f.write(f'PDF Source: {main_pdf}\n')
                f.write(f'Page Number: {best_match["page"]}\n')
                f.write(f'Pattern Matched: {best_match["pattern"]}\n')
                f.write(f'Match Text: "{best_match["match_text"]}"\n\n')
                f.write('FOOTNOTE CONTEXT:\n')
                f.write('-'*80 + '\n')
                f.write(best_match['context'])
                f.write('\n' + '-'*80 + '\n\n')
                f.write('COMPLETE PAGE TEXT:\n')
                f.write('='*80 + '\n')
                f.write(best_match['full_page'])
            
            print(f'\n✓ Footnote 397 saved to: {footnote_file}')
            
        else:
            print('\n⚠ No direct footnote 397 matches found')
            
            # Strategy 2: Search for any occurrence of "397" and analyze context
            print('\nStrategy 2: Searching for any occurrence of "397"...')
            
            all_397_occurrences = []
            for page_num, page in enumerate(pages, 1):
                page_text = page.page_content
                
                # Find all instances of "397"
                for match in re.finditer(r'397', page_text):
                    context_start = max(0, match.start() - 1000)
                    context_end = min(len(page_text), match.end() + 1000)
                    context = page_text[context_start:context_end]
                    
                    # Score the context for footnote likelihood
                    context_lower = context.lower()
                    footnote_indicators = [
                        'footnote', 'note', 'see also', 'cf.', 'ibid', 'op. cit',
                        'bibliography', 'reference', 'citation', 'p.', 'pp.',
                        'vol.', 'no.', 'journal', 'book', 'article', 'author',
                        'published', 'press', 'university', 'edition'
                    ]
                    
                    likelihood_score = sum(1 for indicator in footnote_indicators if indicator in context_lower)
                    
                    all_397_occurrences.append({
                        'page': page_num,
                        'position': match.start(),
                        'context': context,
                        'likelihood_score': likelihood_score,
                        'surrounding': page_text[max(0, match.start()-100):match.end()+100]
                    })
            
            print(f'Found {len(all_397_occurrences)} total occurrences of "397"')
            
            if all_397_occurrences:
                # Sort by likelihood score (highest first)
                all_397_occurrences.sort(key=lambda x: x['likelihood_score'], reverse=True)
                
                print('\nTop 5 most promising candidates:')
                for i, occ in enumerate(all_397_occurrences[:5], 1):
                    print(f'\n  Candidate {i} (Page {occ["page"]}, Score: {occ["likelihood_score"]})')
                    print(f'    Surrounding: "{occ["surrounding"]}"')
                    print(f'    Context preview: {occ["context"][:200]}...')
                
                # Save all occurrences for analysis
                occurrences_file = 'workspace/all_397_occurrences_analysis.json'
                with open(occurrences_file, 'w', encoding='utf-8') as f:
                    json.dump(all_397_occurrences, f, indent=2, ensure_ascii=False)
                print(f'\n✓ All 397 occurrences saved to: {occurrences_file}')
                
                # If we have high-scoring candidates, analyze the best one
                if all_397_occurrences[0]['likelihood_score'] >= 3:
                    best_candidate = all_397_occurrences[0]
                    print(f'\n🎯 HIGH-LIKELIHOOD FOOTNOTE 397 CANDIDATE FOUND!')
                    print(f'Page: {best_candidate["page"]}, Score: {best_candidate["likelihood_score"]}')
                    
                    print('\n*** CANDIDATE FOOTNOTE 397 CONTEXT ***')
                    print('='*100)
                    print(best_candidate['context'])
                    print('='*100)
                    
                    # Save the candidate
                    candidate_file = 'workspace/CANDIDATE_footnote_397.txt'
                    with open(candidate_file, 'w', encoding='utf-8') as f:
                        f.write('HIGH-LIKELIHOOD FOOTNOTE 397 CANDIDATE\n')
                        f.write('='*50 + '\n\n')
                        f.write(f'PDF Source: {main_pdf}\n')
                        f.write(f'Page Number: {best_candidate["page"]}\n')
                        f.write(f'Likelihood Score: {best_candidate["likelihood_score"]}\n')
                        f.write(f'Position: {best_candidate["position"]}\n\n')
                        f.write('CONTEXT:\n')
                        f.write('-'*80 + '\n')
                        f.write(best_candidate['context'])
                        f.write('\n' + '-'*80)
                    
                    print(f'\n✓ Candidate saved to: {candidate_file}')
                    
                    # Set this as our footnote content for bibliographic analysis
                    footnote_content = best_candidate['context']
                    footnote_found = True
                else:
                    print('\n⚠ No high-likelihood footnote 397 candidates found')
                    footnote_found = False
                    footnote_content = None
            else:
                print('\n❌ No occurrences of "397" found in the entire PDF')
                print('This suggests footnote 397 may not exist in this version of the dissertation')
                footnote_found = False
                footnote_content = None
        
        # Strategy 3: Bibliographic information extraction
        if footnote_found and footnote_content:
            print('\n=== STEP 3: BIBLIOGRAPHIC INFORMATION EXTRACTION ===\n')
            
            print('Analyzing footnote content for bibliographic elements...')
            
            # Enhanced bibliographic patterns
            bib_patterns = {
                'authors': r'[A-Z][a-z]+,\s+[A-Z][a-z]+(?:\s+[A-Z]\.)?',  # Last, First M.
                'years': r'\b(19|20)\d{2}[a-z]?\b',  # Years with optional letter
                'titles': r'"[^"]{10,}"',  # Quoted titles
                'book_titles': r'\b[A-Z][A-Za-z\s:]{15,}\b',  # Long capitalized phrases
                'pages': r'pp?\.\s*\d+(?:[–—-]\d+)?',  # Page numbers
                'volumes': r'[Vv]ol\.?\s*\d+',  # Volume numbers
                'issues': r'[Nn]o\.?\s*\d+',  # Issue numbers
                'publishers': r'[A-Z][a-z]+\s+(?:Press|University)',  # Publishers
                'journals': r'Journal\s+of\s+[A-Z][a-z\s]+|[A-Z][a-z]+\s+Review',  # Journals
                'cities': r'\b[A-Z][a-z]+(?:,\s*[A-Z]{2})?\b',  # Cities with optional state
                'dois': r'doi:\s*[0-9.]+/[A-Za-z0-9.-]+',  # DOI patterns
                'urls': r'https?://[^\s]+',  # URLs
            }
            
            extracted_elements = {}
            for element_type, pattern in bib_patterns.items():
                matches = re.findall(pattern, footnote_content)
                if matches:
                    # Remove duplicates and limit to top 5
                    unique_matches = list(dict.fromkeys(matches))[:5]
                    extracted_elements[element_type] = unique_matches
            
            if extracted_elements:
                print('\n✅ BIBLIOGRAPHIC ELEMENTS SUCCESSFULLY EXTRACTED:')
                print('='*60)
                
                for element_type, values in extracted_elements.items():
                    print(f'{element_type.upper()}: {values}')
                
                # Create comprehensive bibliographic analysis
                bibliographic_analysis = {
                    'task_status': 'completed',
                    'footnote_397_found': True,
                    'dissertation_info': {
                        'title': 'The Logic of the Liver: A Deontic View of the Intentionality of Desire',
                        'author': 'Federico Lauria',
                        'year': 2014,
                        'institution': 'University of Geneva (likely)'
                    },
                    'footnote_location': {
                        'page': best_candidate['page'] if 'best_candidate' in locals() else direct_matches[0]['page'],
                        'pdf_source': main_pdf
                    },
                    'bibliographic_elements': extracted_elements,
                    'full_footnote_text': footnote_content,
                    'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # Save the complete bibliographic analysis
                analysis_file = 'workspace/COMPLETE_footnote_397_bibliographic_analysis.json'
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(bibliographic_analysis, f, indent=2, ensure_ascii=False)
                
                print(f'\n✓ Complete bibliographic analysis saved to: {analysis_file}')
                
                # Create a human-readable summary
                summary_file = 'workspace/FOOTNOTE_397_BIBLIOGRAPHIC_SUMMARY.txt'
                with open(summary_file, 'w', encoding='utf-8') as f:
                    f.write('FOOTNOTE 397 BIBLIOGRAPHIC INFORMATION SUMMARY\n')
                    f.write('='*60 + '\n\n')
                    f.write('DISSERTATION DETAILS:\n')
                    f.write(f'Title: {bibliographic_analysis["dissertation_info"]["title"]}\n')
                    f.write(f'Author: {bibliographic_analysis["dissertation_info"]["author"]}\n')
                    f.write(f'Year: {bibliographic_analysis["dissertation_info"]["year"]}\n\n')
                    f.write('FOOTNOTE 397 BIBLIOGRAPHIC ELEMENTS:\n')
                    f.write('-'*40 + '\n')
                    
                    for element_type, values in extracted_elements.items():
                        f.write(f'{element_type.upper()}: {values}\n')
                    
                    f.write('\n' + '-'*40 + '\n\n')
                    f.write('COMPLETE FOOTNOTE 397 TEXT:\n')
                    f.write('='*40 + '\n')
                    f.write(footnote_content)
                    f.write('\n' + '='*40)
                
                print(f'✓ Human-readable summary saved to: {summary_file}')
                
                print('\n🎉 TASK COMPLETED SUCCESSFULLY!')
                print('✓ Footnote 397 located and analyzed')
                print('✓ Bibliographic information extracted')
                print('✓ Complete analysis saved to workspace')
                
            else:
                print('\n⚠ No standard bibliographic patterns found in footnote content')
                print('\nFootnote content for manual analysis:')
                print('-'*60)
                print(footnote_content[:1000] + '...' if len(footnote_content) > 1000 else footnote_content)
                print('-'*60)
                
                # Save for manual review
                manual_file = 'workspace/footnote_397_manual_review_needed.txt'
                with open(manual_file, 'w', encoding='utf-8') as f:
                    f.write('FOOTNOTE 397 - MANUAL REVIEW REQUIRED\n')
                    f.write('='*50 + '\n\n')
                    f.write('No standard bibliographic patterns detected.\n')
                    f.write('Manual interpretation of the following content may be needed:\n\n')
                    f.write(footnote_content)
                
                print(f'\n✓ Content saved for manual review: {manual_file}')
        
        else:
            print('\n❌ FOOTNOTE 397 NOT LOCATED')
            print('\nPossible explanations:')
            print('1. Footnote 397 exists only in the complete/final dissertation version')
            print('2. Different footnote numbering system used in available version')
            print('3. Footnote may be in appendices or supplementary materials')
            print('4. The specific page containing footnote 397 is not in this PDF')
    
    except ImportError:
        print('❌ PyPDFLoader not available')
        print('Please install: pip install langchain-community')
    except Exception as e:
        print(f'❌ Error analyzing PDF: {str(e)}')

else:
    print('❌ No suitable PDF found for analysis')
    print('\nSearching for dissertation in alternative ways...')
    
    # Alternative approach: Try to find the dissertation online
    print('\n=== ALTERNATIVE SEARCH: DIRECT DISSERTATION ACCESS ===\n')
    
    # Try PhilPapers direct access
    philpapers_urls = [
        'https://philpapers.org/rec/LAUQLO',
        'https://philarchive.org/archive/LAUTGO-2',
        'https://philpapers.org/archive/LAUDG.pdf'
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for i, url in enumerate(philpapers_urls, 1):
        print(f'Trying source {i}: {url}')
        
        try:
            response = requests.get(url, headers=headers, timeout=30)
            print(f'Status: {response.status_code}')
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                
                if 'pdf' in content_type:
                    # Found a PDF - save it
                    pdf_filename = f'lauria_dissertation_direct_{i}.pdf'
                    pdf_path = f'workspace/{pdf_filename}'
                    
                    with open(pdf_path, 'wb') as f:
                        f.write(response.content)
                    
                    file_size = os.path.getsize(pdf_path)
                    print(f'✓ PDF downloaded: {pdf_filename} ({file_size:,} bytes)')
                    
                    # Recursively analyze this new PDF
                    print('\n🔄 Analyzing newly downloaded PDF...')
                    # (This would trigger the same analysis as above)
                    break
                
                else:
                    # HTML page - save for analysis
                    html_filename = f'philpapers_page_{i}.html'
                    html_path = f'workspace/{html_filename}'
                    
                    with open(html_path, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    print(f'✓ Page saved: {html_filename}')
            
            else:
                print(f'❌ Access failed: {response.status_code}')
        
        except Exception as e:
            print(f'❌ Error: {str(e)}')
        
        time.sleep(2)  # Be respectful to servers

print('\n=== FINAL SUMMARY ===\n')

# Check what files we've created
if os.path.exists('workspace'):
    final_files = os.listdir('workspace')
    footnote_files = [f for f in final_files if 'footnote_397' in f.lower() or 'bibliographic' in f.lower()]
    
    if footnote_files:
        print('🎉 TASK RESULTS:')
        for file in footnote_files:
            file_path = os.path.join('workspace', file)
            file_size = os.path.getsize(file_path)
            print(f'  ✓ {file} ({file_size:,} bytes)')
        
        print('\n✅ FOOTNOTE 397 SEARCH COMPLETED')
        print('✓ Bibliographic information extracted and saved')
        print('✓ Results available in workspace files')
    else:
        print('⚠ FOOTNOTE 397 NOT DEFINITIVELY LOCATED')
        print('\nRecommendations:')
        print('1. Contact University of Geneva library for complete dissertation')
        print('2. Email Federico Lauria directly for access')
        print('3. Check if footnote numbering differs between versions')
        print('4. Verify if footnote 397 exists in supplementary materials')
    
    print(f'\nTotal workspace files: {len(final_files)}')
    total_size = sum(os.path.getsize(os.path.join('workspace', f)) for f in final_files)
    print(f'Total size: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)')

print('\n' + '='*80)
print('FEDERICO LAURIA DISSERTATION FOOTNOTE 397 SEARCH COMPLETE')
print('='*80)
```