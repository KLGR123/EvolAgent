### Development Step 17: Search and download 'Can Hiccup Supply Enough Fish for a Dragon’s Diet?' PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic researcher automating the discovery and download of the University of Leicester paper “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” by parsing saved DuckDuckGo and Bing search-result HTML and extracting direct ‘le.ac.uk’ links for full-PDF retrieval
- University librarian batch‐inspecting archived search-result pages to identify and harvest official Leicester domain articles for the institutional repository, ensuring all links resolve to ‘le.ac.uk’ landing pages
- Bioinformatics scientist building a pipeline that scans stored search-engine snapshots for ‘le.ac.uk’ URLs to programmatically fetch ecological feeding‐strategy PDFs on mythical and real-world species
- SEO analyst auditing competitor visibility by extracting and listing all ‘le.ac.uk’ links from DuckDuckGo and Bing result dumps, then measuring landing‐page performance and search ranking for specific research topics
- Regulatory compliance officer validating that all external citations in a policy report reference only approved university sources by scanning HTML exports for ‘le.ac.uk’ domains before publication
- EdTech developer integrating a pre‐flight link inspector into course‐creation workflows to automatically gather University of Leicester research PDFs into the LMS when building modules on animal nutrition
- Data journalist mining stored search-result HTML for the official “dragon diet” study on le.ac.uk and downloading the PDF to fact-check and source authoritative quotes in a feature article
- Pharmaceutical R&D specialist aggregating University of Leicester nutritional studies by scanning previously saved Bing and DuckDuckGo pages for ‘le.ac.uk’ links, then downloading PDFs for meta‐analysis of protein sources in novel feed formulations

```
import os
import sys
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# Step 1: Define and verify the workspace directory
workspace = 'workspace'
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    print("Please run the search scripts first to save the DuckDuckGo and Bing HTML files into workspace/.")
    sys.exit(1)

# Step 2: Define expected search-result HTML file paths
ddg_html = os.path.join(workspace, 'site_le_duckduckgo_search.html')
bing_html = os.path.join(workspace, 'site_le_bing_search.html')
for html_file in (ddg_html, bing_html):
    if not os.path.isfile(html_file):
        print(f"ERROR: Required file not found: {html_file}")
        print("Make sure you've saved the search-result pages into workspace/ before running this inspection.")
        sys.exit(1)

print(f"[INFO] Inspecting for 'le.ac.uk' links in:\n - {ddg_html}\n - {bing_html}\n")

# Container for all detected University of Leicester URLs
leicester_urls = []

# Function to inspect one HTML file for le.ac.uk
def inspect_html(path, label):
    print(f"[INSPECT] {label}")
    with open(path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1) Extract direct <a href> and DuckDuckGo uddg redirects
    for a in soup.find_all('a', href=True):
        href = a['href']
        # DuckDuckGo redirect pattern
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                decoded = unquote(q[0])
                host = urlparse(decoded).netloc.lower()
                if 'le.ac.uk' in host:
                    print(f"  [REDIRECT] decoded uddg -> {decoded}")
                    leicester_urls.append(decoded)
        # Direct link on le.ac.uk
        netloc = urlparse(href).netloc.lower()
        if 'le.ac.uk' in netloc:
            print(f"  [DIRECT] {href}")
            leicester_urls.append(href)

    # 2) Raw text scan for any 'le.ac.uk' outside of <a> tags
    lines = html_content.splitlines()
    for idx, line in enumerate(lines):
        if 'le.ac.uk' in line and '<a ' not in line:
            start = max(0, idx - 2)
            end = min(len(lines), idx + 3)
            print(f"  [TEXT] Context around line {idx+1}:")
            for i in range(start, end):
                print(f"    {i+1:4d}: {lines[i].strip()}")
            print("")

# Inspect both search-result pages
inspect_html(ddg_html, 'DuckDuckGo results')
print('-' * 60)
inspect_html(bing_html, 'Bing results')

# Deduplicate URLs
unique_urls = []
for url in leicester_urls:
    if url not in unique_urls:
        unique_urls.append(url)

# Step 4: Write the inspection report
report_path = os.path.join(workspace, 'leicester_link_inspection.txt')
with open(report_path, 'w', encoding='utf-8') as out_f:
    out_f.write('Detected University of Leicester (le.ac.uk) landing URLs and contexts:\n\n')
    for url in unique_urls:
        out_f.write(url + '\n')

print(f"[DONE] Found {len(unique_urls)} unique Leicester-related URLs.")
print(f"[INFO] Detailed list written to: {report_path}")
print("Next: Open and review 'leicester_link_inspection.txt' to identify the correct landing-page URL(s) on le.ac.uk for the target paper.")
```