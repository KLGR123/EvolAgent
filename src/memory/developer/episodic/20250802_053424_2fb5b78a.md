### Development Step 5: Search and Download University of Leicester PDF: ‚ÄúHiccup‚Äôs Fish Supply for a Dragon‚Äôs Diet‚Äù

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Regulatory compliance teams automating monthly downloads of updated environmental impact assessment PDFs from government agency portals for audit records
- Financial analysts fetching quarterly earnings report PDFs from corporate investor-relations websites to feed revenue forecasting models
- Legal departments retrieving patent filing PDFs from university and patent-office domains to build an internal searchable claims database
- Academic research assistants crawling institutional repositories to download full conference proceedings PDFs for comprehensive literature reviews
- Supply chain managers collecting safety data sheet (SDS) PDFs for raw materials from multiple supplier portals to ensure workplace health compliance
- E-learning developers grabbing open-access textbook chapter PDFs from university library sites to integrate into online course modules
- Investigative journalists batch-downloading public records PDFs (legislative bills, committee reports) from government websites to support data-driven stories
- Market research teams scraping competitor product catalog PDFs from manufacturer domains to extract specifications and pricing into analysis pipelines

```
import os
import sys
import time
import requests
from urllib.parse import urlparse, parse_qs, unquote

# Purpose: Locate & download the University of Leicester paper
# "Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?"
# Save PDF to workspace/dragon_diet_fish_bag.pdf

# 1. Prepare workspace
def ensure_workspace():
    ws = 'workspace'
    os.makedirs(ws, exist_ok=True)
    return ws
workspace_dir = ensure_workspace()

# 2. Prepare search parameters
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon‚Äôs Diet?"
query = f'"{title}" University of Leicester PDF'
print(f"Search ‚Äì {query}\n")

# 3. Check manual override
override = os.getenv('DRAGON_PDF_URL')
if override:
    print(f"üîó Using override URL from DRAGON_PDF_URL: {override}\n")
    pdf_link = override
else:
    pdf_link = None

# 4. Primary: SerpAPI with backoff (if key present)
api_key = os.getenv('SERPAPI_API_KEY')
if not pdf_link and api_key:
    params = dict(engine='google', q=query, api_key=api_key,
                  num=10, google_domain='google.com', safe='off')
    for attempt in range(1, 4):
        print(f"[SerpAPI] Attempt {attempt}/3...")
        try:
            r = requests.get('https://serpapi.com/search.json', params=params, timeout=30)
            r.raise_for_status()
            data = r.json()
            for idx, item in enumerate(data.get('organic_results', []), 1):
                link = item.get('link','')
                print(f"  #{idx}: {link}")
                if link.lower().endswith('.pdf') or '.pdf?' in link.lower():
                    pdf_link = link
                    print(f"  ‚Üí Selected PDF via SerpAPI: {pdf_link}\n")
                    break
            if pdf_link:
                break
        except requests.exceptions.HTTPError as e:
            code = getattr(e.response, 'status_code', None)
            if code == 429:
                backoff = 2 ** (attempt-1)
                print(f"  429 Rate limit. Backing off {backoff}s...\n")
                time.sleep(backoff)
                continue
            else:
                print(f"  HTTPError {code}: {e}\n")
                break
        except Exception as e:
            print(f"  Error: {e}\n")
            break
    else:
        print("  SerpAPI attempts exhausted.\n")

# 5. Fallback: DuckDuckGo HTML with decoding uddg links
if not pdf_link:
    print("---\nFallback: DuckDuckGo HTML search + decode uddg links\n---")
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("Install bs4: pip install beautifulsoup4")
        sys.exit(1)
    resp = requests.get('https://duckduckgo.com/html/', params={'q': query},
                        headers={'User-Agent':'Mozilla/5.0'}, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    candidates = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        # decode uddg redirect
        if href.startswith('/l/?uddg='):
            qs = parse_qs(urlparse(href).query)
            real = qs.get('uddg', [None])[0]
            if real:
                url = unquote(real)
                if '.pdf' in url.lower(): candidates.append(url)
        # direct PDF
        elif href.lower().endswith('.pdf'):
            candidates.append(href)
    candidates = list(dict.fromkeys(candidates))
    if candidates:
        for i, u in enumerate(candidates,1): print(f"  {i}. {u}")
        pdf_link = candidates[0]
        print(f"  ‚Üí Selected PDF via DuckDuckGo: {pdf_link}\n")

# 6. Fallback #2: DuckDuckGo site:le.ac.uk search
if not pdf_link:
    print("---\nFallback: DuckDuckGo site:le.ac.uk search\n---")
    resp = requests.get('https://duckduckgo.com/html/',
                        params={'q': f"site:le.ac.uk {title} filetype:pdf"},
                        headers={'User-Agent':'Mozilla/5.0'}, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    sites = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        if href.startswith('/l/?uddg='):
            real = parse_qs(urlparse(href).query).get('uddg',[None])[0]
            if real and real.lower().endswith('.pdf'): sites.append(unquote(real))
    sites = list(dict.fromkeys(sites))
    if sites:
        for i, u in enumerate(sites,1): print(f"  {i}. {u}")
        pdf_link = sites[0]
        print(f"  ‚Üí Selected PDF via site:le.ac.uk fallback: {pdf_link}\n")

# 7. Fallback #3: Google HTML scraping with filetype:pdf
if not pdf_link:
    print("---\nFallback: Google HTML search filetype:pdf\n---")
    resp = requests.get('https://www.google.com/search',
                        params={'q': f"{title} filetype:pdf", 'num':'10', 'hl':'en'},
                        headers={'User-Agent':'Mozilla/5.0'}, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    glinks = []
    for a in soup.find_all('a', href=True):
        h = a['href']
        if h.startswith('/url?q='):
            url = h.split('/url?q=')[1].split('&')[0]
            if url.lower().endswith('.pdf'): glinks.append(url)
    glinks = list(dict.fromkeys(glinks))
    if glinks:
        for i, u in enumerate(glinks,1): print(f"  {i}. {u}")
        pdf_link = glinks[0]
        print(f"  ‚Üí Selected PDF via Google HTML: {pdf_link}\n")

# 8. Abort if still missing
if not pdf_link:
    print("‚ùå Unable to find any .pdf link. Please set DRAGON_PDF_URL to the direct PDF URL.")
    sys.exit(1)

# 9. Download PDF
pdf_path = os.path.join(workspace_dir, 'dragon_diet_fish_bag.pdf')
print(f"Downloading ‚Üí {pdf_link}\n to: {pdf_path}\n")
with requests.get(pdf_link, headers={'User-Agent':'Mozilla/5.0','Accept':'application/pdf'},
                  stream=True, timeout=60) as dl:
    dl.raise_for_status()
    total = 0
    with open(pdf_path, 'wb') as f:
        for chunk in dl.iter_content(8192):
            if chunk:
                f.write(chunk)
                total += len(chunk)
print(f"Download complete: {total} bytes ({total/1024/1024:.2f} MB)")
if total<10000:
    print("‚ö†Ô∏è Warning: File <10KB, may be incomplete.")
else:
    print("‚úÖ PDF appears valid.")
print("Done.")

```