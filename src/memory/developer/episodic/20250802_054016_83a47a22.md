### Development Step 24: Download PDF of University of Leicester paper on dragon’s fish diet

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic researcher automating retrieval and local archiving of niche university papers (e.g., “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?”) for literature reviews.
- Patent attorney conducting rapid searches for technical PDF disclosures to support freedom‐to‐operate opinions in intellectual property cases.
- Pharmaceutical compliance officer downloading and verifying full-text regulatory guidance PDFs from government websites to ensure up-to-date drug approval workflows.
- Market intelligence analyst scraping competitor whitepapers and R&D reports in PDF form to extract key product development insights.
- Environmental NGO building an offline repository of climate impact studies in PDF format for policy briefings and stakeholder presentations.
- Investigative journalist locating and preserving leaked corporate or government PDF documents as evidence in long-form reporting.
- E-learning platform aggregating and indexing course reading materials (PDF syllabi, lecture notes) from multiple universities for centralized student access.

```
import os
import sys
import re
import requests

# Step 1: Ensure workspace directory exists
workspace = 'workspace'
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    sys.exit(1)
print(f"[INFO] Using workspace: {workspace}\n")

# Step 2: Set up headers and query\HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept': 'text/html'
}
query = '"Can Hiccup Supply Enough Fish to Maintain a Dragon\'s Diet?" filetype:pdf'

# Step 3: Perform Bing search and retrieve HTML
search_url = 'https://www.bing.com/search'
print(f"[SEARCH] Querying Bing for HTML dump: {query}")
resp = requests.get(search_url, params={'q': query}, headers=HEADERS, timeout=30)
resp.raise_for_status()
html = resp.text
print(f"[SEARCH] Retrieved {len(html)} characters of HTML from Bing.\n")

# Step 4: Save full HTML for offline inspection
html_path = os.path.join(workspace, 'bing_search.html')
with open(html_path, 'w', encoding='utf-8') as f:
    f.write(html)
print(f"[SAVE] Full Bing HTML saved to: {html_path}\n")

# Step 5: Find all 'pdf' occurrences and dump context snippets
pdf_positions = [m.start() for m in re.finditer(r'pdf', html, flags=re.IGNORECASE)]
print(f"[PARSE] Found {len(pdf_positions)} occurrences of 'pdf' in the HTML.")

snippets = []
for idx, pos in enumerate(pdf_positions[:20]):
    start = max(0, pos - 80)
    end = min(len(html), pos + 80)
    context = html[start:end].replace('\n', ' ')
    snippets.append((pos, context))

# Step 6: Save snippets to file
snip_path = os.path.join(workspace, 'pdf_snippets.txt')
with open(snip_path, 'w', encoding='utf-8') as f:
    for i, (pos, ctx) in enumerate(snippets, 1):
        f.write(f"Occurrence {i} at index {pos}:\n")
        f.write(ctx + "\n" + '-'*80 + "\n")
print(f"[SAVE] First {len(snippets)} context snippets saved to: {snip_path}\n")

# Step 7: Print first few snippets to console for quick inspection
for i, (pos, ctx) in enumerate(snippets[:5], 1):
    print(f"[SNIPPET {i}] Index {pos}: {ctx}\n")

print("[COMPLETE] HTML dump and PDF-context snippets are ready for inspection.")
```