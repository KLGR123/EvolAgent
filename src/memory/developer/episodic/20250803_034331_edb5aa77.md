### Development Step 10: Filter Wikipedia revision history to count page edits through June 30, 2023

**Description**: Filter the extracted revision data from the comprehensive Wikipedia history to count only the edits made from the page's inception until the end of June 2023. Exclude any revisions that occurred after June 30, 2023, and provide the exact count of edits within the specified timeframe.

**Use Cases**:
- Digital humanities research on cultural heritage: counting edits to historical monument Wikipedia pages through June 2023 to gauge community-driven documentation growth
- Political journalism analytics: measuring the volume of revisions on election-related articles up to June 2023 to identify periods of peak editorial activity
- Corporate brand compliance audit: auditing all edits on a company’s Wikipedia page until mid-2023 to verify no unauthorized content changes occurred
- Machine learning dataset preparation: filtering Wikipedia revision histories before July 2023 to build a time-bounded corpus for change-detection model training
- Educational content stability assessment: evaluating the number of edits on science and math topic pages until June 2023 to select stable resources for textbook references
- NGO transparency reporting: quantifying community edits on environmental policy articles through June 2023 to demonstrate stakeholder engagement over time
- SEO and digital marketing performance review: tracking cumulative edits on product and service Wikipedia pages before July 2023 to inform content strategy adjustments
- Fact-checking and misinformation studies: analyzing revision counts on high-profile news event pages up to June 2023 to correlate editorial activity with misinformation spikes

```
import os
import json
from datetime import datetime

print("=== INSPECTING EXISTING REVISION DATA FILES ===\n")

# Check all workspace directories for revision data
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
print(f"Found workspace directories: {workspace_dirs}\n")

revision_files = []
for workspace_dir in workspace_dirs:
    files = os.listdir(workspace_dir)
    for file in files:
        if 'revision' in file.lower() or 'wikipedia' in file.lower():
            file_path = os.path.join(workspace_dir, file)
            file_size = os.path.getsize(file_path)
            revision_files.append({
                'path': file_path,
                'name': file,
                'size': file_size,
                'workspace': workspace_dir
            })
            print(f"Found revision file: {file} ({file_size:,} bytes) in {workspace_dir}")

if not revision_files:
    print("❌ No revision data files found in any workspace")
else:
    # Use the largest/most comprehensive file
    largest_file = max(revision_files, key=lambda x: x['size'])
    print(f"\nUsing largest revision file: {largest_file['name']} ({largest_file['size']:,} bytes)")
    
    # First, inspect the file structure before loading
    print(f"\n=== INSPECTING FILE STRUCTURE: {largest_file['name']} ===\n")
    
    try:
        with open(largest_file['path'], 'r', encoding='utf-8') as f:
            # Read just the beginning to understand structure
            content_preview = f.read(1000)
            print(f"File preview (first 1000 chars):\n{content_preview}\n")
            
            # Reset and load as JSON to inspect structure
            f.seek(0)
            data = json.load(f)
            
        print("JSON structure analysis:")
        if isinstance(data, dict):
            print(f"  Root type: Dictionary with {len(data)} keys")
            for key, value in data.items():
                if isinstance(value, list):
                    print(f"    {key}: List with {len(value)} items")
                    if len(value) > 0:
                        print(f"      Sample item type: {type(value[0]).__name__}")
                        if isinstance(value[0], dict):
                            sample_keys = list(value[0].keys())
                            print(f"      Sample item keys: {sample_keys}")
                elif isinstance(value, dict):
                    print(f"    {key}: Dictionary with {len(value)} keys")
                    nested_keys = list(value.keys())
                    print(f"      Keys: {nested_keys}")
                else:
                    print(f"    {key}: {type(value).__name__} = {str(value)[:100]}")
        
        print(f"\n=== FILTERING REVISIONS TO COUNT EDITS UNTIL JUNE 30, 2023 ===\n")
        
        # Now that I understand the structure, extract revisions safely
        revisions = []
        metadata = {}
        
        # Check different possible structures
        if 'revisions' in data:
            revisions = data['revisions']
            print(f"Found 'revisions' key with {len(revisions)} items")
        elif 'filtered_revisions' in data:
            revisions = data['filtered_revisions']
            print(f"Found 'filtered_revisions' key with {len(revisions)} items")
        elif isinstance(data, list):
            revisions = data
            print(f"Data is a list with {len(revisions)} items")
        else:
            print("❌ Could not identify revisions data structure")
            print(f"Available keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
        
        # Extract metadata if available
        for key in ['extraction_metadata', 'filtering_metadata', 'metadata']:
            if key in data:
                metadata = data[key]
                print(f"Found metadata under '{key}' key")
                break
        
        if not revisions:
            print("❌ No revision data found to process")
        else:
            print(f"\nProcessing {len(revisions)} revisions...")
            
            # Show sample revision structure
            if len(revisions) > 0:
                sample_rev = revisions[0]
                print(f"\nSample revision structure:")
                for key, value in sample_rev.items():
                    print(f"  {key}: {type(value).__name__} = {str(value)[:100]}")
            
            # Filter revisions until June 30, 2023
            cutoff_date = datetime(2023, 6, 30, 23, 59, 59)
            print(f"\nApplying cutoff date: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S')}")
            
            filtered_count = 0
            excluded_count = 0
            earliest_timestamp = None
            latest_timestamp = None
            
            for revision in revisions:
                if 'timestamp' in revision:
                    try:
                        # Parse Wikipedia timestamp format
                        timestamp_str = revision['timestamp']
                        rev_timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        rev_timestamp = rev_timestamp.replace(tzinfo=None)
                        
                        # Track date range
                        if earliest_timestamp is None or timestamp_str < earliest_timestamp:
                            earliest_timestamp = timestamp_str
                        if latest_timestamp is None or timestamp_str > latest_timestamp:
                            latest_timestamp = timestamp_str
                        
                        # Count based on cutoff date
                        if rev_timestamp <= cutoff_date:
                            filtered_count += 1
                        else:
                            excluded_count += 1
                            
                    except Exception as e:
                        print(f"  ⚠️ Error parsing timestamp {revision.get('timestamp', 'N/A')}: {str(e)}")
                        continue
                else:
                    print(f"  ⚠️ Revision missing timestamp: {revision}")
            
            print(f"\n=== FILTERING RESULTS ===\n")
            print(f"Total revisions processed: {len(revisions)}")
            print(f"Edits until end of June 2023: {filtered_count}")
            print(f"Edits excluded (after June 30, 2023): {excluded_count}")
            
            if earliest_timestamp and latest_timestamp:
                print(f"\nRevision date range in data:")
                print(f"  Earliest: {earliest_timestamp}")
                print(f"  Latest: {latest_timestamp}")
            
            # Show metadata if available
            if metadata:
                print(f"\nSource metadata:")
                for key, value in metadata.items():
                    print(f"  {key}: {value}")
            
            # Save the final count result
            result = {
                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source_file': largest_file['name'],
                'cutoff_date': '2023-06-30 23:59:59',
                'total_revisions_in_source': len(revisions),
                'edits_until_june_2023': filtered_count,
                'edits_excluded_after_june_2023': excluded_count,
                'revision_date_range': {
                    'earliest': earliest_timestamp,
                    'latest': latest_timestamp
                },
                'source_metadata': metadata
            }
            
            result_file = os.path.join(largest_file['workspace'], 'final_edit_count_june_2023.json')
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            print(f"\n✅ Final results saved to: {os.path.basename(result_file)}")
            
            # Create summary text file
            summary_file = os.path.join(largest_file['workspace'], 'edit_count_summary_final.txt')
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"WIKIPEDIA EDIT COUNT - FINAL RESULTS\n")
                f.write(f"={'='*45}\n\n")
                f.write(f"Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Source data file: {largest_file['name']}\n")
                f.write(f"Cutoff date: June 30, 2023 23:59:59\n\n")
                f.write(f"FINAL ANSWER:\n")
                f.write(f"🎯 {filtered_count} edits were made from the page's inception until the end of June 2023\n\n")
                f.write(f"BREAKDOWN:\n")
                f.write(f"- Total revisions in source data: {len(revisions)}\n")
                f.write(f"- Edits until end of June 2023: {filtered_count}\n")
                f.write(f"- Edits excluded (after June 30, 2023): {excluded_count}\n\n")
                if earliest_timestamp and latest_timestamp:
                    f.write(f"SOURCE DATA DATE RANGE:\n")
                    f.write(f"- Earliest revision: {earliest_timestamp}\n")
                    f.write(f"- Latest revision: {latest_timestamp}\n")
            
            print(f"✅ Summary saved to: {os.path.basename(summary_file)}")
            
            print(f"\n🎯 FINAL ANSWER: {filtered_count} edits were made from the page's inception until the end of June 2023")
            
    except Exception as e:
        print(f"❌ Error processing revision file: {str(e)}")
        import traceback
        traceback.print_exc()
```