### Development Step 21: Search and Download PDF of University of Leicester’s "Hiccup Dragon Diet Fish Supply" Paper

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Legal compliance teams automating retrieval of regulatory guideline PDFs from government domains for centralized document management
- Pharmaceutical R&D groups programmatically downloading clinical trial publications in PDF from university repositories for literature reviews
- Academic libraries batch-harvesting course syllabi and lecture notes in PDF format to maintain a digital curriculum archive
- Market research analysts extracting quarterly financial report PDFs from corporate websites for automated KPI computation
- Environmental policy researchers scraping whitepaper and legislation PDFs on climate regulations for data-driven impact modeling
- Patent attorneys collecting invention disclosure and patent PDFs from official registries to support infringement and freedom-to-operate analysis
- EdTech platforms syncing open-access textbook PDFs from university domains into learning management systems for student access
- Newsrooms aggregating press release and briefing PDFs from agency websites for real-time reporting and sentiment analysis

```
import os
import sys
import re
import requests
from urllib.parse import urljoin

# Step 1: Ensure workspace directory exists
workspace = 'workspace'
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    sys.exit(1)
print(f"[INFO] Using workspace: {workspace}\n")

# Common headers for requests
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept': 'text/html'
}

# Utility to perform a Bing search and return HTML
def bing_search(query):
    url = 'https://www.bing.com/search'
    print(f"[SEARCH] Bing query: {query}")
    resp = requests.get(url, params={'q': query}, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    print(f"[SEARCH] Retrieved {len(resp.text)} characters of HTML")
    return resp.text

# Phase 1: Look for landing page on le.ac.uk containing the paper title
title_phrase = 'Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?'
phase1_query = f"site:le.ac.uk \"{title_phrase}\""
html1 = bing_search(phase1_query)

# Extract all href URLs from search results
hrefs1 = re.findall(r'href=["'](https?://[^"'> ]+)["']', html1)
le_urls = [u for u in hrefs1 if 'le.ac.uk' in u.lower()]
le_urls = list(dict.fromkeys(le_urls))  # dedupe preserving order
print(f"[PARSE] Found {len(le_urls)} candidate landing URLs on le.ac.uk:")
for i, u in enumerate(le_urls[:5], 1):
    print(f"  {i}. {u}")

pdf_url = None
# If we have a landing URL, fetch it and look for .pdf links
if le_urls:
    landing = le_urls[0]
    print(f"\n[PHASE1] Fetching landing page: {landing}")
    try:
        lp_resp = requests.get(landing, headers=HEADERS, timeout=30)
        lp_resp.raise_for_status()
        lp_html = lp_resp.text
        # Find PDF links on landing page (absolute or relative)
        pdfs_lp = re.findall(r'href=["']([^"'>]+\.pdf(?:\?[^"'>]*)?)["']', lp_html, flags=re.IGNORECASE)
        pdfs_lp = list(dict.fromkeys(pdfs_lp))
        print(f"[PHASE1] Found {len(pdfs_lp)} .pdf links on landing page.")
        for p in pdfs_lp[:5]:
            full = urljoin(landing, p)
            print(f"   -> {full}")
        if pdfs_lp:
            pdf_url = urljoin(landing, pdfs_lp[0])
    except Exception as e:
        print(f"ERROR: Failed to fetch/parse landing page: {e}")

# Phase 2: Fallback search directly for PDF if Phase 1 failed
if not pdf_url:
    print("\n[PHASE2] No PDF found on landing page. Falling back to broad PDF search.")
    phase2_query = f"\"{title_phrase}\" filetype:pdf"
    html2 = bing_search(phase2_query)
    # Extract all PDF URLs (with optional query strings)
    pattern = r"https?://[^\s'\">]+\.pdf(?:\?[^\s'\">]*)?"
    all_pdfs = re.findall(pattern, html2, flags=re.IGNORECASE)
    all_pdfs = list(dict.fromkeys(all_pdfs))
    print(f"[PARSE] Extracted {len(all_pdfs)} total PDF URLs from HTML.")
    for i, p in enumerate(all_pdfs[:10], 1):
        print(f"  {i}. {p}")
    if not all_pdfs:
        print("ERROR: No PDF URLs found in fallback search. Cannot proceed.")
        sys.exit(1)
    # Prefer PDFs on le.ac.uk but allow any
    le_pdfs = [p for p in all_pdfs if 'le.ac.uk' in p.lower()]
    if le_pdfs:
        print(f"[FILTER] {len(le_pdfs)} PDF URLs on le.ac.uk domain found in fallback.")
        pdf_url = le_pdfs[0]
    else:
        print("[FILTER] No le.ac.uk PDFs in fallback. Picking first available PDF URL.")
        pdf_url = all_pdfs[0]
    print(f"[SELECT] PDF URL chosen: {pdf_url}")

# Step: Download the chosen PDF
pdf_path = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')
print(f"\n[DOWNLOAD] Downloading PDF to: {pdf_path}")
try:
    r = requests.get(pdf_url, headers=HEADERS, stream=True, timeout=60)
    r.raise_for_status()
    with open(pdf_path, 'wb') as f:
        for chunk in r.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
    size = os.path.getsize(pdf_path)
    print(f"[SUCCESS] Saved PDF ({size} bytes, {size/1024:.2f} KB)")
    if size < 10 * 1024:
        print("WARNING: File smaller than 10 KB. It may not be the correct PDF.")
        sys.exit(1)
    print("[COMPLETE] PDF download successful and exceeds size threshold.")
except Exception as e:
    print(f"ERROR: Failed to download PDF: {e}")
    if os.path.exists(pdf_path):
        os.remove(pdf_path)
    sys.exit(1)
```