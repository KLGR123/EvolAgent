### Development Step 43: Scrape Survivor Seasons 1–44 Winners from Wikipedia and Save as JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated update of a fan site’s Survivor winners database for enhanced user engagement on reality TV forums
- Integration into a trivia app to dynamically generate questions and leaderboards based on Survivor season champions
- Data pipeline feed for a TV analytics dashboard tracking winner demographics and season performance over time
- Content enrichment for a podcast show notes generator, pulling the latest Survivor winners list for each episode
- Academic social dynamics research analyzing patterns among Reality TV show winners for publication in sociology journals
- Business intelligence reporting for a media agency comparing Survivor winner attributes against audience ratings
- Localization workflow for translating season winner names into multiple languages for international TV guide services
- AI chatbot knowledge base injection to answer user queries about Survivor champions across seasons 1 through 44

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# Ensure workspace exists
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# Fetch the Survivor Wikipedia page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching page: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
})
resp.raise_for_status()
soup = BeautifulSoup(resp.text, 'html.parser')

# Debug: list all H2/H3 headings with their ids and following tables count
debug_lines = []
for header_tag in soup.find_all(['h2','h3']):
    span = header_tag.find('span', class_='mw-headline')
    if not span:
        continue
    hid = span.get('id')
    text = span.get_text()
    # Count immediate <table> siblings until another header appears
    tbl_count = 0
    for sib in header_tag.find_next_siblings():
        if sib.name in ['h2','h3']:
            break
        if sib.name == 'table':
            tbl_count += 1
    debug_lines.append(f"{header_tag.name} id='{hid}' text='{text}' → {tbl_count} table(s) under this section")

# Save debug info
dbg_path = os.path.join(workspace_dir, 'survivor_debug_headings.txt')
with open(dbg_path, 'w', encoding='utf-8') as f:
    f.write("# Debug: H2/H3 headings and number of tables beneath each until next heading\n")
    f.write("\n".join(debug_lines))

print(f"✅ Debug info written to: {dbg_path}")
```