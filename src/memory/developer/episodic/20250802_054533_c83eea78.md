### Development Step 43: Search and download “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated retrieval of institutional research papers by university librarians for populating digital archives
- Bulk download of technical datasheets from electronics manufacturers for embedded system design validation
- Automated collection of ISO standards and regulatory compliance PDFs for corporate quality management systems
- Patent attorneys fetching patent specification documents into legal analysis platforms for prior art review
- Pharmaceutical researchers gathering drug safety and clinical trial PDF reports for meta-analysis studies
- Journalists sourcing public records and PDF transcripts from government sites for investigative reporting
- Financial analysts downloading investment research PDFs from rating agencies for portfolio performance modeling
- Environmental scientists collecting environmental impact assessment PDFs from agency websites for ecosystem risk studies

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup

# --- Configuration ---
workspace = 'workspace'
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# The paper title
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
# DuckDuckGo HTML search endpoint
ddg_url = 'https://html.duckduckgo.com/html/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# Queries to try: first restrict to Leicester domains, then broad
queries = [
    f'filetype:pdf "{title}" site:lra.le.ac.uk',
    f'filetype:pdf "{title}" site:le.ac.uk',
    f'filetype:pdf "{title}"'
]

# Paths
ddg_html = os.path.join(workspace, 'ddg_search.html')
pdf_list = os.path.join(workspace, 'ddg_pdf_urls.txt')
pdf_out = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

all_candidates = []

# Perform searches
for q in queries:
    print(f"[INFO] DuckDuckGo search query: {q}")
    resp = requests.post(ddg_url, data={'q': q}, headers=headers, timeout=30)
    print(f"[INFO] Response status: {resp.status_code}")
    if resp.status_code != 200:
        print(f"[WARN] Non-200 status for query '{q}', skipping.")
        continue

    # Save the first search HTML for inspection
    if not os.path.exists(ddg_html):
        with open(ddg_html, 'w', encoding='utf-8') as f:
            f.write(resp.text)
        print(f"[SAVE] DuckDuckGo HTML → {ddg_html}")

    html = resp.text
    soup = BeautifulSoup(html, 'html.parser')
    candidates = set()

    # 1) Extract anchors with href containing .pdf
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '.pdf' in href.lower():
            candidates.add(href)
            print(f"    [FOUND] PDF link in anchor: {href}")

    # 2) Regex fallback on raw HTML
    pattern = r'https?://[^\s"']+?\.pdf(?:\?[^\s"']*)?'
    for match in re.findall(pattern, html, flags=re.IGNORECASE):
        if match not in candidates:
            candidates.add(match)
            print(f"    [FOUND] PDF via regex: {match}")

    # Normalize and collect full URLs
    for link in candidates:
        # DuckDuckGo sometimes wraps real URLs in '/l/?kh=-1&uddg='
        if 'uddg=' in link:
            # Extract after uddg=
            real = re.search(r'uddg=(.+)', link)
            if real:
                url = requests.utils.unquote(real.group(1))
            else:
                url = link
        else:
            url = link
        all_candidates.append(url)

    if all_candidates:
        print(f"[INFO] Found {len(all_candidates)} PDF candidate(s). Stopping further queries.")
        break
    else:
        print(f"[INFO] No PDF links found for query '{q}'. Trying next query.\n")

# Save all candidates
if all_candidates:
    unique = []
    for u in all_candidates:
        if u not in unique:
            unique.append(u)
    with open(pdf_list, 'w', encoding='utf-8') as f:
        for u in unique:
            f.write(u + '\n')
    print(f"[SAVE] Candidate PDF URLs → {pdf_list}")
else:
    print("[ERROR] No PDF candidates found after all queries.")
    print(f"Please inspect '{ddg_html}' and adjust the query or parsing logic.")
    sys.exit(1)

# Download the first candidate
first = unique[0]
print(f"[INFO] Downloading PDF from: {first}")
try:
    dl = requests.get(first, headers=headers, timeout=60)
    print(f"[INFO] Download status: {dl.status_code}")
except Exception as e:
    print(f"[ERROR] Failed to download PDF: {e}")
    sys.exit(1)

if dl.status_code == 200 and len(dl.content) > 10_000:
    with open(pdf_out, 'wb') as f:
        f.write(dl.content)
    size = os.path.getsize(pdf_out)
    print(f"[SUCCESS] Saved PDF → {pdf_out} ({size} bytes)")
else:
    print("[ERROR] Download failed or file too small (<10 KB).")
    print(f"Content length: {len(dl.content)} bytes")
    print(f"Inspect '{pdf_list}', pick another link, and try manually.")
    sys.exit(1)

```