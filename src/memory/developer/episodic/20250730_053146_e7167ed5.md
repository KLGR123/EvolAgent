### Development Step 5: Retrieve and Save ‘A Dark Trace’ from MUSE, Extract Chapter 2 to Identify Freud’s Influencer

**Description**: Access and download the full text of 'A Dark Trace: Sigmund Freud on the Sense of Guilt' by H. Westerink from Project MUSE using DOI 10.1353/book.24372. Since the book was confirmed to be open access, retrieve the complete text and save it to workspace/dark_trace_freud_book.pdf or appropriate format. Focus on locating and extracting Chapter 2 content to identify the author who influenced Freud's belief in 'endopsychic myths'. If the full book is not directly downloadable, extract Chapter 2 specifically or access the book's table of contents to determine the exact chapter title and content structure.

**Use Cases**:
- Automated academic literature mining and influence mapping for psychoanalysis researchers by extracting Chapter 2 of H. Westerink’s “A Dark Trace” to identify key thinkers behind Freud’s endopsychic myths
- Digital humanities corpus creation and semantic network analysis of 19th- and 20th-century psychology texts by scraping open-access Project MUSE monographs and parsing chapter structures
- Corporate compliance and policy extraction workflow that retrieves specific legal or ethical guidelines from open-access governance textbooks to populate internal risk management databases
- AI training dataset generation for natural language models by bulk‐downloading and normalizing open-access scholarly book chapters, then tagging influential authors and key concepts for fine-tuning
- University library archiving automation to ingest, tag, and store open-access academic books (including chapter metadata) in institutional repositories for long-term preservation
- Pharmaceutical regulatory monitoring by programmatically extracting relevant sections from open-access pharmacology and clinical trial methodology texts to ensure compliance with updated guidelines
- Marketing competitive intelligence and trend analysis through automated scraping of business strategy book chapters on consumer behavior, then performing keyword frequency and sentiment analysis
- Government policy analysis and legislative review automation by sourcing open-access public administration and law volumes from Project MUSE, extracting chapter content, and identifying policy influencers

```
import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

print('=== EXAMINING EXTRACTED CHAPTER CONTENT AND FIXING CONTENT EXTRACTION ===')
print('Objective: Analyze the minimal content extracted and improve parsing to get actual chapter text\n')

# First, let's inspect the saved chapter content file structure
chapter_file = 'workspace/chapter_2_dark_traces_content.json'

if os.path.exists(chapter_file):
    print('=== INSPECTING SAVED CHAPTER CONTENT FILE STRUCTURE ===')
    
    with open(chapter_file, 'r', encoding='utf-8') as f:
        chapter_data = json.load(f)
    
    print(f'Chapter data keys: {list(chapter_data.keys())}')
    
    for key, value in chapter_data.items():
        if isinstance(value, str) and len(value) > 100:
            print(f'{key}: String with {len(value):,} characters')
            print(f'  Preview: "{value[:100]}..."')
        elif isinstance(value, dict):
            print(f'{key}: Dictionary with {len(value)} keys: {list(value.keys())}')
        elif isinstance(value, list):
            print(f'{key}: List with {len(value)} items')
        else:
            print(f'{key}: {value}')
    
    print(f'\nFull extracted text (first 200 chars): "{chapter_data.get("full_text", "")[:200]}..."')
    
    if len(chapter_data.get('full_text', '')) < 100:
        print('\n*** CONTENT EXTRACTION FAILED - Only extracted navigation/interface elements ***')
        print('Need to fix the content parsing logic for Project MUSE structure')
    
else:
    print(f'Chapter content file not found: {chapter_file}')
    print('Available workspace files:')
    if os.path.exists('workspace'):
        for file in os.listdir('workspace'):
            print(f'  - {file}')

print('\n=== IMPROVED PROJECT MUSE CONTENT EXTRACTION ===\n')

# Since we have successful chapter URLs, let's try with better content extraction logic
chapter_url = 'https://muse.jhu.edu/book/24372/chapter/2'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Cookie': 'muse_session=true',  # Try to simulate session
    'Referer': 'https://muse.jhu.edu/book/24372'
}

print(f'Re-accessing chapter URL with improved parsing: {chapter_url}')

try:
    response = requests.get(chapter_url, headers=headers, timeout=30)
    print(f'Status: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    print(f'Content type: {response.headers.get("Content-Type", "unknown")}')
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Get page title for verification
        page_title = soup.find('title')
        if page_title:
            print(f'Page title: {page_title.get_text().strip()}')
        
        # Save the raw HTML for inspection
        html_file = 'workspace/chapter_2_raw_html.html'
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f'Raw HTML saved to: {html_file}')
        
        # Try multiple content extraction strategies specifically for Project MUSE
        print('\n=== TRYING MULTIPLE CONTENT EXTRACTION STRATEGIES ===')
        
        content_strategies = [
            # Strategy 1: Look for main content containers
            ('Main content div', 'div#main-content'),
            ('Content wrapper', 'div.content-wrapper'),
            ('Article content', 'article'),
            ('Main article', 'main'),
            
            # Strategy 2: Project MUSE specific selectors
            ('MUSE content', 'div.muse-content'),
            ('Book content', 'div.book-content'),
            ('Chapter text', 'div.chapter-text'),
            ('Text body', 'div.text-body'),
            
            # Strategy 3: Generic text containers
            ('Content div', 'div.content'),
            ('Text div', 'div.text'),
            ('Body content', 'div.body'),
            ('Reader content', 'div.reader'),
            
            # Strategy 4: Paragraph-based extraction
            ('All paragraphs', 'p'),
            ('Main section paragraphs', 'section p'),
            ('Article paragraphs', 'article p'),
            
            # Strategy 5: Look for specific text indicators
            ('Text containing "dark"', '*:contains("dark")'),
            ('Text containing "Freud"', '*:contains("Freud")'),
            ('Text containing "myth"', '*:contains("myth")')
        ]
        
        successful_extractions = []
        
        for strategy_name, selector in content_strategies:
            try:
                if ':contains' in selector:
                    # Skip contains selectors for now due to deprecation
                    continue
                    
                elements = soup.select(selector)
                
                if elements:
                    if selector == 'p':
                        # For paragraph strategy, combine all paragraphs
                        combined_text = '\n\n'.join([elem.get_text().strip() for elem in elements if elem.get_text().strip()])
                    else:
                        # For container strategies, use the largest element
                        largest_elem = max(elements, key=lambda x: len(x.get_text()))
                        combined_text = largest_elem.get_text().strip()
                    
                    if len(combined_text) > 200:  # Only consider substantial content
                        successful_extractions.append({
                            'strategy': strategy_name,
                            'selector': selector,
                            'text_length': len(combined_text),
                            'content': combined_text,
                            'element_count': len(elements)
                        })
                        
                        print(f'✓ {strategy_name} ({selector}): {len(combined_text):,} chars from {len(elements)} elements')
                        print(f'  Preview: "{combined_text[:150]}..."')
                    else:
                        print(f'✗ {strategy_name} ({selector}): Only {len(combined_text)} chars (too short)')
                        
                else:
                    print(f'✗ {strategy_name} ({selector}): No elements found')
                    
            except Exception as e:
                print(f'✗ {strategy_name} ({selector}): Error - {str(e)}')
        
        print(f'\n=== CONTENT EXTRACTION RESULTS ===')
        print(f'Found {len(successful_extractions)} successful content extraction strategies')
        
        if successful_extractions:
            # Sort by content length and use the longest one
            best_extraction = max(successful_extractions, key=lambda x: x['text_length'])
            
            print(f'\nBest strategy: {best_extraction["strategy"]} ({best_extraction["selector"]})')
            print(f'Content length: {best_extraction["text_length"]:,} characters')
            print(f'Element count: {best_extraction["element_count"]}')
            
            chapter_text = best_extraction['content']
            
            # Search for key terms in the extracted content
            print(f'\n=== SEARCHING FOR KEY TERMS IN EXTRACTED CONTENT ===')
            
            search_terms = [
                'endopsychic myth',
                'endopsychic',
                'myth',
                'mythology',
                'jung',
                'carl jung',
                'nietzsche',
                'schopenhauer',
                'kant',
                'darwin',
                'influenced',
                'influence'
            ]
            
            found_terms = {}
            for term in search_terms:
                count = chapter_text.lower().count(term.lower())
                if count > 0:
                    found_terms[term] = count
                    print(f'Found "{term}": {count} occurrences')
            
            if found_terms:
                print(f'\n=== EXTRACTING RELEVANT PASSAGES FOR ENDOPSYCHIC MYTHS ===')
                
                # Look specifically for "endopsychic" passages
                endopsychic_found = False
                for term in ['endopsychic', 'endopsychic myth']:
                    if term in found_terms:
                        print(f'\nExtracting passages containing "{term}":')
                        
                        text_lower = chapter_text.lower()
                        positions = []
                        start = 0
                        while True:
                            pos = text_lower.find(term.lower(), start)
                            if pos == -1:
                                break
                            positions.append(pos)
                            start = pos + 1
                        
                        for i, pos in enumerate(positions, 1):
                            # Extract larger context around the term
                            context_start = max(0, pos - 500)
                            context_end = min(len(chapter_text), pos + 600)
                            context = chapter_text[context_start:context_end]
                            
                            print(f'\nPassage {i} (position {pos}):')  
                            print(f'{"="*80}')
                            print(context)
                            print(f'{"="*80}')
                            
                            # Look for author names in this passage
                            passage_lower = context.lower()
                            potential_authors = ['jung', 'nietzsche', 'schopenhauer', 'kant', 'darwin', 'hegel']
                            
                            for author in potential_authors:
                                if author in passage_lower:
                                    print(f'*** POTENTIAL INFLUENCE FOUND: {author.upper()} mentioned in this passage ***')
                        
                        endopsychic_found = True
                        break
                
                if not endopsychic_found:
                    print('No "endopsychic" references found. Looking for other relevant terms...')
                    
                    # Look for any author influences mentioned
                    for term in ['influenced', 'influence', 'jung', 'nietzsche']:
                        if term in found_terms:
                            print(f'\nExtracting passages containing "{term}":')
                            
                            text_lower = chapter_text.lower()
                            positions = []
                            start = 0
                            while True:
                                pos = text_lower.find(term.lower(), start)
                                if pos == -1:
                                    break
                                positions.append(pos)
                                start = pos + 1
                            
                            # Show first few occurrences
                            for i, pos in enumerate(positions[:3], 1):
                                context_start = max(0, pos - 300)
                                context_end = min(len(chapter_text), pos + 400)
                                context = chapter_text[context_start:context_end]
                                
                                print(f'\n{term.title()} passage {i}:')
                                print(f'{"-"*60}')
                                print(context)
                                print(f'{"-"*60}')
                            
                            break
            else:
                print('No key terms found in extracted content.')
                print('Content may not be the actual chapter text.')
                print(f'Content preview (first 1000 chars): {chapter_text[:1000]}...')
            
            # Save the improved extraction
            improved_chapter_data = {
                'source_url': chapter_url,
                'extraction_strategy': best_extraction['strategy'],
                'selector_used': best_extraction['selector'],
                'chapter_title': 'Chapter 2: Dark Traces',
                'content_length': len(chapter_text),
                'full_text': chapter_text,
                'search_terms_found': found_terms,
                'all_strategies_tried': [{
                    'strategy': ext['strategy'],
                    'text_length': ext['text_length']
                } for ext in successful_extractions],
                'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            improved_file = 'workspace/chapter_2_improved_extraction.json'
            with open(improved_file, 'w', encoding='utf-8') as f:
                json.dump(improved_chapter_data, f, indent=2, ensure_ascii=False)
            
            print(f'\n*** IMPROVED CHAPTER EXTRACTION SAVED ***')
            print(f'File: {improved_file}')
            print(f'Content length: {len(chapter_text):,} characters')
            print(f'Search terms found: {len(found_terms)}')
            
        else:
            print('\n*** NO SUBSTANTIAL CONTENT EXTRACTED ***')
            print('The page may require authentication or use dynamic loading')
            
            # Let's examine the HTML structure more closely
            print('\n=== ANALYZING HTML STRUCTURE ===')
            
            # Look for any text that might indicate the content is behind a paywall/login
            page_text = soup.get_text().lower()
            access_indicators = [
                'login', 'sign in', 'subscription', 'access', 'paywall',
                'institutional', 'authenticate', 'purchase', 'buy'
            ]
            
            access_issues = []
            for indicator in access_indicators:
                if indicator in page_text:
                    access_issues.append(indicator)
            
            if access_issues:
                print(f'Access restriction indicators found: {access_issues}')
                print('The content may be behind authentication despite open access status')
            
            # Look for any JavaScript that might be loading content dynamically
            script_tags = soup.find_all('script')
            print(f'Found {len(script_tags)} script tags - content may be loaded dynamically')
            
            # Check for any data attributes or hidden content
            hidden_content = soup.find_all(attrs={'style': lambda x: x and 'display:none' in x.replace(' ', '')})
            if hidden_content:
                print(f'Found {len(hidden_content)} hidden elements that might contain content')
    
    else:
        print(f'Failed to access chapter URL: {response.status_code}')
        print(f'Response text preview: {response.text[:300]}...')

except Exception as e:
    print(f'Error during improved content extraction: {str(e)}')

print('\n=== CONTENT EXTRACTION ANALYSIS COMPLETE ===')
print('Status: Attempted multiple strategies to extract Chapter 2 content')
print('Objective: Find the author who influenced Freud\'s concept of "endopsychic myths"')
print('Next: Analyze results and try alternative access methods if needed')
```