### Development Step 2: Title: Alphabetize and Identify the Fifth Federal Rules Category from Cornell LIIâ€™s Federal Rules Section

**Description**: Navigate to Cornell Law School's Legal Information Institute website and locate the federal rules section. Identify all available federal rules categories, arrange them alphabetically, and determine which category represents the fifth section. Extract the complete list of federal rules categories to establish the alphabetical ordering and identify the target section for further analysis.

**Use Cases**:
- Automated legal research tool development for attorneys needing up-to-date, categorized access to federal rules for case preparation
- Academic curriculum design for law professors creating syllabi that require structured lists of federal rules sections in teaching materials
- Compliance monitoring systems for corporate legal departments to track changes or additions to specific federal rules categories
- Legal publishing workflow automation for editors who need to verify, sort, and reference all federal rules categories in annotated legal texts
- Data-driven policy analysis by government agencies examining the breadth and structure of federal procedural rules for reform initiatives
- AI-powered chatbot training for legal tech startups to enable accurate, section-specific responses about federal rules for end users

```
import requests
from bs4 import BeautifulSoup
import os
import json
from datetime import datetime
import re

print('=== ACCESSING CORNELL LAW FEDERAL RULES SECTION ===') 
print('Objective: Access the federal rules page and extract all categories')
print('Goal: Identify all federal rules categories, sort alphabetically, find 5th section\n')

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# First, let's inspect the existing homepage analysis file to understand the structure
print('=== STEP 1: INSPECTING EXISTING ANALYSIS DATA ===')
homepage_file = os.path.join(workspace_dir, 'cornell_law_homepage_analysis.json')

if os.path.exists(homepage_file):
    print(f'Found existing homepage analysis: {homepage_file}')
    
    with open(homepage_file, 'r') as f:
        homepage_data = json.load(f)
    
    print('Homepage analysis structure:')
    for key, value in homepage_data.items():
        if isinstance(value, list):
            print(f'  {key}: List with {len(value)} items')
        elif isinstance(value, dict):
            print(f'  {key}: Dictionary with {len(value)} keys')
        else:
            print(f'  {key}: {value}')
    
    main_federal_rules_url = homepage_data.get('main_federal_rules_url')
    print(f'\nMain federal rules URL from analysis: {main_federal_rules_url}')
else:
    print('No existing homepage analysis found, using direct URL')
    main_federal_rules_url = 'https://www.law.cornell.edu/rules'

print(f'\n=== STEP 2: ACCESSING FEDERAL RULES PAGE ===')
print(f'Target URL: {main_federal_rules_url}')

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

try:
    response = requests.get(main_federal_rules_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f'Successfully accessed federal rules page')
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    
    # Parse the federal rules page
    soup = BeautifulSoup(response.content, 'html.parser')
    
    page_title = soup.find('title').get_text() if soup.find('title') else 'No title found'
    print(f'Page title: {page_title}')
    
    # Save the raw HTML content for inspection
    html_file = os.path.join(workspace_dir, 'cornell_federal_rules_page.html')
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(response.text)
    print(f'Raw HTML saved to: {html_file}')
    
    print('\n=== STEP 3: EXTRACTING FEDERAL RULES CATEGORIES ===')
    
    # Look for the main content area and lists of federal rules
    federal_rules_categories = []
    
    # Strategy 1: Look for lists or navigation elements containing federal rules
    print('Strategy 1: Searching for list elements and navigation...')
    
    # Find all list items, links, and potential category containers
    potential_containers = soup.find_all(['ul', 'ol', 'div', 'nav', 'section'], class_=re.compile(r'rule|federal|nav|menu|list|content', re.I))
    
    print(f'Found {len(potential_containers)} potential category containers')
    
    for i, container in enumerate(potential_containers[:10], 1):  # Examine first 10
        print(f'\nContainer {i}: {container.name} with class="{container.get("class", [])}"')
        
        # Look for links within this container
        links = container.find_all('a', href=True)
        print(f'  Contains {len(links)} links')
        
        for link in links[:5]:  # Show first 5 links
            link_text = link.get_text().strip()
            href = link.get('href')
            if 'rule' in link_text.lower() or 'rule' in href.lower():
                print(f'    Rule-related link: "{link_text}" -> {href}')
    
    # Strategy 2: Look for all links containing "rules" in text or href
    print('\nStrategy 2: Searching for all rule-related links...')
    
    all_links = soup.find_all('a', href=True)
    rule_links = []
    
    for link in all_links:
        link_text = link.get_text().strip()
        href = link.get('href')
        
        # Check if this is a federal rules category
        if ('federal rules' in link_text.lower() or 
            'rules of' in link_text.lower() or
            '/rules/' in href.lower()):
            
            # Clean up the link text and href
            clean_text = re.sub(r'\s+', ' ', link_text).strip()
            
            if clean_text and len(clean_text) > 5:  # Filter out very short or empty links
                rule_links.append({
                    'text': clean_text,
                    'href': href,
                    'full_url': href if href.startswith('http') else 'https://www.law.cornell.edu' + href
                })
    
    print(f'Found {len(rule_links)} rule-related links:')
    
    # Remove duplicates based on text
    seen_texts = set()
    unique_rule_links = []
    
    for link in rule_links:
        if link['text'] not in seen_texts:
            seen_texts.add(link['text'])
            unique_rule_links.append(link)
    
    print(f'After removing duplicates: {len(unique_rule_links)} unique rule categories')
    
    for i, link in enumerate(unique_rule_links, 1):
        print(f'{i}. "{link["text"]}" -> {link["href"]}')
    
    # Strategy 3: Look for specific patterns in page content
    print('\nStrategy 3: Searching page content for federal rules patterns...')
    
    page_text = soup.get_text()
    
    # Look for "Federal Rules of [Something]" patterns
    federal_rules_patterns = re.findall(r'Federal Rules of [A-Za-z\s]+(?:Procedure|Evidence|Practice)', page_text, re.IGNORECASE)
    
    print(f'Found {len(federal_rules_patterns)} "Federal Rules of..." patterns:')
    for pattern in set(federal_rules_patterns):  # Remove duplicates
        print(f'  - {pattern}')
    
    # Combine all findings to create comprehensive category list
    print('\n=== STEP 4: CONSOLIDATING FEDERAL RULES CATEGORIES ===')
    
    all_categories = set()
    
    # Add from unique rule links
    for link in unique_rule_links:
        category_text = link['text']
        # Clean up category names
        if 'federal rules of' in category_text.lower():
            all_categories.add(category_text)
        elif 'rules of' in category_text.lower():
            all_categories.add(category_text)
    
    # Add from pattern matches
    for pattern in set(federal_rules_patterns):
        all_categories.add(pattern)
    
    # Convert to list and clean up
    categories_list = []
    for category in all_categories:
        # Standardize format
        clean_category = re.sub(r'\s+', ' ', category).strip()
        if len(clean_category) > 10:  # Filter out very short entries
            categories_list.append(clean_category)
    
    print(f'Total consolidated categories: {len(categories_list)}')
    
    for i, category in enumerate(categories_list, 1):
        print(f'{i}. {category}')
    
    # Sort alphabetically
    print('\n=== STEP 5: SORTING CATEGORIES ALPHABETICALLY ===')
    
    sorted_categories = sorted(categories_list, key=str.lower)
    
    print(f'Federal Rules Categories in Alphabetical Order:')
    print('=' * 60)
    
    for i, category in enumerate(sorted_categories, 1):
        print(f'{i}. {category}')
    
    # Identify the fifth category
    fifth_category = None
    if len(sorted_categories) >= 5:
        fifth_category = sorted_categories[4]  # Index 4 for 5th item
        print(f'\n*** THE FIFTH FEDERAL RULES CATEGORY (ALPHABETICALLY): ***')
        print(f'5. {fifth_category}')
    else:
        print(f'\nWarning: Only found {len(sorted_categories)} categories, cannot identify fifth category')
    
    # Save comprehensive analysis
    analysis_results = {
        'analysis_date': datetime.now().isoformat(),
        'source_url': main_federal_rules_url,
        'page_title': page_title,
        'total_links_found': len(all_links),
        'rule_related_links_found': len(rule_links),
        'unique_rule_categories_found': len(unique_rule_links),
        'pattern_matches_found': len(set(federal_rules_patterns)),
        'consolidated_categories_count': len(categories_list),
        'all_federal_rules_categories': categories_list,
        'categories_alphabetically_sorted': sorted_categories,
        'fifth_category_alphabetically': fifth_category,
        'detailed_rule_links': unique_rule_links,
        'pattern_matches': list(set(federal_rules_patterns))
    }
    
    results_file = os.path.join(workspace_dir, 'cornell_federal_rules_categories_analysis.json')
    with open(results_file, 'w') as f:
        json.dump(analysis_results, f, indent=2)
    
    print(f'\nComprehensive analysis saved to: {results_file}')
    
    print('\n=== ANALYSIS COMPLETE ===')
    print(f'Successfully identified {len(sorted_categories)} federal rules categories')
    if fifth_category:
        print(f'Fifth category alphabetically: {fifth_category}')
    
except requests.exceptions.RequestException as e:
    print(f'Error accessing federal rules page: {e}')
except Exception as e:
    print(f'Unexpected error during analysis: {e}')
    import traceback
    traceback.print_exc()
```