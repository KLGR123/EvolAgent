### Development Step 3: US Federal Minimum Butterfat Percentage Required for Ice Cream Classification (2020 Wikipedia Data)

**Description**: Research the US federal standards for butterfat content in ice cream as reported by Wikipedia in 2020. Search for the specific minimum butterfat percentage required by federal regulations for a product to be legally classified as ice cream in the United States. Extract the exact percentage value and any relevant context about these standards.

**Use Cases**:
- Regulatory compliance verification for food manufacturers ensuring their ice cream products meet US federal butterfat standards before market release
- Automated quality assurance checks in dairy production facilities to validate product labeling against legal ice cream definitions
- Food import/export documentation review for customs brokers to confirm imported ice cream products comply with US classification requirements
- Academic research on historical changes in food standards, using extracted butterfat regulations as part of a longitudinal analysis
- Consumer advocacy investigations to identify and report brands mislabeling frozen desserts as ice cream without meeting federal butterfat minimums
- Development of nutrition and ingredient databases for food delivery apps, ensuring accurate product categorization based on federal standards
- Legal case preparation for attorneys representing clients in food labeling disputes, using extracted Wikipedia data as supporting evidence
- Automated content curation for food bloggers or journalists reporting on industry trends and regulatory updates in the US ice cream market

```
import requests
from bs4 import BeautifulSoup
import os
import json
from datetime import datetime
import re

print("=== DIRECT ACCESS TO WIKIPEDIA ICE CREAM PAGE FOR BUTTERFAT STANDARDS ===")
print("Objective: Find US federal minimum butterfat percentage for ice cream classification")
print("Strategy: Direct Wikipedia page access using correct API endpoints\n")

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# First, let's try the correct Wikipedia API endpoint to get the Ice cream page
print("=== STEP 1: ACCESSING WIKIPEDIA ICE CREAM PAGE DIRECTLY ===")

try:
    # Use the correct Wikipedia API endpoint
    api_url = 'https://en.wikipedia.org/w/api.php'
    
    # Get the Ice cream page content
    params = {
        'action': 'query',
        'format': 'json',
        'titles': 'Ice cream',
        'prop': 'extracts',
        'exintro': False,  # Get full content
        'explaintext': True,  # Get plain text
        'exsectionformat': 'wiki'
    }
    
    print("Requesting Ice cream page from Wikipedia...")
    response = requests.get(api_url, params=params, timeout=30)
    response.raise_for_status()
    
    data = response.json()
    print(f"API response received (Status: {response.status_code})")
    
    # Extract page content
    if 'query' in data and 'pages' in data['query']:
        pages = data['query']['pages']
        
        for page_id, page_info in pages.items():
            if 'extract' in page_info:
                page_title = page_info.get('title', 'Unknown')
                full_text = page_info['extract']
                
                print(f"\nSuccessfully retrieved: '{page_title}'")
                print(f"Content length: {len(full_text):,} characters")
                
                # Save the full Wikipedia content for reference
                wiki_content_file = os.path.join(workspace_dir, 'wikipedia_ice_cream_full_content.txt')
                with open(wiki_content_file, 'w', encoding='utf-8') as f:
                    f.write(f"WIKIPEDIA ICE CREAM PAGE CONTENT\n")
                    f.write(f"Retrieved: {datetime.now().isoformat()}\n")
                    f.write(f"Page: {page_title}\n")
                    f.write(f"Content Length: {len(full_text):,} characters\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(full_text)
                
                print(f"Full content saved to: {wiki_content_file}")
                
                # Now search for butterfat content information
                print("\n=== STEP 2: ANALYZING CONTENT FOR BUTTERFAT STANDARDS ===")
                
                # Convert to lowercase for case-insensitive searching
                text_lower = full_text.lower()
                
                # Look for butterfat-related content
                if 'butterfat' in text_lower:
                    print("*** BUTTERFAT CONTENT FOUND ***")
                    
                    # Split into sentences for detailed analysis
                    sentences = full_text.split('.')
                    
                    butterfat_sentences = []
                    federal_standard_sentences = []
                    
                    for sentence in sentences:
                        sentence_clean = sentence.strip()
                        sentence_lower = sentence_clean.lower()
                        
                        # Look for sentences containing butterfat
                        if 'butterfat' in sentence_lower:
                            butterfat_sentences.append(sentence_clean)
                            
                            # Check if it mentions federal standards, FDA, or regulations
                            if any(keyword in sentence_lower for keyword in ['federal', 'fda', 'regulation', 'standard', 'minimum', 'require']):
                                federal_standard_sentences.append(sentence_clean)
                    
                    print(f"\nSentences mentioning butterfat: {len(butterfat_sentences)}")
                    print(f"Sentences about federal standards: {len(federal_standard_sentences)}")
                    
                    # Display butterfat sentences
                    if butterfat_sentences:
                        print("\n=== BUTTERFAT CONTENT ANALYSIS ===")
                        
                        for i, sentence in enumerate(butterfat_sentences, 1):
                            print(f"\n{i}. {sentence}")
                            
                            # Extract percentage values from each sentence
                            percentage_patterns = [
                                r'(\d+(?:\.\d+)?)\s*(?:percent|%)',
                                r'(\d+(?:\.\d+)?)\s*(?:per cent)',
                                r'(\d+(?:\.\d+)?)\s*(?:pct)'
                            ]
                            
                            found_percentages = []
                            for pattern in percentage_patterns:
                                matches = re.findall(pattern, sentence, re.IGNORECASE)
                                found_percentages.extend(matches)
                            
                            if found_percentages:
                                print(f"   Percentages found: {found_percentages}")
                                
                                # Check for context indicating minimum federal standard
                                if any(keyword in sentence.lower() for keyword in ['minimum', 'federal', 'fda', 'standard', 'regulation', 'require']):
                                    print(f"   *** POTENTIAL FEDERAL STANDARD: {found_percentages} ***")
                    
                    # Focus on federal standard sentences
                    if federal_standard_sentences:
                        print("\n=== FEDERAL STANDARD SENTENCES ===")
                        
                        federal_standards_found = []
                        
                        for i, sentence in enumerate(federal_standard_sentences, 1):
                            print(f"\n{i}. {sentence}")
                            
                            # Extract percentages from federal standard sentences
                            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*(?:percent|%)', sentence, re.IGNORECASE)
                            
                            if percentages:
                                print(f"   Federal standard percentages: {percentages}")
                                
                                federal_standards_found.append({
                                    'sentence': sentence,
                                    'percentages': percentages,
                                    'context': 'federal_standard'
                                })
                        
                        # Save federal standards analysis
                        if federal_standards_found:
                            standards_file = os.path.join(workspace_dir, 'federal_butterfat_standards.json')
                            with open(standards_file, 'w') as f:
                                json.dump({
                                    'analysis_date': datetime.now().isoformat(),
                                    'source': 'Wikipedia Ice cream page',
                                    'objective': 'US federal minimum butterfat percentage for ice cream',
                                    'federal_standards_found': len(federal_standards_found),
                                    'standards_data': federal_standards_found,
                                    'all_butterfat_sentences': butterfat_sentences
                                }, f, indent=2)
                            
                            print(f"\nFederal standards analysis saved to: {standards_file}")
                            
                            # Extract the most likely federal minimum percentage
                            print("\n=== FEDERAL MINIMUM BUTTERFAT PERCENTAGE EXTRACTION ===")
                            
                            all_federal_percentages = []
                            for standard in federal_standards_found:
                                all_federal_percentages.extend(standard['percentages'])
                            
                            if all_federal_percentages:
                                # Convert to float and find common values
                                percentage_values = []
                                for pct in all_federal_percentages:
                                    try:
                                        percentage_values.append(float(pct))
                                    except ValueError:
                                        continue
                                
                                if percentage_values:
                                    unique_percentages = list(set(percentage_values))
                                    print(f"Unique federal percentages found: {unique_percentages}")
                                    
                                    # Look for the most commonly mentioned percentage
                                    from collections import Counter
                                    percentage_counts = Counter(percentage_values)
                                    most_common = percentage_counts.most_common(1)
                                    
                                    if most_common:
                                        federal_minimum = most_common[0][0]
                                        frequency = most_common[0][1]
                                        
                                        print(f"\n*** FEDERAL MINIMUM BUTTERFAT PERCENTAGE: {federal_minimum}% ***")
                                        print(f"Mentioned {frequency} time(s) in federal standard contexts")
                                        
                                        # Find the specific sentence with this percentage
                                        for standard in federal_standards_found:
                                            if str(federal_minimum) in standard['percentages'] or str(int(federal_minimum)) in standard['percentages']:
                                                print(f"\nSource sentence: {standard['sentence']}")
                                                break
                                        
                                        # Save the final result
                                        result_file = os.path.join(workspace_dir, 'us_federal_ice_cream_butterfat_standard.json')
                                        with open(result_file, 'w') as f:
                                            json.dump({
                                                'analysis_date': datetime.now().isoformat(),
                                                'source': 'Wikipedia Ice cream page (2020 information)',
                                                'federal_minimum_butterfat_percentage': federal_minimum,
                                                'percentage_unit': 'percent',
                                                'context': 'US federal regulations for ice cream classification',
                                                'frequency_mentioned': frequency,
                                                'supporting_evidence': [s['sentence'] for s in federal_standards_found if str(federal_minimum) in s['percentages'] or str(int(federal_minimum)) in s['percentages']],
                                                'all_federal_percentages_found': unique_percentages
                                            }, f, indent=2)
                                        
                                        print(f"\nFinal result saved to: {result_file}")
                                        
                                        print(f"\n=== PLAN OBJECTIVE COMPLETED ===")
                                        print(f"US Federal Minimum Butterfat Content for Ice Cream: {federal_minimum}%")
                                        print(f"Source: Wikipedia (2020 information)")
                                        print(f"Context: Federal regulations for legal ice cream classification")
                
                else:
                    print("No butterfat content found in the Wikipedia Ice cream page.")
                    print("Searching for alternative terms...")
                    
                    # Search for alternative terms
                    alternative_terms = ['fat content', 'milk fat', 'dairy fat', 'cream content', 'fat percentage']
                    
                    for term in alternative_terms:
                        if term in text_lower:
                            print(f"Found alternative term: '{term}'")
                            
                            # Extract sentences with alternative terms
                            sentences = full_text.split('.')
                            relevant_sentences = []
                            
                            for sentence in sentences:
                                if term in sentence.lower():
                                    relevant_sentences.append(sentence.strip())
                            
                            if relevant_sentences:
                                print(f"Sentences with '{term}': {len(relevant_sentences)}")
                                for i, sentence in enumerate(relevant_sentences[:3], 1):  # Show first 3
                                    print(f"  {i}. {sentence[:200]}{'...' if len(sentence) > 200 else ''}")
            else:
                print("No content extract available from the Wikipedia page.")
    else:
        print("Error: Could not retrieve Wikipedia page data.")
        print(f"API response structure: {list(data.keys()) if isinstance(data, dict) else 'Not a dictionary'}")

except requests.exceptions.RequestException as e:
    print(f"Error accessing Wikipedia API: {e}")
    print("Will try alternative approach...")
    
    # Alternative approach: Direct HTML scraping
    print("\n=== ALTERNATIVE APPROACH: DIRECT HTML SCRAPING ===")
    
    try:
        # Direct access to Wikipedia Ice cream page
        wiki_url = 'https://en.wikipedia.org/wiki/Ice_cream'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(f"Accessing Wikipedia Ice cream page directly: {wiki_url}")
        response = requests.get(wiki_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print(f"Successfully accessed Wikipedia page (Status: {response.status_code})")
        print(f"Content length: {len(response.content):,} bytes")
        
        # Parse HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract text content
        page_text = soup.get_text()
        
        print(f"Extracted text length: {len(page_text):,} characters")
        
        # Search for butterfat content
        if 'butterfat' in page_text.lower():
            print("\n*** BUTTERFAT CONTENT FOUND VIA HTML SCRAPING ***")
            
            # Split into paragraphs for analysis
            paragraphs = page_text.split('\n')
            
            butterfat_paragraphs = []
            for paragraph in paragraphs:
                if 'butterfat' in paragraph.lower() and len(paragraph.strip()) > 20:
                    butterfat_paragraphs.append(paragraph.strip())
            
            print(f"Paragraphs mentioning butterfat: {len(butterfat_paragraphs)}")
            
            for i, paragraph in enumerate(butterfat_paragraphs, 1):
                print(f"\n{i}. {paragraph[:300]}{'...' if len(paragraph) > 300 else ''}")
                
                # Look for percentages
                percentages = re.findall(r'(\d+(?:\.\d+)?)\s*(?:percent|%)', paragraph, re.IGNORECASE)
                if percentages:
                    print(f"   Percentages: {percentages}")
                    
                    # Check for federal context
                    if any(keyword in paragraph.lower() for keyword in ['federal', 'fda', 'regulation', 'standard', 'minimum']):
                        print(f"   *** POTENTIAL FEDERAL STANDARD: {percentages} ***")
        
        else:
            print("No butterfat content found via HTML scraping either.")
            print("The Wikipedia page may not contain the specific federal butterfat standards.")
    
    except Exception as scraping_error:
        print(f"HTML scraping also failed: {scraping_error}")

except Exception as e:
    print(f"Unexpected error during Wikipedia analysis: {e}")
    import traceback
    traceback.print_exc()

print("\n=== WIKIPEDIA ANALYSIS COMPLETE ===")
```