### Development Step 8: Title:  
Verify DOI 10.2307/j.ctv9b2xdv and Locate Wikipedia Citations with November Access Dates in References

**Description**: Verify the DOI 10.2307/j.ctv9b2xdv by conducting an independent search to confirm we have the correct publication. Cross-reference the DOI with multiple academic databases and publisher sources to ensure we're analyzing the right book. If the DOI resolves to a different publication than 'The Responsibility of Intellectuals', access that correct publication instead. If the DOI is confirmed correct, perform a more targeted search specifically for endnotes, footnotes, or reference sections that might contain Wikipedia citations with November access dates, including checking for alternative formatting or citation styles that might have been missed in the previous comprehensive search.

**Use Cases**:
- Academic integrity verification for university librarians cross-checking citations in student theses against original publications
- Automated reference auditing for journal editors ensuring all cited DOIs resolve to the correct sources before publication
- Legal research teams validating cited case law and statutes in briefs by independently confirming DOIs and cross-referencing publisher records
- Digital humanities projects analyzing citation patterns in historical texts, including targeted searches for Wikipedia and web-based references
- Research grant compliance audits requiring confirmation that all referenced materials in proposals are correctly attributed and accessible
- Corporate knowledge management systems performing automated checks on internal reports to ensure external sources are properly cited and up-to-date
- Scholarly book publishers conducting pre-publication reviews to detect and correct misattributed or outdated citations in manuscripts
- Library acquisition departments verifying the authenticity and edition of requested academic books before procurement using DOI and metadata cross-referencing

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

print('=== INDEPENDENT DOI VERIFICATION AND TARGETED SEARCH ===') 
print('DOI: 10.2307/j.ctv9b2xdv')
print('Objective: Verify DOI independently and perform targeted search for Wikipedia citations')
print('Status: Previous comprehensive search found no November Wikipedia citations')
print('\n' + '='*100 + '\n')

# First, let's inspect the existing workspace to understand what we have
print('=== INSPECTING EXISTING WORKSPACE FILES ===')
workspace_files = os.listdir('workspace')
print(f'Found {len(workspace_files)} files in workspace:')
for file in sorted(workspace_files):
    file_path = os.path.join('workspace', file)
    file_size = os.path.getsize(file_path)
    print(f'- {file} ({file_size:,} bytes)')

# Let's examine the final bibliographic record to understand the publication details
final_record_path = 'workspace/final_bibliographic_record.json'
if os.path.exists(final_record_path):
    print('\n=== EXAMINING FINAL BIBLIOGRAPHIC RECORD ===')
    with open(final_record_path, 'r', encoding='utf-8') as f:
        biblio_data = json.load(f)
    
    print('Available keys in bibliographic record:')
    for key in biblio_data.keys():
        value = biblio_data[key]
        if isinstance(value, (str, int)):
            print(f'- {key}: {value}')
        elif isinstance(value, list):
            print(f'- {key}: list with {len(value)} items')
            if value and len(value) <= 3:
                for item in value:
                    print(f'  * {item}')
        elif isinstance(value, dict):
            print(f'- {key}: dict with keys {list(value.keys())}')
        else:
            print(f'- {key}: {type(value)}')
else:
    print('Final bibliographic record not found')

print('\n=== INDEPENDENT DOI VERIFICATION ===')

# Let's verify the DOI through multiple independent sources
doi = '10.2307/j.ctv9b2xdv'
print(f'Verifying DOI: {doi}')

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
}

# 1. CrossRef API verification
print('\n1. CrossRef API Verification:')
try:
    crossref_url = f'https://api.crossref.org/works/{doi}'
    response = requests.get(crossref_url, headers=headers, timeout=15)
    print(f'   Status: {response.status_code}')
    
    if response.status_code == 200:
        crossref_data = response.json()
        work = crossref_data.get('message', {})
        
        print(f'   Title: {work.get("title", ["Unknown"])[0] if work.get("title") else "Unknown"}')
        print(f'   Publisher: {work.get("publisher", "Unknown")}')
        print(f'   Type: {work.get("type", "Unknown")}')
        print(f'   Published: {work.get("published-print", {}).get("date-parts", [["Unknown"]])[0]}')
        
        # Check if this matches our expected publication
        title = work.get('title', [''])[0].lower() if work.get('title') else ''
        if 'responsibility' in title and 'intellectuals' in title:
            print('   ✓ Confirmed: This is "The Responsibility of Intellectuals"')
        else:
            print(f'   ⚠ Warning: Title does not match expected publication')
            print(f'   Actual title: {title}')
    else:
        print(f'   ❌ CrossRef lookup failed: {response.status_code}')
except Exception as e:
    print(f'   ❌ CrossRef error: {str(e)}')

# 2. DOI.org resolution verification
print('\n2. DOI.org Resolution Verification:')
try:
    doi_url = f'https://doi.org/{doi}'
    response = requests.get(doi_url, headers=headers, timeout=15, allow_redirects=True)
    print(f'   Status: {response.status_code}')
    print(f'   Final URL: {response.url}')
    
    if response.status_code == 200:
        # Check where it redirects to
        if 'jstor.org' in response.url:
            print('   ✓ Redirects to JSTOR as expected')
        elif 'uclpress' in response.url:
            print('   ✓ Redirects to UCL Press')
        else:
            print(f'   ⚠ Redirects to unexpected domain: {response.url}')
    else:
        print(f'   ❌ DOI resolution failed: {response.status_code}')
except Exception as e:
    print(f'   ❌ DOI resolution error: {str(e)}')

# 3. Alternative identifier verification
print('\n3. Alternative Identifier Verification:')
# Check if there are alternative identifiers from CrossRef
if 'crossref_data' in locals() and crossref_data:
    work = crossref_data.get('message', {})
    
    # Look for ISBN or other identifiers
    if 'ISBN' in work:
        print(f'   ISBN found: {work["ISBN"]}')
    
    # Look for alternative URLs
    if 'URL' in work:
        print(f'   Alternative URL: {work["URL"]}')
    
    # Check for relations to other works
    if 'relation' in work:
        relations = work['relation']
        print(f'   Relations found: {list(relations.keys()) if isinstance(relations, dict) else relations}')
        
        # Look for "is-identical-to" relations
        if isinstance(relations, dict) and 'is-identical-to' in relations:
            identical_works = relations['is-identical-to']
            for related_work in identical_works:
                if 'id' in related_work:
                    alt_doi = related_work['id']
                    print(f'   Alternative DOI found: {alt_doi}')
                    
                    # Try to access this alternative DOI
                    try:
                        alt_url = f'https://doi.org/{alt_doi}'
                        alt_response = requests.get(alt_url, headers=headers, timeout=15, allow_redirects=True)
                        print(f'   Alternative DOI status: {alt_response.status_code}')
                        print(f'   Alternative DOI redirects to: {alt_response.url}')
                        
                        if alt_response.status_code == 200 and 'uclpress' in alt_response.url:
                            print('   ✓ Alternative DOI provides direct UCL Press access')
                    except Exception as alt_e:
                        print(f'   ❌ Alternative DOI error: {str(alt_e)}')

print('\n=== CHECKING FOR DIFFERENT EDITIONS OR VERSIONS ===')

# Search for different versions of the publication
search_queries = [
    'The Responsibility of Intellectuals UCL Press 2019',
    'Responsibility Intellectuals Chomsky UCL Press',
    'The Responsibility of Intellectuals reflections 50 years'
]

for i, query in enumerate(search_queries, 1):
    print(f'\n{i}. Searching for: "{query}"')
    try:
        # Use Google Scholar search
        scholar_url = f'https://scholar.google.com/scholar?q={query.replace(" ", "+")}'
        response = requests.get(scholar_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for result titles
            result_titles = soup.find_all('h3', class_='gs_rt')
            if result_titles:
                print(f'   Found {len(result_titles)} search results:')
                for j, title_elem in enumerate(result_titles[:3], 1):
                    title_text = title_elem.get_text().strip()
                    print(f'   {j}. {title_text[:100]}...')
                    
                    # Look for links in this result
                    links = title_elem.find_all('a', href=True)
                    for link in links:
                        href = link.get('href')
                        if 'uclpress' in href or 'ucl.ac.uk' in href:
                            print(f'      UCL link: {href}')
            else:
                print('   No results found')
        else:
            print(f'   Search failed: {response.status_code}')
    except Exception as search_e:
        print(f'   Search error: {str(search_e)}')
    
    time.sleep(2)  # Be respectful to Google Scholar

print('\n=== TARGETED SEARCH FOR ENDNOTES AND REFERENCES SECTIONS ===')

# Let's check if we have the full book text and search more specifically
full_text_path = 'workspace/full_book_text.txt'
if os.path.exists(full_text_path):
    print('Loading full book text for targeted analysis...')
    
    with open(full_text_path, 'r', encoding='utf-8') as f:
        full_text = f.read()
    
    print(f'Full text length: {len(full_text):,} characters')
    
    # Search for different types of reference sections
    reference_section_patterns = [
        r'\b(notes?)\b[^\n]{0,50}\n',
        r'\b(endnotes?)\b[^\n]{0,50}\n',
        r'\b(references?)\b[^\n]{0,50}\n',
        r'\b(bibliography)\b[^\n]{0,50}\n',
        r'\b(works cited)\b[^\n]{0,50}\n',
        r'\b(sources?)\b[^\n]{0,50}\n',
        r'\b(footnotes?)\b[^\n]{0,50}\n'
    ]
    
    reference_sections = []
    for pattern in reference_section_patterns:
        matches = re.finditer(pattern, full_text, re.IGNORECASE)
        for match in matches:
            section_name = match.group(1)
            position = match.start()
            
            # Get some context around this section
            context_start = max(0, position - 200)
            context_end = min(len(full_text), position + 500)
            context = full_text[context_start:context_end]
            
            reference_sections.append({
                'section_name': section_name,
                'position': position,
                'context': context
            })
    
    if reference_sections:
        print(f'\nFound {len(reference_sections)} potential reference sections:')
        for i, section in enumerate(reference_sections, 1):
            print(f'\n{i}. "{section["section_name"]}" at position {section["position"]:,}')
            print('   Context:')
            print('   ' + '-'*60)
            print('   ' + section['context'][:300].replace('\n', '\n   '))
            print('   ' + '-'*60)
    else:
        print('\nNo clear reference sections found')
    
    # Search for numbered references or citations
    print('\n=== SEARCHING FOR NUMBERED CITATIONS ===')
    
    numbered_citation_patterns = [
        r'\n\s*(\d+)\s*[\.]\s*[^\n]{20,200}',  # Numbered list items
        r'\[(\d+)\][^\n]{20,200}',  # Bracketed numbers
        r'\n\s*(\d+)\s+[A-Z][^\n]{20,200}',  # Number followed by text
    ]
    
    numbered_citations = []
    for pattern in numbered_citation_patterns:
        matches = re.finditer(pattern, full_text, re.IGNORECASE)
        for match in matches:
            number = match.group(1)
            citation_text = match.group(0)
            
            # Only include reasonable citation numbers (1-200)
            if 1 <= int(number) <= 200:
                numbered_citations.append({
                    'number': int(number),
                    'text': citation_text.strip(),
                    'position': match.start()
                })
    
    # Remove duplicates and sort by number
    unique_citations = {}
    for citation in numbered_citations:
        if citation['number'] not in unique_citations:
            unique_citations[citation['number']] = citation
    
    sorted_citations = sorted(unique_citations.values(), key=lambda x: x['number'])
    
    if sorted_citations:
        print(f'Found {len(sorted_citations)} numbered citations')
        print('First 10 numbered citations:')
        for citation in sorted_citations[:10]:
            print(f'   {citation["number"]}. {citation["text"][:100]}...')
        
        # Look specifically for Wikipedia in these numbered citations
        wikipedia_numbered = []
        for citation in sorted_citations:
            if 'wikipedia' in citation['text'].lower():
                wikipedia_numbered.append(citation)
        
        if wikipedia_numbered:
            print(f'\n*** FOUND {len(wikipedia_numbered)} NUMBERED CITATIONS CONTAINING WIKIPEDIA ***')
            for citation in wikipedia_numbered:
                print(f'\nCitation {citation["number"]}:')
                print(f'Position: {citation["position"]:,}')
                print('Text:')
                print('-'*60)
                print(citation['text'])
                print('-'*60)
                
                # Check for November dates in this citation
                november_match = re.search(r'november\s+(\d{1,2})', citation['text'], re.IGNORECASE)
                if november_match:
                    day = november_match.group(1)
                    print(f'*** FOUND NOVEMBER {day} ACCESS DATE ***')
        else:
            print('\nNo numbered citations contain Wikipedia references')
    else:
        print('No numbered citations found')
    
    # Final comprehensive search for any access dates
    print('\n=== FINAL SEARCH FOR ANY ACCESS DATES ===')
    
    access_date_patterns = [
        r'accessed[^\n]{0,100}\d{4}[^\n]{0,100}',
        r'retrieved[^\n]{0,100}\d{4}[^\n]{0,100}',
        r'viewed[^\n]{0,100}\d{4}[^\n]{0,100}',
        r'\d{1,2}[^\n]{0,50}\d{4}[^\n]{0,100}accessed[^\n]{0,100}',
        r'\b\d{1,2}\s+\w+\s+\d{4}\b[^\n]{0,200}'
    ]
    
    access_dates = []
    for pattern in access_date_patterns:
        matches = re.finditer(pattern, full_text, re.IGNORECASE)
        for match in matches:
            date_text = match.group(0)
            access_dates.append({
                'text': date_text,
                'position': match.start()
            })
    
    if access_dates:
        print(f'Found {len(access_dates)} potential access dates:')
        for i, date_info in enumerate(access_dates[:10], 1):
            print(f'{i}. {date_info["text"][:100]}...')
    else:
        print('No access dates found')
else:
    print('Full book text not available for targeted search')

print('\n' + '='*100)
print('INDEPENDENT DOI VERIFICATION AND TARGETED SEARCH COMPLETE')
print('='*100)
print('Summary:')
print('- Verified DOI through multiple independent sources')
print('- Searched for alternative editions or versions')
print('- Performed targeted search for reference sections and numbered citations')
print('- Conducted final search for any access dates in the publication')
print('\nConclusion: If no Wikipedia citations with November access dates were found,')
print('this confirms the publication does not contain the target endnote as described.')
```