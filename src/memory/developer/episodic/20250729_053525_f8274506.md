### Development Step 6: Identify 2022 Legume Wikipedia Content Violations Before December and the ‘R’ in Three Core Policies

**Description**: Search for information about the Legume Wikipedia page and identify what content violations occurred in its public logs during 2022 before December. Focus on finding the specific type of content that was violated and the associated three core policies, particularly identifying what 'R' stands for in those policies.

**Use Cases**:
- Corporate knowledge base compliance auditing: scanning internal wiki logs for policy breaches (e.g., NPOV, Verifiability, NOR) and automatically generating violation reports
- Legal team wiki content review for copyright infringement: parsing edit histories to identify removed or reverted content due to alleged copyright violations in 2022
- Academic research on collaborative content governance: analyzing Wikipedia talk pages and revision logs to study policy violation patterns and their resolutions
- Non-profit NGO media monitoring: tracking changes on advocacy-related Wikipedia pages to detect vandalism or biased edits and ensure neutrality
- Automated documentation pipeline in software development: integrating violation scanning to flag policy breaches in project documentation before publication
- Government agency digital archive auditing: reviewing web archive captures to identify and document policy violations on public information pages for regulatory compliance
- Media company brand monitoring: analyzing Wikipedia page edits and removal incidents to detect vandalism or misinformation about company products and mitigate reputational risks
- University academic integrity assessment: auditing student-contributed wiki revisions to enforce "No Original Research" policy and Verifiability guidelines in coursework

```
import os
import json

print("=== EXAMINING DEEP ANALYSIS RESULTS FOR VIOLATION DETAILS ===\n")
print("Objective: Review saved analysis to identify specific 2022 violations and three core policies\n")

# Locate workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("No workspace directory found.")
    exit()

workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# First, inspect the deep analysis file structure
deep_analysis_file = f'{workspace_dir}/legume_deep_violation_analysis.json'
if os.path.exists(deep_analysis_file):
    print(f"Loading deep analysis file: {os.path.basename(deep_analysis_file)}")
    file_size = os.path.getsize(deep_analysis_file)
    print(f"File size: {file_size:,} bytes\n")
    
    # Inspect JSON structure first
    with open(deep_analysis_file, 'r') as f:
        deep_data = json.load(f)
    
    print("=== DEEP ANALYSIS FILE STRUCTURE ===\n")
    print("Top-level keys:")
    for key, value in deep_data.items():
        if isinstance(value, dict):
            print(f"  {key}: Dictionary with {len(value)} keys")
            # Show nested structure
            for nested_key, nested_value in value.items():
                if isinstance(nested_value, list):
                    print(f"    {nested_key}: List with {len(nested_value)} items")
                elif isinstance(nested_value, dict):
                    print(f"    {nested_key}: Dictionary with {len(nested_value)} keys")
                else:
                    print(f"    {nested_key}: {nested_value}")
        elif isinstance(value, list):
            print(f"  {key}: List with {len(value)} items")
        else:
            print(f"  {key}: {value}")
    
    print("\n=== EXAMINING POLICY ABBREVIATIONS FOUND ===\n")
    
    if 'policy_abbreviations_found' in deep_data:
        policies = deep_data['policy_abbreviations_found']
        print(f"Total policies found: {len(policies)}")
        
        print("\nAll policies with details:")
        for abbrev, details in policies.items():
            print(f"  {abbrev}: {details['full_name']}")
            print(f"    Mentions: {details['total_mentions']} (Abbrev: {details['abbrev_count']}, Full name: {details['name_count']})")
            print(f"    Contains 'R': {details['contains_r']}")
            print()
    
    print("=== EXAMINING R-CONTAINING POLICIES ===\n")
    
    if 'r_containing_policies' in deep_data:
        r_policies = deep_data['r_containing_policies']
        print(f"Policies containing 'R': {len(r_policies)}")
        
        for abbrev, details in r_policies.items():
            print(f"  {abbrev}: {details['full_name']}")
            print(f"    Total mentions: {details['total_mentions']}")
            print(f"    This could be what 'R' stands for in the context")
            print()
    
    print("=== KEY FINDINGS ANALYSIS ===\n")
    
    # Based on Wikipedia's three core content policies, analyze what we found
    print("Wikipedia's three core content policies are typically:")
    print("1. Neutral Point of View (NPOV)")
    print("2. Verifiability (V)")
    print("3. No Original Research (NOR)")
    print()
    
    # Check if we found these in our analysis
    core_policies_found = []
    if 'policy_abbreviations_found' in deep_data:
        policies = deep_data['policy_abbreviations_found']
        
        if 'NPOV' in policies:
            core_policies_found.append('NPOV (Neutral Point of View)')
        if 'V' in policies:
            core_policies_found.append('V (Verifiability)')
        if 'NOR' in policies:
            core_policies_found.append('NOR (No Original Research)')
    
    print(f"Core policies found in analysis: {len(core_policies_found)}")
    for policy in core_policies_found:
        print(f"  - {policy}")
    
    print("\n=== IDENTIFYING WHAT 'R' STANDS FOR ===\n")
    
    # Analyze the R-containing policies to determine the most likely answer
    if 'r_containing_policies' in deep_data:
        r_policies = deep_data['r_containing_policies']
        
        print("Possible meanings of 'R' based on found policies:")
        for abbrev, details in r_policies.items():
            if abbrev == 'NOR':
                print(f"  - 'R' could stand for 'Research' (from No Original Research - NOR)")
                print(f"    Mentions: {details['total_mentions']}")
            elif abbrev == 'RS':
                print(f"  - 'R' could stand for 'Reliable' (from Reliable Sources - RS)")
                print(f"    Mentions: {details['total_mentions']}")
            elif abbrev == 'RV':
                print(f"  - 'R' could stand for 'Revert' (from Revert - RV)")
                print(f"    Mentions: {details['total_mentions']}")
        
        # Determine most likely based on context and mentions
        if 'NOR' in r_policies and 'RS' in r_policies:
            print("\n*** ANALYSIS CONCLUSION ***")
            print("Based on Wikipedia's three core content policies (NPOV, V, NOR):")
            print("'R' most likely stands for 'RESEARCH' (from No Original Research)")
            print("OR 'RELIABLE' (from Reliable Sources)")
            print("Both are fundamental Wikipedia policies containing 'R'")
    
    print("\n=== SEARCHING FOR 2022 VIOLATION SPECIFICS ===\n")
    
    # Now let's look more carefully at the original HTML files for actual violation incidents
    print("Previous analysis found policy references but no specific 2022 violations.")
    print("Let me search the HTML content more directly for violation incidents...")
    
    # Check all HTML files for specific violation patterns
    html_files = [f for f in os.listdir(workspace_dir) if f.endswith('.html')]
    
    violation_incidents = []
    
    for html_file in html_files:
        html_path = os.path.join(workspace_dir, html_file)
        print(f"\nSearching {html_file} for specific violations...")
        
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Search for specific violation patterns in 2022
        violation_patterns = [
            r'2022.*?(?:remov|delet|revert).*?(?:copyright|spam|vandal|polic|violat)',
            r'(?:copyright|spam|vandal|polic|violat).*?2022.*?(?:remov|delet|revert)',
            r'2022.*?(?:January|February|March|April|May|June|July|August|September|October|November).*?(?:violat|polic|remov)',
            r'(?:block|warn|remov).*?2022.*?(?:before|prior to).*?December'
        ]
        
        for pattern in violation_patterns:
            matches = list(re.finditer(pattern, content, re.IGNORECASE | re.DOTALL))
            for match in matches:
                start = max(0, match.start() - 200)
                end = min(len(content), match.end() + 200)
                context = content[start:end]
                
                violation_incidents.append({
                    'source_file': html_file,
                    'pattern': pattern,
                    'match': match.group(),
                    'context': context,
                    'position': match.start()
                })
        
        # Also search for any mentions of content removal or policy enforcement
        simple_patterns = ['removed for', 'deleted due to', 'reverted because', 'policy violation', 'content violation']
        for simple_pattern in simple_patterns:
            if simple_pattern in content.lower() and '2022' in content:
                # Find the section containing both the pattern and 2022
                pattern_pos = content.lower().find(simple_pattern)
                year_pos = content.find('2022')
                
                if abs(pattern_pos - year_pos) < 1000:  # Within 1000 characters
                    start = max(0, min(pattern_pos, year_pos) - 300)
                    end = min(len(content), max(pattern_pos, year_pos) + 300)
                    context = content[start:end]
                    
                    violation_incidents.append({
                        'source_file': html_file,
                        'pattern': f'Simple pattern: {simple_pattern}',
                        'context': context,
                        'type': 'simple_match'
                    })
    
    print(f"\nTotal violation incidents found: {len(violation_incidents)}")
    
    if violation_incidents:
        print("\n=== VIOLATION INCIDENTS DETAILS ===\n")
        for i, incident in enumerate(violation_incidents[:5], 1):  # Show first 5
            print(f"{i}. Violation Incident:")
            print(f"   Source: {incident['source_file']}")
            print(f"   Pattern: {incident.get('pattern', 'N/A')}")
            if 'match' in incident:
                print(f"   Match: {incident['match'][:100]}...")
            print(f"   Context: {incident['context'][:400]}...")
            print()
    
    # Save final comprehensive results
    final_analysis = {
        'research_objective': 'Legume Wikipedia content violations 2022 (before December)',
        'three_core_policies_identified': core_policies_found,
        'r_meaning_analysis': {
            'most_likely_meanings': ['Research (from NOR)', 'Reliable (from RS)', 'Revert (from RV)'],
            'policies_with_r': list(deep_data.get('r_containing_policies', {}).keys()) if 'r_containing_policies' in deep_data else [],
            'conclusion': 'R most likely stands for Research or Reliable in Wikipedia policy context'
        },
        'violation_incidents_2022': violation_incidents,
        'analysis_summary': {
            'policies_found': len(deep_data.get('policy_abbreviations_found', {})),
            'r_policies_found': len(deep_data.get('r_containing_policies', {})),
            'violation_incidents': len(violation_incidents),
            'core_policies_identified': len(core_policies_found)
        }
    }
    
    with open(f'{workspace_dir}/legume_final_violation_analysis.json', 'w') as f:
        json.dump(final_analysis, f, indent=2, default=str)
    
    print(f"\n=== FINAL ANALYSIS RESULTS ===\n")
    print(f"Three core policies found: {core_policies_found}")
    print(f"'R' most likely stands for: Research (NOR) or Reliable (RS)")
    print(f"2022 violation incidents found: {len(violation_incidents)}")
    print(f"\nFinal analysis saved to: {workspace_dir}/legume_final_violation_analysis.json")
    
else:
    print("Deep analysis file not found. Need to run previous analysis first.")
```