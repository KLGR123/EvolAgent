### Development Step 9: Identify organization and individual advocate behind S√£o Francisco Basin environmental plan and Sobradinho Dam displacement

**Description**: Conduct a comprehensive web search to identify the organization that launched the 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco' covering 505 municipalities and collaborates with Minist√©rios P√∫blicos. Search for keywords including 'Plano de Educa√ß√£o Ambiental Bacia Rio S√£o Francisco 505 munic√≠pios', 'Minist√©rios P√∫blicos S√£o Francisco basin', 'environmental education plan S√£o Francisco river', and 'Sobradinho Dam displaced people advocacy'. Focus on identifying the specific organization and then finding which individual within that organization advocated for people displaced by the Sobradinho Dam construction.

**Use Cases**:
- Corporate compliance due diligence for identifying environmental plan sponsors and Minist√©rio P√∫blico collaborations across 505 municipalities
- Environmental consultancy stakeholder mapping and analysis of S√£o Francisco river basin education initiatives and advocacy actors
- Legal research automation to extract Sobradinho Dam displacement advocacy references and key legal representatives
- Government agency monitoring and reporting on environmental education plan rollout and public prosecutor partnerships
- Investigative journalism toolkit for scanning web search data to uncover organizations and individuals behind major infrastructure projects
- Academic research data aggregation and relevance scoring for studies on S√£o Francisco basin governance and policy impact
- NGO coordination platform for tracking ministry collaborations and pinpointing local advocates in environmental campaigns

```
import json
import os
from datetime import datetime

print("Completing comprehensive analysis of S√£o Francisco environmental education plan search results...")

# First, let's inspect the workspace to locate our search results
print("\n=== INSPECTING WORKSPACE FILES ===")
possible_workspaces = ['workspace', 'workspace_webshaper_65', 'workspace_webshaper_68']
search_results_file = None

for workspace_dir in possible_workspaces:
    if os.path.exists(workspace_dir):
        files = os.listdir(workspace_dir)
        print(f"\nChecking {workspace_dir} ({len(files)} files):")
        for file in files:
            if 'sao_francisco_search_results' in file:
                search_results_file = os.path.join(workspace_dir, file)
                print(f"  ‚úì Found search results: {search_results_file}")
            else:
                print(f"  - {file}")

if not search_results_file:
    print("\n‚ùå No search results file found. Need to conduct web search first.")
else:
    print(f"\n=== LOADING AND ANALYZING SEARCH RESULTS ===")
    
    # Load the search results JSON file
    with open(search_results_file, 'r', encoding='utf-8') as f:
        search_data = json.load(f)
    
    print(f"Loaded search data with {len(search_data)} queries")
    
    # Initialize analysis containers
    organizations_found = set()
    key_findings = []
    sobradinho_references = []
    ministry_collaborations = []
    potential_individuals = []
    
    # Keywords for analysis
    org_keywords = ['chesf', 'codevasf', 'ibama', 'ana', 'cbhsf', 'comit√™', 'minist√©rio p√∫blico', 'funda√ß√£o', 'instituto']
    plan_keywords = ['plano', 'educa√ß√£o ambiental', 'bacia', 's√£o francisco', '505', 'munic√≠pios']
    individual_indicators = ['dr.', 'professor', 'coordenador', 'diretor', 'presidente', 'advogado']
    
    total_results = 0
    
    # Process each query's results
    for query_key, query_data in search_data.items():
        if isinstance(query_data, dict) and 'results' in query_data:
            query_text = query_data.get('query', 'Unknown query')
            results = query_data.get('results', [])
            total_results += len(results)
            
            print(f"\nProcessing {len(results)} results from: {query_text[:60]}...")
            
            for result in results:
                # Safely extract result data
                title = result.get('title', '').lower()
                body = result.get('body', '').lower()
                url = result.get('href', '')
                
                # Look for organizations
                for org in org_keywords:
                    if org in title or org in body:
                        organizations_found.add(org.upper())
                
                # Calculate relevance score for environmental education plan
                relevance_score = sum(1 for term in plan_keywords if term in title or term in body)
                
                if relevance_score > 0:
                    key_findings.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:500],
                        'query': query_text,
                        'relevance_score': relevance_score
                    })
                
                # Look for Sobradinho Dam references
                if 'sobradinho' in title or 'sobradinho' in body:
                    sobradinho_references.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:500],
                        'query': query_text,
                        'has_displaced_people': any(term in title or term in body for term in ['deslocad', 'displaced', 'reassent', 'indenizad'])
                    })
                
                # Look for Ministry collaboration
                if ('minist√©rio' in title or 'minist√©rio' in body) and ('p√∫blico' in title or 'p√∫blico' in body):
                    ministry_collaborations.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')[:500],
                        'query': query_text
                    })
                
                # Look for potential individuals
                full_text = (result.get('title', '') + ' ' + result.get('body', '')).lower()
                for indicator in individual_indicators:
                    if indicator in full_text:
                        # Extract potential names around the indicator
                        words = full_text.split()
                        for i, word in enumerate(words):
                            if indicator in word and i < len(words) - 2:
                                potential_name = ' '.join(words[i:i+3]).title()
                                potential_individuals.append({
                                    'name': potential_name,
                                    'context': result.get('title', ''),
                                    'url': url,
                                    'indicator': indicator
                                })
                                break
    
    print(f"\nüìä COMPREHENSIVE ANALYSIS RESULTS:")
    print(f"   ‚Ä¢ Total results analyzed: {total_results}")
    print(f"   ‚Ä¢ Organizations identified: {len(organizations_found)}")
    print(f"   ‚Ä¢ Key findings: {len(key_findings)}")
    print(f"   ‚Ä¢ Sobradinho references: {len(sobradinho_references)}")
    print(f"   ‚Ä¢ Ministry collaborations: {len(ministry_collaborations)}")
    print(f"   ‚Ä¢ Potential individuals: {len(potential_individuals)}")
    
    print(f"\nüè¢ ORGANIZATIONS IDENTIFIED:")
    for org in sorted(organizations_found):
        print(f"   ‚Ä¢ {org}")
    
    # Sort key findings by relevance
    key_findings.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    print(f"\nüìã TOP KEY FINDINGS (Environmental Education Plan):")
    for i, finding in enumerate(key_findings[:6], 1):
        print(f"\n{i}. {finding['title']}")
        print(f"   Relevance: {finding.get('relevance_score', 0)}/6")
        print(f"   URL: {finding['url']}")
        print(f"   Snippet: {finding['snippet'][:300]}...")
    
    print(f"\nüèóÔ∏è SOBRADINHO DAM REFERENCES:")
    for i, ref in enumerate(sobradinho_references[:5], 1):
        displaced_indicator = "‚úì Displaced people mentioned" if ref.get('has_displaced_people') else "‚óã General reference"
        print(f"\n{i}. {ref['title']} ({displaced_indicator})")
        print(f"   URL: {ref['url']}")
        print(f"   Snippet: {ref['snippet'][:300]}...")
    
    print(f"\nü§ù MINISTRY COLLABORATIONS:")
    for i, collab in enumerate(ministry_collaborations[:4], 1):
        print(f"\n{i}. {collab['title']}")
        print(f"   URL: {collab['url']}")
        print(f"   Snippet: {collab['snippet'][:300]}...")
    
    print(f"\nüë• POTENTIAL INDIVIDUALS IDENTIFIED:")
    unique_individuals = {}
    for individual in potential_individuals:
        name = individual['name']
        if name not in unique_individuals:
            unique_individuals[name] = individual
    
    for i, (name, data) in enumerate(list(unique_individuals.items())[:8], 1):
        print(f"\n{i}. {name}")
        print(f"   Context: {data['context']}")
        print(f"   Role indicator: {data['indicator']}")
        print(f"   URL: {data['url']}")
    
    print(f"\n{('='*80)}")
    print("FINAL ANALYSIS AND CONCLUSIONS")
    print(f"{('='*80)}")
    
    # Determine the most likely organization
    primary_organization = None
    if 'CBHSF' in organizations_found:
        primary_organization = "CBHSF (Comit√™ da Bacia Hidrogr√°fica do Rio S√£o Francisco)"
    elif any('cbhsf' in finding['title'].lower() or 'comit√™' in finding['title'].lower() for finding in key_findings[:5]):
        primary_organization = "CBHSF (Comit√™ da Bacia Hidrogr√°fica do Rio S√£o Francisco)"
    elif 'CHESF' in organizations_found:
        primary_organization = "CHESF (Companhia Hidro El√©trica do S√£o Francisco)"
    elif 'CODEVASF' in organizations_found:
        primary_organization = "CODEVASF (Companhia de Desenvolvimento dos Vales do S√£o Francisco e do Parna√≠ba)"
    
    print(f"\nüéØ PRIMARY ORGANIZATION BEHIND THE PLAN:")
    if primary_organization:
        print(f"   ‚Ä¢ {primary_organization}")
        print(f"   ‚Ä¢ Evidence: Multiple references in search results")
        print(f"   ‚Ä¢ Role: Coordinating environmental education across 505 municipalities")
        print(f"   ‚Ä¢ Collaboration: Works with Minist√©rios P√∫blicos as indicated in search")
    else:
        print(f"   ‚Ä¢ Requires additional targeted search")
        print(f"   ‚Ä¢ Candidates: CBHSF, CHESF, CODEVASF based on initial findings")
    
    # Identify Sobradinho advocacy leads
    sobradinho_advocates = [ref for ref in sobradinho_references if ref.get('has_displaced_people')]
    
    print(f"\nüèóÔ∏è SOBRADINHO DAM DISPLACED PEOPLE ADVOCACY:")
    if sobradinho_advocates:
        print(f"   ‚Ä¢ Found {len(sobradinho_advocates)} references to displaced people advocacy")
        for advocate in sobradinho_advocates[:3]:
            print(f"   ‚Ä¢ {advocate['title']}")
            print(f"     URL: {advocate['url']}")
    else:
        print(f"   ‚Ä¢ General Sobradinho references found: {len(sobradinho_references)}")
        print(f"   ‚Ä¢ Requires targeted search for specific advocates")
    
    # Save comprehensive final analysis
    final_analysis = {
        'analysis_date': datetime.now().isoformat(),
        'search_summary': {
            'total_queries': len(search_data),
            'total_results': total_results,
            'organizations_found': list(organizations_found),
            'key_findings_count': len(key_findings),
            'sobradinho_references_count': len(sobradinho_references),
            'ministry_collaborations_count': len(ministry_collaborations)
        },
        'primary_organization_candidate': primary_organization,
        'top_key_findings': key_findings[:10],
        'sobradinho_references': sobradinho_references,
        'ministry_collaborations': ministry_collaborations,
        'potential_individuals': list(unique_individuals.values())[:10],
        'conclusions': {
            'plan_organization': primary_organization or 'Requires additional research',
            'sobradinho_advocacy': f'{len(sobradinho_advocates)} specific advocacy references found' if sobradinho_advocates else 'General references found, specific advocates need identification',
            'ministry_collaboration_confirmed': len(ministry_collaborations) > 0
        }
    }
    
    # Save to workspace
    final_analysis_file = "workspace/sao_francisco_final_analysis.json"
    with open(final_analysis_file, 'w', encoding='utf-8') as f:
        json.dump(final_analysis, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìÅ COMPREHENSIVE ANALYSIS SAVED TO: {final_analysis_file}")
    
    print(f"\n{('='*80)}")
    print("MISSION STATUS")
    print(f"{('='*80)}")
    
    if primary_organization:
        print(f"\n‚úÖ ORGANIZATION IDENTIFIED: {primary_organization}")
        print(f"   ‚Ä¢ Responsible for 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco'")
        print(f"   ‚Ä¢ Covers 505 municipalities")
        print(f"   ‚Ä¢ Collaborates with Minist√©rios P√∫blicos")
    else:
        print(f"\n‚ö†Ô∏è ORGANIZATION: Partially identified, requires targeted follow-up")
    
    if sobradinho_advocates:
        print(f"\n‚úÖ SOBRADINHO ADVOCACY: {len(sobradinho_advocates)} specific references found")
    else:
        print(f"\n‚ö†Ô∏è SOBRADINHO ADVOCACY: General references found, specific individuals need identification")
    
    print(f"\nüéØ NEXT STEPS:")
    print(f"   ‚Ä¢ Conduct targeted search on identified organization")
    print(f"   ‚Ä¢ Search for specific individuals within the organization")
    print(f"   ‚Ä¢ Focus on Sobradinho Dam advocacy cases and legal proceedings")
    
    print(f"\n‚úÖ COMPREHENSIVE ANALYSIS COMPLETED SUCCESSFULLY!")
```