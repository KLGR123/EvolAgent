### Development Step 50: Extract Survivor (US) Winners for Seasons 1–44 into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- TV analytics platform aggregating Survivor winners to visualize winning trends and demographic shifts across seasons for network executives
- Entertainment news site automating the creation of “On this day” articles by pulling past Survivor champion names and win dates
- Data journalism project analyzing the correlation between Survivor winners’ backgrounds (e.g., age, profession) and jury voting outcomes
- Reality TV fan app generating personalized trivia quizzes using season-winner data to engage superfans with custom leaderboards
- Academic study on game theory applying historical Survivor winner data to model strategic alliance patterns over multiple seasons
- Social media scheduling tool auto-posting anniversary shoutouts for each Survivor winner to drive engagement on franchise channels
- Market research consultancy compiling a JSON dataset of Survivor champions to advise casting directors on contestant archetypes
- Machine learning pipeline training a predictive model using past Survivor winners’ profiles and season variables to forecast future victors

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# === DEBUG STEP: Verify fetched HTML and extract all mw-headline spans ===
# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
wiki_url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {wiki_url}")
resp = requests.get(wiki_url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
})
print(f"→ Final URL after redirects: {resp.url}")
print(f"→ HTTP status code: {resp.status_code}")

# 3) Save a snippet of the page for manual inspection
snippet_path = os.path.join(workspace_dir, 'survivor_page_snippet.html')
with open(snippet_path, 'w', encoding='utf-8') as f:
    # Save only first 1000 characters to avoid massive file
    f.write(resp.text[:1000])
print(f"Saved first 1000 chars of page to: {snippet_path}\n")

# 4) Parse HTML and list all <span class="mw-headline"> elements
soup = BeautifulSoup(resp.text, 'html.parser')
headlines = soup.find_all('span', class_='mw-headline')
print(f"Found {len(headlines)} spans with class 'mw-headline'\n")

# 5) Write out the first 30 headline ids and texts for examination
titles_path = os.path.join(workspace_dir, 'survivor_headlines_list.txt')
with open(titles_path, 'w', encoding='utf-8') as f:
    f.write("# List of first 30 <span class='mw-headline'> id/text pairs\n")
    for i, span in enumerate(headlines[:30], start=1):
        hid = span.get('id', '')
        text = span.get_text(strip=True)
        line = f"{i:2d}. id='{hid}' text='{text}'"
        f.write(line + "\n")

print(f"Wrote first {min(30, len(headlines))} headline entries to: {titles_path}\n")
print("=== Sample headlines ===")
for i, span in enumerate(headlines[:30], start=1):
    hid = span.get('id', '')
    text = span.get_text(strip=True)
    print(f"{i:2d}. id='{hid}' text='{text}'")
print("=== End of headlines list ===")

# 6) If no headlines found, warn the tester
if not headlines:
    print("⚠️ Warning: No .mw-headline spans found. The page structure may differ or the fetch failed.")
```