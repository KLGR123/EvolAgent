### Development Step 36: Locate and Download Leicester's “Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?” PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic systematic literature review automation for marine biology researchers collecting university-hosted PDFs on fishery sustainability
- Market intelligence gathering for a biotech startup extracting recent protein analysis publications from university repositories
- Regulatory compliance auditing by legal teams downloading policy and certification documents in PDF from government and academic sites
- Digital archiving workflow for university librarians harvesting departmental theses and dissertations in PDF format for institutional repositories
- Data journalism investigation scraping financial and environmental impact reports in PDF from academic and NGO websites to visualize industry trends
- Intellectual property due diligence by patent lawyers retrieving related technical papers and patent PDFs from university domains
- Insurance risk assessment process automating download of engineering and environmental hazard reports in PDF from academic research portals
- Product design research for automotive engineers extracting technical specification sheets and materials studies in PDF from manufacturer and university publications

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
bing_html = os.path.join(workspace, 'bing_search.html')
candidates_out = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure workspace exists ---
if not os.path.isdir(workspace):
    print(f"[INFO] Creating workspace directory: {workspace}")
    os.makedirs(workspace, exist_ok=True)

# --- Step 1: Perform Bing search for the target PDF ---
query = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet? filetype:pdf site:le.ac.uk"
bing_url = 'https://www.bing.com/search'
params = {'q': query}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:104.0) Gecko/20100101 Firefox/104.0',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"[INFO] Sending Bing search request with query:\n      {query}\n")
response = requests.get(bing_url, params=params, headers=headers, timeout=30)
print(f"[INFO] Bing returned status: {response.status_code}")
if response.status_code != 200:
    print(f"ERROR: Bing search failed with status {response.status_code}. Exiting.")
    sys.exit(1)

# Save raw HTML for inspection
with open(bing_html, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"[SAVE] Bing search HTML → {bing_html}\n")

# --- Step 2: Parse HTML and extract .pdf URLs ---
print("[INFO] Parsing saved HTML to extract .pdf URLs...")
with open(bing_html, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()

# 2a) Direct <a> tags ending with .pdf or containing .pdf?
print("[STEP] Scanning <a> tags for literal .pdf links...")
for a in soup.find_all('a', href=True):
    href = a['href']
    hl = href.lower()
    if hl.endswith('.pdf') or '.pdf?' in hl:
        pdf_urls.add(href)
print(f"    Found {len(pdf_urls)} direct .pdf href(s)")

# 2b) Decode Bing redirect links (/url?q=...)
print("[STEP] Parsing Bing redirect links (/url?q=...pdf)...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q','u','url'):
            if key in qs:
                real = unquote(qs[key][0])
                rl = real.lower()
                if rl.endswith('.pdf') or '.pdf?' in rl:
                    pdf_urls.add(real)
                break
print(f"    Total after redirect parsing: {len(pdf_urls)} candidate(s)")

# 2c) Regex fallback over the raw HTML
print("[STEP] Running regex fallback for http(s)://...pdf patterns...")
# Use a triple-quoted raw string to avoid escaping issues
pattern = r'''https?://[^\s"']+?\.pdf(?:\?[^\s"']*)?'''
matches = re.findall(pattern, html, flags=re.IGNORECASE)
for m in matches:
    pdf_urls.add(m)
print(f"    Total after regex fallback: {len(pdf_urls)} candidate(s)")

# 2d) Filter for Leicester-related domains
print("[STEP] Filtering for Leicester domains (le.ac.uk, lra.le.ac.uk, ...)")
domains = ('le.ac.uk','lra.le.ac.uk','core.ac.uk','hdl.handle.net')
le_urls = [u for u in pdf_urls if any(d in u.lower() for d in domains)]
if le_urls:
    print(f"    Found {len(le_urls)} Leicester-specific PDF URL(s):")
    for u in le_urls:
        print(f"      - {u}")
else:
    print("    No Leicester-specific PDFs found; listing all candidates:")
    for u in sorted(pdf_urls):
        print(f"      - {u}")

# Save all candidates to file
with open(candidates_out, 'w', encoding='utf-8') as f:
    for u in sorted(pdf_urls):
        f.write(u + '\n')
print(f"\n[SAVE] All PDF URL candidates written to: {candidates_out}")
print("[COMPLETE] Extraction phase finished. Inspect 'found_pdf_urls.txt' to pick the correct PDF link.")

```