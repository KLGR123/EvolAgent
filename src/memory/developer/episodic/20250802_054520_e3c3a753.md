### Development Step 42: Retrieve PDF of Leicester’s “Can Hiccup Supply Enough Fish for a Dragon’s Diet?”

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated literature review for marine biologists fetching PDF studies on fish population dynamics from university DSpace repositories
- Intellectual property due diligence in a law firm by downloading patent PDFs from online academic archives for infringement analysis
- Clinical trial monitoring in pharmaceutical R&D by periodically retrieving protocol and study result PDFs from medical school repositories
- Competitive technology scouting for semiconductor engineers by scraping and downloading conference white papers on new fabrication techniques
- Environmental policy analysis by collecting government-funded climate change reports in PDF from institutional research portals
- Machine learning dataset creation for nutrition science by bulk downloading open-access dietary study PDFs from university libraries
- Sustainability reporting in a corporate ESG team by aggregating environmental impact assessment PDFs from academic and government repositories
- Digital humanities research support by systematically retrieving historical theses and archival documents in PDF from online university collections

```
import os
import sys
import requests
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

# --- Suppress SSL warnings in case we still hit HTTPS redirect ---
import requests.packages.urllib3
requests.packages.urllib3.disable_warnings()

# --- Configuration ---
workspace = 'workspace'
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# Try HTTP instead of HTTPS to avoid SSL handshake issues
base_url = 'http://lra.le.ac.uk'
# Paper title (ASCII apostrophe)
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
# URL-encode the title in quotes
encoded = quote(f'"{title}"')
search_url = f'{base_url}/simple-search?query={encoded}'

# Local paths
search_html = os.path.join(workspace, 'lra_search.html')
candidates_txt = os.path.join(workspace, 'lra_pdf_urls.txt')
pdf_out = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

# --- Step 1: Fetch the DSpace search page via HTTP ---
print(f"[INFO] Searching DSpace (HTTP) for paper:\n       {title}")
print(f"[INFO] GET {search_url}")
try:
    resp = requests.get(search_url, timeout=30)
    print(f"[INFO] Received status: {resp.status_code}")
except Exception as e:
    print(f"ERROR: Could not fetch search page via HTTP: {e}")
    sys.exit(1)

if resp.status_code != 200:
    print(f"ERROR: Unexpected status code {resp.status_code} on HTTP search. Exiting.")
    sys.exit(1)

# Save raw HTML for inspection
def save_file(path, content, mode='w'):
    with open(path, mode, encoding='utf-8') as f:
        f.write(content)
    print(f"[SAVE] {path}")

save_file(search_html, resp.text)

# --- Step 2: Parse HTML and collect PDF links ---
soup = BeautifulSoup(resp.text, 'html.parser')
pdf_links = set()

for a in soup.find_all('a', href=True):
    href = a['href']
    txt = a.get_text(strip=True).lower()
    # direct .pdf
    if href.lower().endswith('.pdf'):
        pdf_links.add(href)
    # DSpace bitstream URLs often include 'bitstream' and '.pdf'
    elif 'bitstream' in href.lower() and '.pdf' in href.lower():
        pdf_links.add(href)
    # anchor text mentions PDF
    elif 'pdf' in txt:
        pdf_links.add(href)

# Normalize to full URLs
full_links = []
for link in pdf_links:
    if link.startswith('http'):
        full_links.append(link)
    else:
        full_links.append(urljoin(base_url, link))

# Save candidate list
with open(candidates_txt, 'w', encoding='utf-8') as f:
    for url in full_links:
        f.write(url + '\n')
print(f"[SAVE] Candidate PDF URLs → {candidates_txt}")

# --- Step 3: Download first candidate if exists ---
if full_links:
    first = full_links[0]
    print(f"[INFO] Downloading PDF from: {first}")
    try:
        pdf_resp = requests.get(first, timeout=60)
        print(f"[INFO] PDF status code: {pdf_resp.status_code}")
    except Exception as e:
        print(f"ERROR: Failed to download PDF: {e}")
        sys.exit(1)

    if pdf_resp.status_code == 200 and len(pdf_resp.content) > 10_000:
        with open(pdf_out, 'wb') as f:
            f.write(pdf_resp.content)
        size = os.path.getsize(pdf_out)
        print(f"[SUCCESS] Saved PDF → {pdf_out} ({size} bytes)")
    else:
        print("ERROR: PDF download failed or file too small (<10 KB). Check the link and try again.")
        print(f"Content length: {len(pdf_resp.content)} bytes")
else:
    print("WARN: No PDF links found on search page. Please inspect 'lra_search.html' for correct link patterns.")
    print("Adjust link-selection logic or try a different repository endpoint.")
```