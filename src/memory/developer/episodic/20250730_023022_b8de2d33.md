### Development Step 1: Extract July 3 2023 LOTR Wikipedia outbound links toward A Song of Ice and Fire path

**Description**: Access the archived Wikipedia page for 'The Lord of the Rings' (book) as it appeared at the end of July 3, 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific version from that date. Extract all outbound links from the page content, focusing on internal Wikipedia links that could potentially lead toward 'A Song of Ice and Fire'. Create a comprehensive list of linked pages including literature, fantasy, author, publisher, and genre-related links that might serve as stepping stones in the path-finding process.

**Use Cases**:
- Digital humanities research on literary networks by extracting and analyzing historical outbound links from archived Wikipedia pages to trace thematic connections between epic novels over time
- Knowledge graph construction for fantasy literature publishing firms by automating the extraction and categorization of author, genre, and publisher links from archived encyclopedia entries
- SEO and content strategy optimization for book marketers by comparing archived internal link structures of bestselling fantasy titles to uncover cross-linking opportunities and trending subgenres
- Academic citation network reconstruction in publishing studies by retrieving historical Wikipedia revisions to map evolving references among authors, works, and related genres
- Automated corporate wiki migration for publishing houses by harvesting archived Wikipedia link data to build and cross-reference internal knowledge bases of authors, works, and adaptations
- Media adaptation impact analysis for entertainment studios by tracking changes in outbound links on “The Lord of the Rings” page before and after film or series releases to measure shifts in emphasis on related works
- Digital preservation auditing for libraries and archives by extracting and categorizing outbound links from archived web snapshots to monitor link decay and ensure cultural heritage integrity
- Conversational AI training data enrichment for chatbots by leveraging historical internal link networks from archived encyclopedia pages to provide richer literary context and recommendation paths

```
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime

print("=== ACCESSING ARCHIVED WIKIPEDIA PAGE: THE LORD OF THE RINGS (BOOK) ===\n")
print("Target: Wikipedia page as it appeared at end of July 3, 2023")
print("Objective: Extract all outbound links for path-finding analysis\n")

# The Wikipedia URL for The Lord of the Rings (book)
url = "https://en.wikipedia.org/wiki/The_Lord_of_the_Rings"

# Target date: July 3, 2023 (end of day)
date = "20230703235959"  # Using end of day timestamp

print(f"Checking Wayback Machine availability for: {url}")
print(f"Target date: {date[:4]}-{date[4:6]}-{date[6:8]} {date[8:10]}:{date[10:12]}:{date[12:14]}\n")

# Check if the webpage is available in the Wayback Machine
api_url = f"https://archive.org/wayback/available?url={url}&timestamp={date}"
print(f"Wayback API query: {api_url}\n")

avail_response = requests.get(api_url, timeout=20)

if avail_response.status_code == 200:
    avail_data = avail_response.json()
    print("Wayback Machine response received successfully")
    
    if "archived_snapshots" in avail_data and "closest" in avail_data["archived_snapshots"]:
        closest = avail_data["archived_snapshots"]["closest"]
        if closest["available"]:
            archive_url = closest["url"]
            archive_date = closest["timestamp"]
            print(f"✅ Archived version found!")
            print(f"Archive URL: {archive_url}")
            print(f"Actual archive date: {archive_date[:4]}-{archive_date[4:6]}-{archive_date[6:8]} {archive_date[8:10]}:{archive_date[10:12]}:{archive_date[12:14]}")
            print(f"Days from target: {(datetime.strptime(archive_date[:8], '%Y%m%d') - datetime.strptime('20230703', '%Y%m%d')).days}")
        else:
            print("❌ No archived version available for this date")
            exit()
    else:
        print("❌ No archived snapshots found")
        exit()
else:
    print(f"❌ Error checking archive availability: {avail_response.status_code}")
    exit()

print("\n=== RETRIEVING ARCHIVED PAGE CONTENT ===\n")

# Set appropriate headers for requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

print(f"Fetching archived page from: {archive_url}")

# Get the archived version of the webpage
response = requests.get(archive_url, headers=headers, timeout=30)
response.raise_for_status()

print(f"✅ Page retrieved successfully")
print(f"Content size: {len(response.content):,} bytes")
print(f"Content type: {response.headers.get('content-type', 'unknown')}\n")

# Parse the HTML content
soup = BeautifulSoup(response.content, 'html.parser')

# Remove Wayback Machine elements that might interfere
print("Cleaning Wayback Machine elements...")
for element in soup.find_all(class_=lambda x: x and 'wayback' in x.lower()):
    element.decompose()
for element in soup.find_all(id=lambda x: x and 'wayback' in x.lower()):
    element.decompose()

# Get page title and basic info
page_title = soup.find('title')
if page_title:
    title_text = page_title.get_text().strip()
    print(f"Page title: {title_text}")
else:
    print("Page title not found")

# Find the main article title
main_title = soup.find('h1', class_='firstHeading') or soup.find('h1')
if main_title:
    article_title = main_title.get_text().strip()
    print(f"Article title: {article_title}")
else:
    article_title = "The Lord of the Rings"
    print(f"Using default article title: {article_title}")

print("\n=== EXTRACTING ALL OUTBOUND LINKS ===\n")

# Find all links in the main article content
# Focus on the main content area, avoiding navigation, sidebar, etc.
content_selectors = [
    '#mw-content-text',  # Main content area
    '.mw-parser-output',  # Parser output
    '#content',           # General content
    '.mw-content-ltr'     # Left-to-right content
]

main_content = None
for selector in content_selectors:
    main_content = soup.select_one(selector)
    if main_content:
        print(f"Found main content using selector: {selector}")
        break

if not main_content:
    print("Main content area not found, using entire page")
    main_content = soup

# Extract all links from the main content
all_links = main_content.find_all('a', href=True)
print(f"Total links found in content: {len(all_links)}\n")

# Process and categorize links
wikipedia_links = []
external_links = []
anchor_links = []
other_links = []

base_wikipedia_url = 'https://en.wikipedia.org'

print("Processing and categorizing links...\n")

for link in all_links:
    href = link.get('href', '')
    link_text = link.get_text().strip()
    
    # Skip empty hrefs or javascript links
    if not href or href.startswith('javascript:') or href.startswith('data:'):
        continue
    
    # Make relative URLs absolute
    if href.startswith('/'):
        if href.startswith('/web/'):
            # This is a Wayback Machine URL structure
            # Extract the original URL
            wayback_match = re.search(r'/web/\d+/(https?://[^\s]+)', href)
            if wayback_match:
                original_url = wayback_match.group(1)
                href = original_url
            else:
                href = urljoin('https://en.wikipedia.org', href.split('/')[-1])
        else:
            href = urljoin('https://en.wikipedia.org', href)
    
    # Categorize links
    if '#' in href and not href.startswith('http'):
        # Anchor link within the same page
        anchor_links.append({
            'href': href,
            'text': link_text,
            'type': 'anchor'
        })
    elif 'en.wikipedia.org/wiki/' in href:
        # Internal Wikipedia link
        # Clean the URL by removing fragments and parameters
        clean_href = href.split('#')[0].split('?')[0]
        
        # Extract the article name from the URL
        article_match = re.search(r'/wiki/([^/?#]+)', clean_href)
        if article_match:
            article_name = article_match.group(1)
            
            # Skip certain types of pages
            skip_patterns = [
                'File:', 'Category:', 'Template:', 'User:', 'Talk:', 'Wikipedia:',
                'Help:', 'Portal:', 'Special:', 'Media:'
            ]
            
            if not any(pattern in article_name for pattern in skip_patterns):
                wikipedia_links.append({
                    'href': clean_href,
                    'text': link_text,
                    'article_name': article_name.replace('_', ' '),
                    'type': 'wikipedia_article'
                })
    elif href.startswith('http') and 'wikipedia.org' not in href:
        # External link
        external_links.append({
            'href': href,
            'text': link_text,
            'type': 'external'
        })
    else:
        # Other types of links
        other_links.append({
            'href': href,
            'text': link_text,
            'type': 'other'
        })

print(f"Link categorization complete:")
print(f"  Wikipedia article links: {len(wikipedia_links)}")
print(f"  External links: {len(external_links)}")
print(f"  Anchor links: {len(anchor_links)}")
print(f"  Other links: {len(other_links)}\n")

# Remove duplicates from Wikipedia links based on href
seen_hrefs = set()
unique_wikipedia_links = []
for link in wikipedia_links:
    if link['href'] not in seen_hrefs:
        seen_hrefs.add(link['href'])
        unique_wikipedia_links.append(link)

print(f"After removing duplicates: {len(unique_wikipedia_links)} unique Wikipedia links\n")

# Sort links alphabetically by article name
unique_wikipedia_links.sort(key=lambda x: x['article_name'].lower())

print("=== SAMPLE OF WIKIPEDIA LINKS FOUND ===\n")
for i, link in enumerate(unique_wikipedia_links[:20], 1):
    print(f"{i:2d}. {link['article_name']}")
    print(f"     URL: {link['href']}")
    print(f"     Link text: {link['text'][:60]}{'...' if len(link['text']) > 60 else ''}")
    print()

if len(unique_wikipedia_links) > 20:
    print(f"... and {len(unique_wikipedia_links) - 20} more links\n")

# Identify potentially relevant categories for path-finding to 'A Song of Ice and Fire'
relevant_categories = {
    'fantasy': ['fantasy', 'magic', 'dragon', 'wizard', 'medieval', 'epic', 'quest'],
    'literature': ['literature', 'novel', 'book', 'author', 'writer', 'fiction', 'narrative'],
    'genre': ['genre', 'speculative', 'science fiction', 'adventure', 'mythology'],
    'publishers': ['publisher', 'publishing', 'books', 'edition'],
    'authors': ['author', 'writer', 'tolkien', 'martin', 'george'],
    'adaptations': ['film', 'movie', 'television', 'series', 'adaptation', 'media'],
    'related_works': ['middle-earth', 'hobbit', 'silmarillion', 'game of thrones', 'ice and fire']
}

print("=== CATEGORIZING LINKS BY RELEVANCE FOR PATH-FINDING ===\n")

categorized_links = {category: [] for category in relevant_categories.keys()}
uncategorized_links = []

for link in unique_wikipedia_links:
    article_name_lower = link['article_name'].lower()
    link_text_lower = link['text'].lower()
    combined_text = f"{article_name_lower} {link_text_lower}"
    
    categorized = False
    for category, keywords in relevant_categories.items():
        if any(keyword in combined_text for keyword in keywords):
            categorized_links[category].append(link)
            categorized = True
            break
    
    if not categorized:
        uncategorized_links.append(link)

print("Links categorized by potential relevance:")
for category, links in categorized_links.items():
    print(f"  {category.upper()}: {len(links)} links")
    for link in links[:3]:  # Show first 3 in each category
        print(f"    - {link['article_name']}")
    if len(links) > 3:
        print(f"    ... and {len(links) - 3} more")
    print()

print(f"UNCATEGORIZED: {len(uncategorized_links)} links\n")

# Save comprehensive results to workspace
results = {
    'extraction_metadata': {
        'source_url': url,
        'archive_url': archive_url,
        'archive_date': archive_date,
        'formatted_date': f"{archive_date[:4]}-{archive_date[4:6]}-{archive_date[6:8]} {archive_date[8:10]}:{archive_date[10:12]}:{archive_date[12:14]}",
        'page_title': title_text if 'title_text' in locals() else 'Unknown',
        'article_title': article_title,
        'extraction_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_links_found': len(all_links),
        'wikipedia_links_count': len(unique_wikipedia_links),
        'external_links_count': len(external_links)
    },
    'wikipedia_links': unique_wikipedia_links,
    'categorized_links': categorized_links,
    'uncategorized_links': uncategorized_links,
    'external_links': external_links[:50],  # Limit external links to first 50
    'link_statistics': {
        'total_unique_wikipedia_links': len(unique_wikipedia_links),
        'categorized_counts': {cat: len(links) for cat, links in categorized_links.items()},
        'uncategorized_count': len(uncategorized_links)
    }
}

# Save to workspace
output_file = 'workspace/lotr_wikipedia_links_july_2023.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\n=== EXTRACTION COMPLETE ===\n")
print(f"✅ Successfully extracted links from archived Wikipedia page")
print(f"📅 Archive date: {archive_date[:4]}-{archive_date[4:6]}-{archive_date[6:8]}")
print(f"🔗 Total Wikipedia article links: {len(unique_wikipedia_links)}")
print(f"📂 Results saved to: {output_file}")
print(f"\n📊 Most promising categories for path-finding:")
for category, links in categorized_links.items():
    if links:
        print(f"   {category.upper()}: {len(links)} links")

print(f"\n🎯 Ready for path-finding analysis toward 'A Song of Ice and Fire'")
```