### Development Step 7: Research Abel Hugo’s Napoleonic service, map publications, and 1809 Madrid hospital under Joseph I

**Description**: Research Abel Hugo, Victor Hugo's brother, focusing on his military service during the Napoleonic Wars and his time in Madrid. Search for information about his authorship of works containing maps of Napoleon's camps in Spain, his connection to the Hospital de Saint Louis in Madrid (converted to military hospital in 1809), and identify during which ruler's reign these events occurred. Look for biographical details about Abel Hugo's military career, his publications, and the specific timeframe when he was in Madrid with Victor Hugo.

**Use Cases**:
- Academic historians automating extraction of Abel Hugo’s military service details and timeline during the Napoleonic Wars to populate digital archives with structured biographical records.
- Museum curators gathering and cross‐validating references to the Hospital de Saint Louis in Madrid (1809) across Wikipedia and Google Books to enrich exhibit labels and interactive displays.
- Historical fiction authors fact‐checking Abel Hugo’s publications on Napoleon’s camps in Spain under Joseph Bonaparte’s reign, ensuring narrative accuracy in 19th-century novels.
- Digital humanities teams integrating scraped map mentions and geospatial data of Napoleon’s encampments into GIS tools for visualizing troop movements during the Peninsular War.
- Educational technology developers compiling concise, keyword-filtered summaries of Abel Hugo and Victor Hugo’s time in Madrid to build interactive online history modules for secondary schools.
- Biographical database administrators harmonizing multilingual Wikipedia content and Google Books metadata to maintain up-to-date, structured profiles of lesser-known Napoleonic era figures.
- Cultural heritage researchers monitoring and extracting ruler-context keywords (e.g., Ferdinand VII, Joseph Bonaparte) from diverse sources to validate historical street-name restorations in Madrid’s municipal records.

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Abel Hugo research with completely fixed parsing - final attempt...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# First, let's inspect existing research files to understand what we have
print("\n=== INSPECTING EXISTING RESEARCH FILES ===")
existing_files = ['abel_hugo_final_research.json', 'abel_hugo_complete_research.json', 'abel_hugo_final_comprehensive_research.json']

for filename in existing_files:
    filepath = f'workspace/{filename}'
    if os.path.exists(filepath):
        print(f"\nInspecting: {filename}")
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            print(f"File structure:")
            for key, value in data.items():
                if isinstance(value, dict):
                    print(f"  - {key}: dict with keys: {list(value.keys())}")
                elif isinstance(value, list):
                    print(f"  - {key}: list with {len(value)} items")
                else:
                    print(f"  - {key}: {type(value).__name__}")
            
            # Check for results
            if 'all_results' in data:
                results = data['all_results']
                print(f"  Found {len(results)} total results")
                wikipedia_results = [r for r in results if r.get('source') == 'Wikipedia']
                google_books_results = [r for r in results if r.get('source') == 'Google Books']
                print(f"  Wikipedia: {len(wikipedia_results)}, Google Books: {len(google_books_results)}")
                
        except Exception as e:
            print(f"  Error reading {filename}: {str(e)}")

# Initialize results storage
all_results = []
search_errors = []

print("\n=== BASIC WIKIPEDIA SEARCH FOR ABEL HUGO ===")

# Wikipedia URLs to search
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Abel_Hugo",
    "https://fr.wikipedia.org/wiki/Abel_Hugo",
    "https://en.wikipedia.org/wiki/Victor_Hugo"
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

for url in wikipedia_urls:
    try:
        print(f"\nFetching: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        print(f"Page title: {title}")
        
        # Extract main content - BASIC APPROACH WITHOUT COMPLEX FILTERING
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove only scripts and styles - nothing else
            for script in content_div.find_all('script'):
                script.decompose()
            for style in content_div.find_all('style'):
                style.decompose()
            
            # Get all text content
            content_text = content_div.get_text(separator=' ', strip=True)
            print(f"Content length: {len(content_text)} characters")
            
            # Target keywords for Abel Hugo research
            target_keywords = [
                'abel hugo', 'victor hugo', 'napoleonic wars', 'madrid', 'spain', 
                'military service', 'military hospital', 'hospital',
                'napoleon', 'camps', 'maps', 'publications', 'author', 'brother',
                '1809', 'peninsular war', 'french army', 'officer', 'career',
                'joseph bonaparte', 'ferdinand vii', 'saint louis', 'saint-louis'
            ]
            
            # Find matching keywords - SIMPLE APPROACH
            content_lower = content_text.lower()
            found_keywords = []
            for keyword in target_keywords:
                if keyword in content_lower:
                    found_keywords.append(keyword)
            
            print(f"Keywords found: {', '.join(found_keywords)}")
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content_text[:8000],  # First 8000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                all_results.append(result)
                print(f"✓ Added result with {len(found_keywords)} keyword matches")
                
                # Special analysis for Abel Hugo content
                if 'abel hugo' in content_lower:
                    print("\n*** ABEL HUGO BIOGRAPHICAL CONTENT FOUND ***")
                    
                    # Extract sentences mentioning Abel Hugo
                    sentences = content_text.split('.')
                    abel_sentences = []
                    
                    for sentence in sentences:
                        sentence_clean = sentence.strip()
                        if len(sentence_clean) > 20:
                            sentence_lower = sentence_clean.lower()
                            if 'abel hugo' in sentence_lower:
                                abel_sentences.append(sentence_clean)
                    
                    print(f"Found {len(abel_sentences)} sentences about Abel Hugo:")
                    for i, sentence in enumerate(abel_sentences[:5], 1):
                        print(f"  {i}. {sentence[:350]}...")
                    
                    # Look for specific information
                    military_info = [s for s in abel_sentences if any(term in s.lower() for term in ['military', 'army', 'officer', 'service', 'war'])]
                    madrid_info = [s for s in abel_sentences if 'madrid' in s.lower()]
                    publication_info = [s for s in abel_sentences if any(term in s.lower() for term in ['author', 'wrote', 'published', 'work', 'book'])]
                    
                    if military_info:
                        print(f"\n  Military information ({len(military_info)} sentences):")
                        for info in military_info[:2]:
                            print(f"    - {info[:300]}...")
                    
                    if madrid_info:
                        print(f"\n  Madrid information ({len(madrid_info)} sentences):")
                        for info in madrid_info[:2]:
                            print(f"    - {info[:300]}...")
                    
                    if publication_info:
                        print(f"\n  Publication information ({len(publication_info)} sentences):")
                        for info in publication_info[:2]:
                            print(f"    - {info[:300]}...")
            
        time.sleep(2)  # Be respectful to Wikipedia
        
    except Exception as e:
        error_msg = f"Error fetching {url}: {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

# Load existing Google Books results
print(f"\n=== LOADING EXISTING GOOGLE BOOKS RESULTS ===")
existing_google_books = []

for filename in existing_files:
    filepath = f'workspace/{filename}'
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            if 'all_results' in data:
                results = data['all_results']
                books = [r for r in results if r.get('source') == 'Google Books']
                existing_google_books.extend(books)
                print(f"Loaded {len(books)} Google Books results from {filename}")
                
        except Exception as e:
            print(f"Error loading {filename}: {str(e)}")

# Remove duplicate Google Books results
seen_titles = set()
unique_books = []
for book in existing_google_books:
    title = book.get('title', '')
    if title and title not in seen_titles:
        seen_titles.add(title)
        unique_books.append(book)

print(f"Unique Google Books results: {len(unique_books)}")

# Combine all results
all_results.extend(unique_books)
all_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print(f"\n=== COMPREHENSIVE ANALYSIS OF ALL RESULTS ===")
print(f"Total results: {len(all_results)}")
print(f"Wikipedia results: {len([r for r in all_results if r['source'] == 'Wikipedia'])}")
print(f"Google Books results: {len([r for r in all_results if r['source'] == 'Google Books'])}")

# Analyze all results for Abel Hugo information
abel_hugo_findings = {
    'military_service_details': [],
    'madrid_connections': [],
    'napoleon_maps_camps': [],
    'hospital_saint_louis': [],
    'publications_authorship': [],
    'timeframe_1809': [],
    'ruler_context': [],
    'family_relations': [],
    'biographical_sources': []
}

for result in all_results:
    # Get text content for analysis - SIMPLE APPROACH
    text_content = ''
    original_text = ''
    
    if 'content' in result and result['content']:
        text_content = result['content'].lower()
        original_text = result['content']
    elif 'description' in result and result['description']:
        text_content = result['description'].lower()
        original_text = result['description']
    elif 'title' in result:
        text_content = result['title'].lower()
        original_text = result['title']
    
    # Check for Abel Hugo relevance
    is_relevant = ('abel hugo' in text_content or 
                  ('abel' in text_content and 'hugo' in text_content and 'victor' in text_content) or
                  result['relevance_score'] >= 3)
    
    if is_relevant:
        print(f"\nAnalyzing: {result['title']} (Score: {result['relevance_score']})")
        
        # Military service
        if any(term in text_content for term in ['military', 'army', 'officer', 'service', 'soldier', 'war', 'campaign']):
            abel_hugo_findings['military_service_details'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Military service information found")
        
        # Madrid connections
        if 'madrid' in text_content:
            abel_hugo_findings['madrid_connections'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Madrid connection found")
        
        # Napoleon maps/camps
        if 'napoleon' in text_content and ('maps' in text_content or 'camps' in text_content):
            abel_hugo_findings['napoleon_maps_camps'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Napoleon maps/camps reference found")
        
        # Hospital Saint Louis
        if 'hospital' in text_content and ('saint louis' in text_content or 'saint-louis' in text_content):
            abel_hugo_findings['hospital_saint_louis'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Hospital Saint Louis reference found")
        
        # Publications and authorship
        if any(term in text_content for term in ['author', 'wrote', 'published', 'publication', 'work', 'book']):
            abel_hugo_findings['publications_authorship'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Publication/authorship information found")
        
        # 1809 timeframe
        if '1809' in text_content:
            abel_hugo_findings['timeframe_1809'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ 1809 timeframe reference found")
        
        # Ruler context
        if any(ruler in text_content for ruler in ['joseph bonaparte', 'napoleon', 'ferdinand vii', 'charles iv']):
            abel_hugo_findings['ruler_context'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Ruler context found")
        
        # Family relations
        if 'victor hugo' in text_content and 'brother' in text_content:
            abel_hugo_findings['family_relations'].append({
                'source': result['title'],
                'url': result.get('url', ''),
                'text_sample': original_text[:1000],
                'keywords': result['keywords_found'],
                'score': result['relevance_score']
            })
            print("  ✓ Family relations found")
        
        # Add to biographical sources
        abel_hugo_findings['biographical_sources'].append({
            'source': result['title'],
            'url': result.get('url', ''),
            'text_sample': original_text[:1200],
            'keywords': result['keywords_found'],
            'score': result['relevance_score'],
            'source_type': result['source']
        })

# Save final comprehensive research
final_research_data = {
    'research_date': datetime.now().isoformat(),
    'research_objective': 'Abel Hugo research: military service during Napoleonic Wars, Madrid connections, Napoleon camp maps, Hospital de Saint Louis, publications, ruler context during 1809',
    'search_summary': {
        'total_results': len(all_results),
        'wikipedia_results': len([r for r in all_results if r['source'] == 'Wikipedia']),
        'google_books_results': len([r for r in all_results if r['source'] == 'Google Books']),
        'search_errors': len(search_errors),
        'highest_relevance_score': max([r['relevance_score'] for r in all_results]) if all_results else 0
    },
    'all_research_results': all_results,
    'abel_hugo_detailed_findings': abel_hugo_findings,
    'search_errors': search_errors
}

final_output_file = 'workspace/abel_hugo_complete_final_research.json'
with open(final_output_file, 'w') as f:
    json.dump(final_research_data, f, indent=2)

print(f"\n{'='*80}")
print("ABEL HUGO RESEARCH - FINAL COMPREHENSIVE SUMMARY")
print(f"{'='*80}")
print(f"Complete research saved to: {final_output_file}")
print(f"Total sources analyzed: {len(all_results)}")
print(f"Wikipedia pages successfully processed: {len([r for r in all_results if r['source'] == 'Wikipedia'])}")
print(f"Google Books sources included: {len([r for r in all_results if r['source'] == 'Google Books'])}")

# Display key research findings
print(f"\n=== ABEL HUGO KEY RESEARCH FINDINGS ===")
for category, findings in abel_hugo_findings.items():
    if findings and category != 'biographical_sources':
        print(f"\n{category.replace('_', ' ').title()}: {len(findings)} sources found")
        # Show the highest scoring source for this category
        best_source = max(findings, key=lambda x: x['score'])
        print(f"  Best source: {best_source['source']} (Score: {best_source['score']})")
        print(f"  Keywords: {', '.join(best_source['keywords'])}")
        print(f"  Sample text: {best_source['text_sample'][:400]}...")

# Display top overall sources
print(f"\n=== TOP SOURCES FOR ABEL HUGO RESEARCH ===")
for i, result in enumerate(all_results[:8], 1):
    print(f"\n{i}. {result['title']} (Relevance Score: {result['relevance_score']})")
    print(f"   Source: {result['source']}")
    if 'authors' in result:
        print(f"   Authors: {', '.join(result['authors'])}")
    print(f"   Keywords: {', '.join(result['keywords_found'])}")
    
    # Display content preview
    if 'content' in result and len(result['content']) > 100:
        print(f"   Content preview: {result['content'][:300]}...")
    elif 'description' in result:
        print(f"   Description: {result['description'][:300]}...")

print(f"\n=== RESEARCH ERRORS AND STATUS ===")
if search_errors:
    print(f"Search errors encountered: {len(search_errors)}")
    for error in search_errors:
        print(f"  - {error}")
else:
    print("No search errors - all sources processed successfully!")

print(f"\nAbel Hugo research completed successfully! Full results available in {final_output_file}")
```