### Development Step 2: Identify the Author of Third Volume ‘Francia’s Reign of Terror’ Letters Documenting Four-Year Paraguay Stay

**Description**: Search for information about a collection of letters titled 'Francia's Reign of Terror' that documents a four-year stay under Dictator Francia's rule in Paraguay. Focus on identifying the author of this work, which is described as the third volume that criticizes local laziness and government policies and was written while relying on local hospitality. Search using keywords including 'Francia's Reign of Terror letters Paraguay dictator', 'four year stay Francia Paraguay author', and 'third volume Francia Paraguay criticism government policies'.

**Use Cases**:
- Digital humanities researcher automating the discovery and author attribution of 19th-century Paraguayan correspondence for a scholarly corpus
- Academic historian compiling bibliographic metadata on multi-volume memoirs about Dictator Francia’s policies for publication in a peer-reviewed journal
- Archival librarian enriching catalog records by extracting author names and criticism themes from a digitized letters collection on Francia’s Reign of Terror
- University history department conducting a systematic literature review of primary sources on a four-year stay under Francia’s regime using automated search and scraping
- Cultural heritage institution indexing and annotating third-volume manuscripts that criticize local laziness and government policies for a public digital exhibition
- Graduate student gathering and analyzing excerpts on local hospitality and societal attitudes under Francia’s dictatorship as part of a doctoral dissertation
- Historical society cross-referencing search engine results and Wikipedia content to validate authorship and context of Paraguay correspondence for a curated archive

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Searching for information about 'Francia's Reign of Terror' letters collection...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

def search_with_requests(query, num_results=10):
    """Search using requests and Google search (alternative to DDGS)"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # Use DuckDuckGo HTML search as alternative
    search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"
    
    try:
        print(f"Searching: {query}")
        response = requests.get(search_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # Parse DuckDuckGo results
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:num_results]:
            title_elem = div.find('a', class_='result__a')
            snippet_elem = div.find('a', class_='result__snippet')
            
            if title_elem:
                title = title_elem.get_text(strip=True)
                href = title_elem.get('href', '')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                
                results.append({
                    'title': title,
                    'href': href,
                    'body': snippet,
                    'search_query': query
                })
        
        print(f"Found {len(results)} results for: {query}")
        return results
        
    except Exception as e:
        print(f"Error searching for '{query}': {str(e)}")
        return []

def search_wikipedia_directly():
    """Search Wikipedia directly for Francia-related articles"""
    print("\n=== SEARCHING WIKIPEDIA DIRECTLY ===")
    
    wikipedia_searches = [
        "https://en.wikipedia.org/wiki/Jos%C3%A9_Gaspar_Rodr%C3%ADguez_de_Francia",
        "https://en.wikipedia.org/wiki/Paraguay",
        "https://en.wikipedia.org/wiki/History_of_Paraguay"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    wikipedia_results = []
    
    for url in wikipedia_searches:
        try:
            print(f"Fetching Wikipedia page: {url}")
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract title
            title_elem = soup.find('h1', class_='firstHeading')
            title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
            
            # Extract main content
            content_div = soup.find('div', {'id': 'mw-content-text'})
            if content_div:
                # Remove unwanted elements
                for elem in content_div.find_all(['script', 'style', 'table', 'div']):
                    if elem.get('class') and any(cls in str(elem.get('class')) for cls in ['navbox', 'infobox', 'reference']):
                        elem.decompose()
                
                content = content_div.get_text(separator=' ', strip=True)
                
                # Look for Francia-related keywords
                content_lower = content.lower()
                francia_keywords = ['francia', 'dictator', 'paraguay', 'reign', 'terror', 'letters', 'correspondence', 'memoir', 'account']
                
                found_keywords = [kw for kw in francia_keywords if kw in content_lower]
                
                if found_keywords:
                    wikipedia_results.append({
                        'title': title,
                        'href': url,
                        'body': content[:2000],  # First 2000 characters
                        'keywords_found': found_keywords,
                        'source': 'Wikipedia'
                    })
                    print(f"Found relevant content in {title} - Keywords: {', '.join(found_keywords)}")
            
            time.sleep(1)  # Be respectful to Wikipedia
            
        except Exception as e:
            print(f"Error fetching Wikipedia page {url}: {str(e)}")
    
    return wikipedia_results

def search_francia_letters():
    """Main search function for Francia's Reign of Terror letters"""
    print("\n=== SEARCHING FOR FRANCIA'S REIGN OF TERROR LETTERS ===")
    
    # Define search queries
    search_queries = [
        "Francia's Reign of Terror letters Paraguay dictator",
        "four year stay Francia Paraguay author",
        "third volume Francia Paraguay criticism government policies",
        "Francia Paraguay dictator letters collection",
        "Paraguay Francia reign terror author four years",
        "Francia dictator Paraguay third volume letters",
        "Paraguay Francia government criticism letters",
        "Francia Paraguay local hospitality author letters",
        "Jose Gaspar Rodriguez de Francia letters memoir",
        "Paraguay dictator Francia correspondence"
    ]
    
    all_search_results = []
    
    # Search using requests-based method
    for i, query in enumerate(search_queries, 1):
        print(f"\nSearch {i}/{len(search_queries)}: {query}")
        results = search_with_requests(query, num_results=8)
        all_search_results.extend(results)
        time.sleep(2)  # Be respectful to search engines
    
    # Also search Wikipedia directly
    wikipedia_results = search_wikipedia_directly()
    all_search_results.extend(wikipedia_results)
    
    return all_search_results

def analyze_search_results(results):
    """Analyze search results for relevant information about Francia's letters"""
    print(f"\n=== ANALYZING {len(results)} TOTAL SEARCH RESULTS ===")
    
    # Keywords to look for in results
    relevant_keywords = [
        'francia',
        'paraguay',
        'dictator',
        'letters',
        'reign of terror',
        'four year',
        'four-year',
        'third volume',
        'author',
        'criticism',
        'government policies',
        'local hospitality',
        'laziness',
        'stay',
        'collection',
        'memoir',
        'correspondence',
        'account',
        'documented'
    ]
    
    relevant_results = []
    
    for result in results:
        title = result.get('title', '').lower()
        body = result.get('body', '').lower()
        combined_text = title + ' ' + body
        
        # Count keyword matches
        keyword_matches = []
        for keyword in relevant_keywords:
            if keyword in combined_text:
                keyword_matches.append(keyword)
        
        # Consider result relevant if it has multiple keyword matches
        if len(keyword_matches) >= 2:
            result['keyword_matches'] = keyword_matches
            result['relevance_score'] = len(keyword_matches)
            relevant_results.append(result)
    
    # Sort by relevance score
    relevant_results.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    print(f"Found {len(relevant_results)} highly relevant results")
    
    return relevant_results

def extract_author_information(results):
    """Extract potential author information from search results"""
    print("\n=== EXTRACTING AUTHOR INFORMATION ===")
    
    author_candidates = []
    
    # Common author name patterns in historical texts
    potential_authors = [
        'robertson',
        'washburn',
        'masterman',
        'rengger',
        'longchamp',
        'wisner',
        'parish',
        'henderson',
        'thompson',
        'carlyle'
    ]
    
    for result in results:
        title = result.get('title', '')
        body = result.get('body', '')
        combined_text = title + ' ' + body
        combined_lower = combined_text.lower()
        
        # Look for author-related patterns
        author_indicators = [
            'written by',
            'author',
            'by ',
            'memoir',
            'account',
            'letters',
            'correspondence',
            'documented by',
            'recorded by',
            'volume',
            'published'
        ]
        
        # Check for potential author names
        found_authors = []
        for author in potential_authors:
            if author in combined_lower:
                found_authors.append(author)
        
        for indicator in author_indicators:
            if indicator in combined_lower:
                # Extract surrounding text that might contain author name
                pos = combined_lower.find(indicator)
                if pos != -1:
                    # Get text around the indicator
                    start = max(0, pos - 150)
                    end = min(len(combined_text), pos + 300)
                    context = combined_text[start:end]
                    
                    author_candidates.append({
                        'source_title': title,
                        'source_url': result.get('href', ''),
                        'indicator': indicator,
                        'context': context,
                        'relevance_score': result.get('relevance_score', 0),
                        'potential_authors': found_authors
                    })
    
    return author_candidates

def analyze_for_specific_details(results):
    """Look for specific details mentioned in the plan"""
    print("\n=== ANALYZING FOR SPECIFIC DETAILS ===")
    
    specific_details = {
        'four_year_stay': [],
        'third_volume': [],
        'criticism_laziness': [],
        'government_policies': [],
        'local_hospitality': [],
        'reign_of_terror': []
    }
    
    for result in results:
        title = result.get('title', '')
        body = result.get('body', '')
        combined_text = (title + ' ' + body).lower()
        
        # Look for four-year stay mentions
        if 'four year' in combined_text or 'four-year' in combined_text:
            specific_details['four_year_stay'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for third volume mentions
        if 'third volume' in combined_text or 'volume 3' in combined_text or 'vol. 3' in combined_text:
            specific_details['third_volume'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for criticism of laziness
        if 'laziness' in combined_text or 'lazy' in combined_text:
            specific_details['criticism_laziness'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for government policies criticism
        if 'government policies' in combined_text or 'policy' in combined_text:
            specific_details['government_policies'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for local hospitality mentions
        if 'hospitality' in combined_text or 'local hospitality' in combined_text:
            specific_details['local_hospitality'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
        
        # Look for reign of terror mentions
        if 'reign of terror' in combined_text:
            specific_details['reign_of_terror'].append({
                'source': title,
                'url': result.get('href', ''),
                'context': body[:500]
            })
    
    return specific_details

# Execute the search
print("Starting comprehensive search for Francia's Reign of Terror letters...")
search_results = search_francia_letters()

print(f"\nTotal search results collected: {len(search_results)}")

# Analyze results for relevance
relevant_results = analyze_search_results(search_results)

# Extract author information
author_candidates = extract_author_information(relevant_results)

# Analyze for specific details
specific_details = analyze_for_specific_details(relevant_results)

# Save all results to workspace
all_data = {
    'search_date': datetime.now().isoformat(),
    'search_summary': {
        'total_results': len(search_results),
        'relevant_results': len(relevant_results),
        'author_candidates': len(author_candidates)
    },
    'all_search_results': search_results,
    'relevant_results': relevant_results,
    'author_candidates': author_candidates,
    'specific_details': specific_details
}

output_file = 'workspace/francia_letters_search_results.json'
with open(output_file, 'w') as f:
    json.dump(all_data, f, indent=2)

print(f"\nAll search data saved to: {output_file}")

# Display top relevant results
print("\n=== TOP RELEVANT RESULTS ===")
for i, result in enumerate(relevant_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result.get('title', 'No title')}")
    print(f"URL: {result.get('href', 'No URL')}")
    print(f"Keywords found: {', '.join(result['keyword_matches'])}")
    print(f"Description: {result.get('body', 'No description')[:400]}...")

# Display author candidates
print("\n=== POTENTIAL AUTHOR INFORMATION ===")
for i, candidate in enumerate(author_candidates[:5], 1):
    print(f"\nAuthor Candidate {i}:")
    print(f"Source: {candidate['source_title']}")
    print(f"URL: {candidate['source_url']}")
    print(f"Indicator: {candidate['indicator']}")
    print(f"Potential Authors: {', '.join(candidate['potential_authors']) if candidate['potential_authors'] else 'None identified'}")
    print(f"Context: {candidate['context'][:300]}...")

# Display specific details found
print("\n=== SPECIFIC DETAILS ANALYSIS ===")
for detail_type, details in specific_details.items():
    if details:
        print(f"\n{detail_type.replace('_', ' ').title()}: {len(details)} mentions found")
        for detail in details[:2]:  # Show first 2 mentions
            print(f"  - Source: {detail['source']}")
            print(f"    Context: {detail['context'][:200]}...")

print(f"\n=== SEARCH SUMMARY ===")
print(f"Total queries executed: 10")
print(f"Total results found: {len(search_results)}")
print(f"Relevant results: {len(relevant_results)}")
print(f"Author candidates identified: {len(author_candidates)}")
print(f"\nSearch completed successfully!")
```