### Development Step 41: Scrape Survivor Seasons 1–44 Winners from Wikipedia and Save to JSON

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automated fan website updates for a Survivor fandom portal that displays the latest season winners and highlights all past champions on interactive timelines
- Data journalism pipeline for a TV industry analyst to generate season-by-season visualizations of Survivor winners’ demographics in JSON format for web articles
- Trivia mobile app backend that populates quiz questions with accurate Survivor winner names and seasons, ensuring the app stays current without manual edits
- Sports betting analytics model that ingests historical reality-show outcomes to calculate trends and odds for upcoming competition-based series
- Academic research on diversity in reality television winners, where scholars extract winner lists to correlate with age, gender, and ethnicity data across seasons
- Social media automation bot for a TV network that schedules “On This Day” posts featuring Survivor winners from past seasons, pulling data directly from the JSON file
- Content aggregator service compiling a unified dataset of reality-show champions across networks, using the Survivor winners JSON as one component of a broader entertainment database

```
import os
import sys
import re
import json
import requests
from bs4 import BeautifulSoup

# 1) Locate the active workspace directory
dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor Wikipedia page
page_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/wiki/{page_title}"
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0',
    'Accept-Language': 'en-US,en;q=0.9'
})
resp.raise_for_status()
print(f"Page fetched successfully (status {resp.status_code})\n")

# 3) Parse the HTML
txt = resp.text
soup = BeautifulSoup(txt, 'html.parser')

# 4) FIRST PASS: scan every <table> for an exact two-column header ['Season','Winner']
print("Scanning all <table> tags for exact 2-column [Season, Winner] header...\n")
tables = soup.find_all('table')
print(f"Total tables found: {len(tables)}")
target = None
for idx, tbl in enumerate(tables, 1):
    first_row = tbl.find('tr')
    if not first_row:
        continue
    hdr_cells = first_row.find_all(['th','td'], recursive=False)
    hdr_texts = [c.get_text(strip=True).lower() for c in hdr_cells]
    if len(hdr_texts) == 2:
        print(f"  Table #{idx} header: {hdr_texts}")
        if hdr_texts == ['season', 'winner']:
            target = tbl
            print(f"→ Selected table #{idx} as the exact two-column Winners table.\n")
            break

# 5) SECOND PASS: fallback to any .wikitable containing both 'season' & 'winner'
if not target:
    print("No exact 2-column table found. Falling back to scanning .wikitable tables...\n")
    wikitables = soup.find_all('table', class_=lambda v: v and 'wikitable' in v)
    for idx, tbl in enumerate(wikitables, 1):
        first_row = tbl.find('tr')
        if not first_row:
            continue
        hdr_cells = first_row.find_all(['th','td'], recursive=False)
        hdr_texts = [c.get_text(strip=True).lower() for c in hdr_cells]
        print(f"  Wikitable #{idx} header: {hdr_texts}")
        # ensure both keywords present
        if 'season' in hdr_texts and 'winner' in hdr_texts:
            target = tbl
            print(f"→ Selected wikitable #{idx} containing Season & Winner.\n")
            break

# 6) Abort if still not found
def bail():
    print("❌ Could not find a suitable winners table. Exiting.")
    sys.exit(1)

if not target:
    bail()

# 7) Determine indices for Season and Winner columns
def get_header_indices(tbl):
    first_row = tbl.find('tr')
    cells = first_row.find_all(['th','td'], recursive=False)
    texts = [c.get_text(strip=True).lower() for c in cells]
    return texts.index('season'), texts.index('winner')

season_i, winner_i = get_header_indices(target)
print(f"Column indices -> season: {season_i}, winner: {winner_i}\n")

# 8) Extract season->winner pairs
winners = []
for row in target.find_all('tr')[1:]:  # skip header row
    cells = row.find_all(['th','td'], recursive=False)
    if len(cells) <= max(season_i, winner_i):
        continue
    # parse season number
    s_txt = cells[season_i].get_text(strip=True)
    m = re.match(r"^(\d+)", s_txt)
    if not m:
        continue
    season = int(m.group(1))
    if not (1 <= season <= 44):
        continue
    # extract winner name(s)
    w_cell = cells[winner_i]
    links = w_cell.find_all('a')
    if links:
        names = [a.get_text(strip=True) for a in links if re.search(r"[A-Za-z]", a.get_text())]
        name = ' & '.join(names) if names else w_cell.get_text(strip=True)
    else:
        name = w_cell.get_text(strip=True)
    print(f"Parsed Season {season} → Winner: '{name}'")
    winners.append({'season': season, 'winner': name})

# 9) Sort and verify count
winners_sorted = sorted(winners, key=lambda x: x['season'])
print(f"\nTotal winners extracted: {len(winners_sorted)} (expected 44)")
if len(winners_sorted) != 44:
    print("⚠️ Warning: Extracted count != 44. Verify table selection and page structure.")
else:
    print("✅ Correct number of winners found.")

# 10) Save to JSON
out_path = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_path, 'w', encoding='utf-8') as f:
    json.dump(winners_sorted, f, indent=2)
print(f"\n✅ Winners list saved to: {out_path}")
```