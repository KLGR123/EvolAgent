### Development Step 30: Download Leicester paper 'Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?' PDF.

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated legal e-discovery and bulk download of PDF court opinions from federal and state judiciary websites for a law firm’s case preparation
- Clinical trial protocol aggregation by scraping PDF reports from university medical school repositories to support pharmaceutical research compliance
- Environmental policy monitoring via automated retrieval of regulatory PDF documents from government domains (epa.gov, defra.gov.uk) for corporate sustainability teams
- Supply chain quality assurance through extraction of PDF technical datasheets and compliance certificates from manufacturer websites in procurement automation workflows
- Competitive intelligence gathering by bulk-downloading industry whitepapers and market research PDFs from association and think tank domains for strategic planning
- Academic meta‐analysis support by harvesting peer-reviewed PDF articles from institutional archives (e.g., le.ac.uk, core.ac.uk) for systematic literature reviews
- Financial due diligence automation by scraping PDF annual reports and audit statements from company investor relations portals for M&A analysts

```
import os
import sys
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
html_path = os.path.join(workspace, 'bing_search.html')
out_path = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Preconditions ---
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist. Run the search script first.")
    sys.exit(1)
if not os.path.isfile(html_path):
    print(f"ERROR: Bing search HTML not found at {html_path}. Please generate it first.")
    sys.exit(1)
print(f"[INFO] Reading HTML from: {html_path}\n")

# --- Load HTML ---
with open(html_path, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()

# 1) Extract direct <a href> links containing ".pdf"
print("[STEP] Extracting literal .pdf hrefs from <a> tags...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.lower().endswith('.pdf') or '.pdf?' in href.lower():
        pdf_urls.add(href)
print(f"    Found {len(pdf_urls)} direct .pdf hrefs so far.")

# 2) Handle Bing redirect links (/url? q=...pdf)
print("[STEP] Extracting Bing redirect links (/url?q=...pdf)...")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q', 'u', 'url'):
            if key in qs:
                real = qs[key][0]
                # decode percent-encoding
                real = unquote(real)
                if real.lower().endswith('.pdf') or '.pdf?' in real.lower():
                    pdf_urls.add(real)
                break
print(f"    Total after redirect extraction: {len(pdf_urls)} candidates.")

# 3) Regex fallback: full URLs in HTML text
print("[STEP] Running regex fallback to catch any PDF URLs in page text...")
pattern = r'https?://[^\s"\']+?\.pdf(?:\?[^\s"\']*)?'
for match in re.findall(pattern, html, flags=re.IGNORECASE):
    pdf_urls.add(match)
print(f"    Total after regex fallback: {len(pdf_urls)} candidates.")

# 4) Filter for likely University of Leicester domains
print("[STEP] Filtering for Leicester-related domains...")
domains = ('le.ac.uk', 'lra.le.ac.uk', 'core.ac.uk', 'hdl.handle.net')
leicester_links = [u for u in pdf_urls if any(d in u.lower() for d in domains)]
if leicester_links:
    print(f"    {len(leicester_links)} candidate(s) match Leicester domains:")
    for u in leicester_links:
        print(f"      - {u}")
else:
    print("    No Leicester-specific candidates found; reporting all.")

# --- Save all candidates for manual inspection ---
with open(out_path, 'w', encoding='utf-8') as f:
    for url in sorted(pdf_urls):
        f.write(url + '\n')
print(f"\n[SAVE] All PDF URL candidates written to: {out_path}")

print("[COMPLETE] Extraction of PDF link candidates is done. Inspect the output file and domain-filtered list.")
```