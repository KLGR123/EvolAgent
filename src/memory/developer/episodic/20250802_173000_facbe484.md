### Development Step 7: Identify who nominated Giganotosaurus for Featured Article status in November 2016

**Description**: Access the Wikipedia Featured Article Candidates (FAC) page or nomination history for the Giganotosaurus article to identify who originally nominated it for Featured Article status in November 2016. Search for the FAC discussion page, nomination details, or article talk page archives that show the nomination process, including the username of the person who submitted the initial nomination for Featured Article consideration.

**Use Cases**:
- Wikipedia community coordinators auditing the nomination workflow and contributor history for Giganotosaurus to improve Featured Article project guidelines
- Academic researchers analyzing the patterns of user nominations and discussion timelines in November 2016 for a study on editorial behavior
- Data journalists investigating transparency in Wikipedia‚Äôs Featured Article process by extracting who nominated high-profile pages and when
- Automated bots monitoring nomination archives across multiple species articles to generate alerts when a Featured Article candidate is submitted
- Quality assurance teams validating archive completeness and nomination metadata for Wikipedia administrators during annual site reviews
- Digital librarians archiving the provenance of article nominations and talk page discussions for long-term historical records
- Machine learning engineers building a training dataset of nomination events and editor usernames for predicting future Featured Article successes
- Non-profit project managers compiling nomination and discussion logs to prepare collaboration reports for volunteer Wiki editors

```
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import re
import os

print("=== SEARCHING FOR GIGANOTOSAURUS FEATURED ARTICLE NOMINATION (NOVEMBER 2016) ===")
print("Objective: Find who nominated Giganotosaurus for Featured Article status in November 2016\n")

# Create workspace directory
workspace_dir = "workspace"
os.makedirs(workspace_dir, exist_ok=True)

# Request headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print("=== STEP 1: ACCESSING WIKIPEDIA FEATURED ARTICLE CANDIDATES ARCHIVES ===")

# Try to access the FAC archives for 2016
fac_urls = [
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/archived_nominations/2016",
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Giganotosaurus",
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Giganotosaurus/archive1"
]

for i, url in enumerate(fac_urls, 1):
    print(f"\n{i}. Trying: {url}")
    
    try:
        response = requests.get(url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            print(f"   ‚úÖ Successfully accessed (Content length: {len(response.content):,} bytes)")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_title = soup.find('title')
            print(f"   Page title: {page_title.get_text().strip() if page_title else 'Unknown'}")
            
            # Save the HTML for analysis
            filename = f"fac_page_{i}.html"
            filepath = os.path.join(workspace_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"   üìÅ Saved to: {filename}")
            
            # Look for Giganotosaurus mentions
            page_text = soup.get_text().lower()
            if 'giganotosaurus' in page_text:
                print(f"   üéØ Found 'Giganotosaurus' mentions on this page!")
                
                # Extract sections containing Giganotosaurus
                giganto_sections = []
                for element in soup.find_all(text=re.compile(r'giganotosaurus', re.IGNORECASE)):
                    parent = element.parent
                    if parent:
                        # Get surrounding context
                        context = parent.get_text().strip()
                        if len(context) > 50:  # Only meaningful contexts
                            giganto_sections.append(context)
                
                if giganto_sections:
                    print(f"   Found {len(giganto_sections)} sections mentioning Giganotosaurus")
                    for j, section in enumerate(giganto_sections[:3], 1):  # Show first 3
                        print(f"   Section {j}: {section[:200]}..." if len(section) > 200 else f"   Section {j}: {section}")
            else:
                print(f"   ‚ùå No 'Giganotosaurus' mentions found")
                
        elif response.status_code == 404:
            print(f"   ‚ùå Page not found (404)")
        else:
            print(f"   ‚ùå HTTP error: {response.status_code}")
            
    except Exception as e:
        print(f"   ‚ùå Error accessing URL: {str(e)}")
    
    # Add delay between requests
    time.sleep(2)

print("\n=== STEP 2: SEARCHING WIKIPEDIA FAC MAIN PAGES ===")

# Try the main FAC page and look for archives
main_fac_urls = [
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates",
    "https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Archived_nominations"
]

for i, url in enumerate(main_fac_urls, 1):
    print(f"\n{i}. Accessing main FAC page: {url}")
    
    try:
        response = requests.get(url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            print(f"   ‚úÖ Successfully accessed")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for archive links to 2016
            archive_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                link_text = link.get_text().strip()
                
                if '2016' in href or '2016' in link_text:
                    archive_links.append({
                        'text': link_text,
                        'href': href,
                        'full_url': f"https://en.wikipedia.org{href}" if href.startswith('/') else href
                    })
            
            if archive_links:
                print(f"   Found {len(archive_links)} links related to 2016:")
                for j, link in enumerate(archive_links[:5], 1):
                    print(f"   {j}. {link['text']} -> {link['href']}")
                
                # Save archive links for further exploration
                archive_file = os.path.join(workspace_dir, f"fac_2016_archive_links_{i}.json")
                with open(archive_file, 'w', encoding='utf-8') as f:
                    json.dump(archive_links, f, indent=2, ensure_ascii=False)
                print(f"   üìÅ Archive links saved to: {os.path.basename(archive_file)}")
            else:
                print(f"   ‚ùå No 2016-related archive links found")
                
        else:
            print(f"   ‚ùå HTTP error: {response.status_code}")
            
    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")
    
    time.sleep(2)

print("\n=== STEP 3: SEARCHING GIGANOTOSAURUS TALK PAGE ===")

# Check the Giganotosaurus article talk page for FAC discussions
talk_urls = [
    "https://en.wikipedia.org/wiki/Talk:Giganotosaurus",
    "https://en.wikipedia.org/wiki/Talk:Giganotosaurus/Archive_1",
    "https://en.wikipedia.org/wiki/Talk:Giganotosaurus/Archive_2"
]

for i, url in enumerate(talk_urls, 1):
    print(f"\n{i}. Accessing talk page: {url}")
    
    try:
        response = requests.get(url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            print(f"   ‚úÖ Successfully accessed")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text().lower()
            
            # Look for FAC-related keywords
            fac_keywords = ['featured article', 'fac', 'nomination', 'nominate', 'featured status', 'november 2016']
            found_keywords = []
            
            for keyword in fac_keywords:
                if keyword in page_text:
                    found_keywords.append(keyword)
            
            if found_keywords:
                print(f"   üéØ Found FAC-related keywords: {found_keywords}")
                
                # Extract sections with these keywords
                fac_sections = []
                for element in soup.find_all(text=re.compile(r'featured|fac|nominat', re.IGNORECASE)):
                    parent = element.parent
                    if parent:
                        context = parent.get_text().strip()
                        if len(context) > 30 and any(kw in context.lower() for kw in fac_keywords):
                            fac_sections.append(context)
                
                if fac_sections:
                    print(f"   Found {len(fac_sections)} relevant sections:")
                    for j, section in enumerate(fac_sections[:3], 1):
                        print(f"   Section {j}: {section[:300]}..." if len(section) > 300 else f"   Section {j}: {section}")
                        print()
                
                # Save the talk page
                talk_file = os.path.join(workspace_dir, f"giganotosaurus_talk_{i}.html")
                with open(talk_file, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"   üìÅ Talk page saved to: {os.path.basename(talk_file)}")
            else:
                print(f"   ‚ùå No FAC-related keywords found")
                
        elif response.status_code == 404:
            print(f"   ‚ùå Page not found (404)")
        else:
            print(f"   ‚ùå HTTP error: {response.status_code}")
            
    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")
    
    time.sleep(2)

print("\n=== STEP 4: SEARCHING WIKIPEDIA SEARCH FOR GIGANOTOSAURUS FAC ===")

# Use Wikipedia's search to find FAC-related pages
search_queries = [
    "Giganotosaurus featured article candidates",
    "Giganotosaurus FAC November 2016",
    "Giganotosaurus nomination featured"
]

for i, query in enumerate(search_queries, 1):
    print(f"\n{i}. Searching Wikipedia for: '{query}'")
    
    try:
        # Use Wikipedia's search API
        search_url = "https://en.wikipedia.org/api/rest_v1/page/search/{}"
        url = search_url.format(requests.utils.quote(query))
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            
            if 'pages' in data and data['pages']:
                print(f"   ‚úÖ Found {len(data['pages'])} search results:")
                
                search_results = []
                for j, page in enumerate(data['pages'][:5], 1):  # Top 5 results
                    title = page.get('title', 'Unknown')
                    description = page.get('description', 'No description')
                    key = page.get('key', '')
                    
                    print(f"   {j}. {title}")
                    print(f"      Description: {description}")
                    print(f"      Key: {key}")
                    
                    search_results.append({
                        'title': title,
                        'description': description,
                        'key': key,
                        'url': f"https://en.wikipedia.org/wiki/{key}" if key else None
                    })
                    print()
                
                # Save search results
                search_file = os.path.join(workspace_dir, f"search_results_{i}.json")
                with open(search_file, 'w', encoding='utf-8') as f:
                    json.dump(search_results, f, indent=2, ensure_ascii=False)
                print(f"   üìÅ Search results saved to: {os.path.basename(search_file)}")
            else:
                print(f"   ‚ùå No search results found")
        else:
            print(f"   ‚ùå Search API error: {response.status_code}")
            
    except Exception as e:
        print(f"   ‚ùå Search error: {str(e)}")
    
    time.sleep(2)

print(f"\n=== INITIAL SEARCH COMPLETE ===")
print(f"üìÅ All data saved to workspace directory: {workspace_dir}")
print(f"üîç Next steps: Analyze saved HTML files and search results for nomination details")
print(f"üìä Files created:")

for filename in os.listdir(workspace_dir):
    filepath = os.path.join(workspace_dir, filename)
    file_size = os.path.getsize(filepath)
    print(f"   - {filename} ({file_size:,} bytes)")
```