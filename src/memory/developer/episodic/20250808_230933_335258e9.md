### Development Step 1: Title: Identify 'Dragon' Wikipedia Edits on Feb 29, 2000/2004 Removing Jokes or Humor Content

**Description**: Search for Wikipedia revision history of the 'Dragon' page to identify edits made on leap days (February 29) before 2008. Focus on February 29, 2000 and February 29, 2004 as the only leap days in that timeframe. Look for edit summaries or revision comparisons that mention joke removal, humor deletion, or similar content changes. Extract the specific revision data showing what content was removed on those dates.

**Use Cases**:
- Academic research on the evolution of Wikipedia articles to study how humor and non-encyclopedic content is filtered out over time, using the 'Dragon' page as a case study.
- Quality assurance for Wikipedia editors or administrators to audit and document the removal of inappropriate or joke content on significant dates, such as leap days, for compliance and transparency.
- Digital humanities projects analyzing patterns of community moderation and content curation on collaborative platforms, focusing on specific cultural or temporal events.
- Automated detection of vandalism or non-serious edits for Wikipedia monitoring bots, using leap day edits as a targeted anomaly detection scenario.
- Journalism or media investigations into the history of internet folklore and mythological topics, tracing how public contributions and editorial standards have changed on notable dates.
- Training datasets creation for machine learning models that classify Wikipedia edit comments or revision types, especially for distinguishing between humor removal and other edit actions.
- Educational workshops or classroom exercises in digital literacy, teaching students how to trace the provenance and editorial changes of online encyclopedia entries.
- Archival documentation for libraries or digital preservationists seeking to capture and analyze the evolution of notable Wikipedia articles around rare calendar events like leap days.

```
import requests
import json
import os
from datetime import datetime
import time

print("=== EXTRACTING WIKIPEDIA DRAGON PAGE REVISION HISTORY ===\n")
print("Objective: Find edits made on leap days (Feb 29) before 2008 that removed jokes/humor\n")

# Create workspace directory if it doesn't exist
workspace_dir = 'workspace'
if not os.path.exists(workspace_dir):
    os.makedirs(workspace_dir)
    print(f"Created workspace directory: {workspace_dir}")
else:
    print(f"Using existing workspace directory: {workspace_dir}")

# Target leap days before 2008
target_dates = [
    '2000-02-29',  # February 29, 2000
    '2004-02-29'   # February 29, 2004
]

print(f"Target leap days to search: {target_dates}\n")

# Wikipedia API endpoint
api_url = "https://en.wikipedia.org/w/api.php"
page_title = "Dragon"

print(f"Extracting revision history for Wikipedia page: {page_title}\n")

# Parameters for getting revision history
params = {
    'action': 'query',
    'format': 'json',
    'prop': 'revisions',
    'titles': page_title,
    'rvlimit': 'max',  # Get maximum revisions per request (500)
    'rvprop': 'timestamp|user|comment|ids|size',
    'rvdir': 'newer',  # Start from oldest revisions
    'rvstart': '1999-01-01T00:00:00Z',  # Start from 1999 to capture 2000 leap day
    'rvend': '2008-01-01T00:00:00Z'     # End before 2008 as specified
}

print("=== FETCHING DRAGON PAGE REVISION DATA FROM WIKIPEDIA API ===\n")

all_revisions = []
rvcontinue = None
request_count = 0
max_requests = 20  # Reasonable limit to get revisions from 1999-2008

while request_count < max_requests:
    request_count += 1
    
    # Add continuation parameter if we have one
    current_params = params.copy()
    if rvcontinue:
        current_params['rvcontinue'] = rvcontinue
    
    print(f"Request {request_count}: Fetching Dragon page revisions...")
    
    try:
        response = requests.get(api_url, params=current_params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        # Check for API errors
        if 'error' in data:
            print(f"  âŒ API Error: {data['error']}")
            break
        
        # Extract revisions from response
        if 'query' in data and 'pages' in data['query']:
            pages = data['query']['pages']
            page_id = list(pages.keys())[0]
            
            if page_id == '-1':
                print("  âŒ Page not found")
                break
                
            if 'revisions' in pages[page_id]:
                revisions = pages[page_id]['revisions']
                all_revisions.extend(revisions)
                print(f"  Retrieved {len(revisions)} revisions (total so far: {len(all_revisions)})")
                
                # Show sample of timestamps to track progress
                if revisions:
                    first_ts = revisions[0]['timestamp']
                    last_ts = revisions[-1]['timestamp']
                    print(f"  Date range: {first_ts} to {last_ts}")
            else:
                print("  No revisions found in response")
                break
        else:
            print("  No page data found in response")
            break
        
        # Check if there are more revisions to fetch
        if 'continue' in data and 'rvcontinue' in data['continue']:
            rvcontinue = data['continue']['rvcontinue']
            print(f"  More revisions available, continuing...")
        else:
            print("  All revisions in date range retrieved")
            break
        
        # Be respectful to Wikipedia's servers
        time.sleep(1)
        
    except Exception as e:
        print(f"  âŒ Error fetching revisions: {str(e)}")
        break

print(f"\n=== REVISION EXTRACTION COMPLETE ===\n")
print(f"Total revisions extracted: {len(all_revisions)}")
print(f"API requests made: {request_count}")

if len(all_revisions) == 0:
    print("âŒ No revision data extracted. Cannot proceed with leap day analysis.")
else:
    # Save the raw revision data
    raw_data = {
        'extraction_metadata': {
            'page_title': page_title,
            'extraction_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_revisions': len(all_revisions),
            'api_requests': request_count,
            'date_range': '1999-01-01 to 2008-01-01',
            'target_leap_days': target_dates
        },
        'revisions': all_revisions
    }
    
    raw_file = os.path.join(workspace_dir, 'dragon_wikipedia_revisions_raw.json')
    with open(raw_file, 'w', encoding='utf-8') as f:
        json.dump(raw_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Raw revision data saved to: {os.path.basename(raw_file)}")
    print(f"   File size: {os.path.getsize(raw_file):,} bytes")
    
    # Analyze the data structure
    print(f"\n=== ANALYZING REVISION DATA STRUCTURE ===\n")
    
    if all_revisions:
        sample_revision = all_revisions[0]
        print(f"Sample revision structure:")
        for key, value in sample_revision.items():
            print(f"  {key}: {type(value).__name__} = {str(value)[:100]}")
        
        # Show date range of all revisions
        timestamps = [rev['timestamp'] for rev in all_revisions if 'timestamp' in rev]
        if timestamps:
            print(f"\nRevision date range:")
            print(f"  Earliest: {min(timestamps)}")
            print(f"  Latest: {max(timestamps)}")
        
        print(f"\nSample timestamps:")
        for i, rev in enumerate(all_revisions[:5]):
            if 'timestamp' in rev:
                print(f"  {i+1}. {rev['timestamp']} - {rev.get('comment', 'No comment')[:50]}...")
    
    print(f"\n=== FILTERING FOR LEAP DAY REVISIONS ===\n")
    
    leap_day_revisions = []
    
    # Check each revision for leap day dates
    for revision in all_revisions:
        if 'timestamp' in revision:
            timestamp = revision['timestamp']
            # Extract date part (YYYY-MM-DD)
            date_part = timestamp.split('T')[0]
            
            if date_part in target_dates:
                leap_day_revisions.append(revision)
                print(f"ðŸŽ¯ LEAP DAY REVISION FOUND!")
                print(f"  Date: {date_part}")
                print(f"  Time: {timestamp}")
                print(f"  User: {revision.get('user', 'Unknown')}")
                print(f"  Comment: {revision.get('comment', 'No comment')}")
                print(f"  Revision ID: {revision.get('revid', 'Unknown')}")
                print(f"  Size: {revision.get('size', 'Unknown')} bytes")
                
                # Check if comment mentions joke/humor removal keywords
                comment = revision.get('comment', '').lower()
                joke_keywords = ['joke', 'humor', 'humour', 'funny', 'remove', 'delete', 'clean', 'vandal']
                found_keywords = [kw for kw in joke_keywords if kw in comment]
                
                if found_keywords:
                    print(f"  ðŸ” POTENTIAL JOKE/HUMOR REMOVAL: Found keywords {found_keywords}")
                else:
                    print(f"  â„¹ï¸ No obvious joke/humor removal keywords in comment")
                print()
    
    print(f"=== LEAP DAY ANALYSIS RESULTS ===\n")
    print(f"Total revisions analyzed: {len(all_revisions)}")
    print(f"Leap day revisions found: {len(leap_day_revisions)}")
    
    if leap_day_revisions:
        print(f"\nðŸŽ‰ SUCCESS: Found {len(leap_day_revisions)} revision(s) on target leap days!\n")
        
        # Save leap day revisions
        leap_day_data = {
            'analysis_metadata': {
                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'target_leap_days': target_dates,
                'total_revisions_analyzed': len(all_revisions),
                'leap_day_revisions_found': len(leap_day_revisions)
            },
            'leap_day_revisions': leap_day_revisions
        }
        
        leap_day_file = os.path.join(workspace_dir, 'dragon_leap_day_revisions.json')
        with open(leap_day_file, 'w', encoding='utf-8') as f:
            json.dump(leap_day_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Leap day revision data saved to: {os.path.basename(leap_day_file)}")
        
        # Create summary report
        summary_file = os.path.join(workspace_dir, 'leap_day_analysis_summary.txt')
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"WIKIPEDIA DRAGON PAGE LEAP DAY REVISION ANALYSIS\n")
            f.write(f"={'='*50}\n\n")
            f.write(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Page Analyzed: {page_title}\n")
            f.write(f"Date Range: 1999-01-01 to 2008-01-01\n")
            f.write(f"Target Leap Days: {', '.join(target_dates)}\n\n")
            f.write(f"RESULTS:\n")
            f.write(f"- Total revisions analyzed: {len(all_revisions)}\n")
            f.write(f"- Leap day revisions found: {len(leap_day_revisions)}\n\n")
            
            if leap_day_revisions:
                f.write(f"LEAP DAY REVISIONS DETAILS:\n")
                for i, rev in enumerate(leap_day_revisions, 1):
                    f.write(f"\n{i}. Revision on {rev['timestamp'].split('T')[0]}:\n")
                    f.write(f"   - Timestamp: {rev['timestamp']}\n")
                    f.write(f"   - User: {rev.get('user', 'Unknown')}\n")
                    f.write(f"   - Revision ID: {rev.get('revid', 'Unknown')}\n")
                    f.write(f"   - Size: {rev.get('size', 'Unknown')} bytes\n")
                    f.write(f"   - Comment: {rev.get('comment', 'No comment')}\n")
                    
                    # Check for joke/humor keywords
                    comment = rev.get('comment', '').lower()
                    joke_keywords = ['joke', 'humor', 'humour', 'funny', 'remove', 'delete', 'clean', 'vandal']
                    found_keywords = [kw for kw in joke_keywords if kw in comment]
                    if found_keywords:
                        f.write(f"   - POTENTIAL JOKE/HUMOR REMOVAL: Keywords found: {found_keywords}\n")
        
        print(f"âœ… Summary report saved to: {os.path.basename(summary_file)}")
        
    else:
        print(f"âš ï¸ No revisions found on the target leap days ({', '.join(target_dates)})")
        print(f"\nThis could mean:")
        print(f"- No edits were made to the Dragon page on those specific dates")
        print(f"- The page didn't exist yet on those dates")
        print(f"- The revisions were outside our date range filter")
        
        # Show some revisions around the target dates for context
        print(f"\n=== REVISIONS NEAR TARGET DATES FOR CONTEXT ===\n")
        
        for target_date in target_dates:
            target_year = target_date.split('-')[0]
            print(f"Revisions from {target_year}:")
            
            year_revisions = [rev for rev in all_revisions 
                            if rev.get('timestamp', '').startswith(target_year)]
            
            if year_revisions:
                print(f"  Found {len(year_revisions)} revisions in {target_year}")
                for rev in year_revisions[:3]:  # Show first 3
                    print(f"    {rev['timestamp']} - {rev.get('comment', 'No comment')[:60]}...")
            else:
                print(f"  No revisions found in {target_year}")
            print()
    
    print(f"\nðŸŽ¯ ANALYSIS COMPLETE")
    print(f"ðŸ“ Key files created:")
    print(f"  - Raw data: {os.path.basename(raw_file)}")
    if leap_day_revisions:
        print(f"  - Leap day data: {os.path.basename(leap_day_file)}")
        print(f"  - Summary report: {os.path.basename(summary_file)}")
    
    print(f"\nðŸ” Next steps: If leap day revisions were found, analyze the specific content changes")
    print(f"   If no leap day revisions found, may need to expand search or check different pages")
```