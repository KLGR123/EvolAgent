### Development Step 3: Extract 3 July 2023 LotR Wikipedia Links Toward A Song of Ice and Fire

**Description**: Access the archived Wikipedia page for 'The Lord of the Rings' (book) as it appeared at the end of July 3, 2023. Use the Wayback Machine or Wikipedia's revision history to retrieve the specific version from that date. Extract all outbound links from the page content, focusing on internal Wikipedia links that could potentially lead toward 'A Song of Ice and Fire'. Create a comprehensive list of linked pages including literature, fantasy, author, publisher, and genre-related links that might serve as stepping stones in the path-finding process.

**Use Cases**:
- Literary influence mapping for a graduate thesis: tracing thematic and authorial connections from ‚ÄúThe Lord of the Rings‚Äù to ‚ÄúA Song of Ice and Fire‚Äù using archived Wikipedia link paths
- Recommendation engine enhancement for an online bookstore: leveraging BFS-driven Wikipedia link graphs to suggest related fantasy series and authors to shoppers
- Knowledge graph enrichment for a publisher‚Äôs metadata system: automatically linking books, genres, and authors by crawling archived Wikipedia pages and extracting internal link relationships
- Digital humanities classroom project: teaching students graph algorithms through hands-on exploration of real Wikipedia revisions and path-finding between classic fantasy novels
- SEO optimization for a fan news website: identifying high-traffic internal Wikipedia links between iconic fantasy properties to improve cross-linking and search rankings
- Virtual library assistant development: automating conversational context by dynamically finding shortest Wikipedia link routes between user queries on Tolkien and Martin works
- Game narrative tool for RPG designers: analyzing themed link networks in archived Wikipedia content to inspire quest chains and world-building elements based on fantasy literature lineages

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
from collections import deque
from datetime import datetime
import re

print("=== IMPLEMENTING BREADTH-FIRST SEARCH PATH-FINDING ALGORITHM ===")
print("Objective: Find path from LOTR links to 'A Song of Ice and Fire'\n")

# First, locate workspace and load the LOTR links data
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("‚ùå No workspace directory found")
    exit()

workspace_dir = workspace_dirs[0]
lotr_file = os.path.join(workspace_dir, 'lotr_wikipedia_links_july_2023.json')

if not os.path.exists(lotr_file):
    print("‚ùå LOTR links file not found")
    exit()

print(f"Loading LOTR links from: {os.path.basename(lotr_file)}\n")

with open(lotr_file, 'r', encoding='utf-8') as f:
    lotr_data = json.load(f)

# Extract starting nodes from the most promising categories
starting_nodes = set()
target_article = "A Song of Ice and Fire"
target_variations = [
    "A Song of Ice and Fire",
    "Game of Thrones", 
    "George R. R. Martin",
    "George R.R. Martin",
    "George Martin"
]

print("=== PREPARING STARTING NODES FOR BFS ===")
print("Selecting high-priority links from categorized data...\n")

# Priority categories for fantasy literature connections
priority_categories = ['fantasy', 'literature', 'authors', 'genre']

for category in priority_categories:
    if category in lotr_data.get('categorized_links', {}):
        links = lotr_data['categorized_links'][category]
        print(f"Adding {len(links)} links from {category.upper()} category")
        for link in links:
            if isinstance(link, dict) and 'article_name' in link:
                # Clean article name (decode URL encoding)
                article_name = link['article_name'].replace('_', ' ')
                article_name = requests.utils.unquote(article_name)
                starting_nodes.add(article_name)

print(f"\nTotal starting nodes: {len(starting_nodes)}")
print("Sample starting nodes:")
for i, node in enumerate(list(starting_nodes)[:10], 1):
    print(f"  {i:2d}. {node}")
if len(starting_nodes) > 10:
    print(f"  ... and {len(starting_nodes) - 10} more\n")

# BFS Implementation
print("=== STARTING BREADTH-FIRST SEARCH ===")
print(f"Target: {target_article} (and variations)\n")

# Initialize BFS data structures
queue = deque()
visited = set()
parent = {}  # To track the path
depth = {}   # Track search depth
found_paths = []
max_depth = 3  # Limit search depth to avoid infinite searches
max_requests = 50  # Limit total API requests
request_count = 0

# Add starting nodes to queue
for node in starting_nodes:
    queue.append(node)
    depth[node] = 0
    parent[node] = None

print(f"Initialized BFS queue with {len(queue)} starting nodes")
print(f"Search parameters: max_depth={max_depth}, max_requests={max_requests}\n")

# Request headers for Wikipedia API
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'application/json'
}

# Function to get Wikipedia page links via API
def get_wikipedia_links(page_title, max_links=100):
    """Get outbound links from a Wikipedia page using the API"""
    global request_count
    
    if request_count >= max_requests:
        return []
    
    try:
        # Use Wikipedia API to get page links
        api_url = "https://en.wikipedia.org/api/rest_v1/page/links/{}"
        url = api_url.format(requests.utils.quote(page_title.replace(' ', '_')))
        
        print(f"  Fetching links from: {page_title} (Request #{request_count + 1})")
        
        response = requests.get(url, headers=headers, timeout=10)
        request_count += 1
        
        if response.status_code == 200:
            data = response.json()
            links = []
            
            # Extract article titles from the response
            if 'items' in data:
                for item in data['items'][:max_links]:  # Limit number of links
                    if 'title' in item:
                        title = item['title']
                        # Filter out non-article pages
                        if not any(prefix in title for prefix in ['File:', 'Category:', 'Template:', 'User:', 'Talk:', 'Wikipedia:', 'Help:', 'Portal:', 'Special:', 'Media:']):
                            links.append(title)
            
            print(f"    Found {len(links)} article links")
            return links
            
        elif response.status_code == 404:
            print(f"    Page not found: {page_title}")
            return []
        else:
            print(f"    API error {response.status_code} for {page_title}")
            return []
            
    except Exception as e:
        print(f"    Error fetching {page_title}: {str(e)}")
        return []

# Function to check if we found our target
def is_target(page_title):
    """Check if the page title matches our target variations"""
    page_lower = page_title.lower()
    return any(target.lower() in page_lower or page_lower in target.lower() 
               for target in target_variations)

# Function to reconstruct path
def get_path(node, parent_dict):
    """Reconstruct the path from start to target node"""
    path = []
    current = node
    while current is not None:
        path.append(current)
        current = parent_dict.get(current)
    return list(reversed(path))

# Main BFS loop
print("Starting BFS traversal...\n")
start_time = datetime.now()

while queue and request_count < max_requests:
    current_node = queue.popleft()
    
    if current_node in visited:
        continue
        
    visited.add(current_node)
    current_depth = depth[current_node]
    
    print(f"\n--- Processing: {current_node} (depth {current_depth}) ---")
    
    # Check if we found the target
    if is_target(current_node):
        path = get_path(current_node, parent)
        found_paths.append({
            'target_found': current_node,
            'path': path,
            'depth': current_depth,
            'path_length': len(path)
        })
        print(f"\nüéØ TARGET FOUND: {current_node}")
        print(f"Path length: {len(path)} steps")
        print(f"Path: {' ‚Üí '.join(path)}")
        break
    
    # Don't go deeper than max_depth
    if current_depth >= max_depth:
        print(f"  Reached max depth ({max_depth}), skipping expansion")
        continue
    
    # Get outbound links from current node
    outbound_links = get_wikipedia_links(current_node)
    
    # Process each outbound link
    new_nodes_added = 0
    target_hints = []
    
    for link in outbound_links:
        if link not in visited and link not in [item for item in queue]:
            # Check for target hints
            if any(hint in link.lower() for hint in ['fantasy', 'martin', 'song', 'ice', 'fire', 'game', 'thrones']):
                target_hints.append(link)
            
            queue.append(link)
            depth[link] = current_depth + 1
            parent[link] = current_node
            new_nodes_added += 1
    
    print(f"  Added {new_nodes_added} new nodes to queue")
    
    if target_hints:
        print(f"  üîç Target hints found: {target_hints[:3]}{'...' if len(target_hints) > 3 else ''}")
    
    # Add small delay to be respectful to Wikipedia's servers
    time.sleep(0.5)
    
    # Progress update
    if len(visited) % 5 == 0:
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"\nüìä Progress: {len(visited)} nodes visited, {len(queue)} in queue, {request_count} requests made")
        print(f"   Elapsed time: {elapsed:.1f}s, Queue size: {len(queue)}")

print(f"\n=== BFS SEARCH COMPLETE ===")
elapsed = (datetime.now() - start_time).total_seconds()
print(f"Search completed in {elapsed:.1f} seconds")
print(f"Nodes visited: {len(visited)}")
print(f"API requests made: {request_count}")
print(f"Paths found: {len(found_paths)}\n")

# Save search results
search_results = {
    'search_metadata': {
        'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),
        'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'elapsed_seconds': elapsed,
        'target_article': target_article,
        'target_variations': target_variations,
        'max_depth': max_depth,
        'max_requests': max_requests,
        'requests_made': request_count,
        'nodes_visited': len(visited),
        'queue_size_final': len(queue)
    },
    'starting_nodes': list(starting_nodes),
    'paths_found': found_paths,
    'visited_nodes': list(visited),
    'search_statistics': {
        'total_starting_nodes': len(starting_nodes),
        'paths_discovered': len(found_paths),
        'search_completed': request_count < max_requests,
        'reason_stopped': 'Target found' if found_paths else ('Max requests reached' if request_count >= max_requests else 'Queue exhausted')
    }
}

# Save comprehensive results
results_file = os.path.join(workspace_dir, 'bfs_pathfinding_results.json')
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(search_results, f, indent=2, ensure_ascii=False, default=str)

print(f"=== FINAL RESULTS ===")
if found_paths:
    print(f"\nüéâ SUCCESS: Found {len(found_paths)} path(s) to target!\n")
    for i, path_info in enumerate(found_paths, 1):
        print(f"Path {i}:")
        print(f"  Target: {path_info['target_found']}")
        print(f"  Length: {path_info['path_length']} steps")
        print(f"  Depth: {path_info['depth']}")
        print(f"  Route: {' ‚Üí '.join(path_info['path'])}")
        print()
else:
    print(f"\n‚ùå No direct paths found within {max_depth} steps using {max_requests} requests")
    print("   This suggests the connection requires deeper exploration or different starting points")
    
    # Show most promising leads that were in the queue
    promising_leads = []
    for item in list(queue)[:10]:
        if any(hint in str(item).lower() for hint in ['fantasy', 'martin', 'epic', 'series', 'literature']):
            promising_leads.append(item)
    
    if promising_leads:
        print(f"\nüîç Promising leads that were queued for exploration:")
        for lead in promising_leads[:5]:
            print(f"   - {lead}")

print(f"\nüìÅ Detailed results saved to: {os.path.basename(results_file)}")
print(f"üîÑ Ready for analysis or extended search with different parameters")
```