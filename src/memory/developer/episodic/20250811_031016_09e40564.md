### Development Step 7: **Title:**  
Extract Wikipedia Access Date from Endnote on Page 11 of 'The Responsibility of Intellectuals' (2019)

**Description**: Access and download the full text of 'The Responsibility of Intellectuals' (DOI 10.2307/j.ctv9b2xdv) from JSTOR or UCL Press. Since this is a 2019 UCL Press publication available through JSTOR, retrieve the complete book content and save it to the workspace. Focus on locating page 11, identifying the second-to-last paragraph on that page, and extracting the specific endnote referenced in that paragraph. The endnote should contain a Wikipedia article citation with a November access date - extract the exact day of the month when the Wikipedia article was accessed.

**Use Cases**:
- Academic integrity auditing by university librarians to verify citation accuracy and access dates in open-access scholarly books
- Automated literature review tools for researchers needing to extract and cross-reference Wikipedia citations and their access dates across large PDF collections
- Digital humanities projects analyzing citation patterns and sources in contemporary intellectual history publications
- Legal compliance checks for publishers ensuring all Wikipedia references in academic works include proper access dates as per citation standards
- Bibliometric analysis for research impact studies, tracking the prevalence and recency of Wikipedia citations in academic monographs
- Content verification workflows for fact-checkers reviewing the reliability and timeliness of Wikipedia-sourced references in published books
- Automated metadata enrichment for digital repositories, extracting and structuring citation details (including access dates) from full-text PDFs
- Quality assurance processes in academic publishing, detecting missing or outdated access dates in Wikipedia citations before final release

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

print('=== COMPREHENSIVE SEARCH FOR WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATES ===')
print('DOI: 10.2307/j.ctv9b2xdv')
print('Objective: Search entire book for Wikipedia citations with November access dates')
print('Status: Page 11 second-to-last paragraph had no endnote references')
print('\n' + '='*100 + '\n')

# First, let's check if we have the full book PDF downloaded
pdf_path = None
workspace_files = os.listdir('workspace')
for file in workspace_files:
    if file.endswith('.pdf') and 'responsibility' in file.lower():
        pdf_path = os.path.join('workspace', file)
        break

if not pdf_path:
    print('‚ùå Full book PDF not found in workspace')
    print('Available files:')
    for file in workspace_files:
        print(f'  - {file}')
    exit()

print(f'Found PDF: {pdf_path}')
file_size = os.path.getsize(pdf_path)
print(f'PDF size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')

print('\n=== EXTRACTING FULL BOOK TEXT FOR COMPREHENSIVE SEARCH ===')

try:
    from langchain_community.document_loaders import PyPDFLoader
    
    print('Loading complete PDF...')
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    
    print(f'‚úì PDF loaded with {len(pages)} pages')
    
    # Combine all pages into full text
    full_book_text = '\n\n'.join([page.page_content for page in pages])
    print(f'Total book text: {len(full_book_text):,} characters')
    
    # Save full text for reference
    with open('workspace/full_book_text.txt', 'w', encoding='utf-8') as f:
        f.write('THE RESPONSIBILITY OF INTELLECTUALS - FULL BOOK TEXT\n')
        f.write('Source: UCL Press Open Access PDF\n')
        f.write('='*80 + '\n\n')
        f.write(full_book_text)
    
    print('‚úì Full book text saved to workspace/full_book_text.txt')
    
    print('\n=== SEARCHING FOR ALL WIKIPEDIA REFERENCES ===')
    
    # First, let's find all Wikipedia references regardless of date
    wikipedia_general_patterns = [
        r'wikipedia[^\n]{0,300}',
        r'en\.wikipedia\.org[^\n]{0,300}',
        r'\bwikipedia\b[^\n]{0,300}'
    ]
    
    all_wikipedia_refs = []
    for pattern in wikipedia_general_patterns:
        matches = re.finditer(pattern, full_book_text, re.IGNORECASE)
        for match in matches:
            ref_text = match.group(0)
            all_wikipedia_refs.append({
                'text': ref_text,
                'position': match.start(),
                'pattern_used': pattern
            })
    
    # Remove duplicates based on position
    unique_wiki_refs = []
    seen_positions = set()
    for ref in all_wikipedia_refs:
        if ref['position'] not in seen_positions:
            seen_positions.add(ref['position'])
            unique_wiki_refs.append(ref)
    
    print(f'Found {len(unique_wiki_refs)} total Wikipedia references in the book')
    
    if unique_wiki_refs:
        print('\nFirst 10 Wikipedia references:')
        for i, ref in enumerate(unique_wiki_refs[:10], 1):
            print(f'{i}. Position {ref["position"]:,}: {ref["text"][:100]}...')
    
    print('\n=== SEARCHING FOR WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATES ===')
    
    # Comprehensive patterns for Wikipedia citations with November dates
    november_wikipedia_patterns = [
        # Wikipedia followed by November and day
        r'wikipedia[^\n]{0,400}november[^\n]{0,100}\d{1,2}[^\n]{0,100}',
        r'en\.wikipedia\.org[^\n]{0,400}november[^\n]{0,100}\d{1,2}[^\n]{0,100}',
        
        # November and day followed by Wikipedia
        r'november[^\n]{0,100}\d{1,2}[^\n]{0,200}wikipedia[^\n]{0,300}',
        r'\d{1,2}[^\n]{0,50}november[^\n]{0,200}wikipedia[^\n]{0,300}',
        
        # Accessed patterns
        r'accessed[^\n]{0,200}november[^\n]{0,100}\d{1,2}[^\n]{0,200}wikipedia[^\n]{0,200}',
        r'wikipedia[^\n]{0,400}accessed[^\n]{0,200}november[^\n]{0,100}\d{1,2}[^\n]{0,100}',
        
        # More flexible patterns
        r'\bwikipedia\b[^\n]{0,500}\bnovember\b[^\n]{0,150}\b\d{1,2}\b[^\n]{0,150}',
        r'\bnovember\b[^\n]{0,150}\b\d{1,2}\b[^\n]{0,300}\bwikipedia\b[^\n]{0,300}',
        
        # URL patterns with dates
        r'https?://[^\s]*wikipedia[^\s]*[^\n]{0,200}november[^\n]{0,100}\d{1,2}[^\n]{0,100}',
        r'november[^\n]{0,100}\d{1,2}[^\n]{0,200}https?://[^\s]*wikipedia[^\s]*[^\n]{0,100}'
    ]
    
    november_citations = []
    for pattern in november_wikipedia_patterns:
        matches = re.finditer(pattern, full_book_text, re.IGNORECASE | re.DOTALL)
        for match in matches:
            citation_text = match.group(0)
            
            # Extract the day from November date using multiple patterns
            day_patterns = [
                r'november\s+(\d{1,2})',
                r'(\d{1,2})\s+november',
                r'november\s+(\d{1,2})(?:st|nd|rd|th)?',
                r'(\d{1,2})(?:st|nd|rd|th)?\s+november',
                r'november\s*,?\s*(\d{1,2})',
                r'(\d{1,2})\s*,?\s*november',
                r'november\s+(\d{1,2})\s*,?\s*\d{4}',
                r'(\d{1,2})\s+november\s+\d{4}'
            ]
            
            day_found = None
            for day_pattern in day_patterns:
                day_match = re.search(day_pattern, citation_text, re.IGNORECASE)
                if day_match:
                    day_found = day_match.group(1)
                    break
            
            if day_found and 1 <= int(day_found) <= 31:  # Valid day
                # Get broader context around the citation
                context_start = max(0, match.start() - 1000)
                context_end = min(len(full_book_text), match.end() + 1000)
                citation_context = full_book_text[context_start:context_end]
                
                # Determine which page this citation appears on
                char_count = 0
                page_number = 0
                for page_idx, page in enumerate(pages):
                    if char_count + len(page.page_content) >= match.start():
                        page_number = page_idx + 1
                        break
                    char_count += len(page.page_content) + 2  # +2 for \n\n separator
                
                november_citations.append({
                    'citation': citation_text,
                    'november_day': day_found,
                    'position': match.start(),
                    'context': citation_context,
                    'page_number': page_number,
                    'pattern_used': pattern
                })
    
    # Remove duplicates based on citation text and day
    unique_november_citations = []
    seen_citations = set()
    for citation in november_citations:
        citation_key = (citation['citation'].strip().lower(), citation['november_day'])
        if citation_key not in seen_citations:
            seen_citations.add(citation_key)
            unique_november_citations.append(citation)
    
    if unique_november_citations:
        print(f'\nüéØ FOUND {len(unique_november_citations)} UNIQUE WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATES:')
        
        for i, citation in enumerate(unique_november_citations, 1):
            print(f'\nCitation {i}:')
            print(f'November day: {citation["november_day"]}')
            print(f'Page number: {citation["page_number"]}')
            print(f'Position in book: {citation["position"]:,}')
            print(f'Pattern used: {citation["pattern_used"]}')
            print('Citation text:')
            print('='*80)
            print(citation['citation'])
            print('='*80)
            
            # Show relevant context
            context_preview = citation['context'][:500] + '...' if len(citation['context']) > 500 else citation['context']
            print(f'Context: {context_preview}')
            print('-'*80)
        
        # Save the complete analysis
        final_analysis = {
            'source_pdf': pdf_path,
            'book_title': 'The Responsibility of Intellectuals',
            'publisher': 'UCL Press',
            'year': 2019,
            'total_pages': len(pages),
            'total_wikipedia_references': len(unique_wiki_refs),
            'wikipedia_citations_with_november_dates': unique_november_citations,
            'search_patterns_used': november_wikipedia_patterns,
            'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open('workspace/comprehensive_wikipedia_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(final_analysis, f, indent=2, ensure_ascii=False)
        
        print('\n‚úì Complete analysis saved to workspace/comprehensive_wikipedia_analysis.json')
        
        # Determine the final answer
        if len(unique_november_citations) == 1:
            answer_day = unique_november_citations[0]['november_day']
            page_num = unique_november_citations[0]['page_number']
            print(f'\n*** FINAL ANSWER: The Wikipedia article was accessed on November {answer_day} ***')
            print(f'(Found on page {page_num} of the book)')
        elif len(unique_november_citations) > 1:
            print(f'\n*** MULTIPLE WIKIPEDIA CITATIONS WITH NOVEMBER DATES FOUND ***')
            print('All November access dates found:')
            for i, citation in enumerate(unique_november_citations, 1):
                print(f'{i}. November {citation["november_day"]} (page {citation["page_number"]})')
            
            # Look for the one closest to page 11 or in endnotes section
            closest_to_page_11 = None
            min_distance = float('inf')
            
            for citation in unique_november_citations:
                distance = abs(citation['page_number'] - 11)
                if distance < min_distance:
                    min_distance = distance
                    closest_to_page_11 = citation
            
            if closest_to_page_11:
                answer_day = closest_to_page_11['november_day']
                page_num = closest_to_page_11['page_number']
                print(f'\n*** MOST LIKELY ANSWER (closest to page 11): November {answer_day} ***')
                print(f'(Found on page {page_num}, distance from page 11: {min_distance} pages)')
            else:
                # Default to first citation
                answer_day = unique_november_citations[0]['november_day']
                print(f'\nDefaulting to first citation: November {answer_day}')
    
    else:
        print('\n‚ö† No Wikipedia citations with November access dates found')
        
        # Let's search for any date patterns with Wikipedia
        print('\nSearching for Wikipedia citations with any date patterns...')
        
        date_patterns = [
            r'wikipedia[^\n]{0,300}\d{1,2}[^\n]{0,100}\d{4}[^\n]{0,100}',  # Any date
            r'wikipedia[^\n]{0,300}accessed[^\n]{0,200}\d{4}[^\n]{0,100}',  # Accessed with year
            r'accessed[^\n]{0,200}wikipedia[^\n]{0,300}\d{4}[^\n]{0,100}',  # Accessed before wikipedia
        ]
        
        any_date_citations = []
        for pattern in date_patterns:
            matches = re.finditer(pattern, full_book_text, re.IGNORECASE)
            for match in matches:
                citation_text = match.group(0)
                any_date_citations.append(citation_text)
        
        if any_date_citations:
            print(f'Found {len(any_date_citations)} Wikipedia citations with any date patterns:')
            for i, citation in enumerate(any_date_citations[:5], 1):
                print(f'{i}. {citation[:150]}...')
        else:
            print('No Wikipedia citations with any date patterns found')
        
        # Final fallback: search for month names with Wikipedia
        print('\nSearching for Wikipedia citations with any month names...')
        
        month_patterns = [
            r'wikipedia[^\n]{0,300}(?:january|february|march|april|may|june|july|august|september|october|november|december)[^\n]{0,100}\d{1,2}[^\n]{0,100}',
            r'(?:january|february|march|april|may|june|july|august|september|october|november|december)[^\n]{0,100}\d{1,2}[^\n]{0,200}wikipedia[^\n]{0,300}'
        ]
        
        month_citations = []
        for pattern in month_patterns:
            matches = re.finditer(pattern, full_book_text, re.IGNORECASE)
            for match in matches:
                citation_text = match.group(0)
                month_citations.append(citation_text)
        
        if month_citations:
            print(f'Found {len(month_citations)} Wikipedia citations with month names:')
            for i, citation in enumerate(month_citations[:5], 1):
                print(f'{i}. {citation[:150]}...')
        else:
            print('No Wikipedia citations with month names found')

except ImportError:
    print('‚ùå PyPDFLoader not available - cannot extract text from PDF')
except Exception as e:
    print(f'‚ùå Error during comprehensive search: {str(e)}')

print('\n' + '='*100)
print('COMPREHENSIVE WIKIPEDIA SEARCH COMPLETE')
print('='*100)
print('Objective: Find Wikipedia citation with November access date from entire book')
print('Status: Searched all pages since page 11 paragraph had no endnote references')
print('\nFiles created:')
if os.path.exists('workspace'):
    for file in sorted(os.listdir('workspace')):
        if file.endswith(('.txt', '.json')):
            file_path = os.path.join('workspace', file)
            file_size = os.path.getsize(file_path)
            print(f'- {file} ({file_size:,} bytes)')
```