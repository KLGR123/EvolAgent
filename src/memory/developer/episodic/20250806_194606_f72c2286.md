### Development Step 6: Extract and Alphabetize Chemicals for EC 1.11.1.7 and 3.1.3.1 from 2016 Wiley Virus Paper

**Description**: Access the identified 2016 Wiley paper 'Effects of Sweet Potato Feathery Mottle Virus and Sweet Potato Chlorotic Stunt Virus' and extract the specific chemicals with EC numbers 1.11.1.7 and 3.1.3.1 used in the virus testing methods. Identify the chemical names corresponding to these EC numbers, alphabetize them, and format the EC numbers in the required semicolon-separated order.

**Use Cases**:
- Plant pathology diagnostic lab protocol standardization by extracting EC numbers and reagent names from the 2016 Wiley paper for consistent virus detection workflows
- Agricultural R&D team integrating EC 1.11.1.7 and 3.1.3.1 enzyme details into high-throughput sweet potato resistance screening assays
- Biotech supply chain automation that queries literature to auto-populate purchase orders with correct chemical names and EC numbers for virus testing
- Regulatory affairs dossier preparation for agrochemical approval, mining peer-reviewed methods to document enzyme reagents and safety data
- Digital library curation of enzyme-based virus assay protocols, populating a searchable repository with standardized EC numbers and chemical names
- Grant proposal development for plant virology research, extracting precise reagent information to strengthen methodological sections and budget forecasts
- Patent prior-art analysis in agricultural biotechnology, harvesting EC number and chemical name data to validate novelty of sweet potato virus detection methods

```
import os
import requests
import json
from datetime import datetime

# First, let's examine the workspace directory to see what files are available
print("Examining workspace directory...")
print("=" * 80)

if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    print(f"Found {len(workspace_files)} files in workspace:")
    for file in workspace_files:
        print(f"  - {file}")
else:
    print("No workspace directory found. Creating workspace directory...")
    os.makedirs('workspace', exist_ok=True)
    print("Workspace directory created.")

print("\n" + "=" * 80)
print("SEARCHING FOR 2016 WILEY PAPER ON SWEET POTATO VIRUSES")
print("=" * 80)

# Get SerpAPI key from environment variables
api_key = os.getenv("SERPAPI_API_KEY")

if api_key is None:
    print("Error: Missing API key. Make sure you have SERPAPI_API_KEY in your environment variables.")
else:
    print("API key found, proceeding with paper search...")
    
    # Define specific search queries to find the 2016 Wiley paper
    search_queries = [
        '"Effects of Sweet Potato Feathery Mottle Virus and Sweet Potato Chlorotic Stunt Virus" 2016 Wiley',
        'Sweet Potato Feathery Mottle Virus Sweet Potato Chlorotic Stunt Virus 2016 site:wiley.com',
        '"Sweet Potato Feathery Mottle Virus" "Sweet Potato Chlorotic Stunt Virus" 2016 EC 1.11.1.7 3.1.3.1',
        'Sweet Potato virus testing methods EC numbers 1.11.1.7 3.1.3.1 2016',
        '"Sweet Potato Feathery Mottle Virus" "Sweet Potato Chlorotic Stunt Virus" Wiley 2016 chemicals'
    ]
    
    print(f"Executing {len(search_queries)} targeted searches...\n")
    
    # Store all search results
    all_search_results = []
    
    for i, query in enumerate(search_queries, 1):
        print(f"Search {i}/{len(search_queries)}: {query}")
        print("-" * 60)
        
        # Prepare API request parameters
        params = {
            "q": query,
            "api_key": api_key,
            "engine": "google",
            "google_domain": "google.com",
            "safe": "off",
            "num": 15,
            "type": "search"
        }
        
        try:
            # Make API request to SerpAPI
            response = requests.get("https://serpapi.com/search.json", params=params, timeout=30)
            
            if response.status_code == 200:
                results = response.json()
                
                if results.get("organic_results"):
                    print(f"Found {len(results['organic_results'])} results")
                    
                    # Store results with query context
                    query_results = {
                        'query': query,
                        'results': results['organic_results'],
                        'search_number': i,
                        'timestamp': datetime.now().isoformat()
                    }
                    all_search_results.append(query_results)
                    
                    # Display and analyze top results for this query
                    paper_candidates = []
                    for j, result in enumerate(results['organic_results'][:8], 1):
                        title = result.get('title', 'No title')
                        link = result.get('link', 'No URL')
                        snippet = result.get('snippet', 'No snippet')
                        
                        # Check for paper-specific indicators
                        title_lower = title.lower()
                        snippet_lower = snippet.lower()
                        combined_text = f"{title_lower} {snippet_lower}"
                        
                        # Key indicators for the specific paper
                        key_indicators = [
                            'sweet potato feathery mottle virus',
                            'sweet potato chlorotic stunt virus',
                            '2016',
                            'wiley',
                            'effects',
                            'ec',
                            'enzyme',
                            'testing',
                            'methods'
                        ]
                        
                        matching_indicators = [ind for ind in key_indicators if ind in combined_text]
                        
                        print(f"\n  Result {j}:")
                        print(f"    Title: {title}")
                        print(f"    URL: {link}")
                        print(f"    Snippet: {snippet[:300]}{'...' if len(snippet) > 300 else ''}")
                        
                        if matching_indicators:
                            print(f"    ‚≠ê MATCHING INDICATORS: {', '.join(matching_indicators)}")
                            
                            # Special attention to Wiley sites and academic databases
                            if any(domain in link.lower() for domain in ['wiley.com', 'onlinelibrary.wiley.com', 'doi.org', 'pubmed', 'scholar.google']):
                                print(f"    üéØ HIGH-PRIORITY SOURCE: Academic/Publisher result")
                                paper_candidates.append({
                                    'title': title,
                                    'link': link,
                                    'snippet': snippet,
                                    'matching_indicators': matching_indicators,
                                    'priority': 'HIGH'
                                })
                            else:
                                paper_candidates.append({
                                    'title': title,
                                    'link': link,
                                    'snippet': snippet,
                                    'matching_indicators': matching_indicators,
                                    'priority': 'MEDIUM'
                                })
                    
                    if not paper_candidates:
                        print("    No highly relevant results found for this query")
                        
                else:
                    print("No organic results found for this query")
                    all_search_results.append({
                        'query': query,
                        'results': [],
                        'search_number': i,
                        'timestamp': datetime.now().isoformat()
                    })
            else:
                print(f"Error: API request failed with status {response.status_code}: {response.text}")
                
        except Exception as e:
            print(f"Error during search {i}: {str(e)}")
            continue
        
        print("\n")
    
    # Save all search results to workspace for analysis
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"workspace/sweet_potato_virus_paper_search_{timestamp}.json"
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'target_paper': 'Effects of Sweet Potato Feathery Mottle Virus and Sweet Potato Chlorotic Stunt Virus',
            'target_year': '2016',
            'target_publisher': 'Wiley',
            'target_ec_numbers': ['1.11.1.7', '3.1.3.1'],
            'search_timestamp': timestamp,
            'total_queries': len(search_queries),
            'queries_executed': search_queries,
            'search_results': all_search_results
        }, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 80)
    print("SEARCH RESULTS ANALYSIS")
    print("=" * 80)
    
    # Analyze all results to find the most promising paper candidates
    all_candidates = []
    total_results = sum(len(query_result['results']) for query_result in all_search_results)
    
    print(f"Total search results collected: {total_results}")
    print(f"Search results saved to: {results_file}")
    
    # Extract and rank all paper candidates
    for query_result in all_search_results:
        for result in query_result['results']:
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            link = result.get('link', '').lower()
            combined = f"{title} {snippet} {link}"
            
            # Score based on key terms
            score = 0
            if 'sweet potato feathery mottle virus' in combined:
                score += 10
            if 'sweet potato chlorotic stunt virus' in combined:
                score += 10
            if '2016' in combined:
                score += 5
            if 'wiley' in combined:
                score += 5
            if 'effects' in combined:
                score += 3
            if any(ec in combined for ec in ['1.11.1.7', '3.1.3.1', 'ec number', 'enzyme']):
                score += 8
            
            if score >= 15:  # High relevance threshold
                all_candidates.append({
                    'title': result.get('title'),
                    'link': result.get('link'),
                    'snippet': result.get('snippet'),
                    'score': score,
                    'query': query_result['query']
                })
    
    # Sort candidates by relevance score
    all_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    print(f"\nüìä TOP PAPER CANDIDATES (Score >= 15):")
    if all_candidates:
        for i, candidate in enumerate(all_candidates[:5], 1):
            print(f"\n  {i}. SCORE: {candidate['score']}")
            print(f"     Title: {candidate['title']}")
            print(f"     URL: {candidate['link']}")
            print(f"     From Query: {candidate['query']}")
            print(f"     Snippet: {candidate['snippet'][:200]}...")
            
            # Check if this is a direct Wiley publication link
            if 'wiley.com' in candidate['link'].lower() or 'onlinelibrary.wiley.com' in candidate['link'].lower():
                print(f"     üéØ DIRECT WILEY ACCESS - High priority for content extraction")
    else:
        print("No high-scoring candidates found. May need to adjust search strategy.")
    
    print(f"\nüìã NEXT STEPS:")
    if all_candidates:
        print(f"1. Access the highest-scoring paper candidate")
        print(f"2. Extract content to find EC numbers 1.11.1.7 and 3.1.3.1")
        print(f"3. Identify corresponding chemical names")
        print(f"4. Alphabetize and format as required")
    else:
        print(f"1. Try alternative search strategies")
        print(f"2. Search for EC numbers directly")
        print(f"3. Look for virus testing methodology papers")
    
    print(f"\nSearch data saved to: {results_file}")
    print("Ready to proceed with paper content extraction if suitable candidates found.")
```