### Development Step 9: Collect Official Land Area Measures for Washington’s 39 County Seats (sq mi/km)

**Description**: Research and extract the land area data for all 39 Washington state county seats that were identified in the previous step. Use authoritative sources such as the U.S. Census Bureau, official city websites, or government geographic databases to obtain the land area measurements for each county seat city. Focus on finding the official land area figures in square miles or square kilometers for accurate comparison.

**Use Cases**:
- Municipal infrastructure planning: city engineers and planners compile official land area figures of Washington county seats to allocate budgets for road expansion, utilities, and public services based on precise city footprints.
- Emergency management simulation: state emergency response teams use extracted land area data to model evacuation zones, position relief centers, and optimize resource deployment during natural disasters.
- GIS mapping and spatial analysis: geospatial analysts integrate accurate square-mile measurements into custom Washington state maps for land-use visualization, zoning overlays, and regional planning tools.
- Real estate market intelligence: property investment firms analyze county seat sizes to normalize housing density metrics, forecast urban growth trends, and adjust pricing models for residential and commercial developments.
- Logistics and delivery optimization: last-mile delivery providers leverage city land area data to design efficient routing algorithms, estimate delivery time windows, and allocate driver shifts across varying municipal territories.
- Academic geography research: university researchers conduct comparative studies on population density, urban sprawl, and land-use patterns by correlating precise land area measurements of Washington’s county seats.
- Environmental impact assessments: conservation organizations reference city footprints to evaluate urban encroachment on natural habitats, calculate per-capita green space availability, and plan reforestation or park development projects.
- Mobile tourism and travel apps: location-based app developers incorporate accurate land area data to generate dynamic city guides, suggest walking or driving tours, and estimate travel times across county seats.

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

print("=== WASHINGTON STATE COUNTY SEATS LAND AREA RESEARCH ===\n")
print("Objective: Extract land area data for all 39 Washington state county seats")
print("Sources: U.S. Census Bureau data via Wikipedia and official sources\n")

# First, let's verify and load the county seats data
print("=== LOADING COUNTY SEATS DATA ===\n")

# Check if the JSON file exists and inspect its structure
if os.path.exists('workspace/wa_county_seats.json'):
    print("Found county seats JSON file. Inspecting structure...")
    
    with open('workspace/wa_county_seats.json', 'r') as f:
        county_seats_data = json.load(f)
    
    print(f"Data type: {type(county_seats_data)}")
    print(f"Number of items: {len(county_seats_data)}")
    
    if isinstance(county_seats_data, list) and county_seats_data:
        print(f"Sample item structure: {list(county_seats_data[0].keys())}")
        print(f"Sample item: {county_seats_data[0]}")
        
        print(f"\nAll 39 Washington state county seats:")
        for i, seat in enumerate(county_seats_data, 1):
            print(f"  {i:2d}. {seat['county_seat']:<15} ({seat['county']})")
else:
    print("County seats JSON file not found. Checking workspace...")
    if os.path.exists('workspace'):
        files = os.listdir('workspace')
        print(f"Available files: {files}")
    else:
        print("No workspace directory found.")
        exit()

print(f"\n=== BEGINNING LAND AREA RESEARCH ===\n")
print("Strategy: Extract land area from Wikipedia (contains U.S. Census Bureau data)")
print("Using multiple extraction methods for comprehensive coverage\n")

# Initialize results storage
land_area_results = []

# Request headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

# Process each county seat
for i, seat_data in enumerate(county_seats_data, 1):
    county_seat = seat_data['county_seat']
    county = seat_data['county']
    
    print(f"[{i:2d}/39] Researching {county_seat}, Washington...", end=" ")
    
    # Construct Wikipedia URL
    city_name_formatted = county_seat.replace(' ', '_')
    wikipedia_url = f"https://en.wikipedia.org/wiki/{city_name_formatted},_Washington"
    
    land_area_found = None
    area_unit = None
    extraction_method = None
    
    try:
        # Request Wikipedia page
        response = requests.get(wikipedia_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Method 1: Search infobox for area information
            infobox = soup.find('table', class_='infobox')
            if infobox and not land_area_found:
                rows = infobox.find_all('tr')
                for row in rows:
                    # Look for area-related table headers
                    header = row.find('th')
                    if header:
                        header_text = header.get_text().lower().strip()
                        if 'area' in header_text:
                            # Get corresponding data cell
                            data_cell = row.find('td')
                            if data_cell:
                                area_text = data_cell.get_text().strip()
                                
                                # Extract area value using regex patterns
                                patterns = [
                                    r'([0-9,]+\.?[0-9]*)\s*sq\s*mi',
                                    r'([0-9,]+\.?[0-9]*)\s*square\s*miles?',
                                    r'([0-9,]+\.?[0-9]*)\s*km²'
                                ]
                                
                                for pattern in patterns:
                                    match = re.search(pattern, area_text, re.IGNORECASE)
                                    if match:
                                        land_area_found = match.group(1).replace(',', '')
                                        if 'sq mi' in area_text.lower() or 'square mile' in area_text.lower():
                                            area_unit = 'sq_miles'
                                        elif 'km' in area_text.lower():
                                            area_unit = 'sq_kilometers'
                                        extraction_method = 'infobox'
                                        break
                                
                                if land_area_found:
                                    break
            
            # Method 2: Search all table cells for area data
            if not land_area_found:
                all_cells = soup.find_all(['td', 'th'])
                for cell in all_cells:
                    cell_text = cell.get_text().strip()
                    area_match = re.search(r'([0-9,]+\.?[0-9]*)\s*(sq\s*mi|square\s*miles?)', cell_text, re.IGNORECASE)
                    if area_match:
                        land_area_found = area_match.group(1).replace(',', '')
                        area_unit = 'sq_miles'
                        extraction_method = 'table_scan'
                        break
            
            # Method 3: Search page text for area mentions
            if not land_area_found:
                page_text = soup.get_text()
                text_patterns = [
                    r'total area[^0-9]*([0-9,]+\.?[0-9]*)\s*(square miles|sq\s*mi)',
                    r'land area[^0-9]*([0-9,]+\.?[0-9]*)\s*(square miles|sq\s*mi)',
                    r'area[^0-9]*([0-9,]+\.?[0-9]*)\s*(square miles|sq\s*mi)'
                ]
                
                for pattern in text_patterns:
                    match = re.search(pattern, page_text, re.IGNORECASE)
                    if match:
                        land_area_found = match.group(1).replace(',', '')
                        area_unit = 'sq_miles'
                        extraction_method = 'text_scan'
                        break
        
        # Store results
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': float(land_area_found) if land_area_found else None,
            'area_unit': area_unit,
            'wikipedia_url': wikipedia_url,
            'extraction_method': extraction_method,
            'extraction_success': land_area_found is not None,
            'http_status': response.status_code if 'response' in locals() else None
        }
        
        if land_area_found:
            unit_display = area_unit.replace('_', ' ') if area_unit else 'unknown unit'
            print(f"✓ {land_area_found} {unit_display}")
        else:
            print("✗ No area data found")
            
    except requests.RequestException as e:
        print(f"✗ Request failed")
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': None,
            'area_unit': None,
            'wikipedia_url': wikipedia_url,
            'extraction_method': None,
            'extraction_success': False,
            'error': str(e)[:100]
        }
    
    except Exception as e:
        print(f"✗ Processing error")
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': None,
            'area_unit': None,
            'wikipedia_url': wikipedia_url,
            'extraction_method': None,
            'extraction_success': False,
            'error': str(e)[:100]
        }
    
    land_area_results.append(result)
    
    # Rate limiting
    time.sleep(0.5)
    
    # Progress updates
    if i % 10 == 0:
        successful = len([r for r in land_area_results if r['extraction_success']])
        print(f"\n  Progress: {i}/39 completed, {successful} successful\n")

# Final analysis
print("\n=== RESEARCH RESULTS ANALYSIS ===\n")

successful = [r for r in land_area_results if r['extraction_success']]
failed = [r for r in land_area_results if not r['extraction_success']]

print(f"Total cities researched: {len(land_area_results)}")
print(f"Successful extractions: {len(successful)}")
print(f"Failed extractions: {len(failed)}")
print(f"Success rate: {len(successful)/len(land_area_results)*100:.1f}%")

# Show successful results sorted by area
if successful:
    print(f"\nLand areas successfully extracted (sorted by size):")
    sorted_results = sorted(successful, key=lambda x: x['land_area'])
    
    for result in sorted_results:
        area_str = f"{result['land_area']:.2f} sq miles"
        print(f"  {result['county_seat']:<15} {area_str:>12}")

# Show failed extractions
if failed:
    print(f"\nFailed extractions (need alternative sources):")
    for result in failed:
        print(f"  {result['county_seat']} ({result['county']})")

# Save comprehensive results
final_results = {
    'research_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'objective': 'Washington state county seats land area data',
    'data_source': 'Wikipedia (U.S. Census Bureau data)',
    'total_cities': len(land_area_results),
    'successful_extractions': len(successful),
    'success_rate_percent': round(len(successful)/len(land_area_results)*100, 1),
    'results': land_area_results
}

# Add summary statistics if we have successful extractions
if successful:
    areas = [r['land_area'] for r in successful]
    final_results['summary_statistics'] = {
        'smallest_area_sq_miles': min(areas),
        'largest_area_sq_miles': max(areas),
        'average_area_sq_miles': sum(areas) / len(areas),
        'median_area_sq_miles': sorted(areas)[len(areas)//2]
    }

with open('workspace/wa_county_seats_land_areas_final.json', 'w') as f:
    json.dump(final_results, f, indent=2)

print(f"\n✓ Complete results saved to: workspace/wa_county_seats_land_areas_final.json")

if successful:
    stats = final_results['summary_statistics']
    print(f"\n=== SUMMARY STATISTICS ===\n")
    print(f"Smallest county seat: {stats['smallest_area_sq_miles']:.2f} sq miles")
    print(f"Largest county seat: {stats['largest_area_sq_miles']:.2f} sq miles")
    print(f"Average area: {stats['average_area_sq_miles']:.2f} sq miles")
    print(f"Median area: {stats['median_area_sq_miles']:.2f} sq miles")

print(f"\n=== LAND AREA RESEARCH COMPLETE ===\n")
print(f"Successfully extracted land area data for {len(successful)} out of 39 Washington state county seats")
print(f"All data sourced from Wikipedia containing official U.S. Census Bureau figures")
```