### Development Step 25: Search for 2011 Bulgarian Census Tertiary Education Completion Data by Gender

**Description**: Search for the official 2011 Bulgarian census data from the Bulgarian National Statistical Institute (NSI) or other authoritative government sources that specifically contain the tertiary education completion numbers by gender. Focus on finding the exact counts or percentages of men and women who completed tertiary education according to the 2011 census results.

**Use Cases**:
- Education ministry policy analysis and gender parity monitoring using 2011 tertiary completion rates by region
- University academic research on historical gender achievement gaps in Bulgarian higher education for a peer-reviewed publication
- NGO evaluation of past gender equality initiatives by comparing male/female tertiary graduation figures from the 2011 census
- Corporate HR workforce planning and campus recruitment strategy based on regional tertiary education demographics by gender
- Data journalism infographic and interactive map creation to illustrate Bulgaria’s gender gap in university attainment
- EdTech product market segmentation and localization using historical tertiary education completion patterns across genders
- Real estate investment forecasting by correlating neighborhood education levels by gender with property value trends
- Public health epidemiological study investigating links between tertiary education attainment by gender and district-level health outcomes

```
import requests
import os
import json
from bs4 import BeautifulSoup
import time
import re

# Create workspace directory if it doesn't exist
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

print("=== SEARCHING FOR 2011 BULGARIAN CENSUS TERTIARY EDUCATION DATA ===\n")
print("Target: Official NSI (Bulgarian National Statistical Institute) census data")
print("Focus: Tertiary education completion by gender\n")

# Function to perform web requests with proper headers
def fetch_with_headers(url, max_retries=3):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5,bg;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive'
    }
    
    for attempt in range(max_retries):
        try:
            print(f"Attempt {attempt + 1} to fetch: {url}")
            response = requests.get(url, headers=headers, timeout=20)
            print(f"Response status: {response.status_code}")
            return response
        except requests.exceptions.RequestException as e:
            wait_time = 2 ** attempt
            if attempt < max_retries - 1:
                print(f"Error: {e}. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Failed after {max_retries} attempts: {e}")
                return None

# Primary target URLs for Bulgarian National Statistical Institute
print("Step 1: Accessing Bulgarian National Statistical Institute (NSI) sources...\n")

target_urls = [
    "https://www.nsi.bg/en",  # English version of NSI
    "https://www.nsi.bg",     # Main NSI site
    "https://www.nsi.bg/en/content/6704/population-education",  # Direct education statistics
    "https://www.nsi.bg/en/content/6710/census-2011",  # Census 2011 page
    "https://www.nsi.bg/census2011/indexen.php",  # Alternative census URL
]

successful_sources = []
failed_sources = []

for i, url in enumerate(target_urls, 1):
    print(f"\n--- Source {i}: {url} ---")
    
    response = fetch_with_headers(url)
    
    if response and response.status_code == 200:
        print(f"✓ Successfully accessed {url}")
        print(f"Content length: {len(response.content):,} bytes")
        
        # Save the content for analysis
        filename = f'nsi_source_{i}.html'
        filepath = os.path.join(workspace_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        # Parse and analyze content
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('title')
        title_text = title.get_text().strip() if title else 'No title found'
        
        # Look for education and census related content
        content_text = soup.get_text().lower()
        education_keywords = ['tertiary education', 'higher education', 'university', 'education level', 'educational attainment', 'census 2011', 'висше образование']
        gender_keywords = ['gender', 'men', 'women', 'male', 'female', 'мъже', 'жени']
        
        has_education_content = any(keyword in content_text for keyword in education_keywords)
        has_gender_content = any(keyword in content_text for keyword in gender_keywords)
        has_2011_content = '2011' in content_text
        
        # Look for links to census data or education statistics
        links = soup.find_all('a', href=True)
        relevant_links = []
        
        for link in links:
            href = link.get('href', '')
            link_text = link.get_text().strip().lower()
            
            if any(keyword in (href.lower() + ' ' + link_text) for keyword in ['census', '2011', 'education', 'образование']):
                full_url = href if href.startswith('http') else f"https://www.nsi.bg{href}" if href.startswith('/') else f"https://www.nsi.bg/{href}"
                relevant_links.append({
                    'text': link.get_text().strip(),
                    'url': full_url
                })
        
        source_info = {
            'url': url,
            'title': title_text,
            'filename': filepath,
            'content_length': len(response.text),
            'has_education_content': has_education_content,
            'has_gender_content': has_gender_content,
            'has_2011_content': has_2011_content,
            'relevant_links': relevant_links[:10]  # Limit to first 10 relevant links
        }
        
        successful_sources.append(source_info)
        
        print(f"Title: {title_text}")
        print(f"Contains education content: {has_education_content}")
        print(f"Contains gender content: {has_gender_content}")
        print(f"Contains 2011 content: {has_2011_content}")
        print(f"Found {len(relevant_links)} relevant links")
        
        if relevant_links:
            print("Top relevant links found:")
            for j, link in enumerate(relevant_links[:5], 1):
                print(f"  {j}. {link['text']} -> {link['url']}")
        
    else:
        error_info = {
            'url': url,
            'status': response.status_code if response else 'No response',
            'error': 'Failed to fetch'
        }
        failed_sources.append(error_info)
        print(f"✗ Failed to access {url}")
    
    time.sleep(2)  # Be respectful to the server

print(f"\n=== INITIAL SEARCH RESULTS ===\n")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources")

# Analyze the most promising sources
priority_sources = []
for source in successful_sources:
    priority_score = 0
    if source['has_education_content']:
        priority_score += 3
    if source['has_gender_content']:
        priority_score += 2
    if source['has_2011_content']:
        priority_score += 2
    if len(source['relevant_links']) > 0:
        priority_score += 1
    
    source['priority_score'] = priority_score
    if priority_score >= 4:  # High priority threshold
        priority_sources.append(source)

# Sort by priority score
priority_sources.sort(key=lambda x: x['priority_score'], reverse=True)

print(f"\n=== HIGH PRIORITY SOURCES FOR DETAILED ANALYSIS ===\n")
print(f"Found {len(priority_sources)} high-priority sources")

for i, source in enumerate(priority_sources, 1):
    print(f"\n{i}. {source['url']} (Priority Score: {source['priority_score']})")
    print(f"   Title: {source['title']}")
    print(f"   Education content: {source['has_education_content']}")
    print(f"   Gender content: {source['has_gender_content']}")
    print(f"   2011 content: {source['has_2011_content']}")
    print(f"   Relevant links: {len(source['relevant_links'])}")

# Follow up on the most relevant links from priority sources
print(f"\n=== FOLLOWING UP ON RELEVANT LINKS ===\n")

additional_sources = []
for source in priority_sources[:2]:  # Check top 2 priority sources
    print(f"Following links from: {source['url']}")
    
    for link_info in source['relevant_links'][:3]:  # Check top 3 links from each source
        link_url = link_info['url']
        link_text = link_info['text']
        
        print(f"\nChecking link: {link_text}")
        print(f"URL: {link_url}")
        
        response = fetch_with_headers(link_url)
        
        if response and response.status_code == 200:
            print(f"✓ Successfully accessed link")
            
            # Save this content too
            safe_filename = re.sub(r'[^\w\-_\.]', '_', link_text)[:50]
            filename = f'nsi_link_{safe_filename}.html'
            filepath = os.path.join(workspace_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # Quick analysis of this linked content
            soup = BeautifulSoup(response.content, 'html.parser')
            content_text = soup.get_text().lower()
            
            # Look for specific tertiary education data
            tertiary_indicators = ['tertiary education', 'higher education', 'university degree', 'bachelor', 'master', 'висше образование']
            has_tertiary_data = any(indicator in content_text for indicator in tertiary_indicators)
            
            # Look for statistical tables or data
            tables = soup.find_all('table')
            has_tables = len(tables) > 0
            
            additional_info = {
                'source_url': source['url'],
                'link_text': link_text,
                'link_url': link_url,
                'filename': filepath,
                'has_tertiary_data': has_tertiary_data,
                'has_tables': has_tables,
                'table_count': len(tables),
                'content_length': len(response.text)
            }
            
            additional_sources.append(additional_info)
            
            print(f"   Saved to: {filename}")
            print(f"   Has tertiary education data: {has_tertiary_data}")
            print(f"   Has tables: {has_tables} ({len(tables)} tables found)")
            
        else:
            print(f"✗ Failed to access link")
        
        time.sleep(2)

# Save comprehensive research summary
research_summary = {
    'research_target': '2011 Bulgarian Census - Tertiary Education by Gender',
    'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'total_sources_attempted': len(target_urls),
    'successful_sources': len(successful_sources),
    'priority_sources': len(priority_sources),
    'additional_links_checked': len(additional_sources),
    'sources_data': successful_sources,
    'priority_sources_data': priority_sources,
    'additional_sources_data': additional_sources,
    'failed_sources': failed_sources
}

summary_file = os.path.join(workspace_dir, 'bulgarian_census_research_summary.json')
with open(summary_file, 'w', encoding='utf-8') as f:
    json.dump(research_summary, f, indent=2, ensure_ascii=False)

print(f"\n=== RESEARCH PHASE 1 COMPLETE ===\n")
print(f"Files created in workspace/:")
print(f"  - bulgarian_census_research_summary.json (comprehensive research summary)")

for source in successful_sources:
    print(f"  - {os.path.basename(source['filename'])} (main NSI page)")

for source in additional_sources:
    print(f"  - {os.path.basename(source['filename'])} (linked content)")

print(f"\nNext steps:")
print(f"1. Parse downloaded HTML files for statistical tables")
print(f"2. Look for census data with education levels broken down by gender")
print(f"3. Extract specific tertiary education completion numbers")

if priority_sources:
    print(f"\nMost promising source: {priority_sources[0]['url']}")
    print(f"Priority score: {priority_sources[0]['priority_score']}")
else:
    print(f"\nNo high-priority sources identified. Will need alternative search strategy.")
```