### Development Step 3: **Title:**  
Extract Wikipedia Access Date from Endnote on Page 11 of 'The Responsibility of Intellectuals' (2019)

**Description**: Access and download the full text of 'The Responsibility of Intellectuals' (DOI 10.2307/j.ctv9b2xdv) from JSTOR or UCL Press. Since this is a 2019 UCL Press publication available through JSTOR, retrieve the complete book content and save it to the workspace. Focus on locating page 11, identifying the second-to-last paragraph on that page, and extracting the specific endnote referenced in that paragraph. The endnote should contain a Wikipedia article citation with a November access date - extract the exact day of the month when the Wikipedia article was accessed.

**Use Cases**:
- Academic integrity verification by university librarians checking the accuracy and access dates of Wikipedia citations in scholarly books
- Automated extraction of citation metadata for digital humanities researchers compiling bibliometric datasets from open-access monographs
- Legal teams auditing referenced online materials in published works to confirm compliance with copyright and citation standards
- Publishers conducting quality control to ensure endnotes in digital books properly reference and date online sources
- Research assistants preparing annotated bibliographies by programmatically identifying and extracting Wikipedia access dates from book endnotes
- Journalists fact-checking claims in recent academic books by tracing the exact Wikipedia versions cited at specific dates
- Digital archivists preserving citation trails by extracting and archiving referenced web pages as they appeared on the cited access date
- Educational technology developers building tools that highlight and verify online references in course materials for instructors

```
import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import re

print('=== ACCESSING FULL TEXT OF "THE RESPONSIBILITY OF INTELLECTUALS" ===')
print('DOI: 10.2307/j.ctv9b2xdv')
print('Objective: Locate page 11, find second-to-last paragraph, extract endnote with Wikipedia citation (November access date)')
print('\n' + '='*100 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# First, let's examine the existing workspace files to understand what we have
print('=== INSPECTING EXISTING WORKSPACE FILES ===')
workspace_files = os.listdir('workspace')
if workspace_files:
    print(f'Found {len(workspace_files)} files in workspace:')
    for file in workspace_files:
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f'- {file} ({file_size:,} bytes)')
else:
    print('No existing files in workspace')

# Check if we have the final bibliographic record
final_record_path = 'workspace/final_bibliographic_record.json'
if os.path.exists(final_record_path):
    print('\n=== INSPECTING FINAL BIBLIOGRAPHIC RECORD ===')
    with open(final_record_path, 'r', encoding='utf-8') as f:
        biblio_data = json.load(f)
    
    print('Available keys in bibliographic record:')
    for key in biblio_data.keys():
        print(f'- {key}: {type(biblio_data[key])}')
    
    print(f'\nKey information:')
    print(f'Title: {biblio_data.get("title", "Unknown")}')
    print(f'Publisher: {biblio_data.get("publisher", "Unknown")}')
    print(f'Year: {biblio_data.get("publication_year", "Unknown")}')
    print(f'DOI URL: {biblio_data.get("doi_url", "Unknown")}')
    print(f'JSTOR URL: {biblio_data.get("jstor_url", "Unknown")}')
    
    # Check chapters/sections structure
    if 'chapters_sections' in biblio_data and biblio_data['chapters_sections']:
        print(f'\nBook structure: {len(biblio_data["chapters_sections"])} chapters/sections')
        for i, chapter in enumerate(biblio_data['chapters_sections'][:3], 1):
            print(f'{i}. {chapter.get("title", "No title")}')
            print(f'   URL: {chapter.get("url", "No URL")}')
else:
    print('Final bibliographic record not found')

# Now let's try to access the full text through JSTOR
print('\n=== ATTEMPTING TO ACCESS FULL TEXT VIA JSTOR ===')

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# Try to access the main JSTOR book page
jstor_main_url = 'https://www.jstor.org/stable/j.ctv9b2xdv'
print(f'Accessing main JSTOR page: {jstor_main_url}')

try:
    response = requests.get(jstor_main_url, headers=headers, timeout=30)
    print(f'JSTOR main page status: {response.status_code}')
    print(f'Final URL: {response.url}')
    print(f'Content length: {len(response.content):,} bytes')
    
    if response.status_code == 200:
        # Save the main page for analysis
        with open('workspace/jstor_main_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print('‚úì JSTOR main page saved to workspace/jstor_main_page.html')
        
        # Parse the page to look for full-text access options
        soup = BeautifulSoup(response.content, 'html.parser')
        page_text = soup.get_text().lower()
        
        # Look for "read online", "full text", "PDF", or similar access options
        access_indicators = [
            'read online', 'full text', 'download pdf', 'view pdf',
            'open access', 'free access', 'read book', 'view book'
        ]
        
        found_access_options = []
        for indicator in access_indicators:
            if indicator in page_text:
                found_access_options.append(indicator)
        
        if found_access_options:
            print(f'\n‚úì Found access indicators: {found_access_options}')
        else:
            print('\n‚ö† No obvious access indicators found in page text')
        
        # Look for links that might provide full-text access
        access_links = []
        
        # Search for various types of access links
        link_selectors = [
            'a[href*="pdf"]',
            'a[href*="read"]',
            'a[href*="view"]',
            'a[href*="download"]',
            'a[href*="full"]',
            'a[href*="text"]',
            '.pdf-link a',
            '.read-link a',
            '.download-link a',
            '.access-link a'
        ]
        
        for selector in link_selectors:
            try:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href:
                        # Convert relative URLs to absolute
                        if href.startswith('/'):
                            href = urljoin(jstor_main_url, href)
                        
                        link_text = link.get_text().strip()
                        if len(link_text) > 0 and len(link_text) < 100:  # Reasonable link text length
                            access_links.append({
                                'url': href,
                                'text': link_text,
                                'selector': selector
                            })
            except Exception as e:
                print(f'Error with selector {selector}: {str(e)}')
        
        # Remove duplicates
        unique_links = []
        seen_urls = set()
        for link in access_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        print(f'\nFound {len(unique_links)} potential access links:')
        for i, link in enumerate(unique_links[:10], 1):  # Show first 10
            print(f'{i}. "{link["text"]}" -> {link["url"]}')
            print(f'   (Found via: {link["selector"]})')
        
        # Look specifically for chapter/section links that might contain page 11
        chapter_links = []
        for link in unique_links:
            link_url = link['url'].lower()
            link_text = link['text'].lower()
            
            # Check if this might be a chapter or section link
            if any(indicator in link_url or indicator in link_text for indicator in 
                   ['chapter', 'section', 'pdf', 'ctv9b2xdv']):
                chapter_links.append(link)
        
        if chapter_links:
            print(f'\n*** FOUND {len(chapter_links)} POTENTIAL CHAPTER/SECTION LINKS ***')
            for i, link in enumerate(chapter_links[:5], 1):
                print(f'{i}. "{link["text"]}" -> {link["url"]}')
        
        # Try to access the first promising link
        if chapter_links:
            print('\n=== ATTEMPTING TO ACCESS FIRST CHAPTER/SECTION LINK ===')
            first_link = chapter_links[0]
            print(f'Trying: {first_link["text"]} -> {first_link["url"]}')
            
            try:
                chapter_response = requests.get(first_link['url'], headers=headers, timeout=30)
                print(f'Chapter access status: {chapter_response.status_code}')
                print(f'Content type: {chapter_response.headers.get("content-type", "unknown")}')
                print(f'Content length: {len(chapter_response.content):,} bytes')
                
                if chapter_response.status_code == 200:
                    content_type = chapter_response.headers.get('content-type', '').lower()
                    
                    if 'pdf' in content_type:
                        print('\n*** PDF CONTENT DETECTED ***')
                        pdf_path = 'workspace/responsibility_intellectuals_chapter.pdf'
                        
                        with open(pdf_path, 'wb') as pdf_file:
                            pdf_file.write(chapter_response.content)
                        
                        file_size = os.path.getsize(pdf_path)
                        print(f'‚úì PDF saved to: {pdf_path}')
                        print(f'File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                        
                        # Try to extract text from PDF if possible
                        try:
                            print('\nAttempting to extract text from PDF...')
                            from langchain_community.document_loaders import PyPDFLoader
                            
                            loader = PyPDFLoader(pdf_path)
                            pages = loader.load_and_split()
                            
                            print(f'‚úì PDF loaded successfully with {len(pages)} pages')
                            
                            # Look for page 11 specifically
                            if len(pages) >= 11:
                                page_11_content = pages[10].page_content  # Page 11 is index 10
                                print(f'\n=== PAGE 11 CONTENT FOUND ===') 
                                print(f'Page 11 length: {len(page_11_content):,} characters')
                                print(f'\nFirst 500 characters of page 11:')
                                print('='*80)
                                print(page_11_content[:500] + '...')
                                print('='*80)
                                
                                # Save page 11 content
                                with open('workspace/page_11_content.txt', 'w', encoding='utf-8') as f:
                                    f.write(page_11_content)
                                print('\n‚úì Page 11 content saved to workspace/page_11_content.txt')
                                
                                # Look for the second-to-last paragraph
                                paragraphs = [p.strip() for p in page_11_content.split('\n\n') if p.strip()]
                                print(f'\nFound {len(paragraphs)} paragraphs on page 11')
                                
                                if len(paragraphs) >= 2:
                                    second_to_last_para = paragraphs[-2]
                                    print(f'\n=== SECOND-TO-LAST PARAGRAPH ON PAGE 11 ===')
                                    print('='*80)
                                    print(second_to_last_para)
                                    print('='*80)
                                    
                                    # Look for endnote references in this paragraph
                                    endnote_patterns = [
                                        r'\b(\d+)\b',  # Simple numbers
                                        r'\[(\d+)\]',  # Numbers in brackets
                                        r'\((\d+)\)',  # Numbers in parentheses
                                        r'\b(\d+)\.',  # Numbers with periods
                                        r'see note (\d+)',  # "see note X" format
                                        r'note (\d+)',  # "note X" format
                                    ]
                                    
                                    found_endnotes = []
                                    for pattern in endnote_patterns:
                                        matches = re.findall(pattern, second_to_last_para, re.IGNORECASE)
                                        if matches:
                                            for match in matches:
                                                if match.isdigit() and int(match) <= 100:  # Reasonable endnote number
                                                    found_endnotes.append(int(match))
                                    
                                    # Remove duplicates and sort
                                    found_endnotes = sorted(list(set(found_endnotes)))
                                    
                                    if found_endnotes:
                                        print(f'\n*** FOUND ENDNOTE REFERENCES: {found_endnotes} ***')
                                        
                                        # Now we need to find the actual endnotes
                                        print('\n=== SEARCHING FOR ENDNOTES SECTION ===')
                                        
                                        # Combine all pages to search for endnotes
                                        full_text = '\n\n'.join([page.page_content for page in pages])
                                        
                                        # Look for endnotes section
                                        endnotes_indicators = [
                                            'notes', 'endnotes', 'references', 'footnotes',
                                            'bibliography', 'works cited'
                                        ]
                                        
                                        endnotes_section_found = False
                                        for indicator in endnotes_indicators:
                                            pattern = rf'\b{indicator}\b'
                                            matches = list(re.finditer(pattern, full_text, re.IGNORECASE))
                                            if matches:
                                                print(f'Found "{indicator}" section at {len(matches)} locations')
                                                endnotes_section_found = True
                                        
                                        # Search for specific endnote numbers with Wikipedia citations
                                        print('\n=== SEARCHING FOR WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATE ===')
                                        
                                        # Look for Wikipedia citations with November access dates
                                        wikipedia_patterns = [
                                            r'wikipedia[^\n]*november[^\n]*accessed[^\n]*',
                                            r'en\.wikipedia\.org[^\n]*november[^\n]*',
                                            r'accessed[^\n]*november[^\n]*wikipedia[^\n]*',
                                            r'november[^\n]*\d{1,2}[^\n]*wikipedia[^\n]*',
                                            r'wikipedia[^\n]*accessed[^\n]*november[^\n]*\d{1,2}[^\n]*'
                                        ]
                                        
                                        wikipedia_citations = []
                                        for pattern in wikipedia_patterns:
                                            matches = re.finditer(pattern, full_text, re.IGNORECASE | re.DOTALL)
                                            for match in matches:
                                                citation_text = match.group(0)
                                                # Extract the day from November date
                                                day_match = re.search(r'november\s+(\d{1,2})', citation_text, re.IGNORECASE)
                                                if day_match:
                                                    day = day_match.group(1)
                                                    wikipedia_citations.append({
                                                        'citation': citation_text,
                                                        'november_day': day,
                                                        'position': match.start()
                                                    })
                                        
                                        if wikipedia_citations:
                                            print(f'\nüéØ FOUND {len(wikipedia_citations)} WIKIPEDIA CITATIONS WITH NOVEMBER ACCESS DATES:')
                                            
                                            for i, citation in enumerate(wikipedia_citations, 1):
                                                print(f'\nCitation {i}:')
                                                print(f'November day: {citation["november_day"]}')
                                                print(f'Position in text: {citation["position"]}')
                                                print('Citation text:')
                                                print('='*60)
                                                print(citation['citation'])
                                                print('='*60)
                                            
                                            # Save the Wikipedia citations
                                            citations_data = {
                                                'source_file': pdf_path,
                                                'page_11_paragraph_count': len(paragraphs),
                                                'second_to_last_paragraph': second_to_last_para,
                                                'endnote_references_found': found_endnotes,
                                                'wikipedia_citations': wikipedia_citations,
                                                'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                                            }
                                            
                                            with open('workspace/wikipedia_citations_analysis.json', 'w', encoding='utf-8') as f:
                                                json.dump(citations_data, f, indent=2, ensure_ascii=False)
                                            
                                            print('\n‚úì Wikipedia citations analysis saved to workspace/wikipedia_citations_analysis.json')
                                            
                                            # Extract the specific day for the answer
                                            if len(wikipedia_citations) == 1:
                                                answer_day = wikipedia_citations[0]['november_day']
                                                print(f'\n*** ANSWER FOUND: The Wikipedia article was accessed on November {answer_day} ***')
                                            elif len(wikipedia_citations) > 1:
                                                print(f'\n*** MULTIPLE WIKIPEDIA CITATIONS FOUND - Need to determine which is from page 11 endnote ***')
                                                for i, citation in enumerate(wikipedia_citations, 1):
                                                    print(f'Option {i}: November {citation["november_day"]}')
                                        else:
                                            print('\n‚ö† No Wikipedia citations with November access dates found')
                                            print('Searching for any Wikipedia references...')
                                            
                                            # Broader search for Wikipedia
                                            wiki_matches = re.finditer(r'wikipedia[^\n]{0,200}', full_text, re.IGNORECASE)
                                            wiki_refs = [match.group(0) for match in wiki_matches]
                                            
                                            if wiki_refs:
                                                print(f'Found {len(wiki_refs)} general Wikipedia references:')
                                                for i, ref in enumerate(wiki_refs[:5], 1):
                                                    print(f'{i}. {ref[:100]}...')
                                    else:
                                        print('\n‚ö† No endnote references found in second-to-last paragraph')
                                        print('Showing paragraph content for manual inspection:')
                                        print(second_to_last_para)
                                else:
                                    print(f'\n‚ö† Page 11 has fewer than 2 paragraphs ({len(paragraphs)} found)')
                                    if paragraphs:
                                        print('Available paragraphs:')
                                        for i, para in enumerate(paragraphs, 1):
                                            print(f'{i}. {para[:100]}...')
                            else:
                                print(f'\n‚ö† PDF has only {len(pages)} pages, page 11 not available')
                                print('Available pages:')
                                for i, page in enumerate(pages[:5], 1):
                                    preview = page.page_content[:100].replace('\n', ' ')
                                    print(f'Page {i}: {preview}...')
                        
                        except ImportError:
                            print('‚ö† PyPDFLoader not available - PDF saved but text extraction skipped')
                        except Exception as pdf_error:
                            print(f'‚ùå PDF text extraction error: {str(pdf_error)}')
                    
                    elif 'html' in content_type:
                        print('\n*** HTML CONTENT DETECTED ***')
                        
                        # Save HTML content
                        with open('workspace/chapter_content.html', 'w', encoding='utf-8') as f:
                            f.write(chapter_response.text)
                        print('‚úì HTML content saved to workspace/chapter_content.html')
                        
                        # Parse HTML for text content
                        chapter_soup = BeautifulSoup(chapter_response.content, 'html.parser')
                        
                        # Remove navigation and non-content elements
                        for element in chapter_soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                            element.decompose()
                        
                        chapter_text = chapter_soup.get_text()
                        print(f'\nExtracted HTML text length: {len(chapter_text):,} characters')
                        print(f'First 300 characters: {chapter_text[:300]}...')
                        
                        # Save extracted text
                        with open('workspace/chapter_text_content.txt', 'w', encoding='utf-8') as f:
                            f.write(chapter_text)
                        print('‚úì Chapter text content saved to workspace/chapter_text_content.txt')
                    
                    else:
                        print(f'\n*** UNKNOWN CONTENT TYPE: {content_type} ***')
                        print('Saving raw content for analysis')
                        
                        with open('workspace/unknown_content.txt', 'w', encoding='utf-8') as f:
                            f.write(chapter_response.text)
                        print('‚úì Raw content saved to workspace/unknown_content.txt')
                
                else:
                    print(f'‚ùå Chapter access failed with status: {chapter_response.status_code}')
            
            except Exception as chapter_error:
                print(f'‚ùå Error accessing chapter: {str(chapter_error)}')
        
        else:
            print('\n‚ö† No chapter/section links found to try')
    
    else:
        print(f'‚ùå JSTOR main page access failed with status: {response.status_code}')

except Exception as e:
    print(f'‚ùå Error accessing JSTOR: {str(e)}')

print('\n' + '='*100)
print('FULL TEXT ACCESS ATTEMPT SUMMARY')
print('='*100)
print('Objective: Access full text to find page 11, second-to-last paragraph, and Wikipedia endnote with November access date')
print('\nFiles created in workspace:')
if os.path.exists('workspace'):
    for file in os.listdir('workspace'):
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f'- {file} ({file_size:,} bytes)')

print('\n*** NEXT STEP: Analyze extracted content for the specific Wikipedia citation ***')
```