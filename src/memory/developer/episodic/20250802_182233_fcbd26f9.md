### Development Step 7: Targeted ClinicalTrials.gov H. pylori & Acne Vulgaris Trials Search with Synonyms Extended Jan‚ÄìMay 2018 Registration

**Description**: Conduct a more targeted search specifically for clinical trials with exact phrase matching for 'H. pylori' AND 'acne vulgaris' on ClinicalTrials.gov, then expand the search timeframe to include trials that may have been registered or updated during Jan-May 2018 but had different start/end dates. Also search for variations like 'Helicobacter pylori' and 'acne' to ensure comprehensive coverage of potential trial listings that match the TASK criteria.

**Use Cases**:
- Clinical research coordinators automating identification of registered trials combining H. pylori eradication and acne vulgaris treatment for streamlined protocol development
- Dermatology clinicians seeking up-to-date trial registries on Helicobacter pylori‚Äìrelated acne interventions to inform off-label antibiotic prescribing
- Pharmaceutical market intelligence analysts tracking emerging clinical studies on microbiome-targeted acne therapies for competitive landscape assessment
- Regulatory affairs specialists compiling exhaustive lists of H. pylori and acne vulgaris trials registered or updated between January and May 2018 for FDA submission dossiers
- Biomedical data scientists integrating scraped ClinicalTrials.gov metadata into machine learning pipelines to predict therapeutic outcomes in acne vulgaris
- Systematic review authors cross-referencing individual NCT identifiers from separate H. pylori and acne searches to identify overlapping studies for meta-analysis inclusion
- Grant writers gathering evidence of existing clinical trial activity on Helicobacter pylori‚Äôs role in dermatologic conditions to support funding applications
- Health informatics teams building automated dashboards that monitor new or updated clinical trials on H. pylori and acne to alert stakeholders of research trends

```
# Since both SerpAPI and the old ClinicalTrials.gov API failed, I'll implement
# direct web scraping of the ClinicalTrials.gov website to search for clinical trials
# involving both H. pylori and acne vulgaris

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
from urllib.parse import urlencode, quote
import re

print("Implementing direct web scraping of ClinicalTrials.gov")
print("=" * 80)
print("WEB SCRAPING SEARCH: H. PYLORI AND ACNE VULGARIS CLINICAL TRIALS")
print("=" * 80)

# ClinicalTrials.gov search URL (using their web interface)
base_search_url = "https://clinicaltrials.gov/search"

# Headers to mimic a real browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# Define search terms and combinations
search_queries = [
    "H. pylori acne vulgaris",
    "Helicobacter pylori acne vulgaris", 
    "H. pylori acne",
    "Helicobacter pylori acne",
    "H pylori acne vulgaris",
    "pylori acne treatment"
]

all_found_trials = []
print(f"Searching ClinicalTrials.gov for {len(search_queries)} different search terms...\n")

for i, query in enumerate(search_queries, 1):
    print(f"Search {i}/{len(search_queries)}: '{query}'")
    print("-" * 60)
    
    # Construct search parameters
    search_params = {
        'term': query,
        'aggFilters': 'status:rec,not,unk,avail,enroll,active,comp,term,sus,with',  # All statuses
        'distance': '50',
        'page': '1'
    }
    
    try:
        # Make request to ClinicalTrials.gov search
        search_url = f"{base_search_url}?{urlencode(search_params)}"
        print(f"Search URL: {search_url}")
        
        response = requests.get(search_url, headers=headers, timeout=30)
        print(f"Response Status: {response.status_code}")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for search results
            # ClinicalTrials.gov typically shows results in specific containers
            result_containers = soup.find_all(['div', 'article'], class_=re.compile(r'.*result.*|.*study.*|.*trial.*', re.I))
            
            # Also look for any links to individual studies (NCT numbers)
            nct_links = soup.find_all('a', href=re.compile(r'/study/NCT\d+'))
            
            # Look for study titles and descriptions
            study_titles = soup.find_all(['h2', 'h3', 'h4'], string=re.compile(r'.*\w.*'))
            
            print(f"Found {len(result_containers)} result containers")
            print(f"Found {len(nct_links)} NCT study links")
            print(f"Found {len(study_titles)} potential study titles")
            
            # Extract information from NCT links
            for link in nct_links:
                nct_id = re.search(r'NCT\d+', link.get('href', ''))
                if nct_id:
                    nct_number = nct_id.group()
                    title = link.get_text(strip=True)
                    
                    study_info = {
                        'search_query': query,
                        'nct_id': nct_number,
                        'title': title,
                        'url': f"https://clinicaltrials.gov{link.get('href')}",
                        'found_via': 'NCT link extraction'
                    }
                    
                    all_found_trials.append(study_info)
                    
                    print(f"\n  Found Study: {nct_number}")
                    print(f"  Title: {title}")
                    print(f"  URL: {study_info['url']}")
            
            # Look for results count or "no results" messages
            results_text = soup.get_text().lower()
            
            if 'no studies found' in results_text or 'no results' in results_text:
                print("  No studies found for this search term")
            elif 'studies found' in results_text:
                # Try to extract the number of studies found
                results_match = re.search(r'(\d+)\s+studies?\s+found', results_text)
                if results_match:
                    num_results = results_match.group(1)
                    print(f"  {num_results} studies found according to page text")
            
            # Save the raw HTML for manual inspection if needed
            html_file = f"workspace/clinicaltrials_search_{i}_{query.replace(' ', '_').replace('.', '')}.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"  Raw HTML saved to: {html_file}")
            
        else:
            print(f"  Error: HTTP {response.status_code}")
            print(f"  Response: {response.text[:300]}")
            
    except Exception as e:
        print(f"  Error during web scraping: {str(e)}")
    
    # Delay between requests to be respectful
    time.sleep(3)
    print()

# Now let's try a more direct approach - search for individual terms and cross-reference
print("\n" + "=" * 80)
print("INDIVIDUAL TERM SEARCHES FOR CROSS-REFERENCING")
print("=" * 80)

# Search for H. pylori trials and acne trials separately
individual_searches = [
    "H. pylori",
    "Helicobacter pylori", 
    "acne vulgaris",
    "acne"
]

individual_results = {}

for term in individual_searches:
    print(f"\nSearching for: '{term}'")
    print("-" * 40)
    
    search_params = {
        'term': term,
        'aggFilters': 'status:rec,not,unk,avail,enroll,active,comp,term,sus,with',
        'page': '1'
    }
    
    try:
        search_url = f"{base_search_url}?{urlencode(search_params)}"
        response = requests.get(search_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            nct_links = soup.find_all('a', href=re.compile(r'/study/NCT\d+'))
            
            term_studies = []
            for link in nct_links[:10]:  # Limit to first 10 for each term
                nct_id = re.search(r'NCT\d+', link.get('href', ''))
                if nct_id:
                    nct_number = nct_id.group()
                    title = link.get_text(strip=True)
                    
                    term_studies.append({
                        'nct_id': nct_number,
                        'title': title,
                        'url': f"https://clinicaltrials.gov{link.get('href')}"
                    })
            
            individual_results[term] = term_studies
            print(f"Found {len(term_studies)} studies for '{term}'")
            
            # Display first few results
            for study in term_studies[:3]:
                print(f"  - {study['nct_id']}: {study['title'][:80]}...")
        
    except Exception as e:
        print(f"Error searching for '{term}': {str(e)}")
    
    time.sleep(2)

print("\n" + "=" * 80)
print("ANALYSIS OF FOUND CLINICAL TRIALS")
print("=" * 80)

# Save all results to workspace
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
results_file = f"workspace/clinicaltrials_web_scraping_results_{timestamp}.json"

with open(results_file, 'w', encoding='utf-8') as f:
    json.dump({
        'search_method': 'Direct web scraping of ClinicalTrials.gov',
        'search_timestamp': timestamp,
        'combined_searches': search_queries,
        'individual_searches': individual_searches,
        'combined_search_results': all_found_trials,
        'individual_search_results': individual_results,
        'total_combined_trials': len(all_found_trials)
    }, f, indent=2, ensure_ascii=False)

print(f"Results saved to: {results_file}")
print(f"Total trials from combined searches: {len(all_found_trials)}")

# Analyze for potential matches
if all_found_trials:
    print(f"\nüìä COMBINED SEARCH RESULTS ANALYSIS:")
    for trial in all_found_trials:
        print(f"\nüîç Trial: {trial['nct_id']}")
        print(f"  Title: {trial['title']}")
        print(f"  Search Query: {trial['search_query']}")
        print(f"  URL: {trial['url']}")
        
        # Check if title contains both H. pylori and acne terms
        title_lower = trial['title'].lower()
        has_h_pylori = any(term.lower() in title_lower for term in ['h. pylori', 'helicobacter pylori', 'h pylori'])
        has_acne = any(term.lower() in title_lower for term in ['acne vulgaris', 'acne'])
        
        if has_h_pylori and has_acne:
            print(f"  üéØ POTENTIAL MATCH: Contains both H. pylori and acne terms!")
else:
    print("\n‚ùå No trials found from combined searches")

# Cross-reference individual search results
print(f"\nüìã INDIVIDUAL SEARCH RESULTS:")
for term, studies in individual_results.items():
    print(f"\n{term}: {len(studies)} studies")
    if studies:
        for study in studies[:2]:  # Show first 2 for each term
            print(f"  - {study['nct_id']}: {study['title'][:60]}...")

# Look for overlapping NCT IDs between H. pylori and acne searches
h_pylori_ncts = set()
acne_ncts = set()

for term, studies in individual_results.items():
    if 'pylori' in term.lower():
        h_pylori_ncts.update(study['nct_id'] for study in studies)
    elif 'acne' in term.lower():
        acne_ncts.update(study['nct_id'] for study in studies)

overlapping_ncts = h_pylori_ncts.intersection(acne_ncts)

print(f"\nüîç CROSS-REFERENCE ANALYSIS:")
print(f"H. pylori related trials: {len(h_pylori_ncts)}")
print(f"Acne related trials: {len(acne_ncts)}")
print(f"Overlapping NCT IDs: {len(overlapping_ncts)}")

if overlapping_ncts:
    print(f"\nüéØ POTENTIAL MATCHES (trials found in both H. pylori and acne searches):")
    for nct_id in overlapping_ncts:
        print(f"  - {nct_id}")
        # Find the trial details from our individual results
        for term, studies in individual_results.items():
            for study in studies:
                if study['nct_id'] == nct_id:
                    print(f"    Title: {study['title']}")
                    print(f"    URL: {study['url']}")
                    break
else:
    print("\n‚ùå No overlapping trials found between H. pylori and acne searches")

print(f"\n" + "=" * 80)
print("WEB SCRAPING SEARCH COMPLETED")
print("=" * 80)
print("\nNote: If no direct matches were found, this suggests that clinical trials")
print("studying both H. pylori and acne vulgaris simultaneously may be rare or")
print("may not exist in the ClinicalTrials.gov database for the specified timeframe.")
```