### Development Step 1: Abel Hugo’s Napoleonic Wars service and Madrid tenure: camp maps, Saint Louis Hospital, ruling monarch

**Description**: Research Abel Hugo, Victor Hugo's brother, focusing on his military service during the Napoleonic Wars and his time in Madrid. Search for information about his authorship of works containing maps of Napoleon's camps in Spain, his connection to the Hospital de Saint Louis in Madrid (converted to military hospital in 1809), and identify during which ruler's reign these events occurred. Look for biographical details about Abel Hugo's military career, his publications, and the specific timeframe when he was in Madrid with Victor Hugo.

**Use Cases**:
- Academic historians aggregating biographical and military service data on obscure Napoleonic figures by scraping Wikipedia and Google Books for publications, campaign maps, and hospital records, then exporting structured JSON for in-depth analysis.
- Genealogy platforms automating the extraction of family connections, author attributions, and timeline events from online encyclopedias and digital book repositories to enrich user family trees with verified historical sources.
- Museum exhibit curators pulling structured summaries, period maps, and contextual details from open-source websites and library APIs to build interactive panels on the Peninsular War without manual data entry.
- EdTech companies generating customized study materials—author profiles, event timelines, and reading lists—by consolidating and filtering historical data on Victor Hugo’s family from multiple web sources for literature and history courses.
- Military historians and defense analysts automating retrieval of Napoleonic campaign maps, officer service records, and converted hospital timelines from online archives to support scholarly publications on the Peninsular War.
- Digital humanities teams creating open-access datasets by normalizing multi-source information—biographical texts, publication metadata, and keyword-tagged content—on 19th-century figures for computational history projects.
- Scholarly publishers integrating web scraping and API searches to verify authorship details, publication dates, and embedded map content when producing annotated editions of 19th-century travelogues and military memoirs.
- Data journalists mining and analyzing historical articles, books, and encyclopedia entries to uncover forgotten narratives, producing interactive online stories about Victor Hugo’s family roles during the Napoleonic Wars.

```
import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import time

print("Researching Abel Hugo, Victor Hugo's brother, focusing on military service and Madrid connections...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Initialize results storage
all_results = []
search_errors = []

print("\n=== DIRECT WIKIPEDIA SEARCH FOR ABEL HUGO INFORMATION ===")

# Search Wikipedia pages directly for Abel Hugo and related topics
wikipedia_urls = [
    "https://en.wikipedia.org/wiki/Abel_Hugo",
    "https://en.wikipedia.org/wiki/Victor_Hugo",
    "https://en.wikipedia.org/wiki/Napoleonic_Wars",
    "https://en.wikipedia.org/wiki/Peninsular_War",
    "https://en.wikipedia.org/wiki/Madrid",
    "https://fr.wikipedia.org/wiki/Abel_Hugo",  # French Wikipedia might have more details
    "https://en.wikipedia.org/wiki/Hospital_de_San_Carlos"  # Madrid hospital
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

for url in wikipedia_urls:
    try:
        print(f"\nFetching: {url}")
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract title
        title_elem = soup.find('h1', class_='firstHeading')
        title = title_elem.get_text(strip=True) if title_elem else 'Unknown Title'
        print(f"Page title: {title}")
        
        # Extract main content
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if content_div:
            # Remove scripts, styles, and navigation elements
            for elem in content_div.find_all(['script', 'style', 'table']):
                if elem.get('class') and any(cls in str(elem.get('class')) for cls in ['navbox', 'infobox']):
                    elem.decompose()
                elif elem.name in ['script', 'style']:
                    elem.decompose()
            
            content = content_div.get_text(separator=' ', strip=True)
            print(f"Content length: {len(content)} characters")
            
            # Create lowercase version for keyword searching
            content_text = content.lower()
            
            # Target keywords from the plan
            target_keywords = [
                'abel hugo', 'victor hugo', 'napoleonic wars', 'madrid', 'spain', 
                'military service', 'military hospital', 'hospital de saint louis',
                'napoleon', 'camps', 'maps', 'publications', 'author', 'brother',
                '1809', 'peninsular war', 'french army', 'officer', 'career'
            ]
            
            # Find matching keywords
            found_keywords = []
            for keyword in target_keywords:
                if keyword in content_text:
                    found_keywords.append(keyword)
            
            print(f"Keywords found: {', '.join(found_keywords)}")
            
            if found_keywords:
                result = {
                    'title': title,
                    'url': url,
                    'content': content[:3000],  # First 3000 characters
                    'keywords_found': found_keywords,
                    'source': 'Wikipedia',
                    'relevance_score': len(found_keywords)
                }
                all_results.append(result)
                print(f"Added relevant result with {len(found_keywords)} keyword matches")
            
        time.sleep(2)  # Be respectful
        
    except Exception as e:
        error_msg = f"Error fetching {url}: {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

print(f"\n=== GOOGLE BOOKS API SEARCH FOR ABEL HUGO WORKS ===")

# Search Google Books API for Abel Hugo's publications
book_queries = [
    "Abel Hugo Napoleon camps Spain",
    "Abel Hugo military maps Napoleon", 
    "Abel Hugo Madrid 1809",
    "Abel Hugo Napoleonic Wars",
    "Abel Hugo Victor Hugo brother",
    "Abel Hugo publications military",
    "Abel Hugo Hospital Saint Louis Madrid",
    "Abel Hugo Peninsular War Spain"
]

for query in book_queries:
    try:
        print(f"\nSearching Google Books for: {query}")
        api_url = "https://www.googleapis.com/books/v1/volumes"
        params = {
            'q': query,
            'maxResults': 10,
            'printType': 'books',
            'langRestrict': 'en'
        }
        
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            print(f"Found {len(data['items'])} books")
            
            for item in data['items']:
                volume_info = item.get('volumeInfo', {})
                title = volume_info.get('title', 'Unknown Title')
                authors = volume_info.get('authors', ['Unknown Author'])
                description = volume_info.get('description', '')
                published_date = volume_info.get('publishedDate', 'Unknown Date')
                
                # Create text for keyword analysis
                book_text = (title + ' ' + ' '.join(authors) + ' ' + description).lower()
                
                # Check for relevant keywords
                target_keywords = [
                    'abel hugo', 'napoleon', 'madrid', 'spain', 'military', 'maps',
                    'camps', 'hospital', 'saint louis', '1809', 'peninsular war',
                    'victor hugo', 'brother', 'napoleonic', 'french army', 'officer'
                ]
                
                # Find matching keywords
                book_keywords = []
                for keyword in target_keywords:
                    if keyword in book_text:
                        book_keywords.append(keyword)
                
                # Only include if relevant (at least 2 keywords)
                if len(book_keywords) >= 2:
                    result = {
                        'title': title,
                        'authors': authors,
                        'description': description,
                        'published_date': published_date,
                        'keywords_found': book_keywords,
                        'source': 'Google Books',
                        'search_query': query,
                        'relevance_score': len(book_keywords)
                    }
                    all_results.append(result)
                    print(f"Added book: {title}")
                    print(f"Authors: {', '.join(authors)}")
                    print(f"Keywords: {', '.join(book_keywords)}")
        else:
            print(f"No books found for: {query}")
        
        time.sleep(1)  # Be respectful to API
        
    except Exception as e:
        error_msg = f"Error searching Google Books for '{query}': {str(e)}"
        print(error_msg)
        search_errors.append(error_msg)

print(f"\n=== ANALYZING RESULTS FOR ABEL HUGO RESEARCH ===")

# Sort results by relevance score
all_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print(f"Total results collected: {len(all_results)}")
print(f"Total errors: {len(search_errors)}")

# Analyze for specific details from the plan
specific_analysis = {
    'military_service': [],
    'madrid_connection': [],
    'napoleon_maps': [],
    'hospital_saint_louis': [],
    'publications': [],
    'timeframe_1809': [],
    'victor_hugo_brother': [],
    'ruler_reign': [],
    'potential_works': set()
}

for result in all_results:
    # Get text content for analysis
    if 'content' in result:
        text_content = result['content'].lower()
    elif 'description' in result:
        text_content = result['description'].lower()
    else:
        text_content = result.get('title', '').lower()
    
    # Look for specific details mentioned in the plan
    if 'military' in text_content and ('service' in text_content or 'officer' in text_content or 'army' in text_content):
        specific_analysis['military_service'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if 'madrid' in text_content:
        specific_analysis['madrid_connection'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if 'napoleon' in text_content and ('maps' in text_content or 'camps' in text_content):
        specific_analysis['napoleon_maps'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if 'hospital' in text_content and ('saint louis' in text_content or 'military hospital' in text_content):
        specific_analysis['hospital_saint_louis'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if 'publication' in text_content or 'author' in text_content or 'wrote' in text_content:
        specific_analysis['publications'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if '1809' in text_content:
        specific_analysis['timeframe_1809'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if 'victor hugo' in text_content and 'brother' in text_content:
        specific_analysis['victor_hugo_brother'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    if any(ruler in text_content for ruler in ['joseph bonaparte', 'napoleon', 'ferdinand vii', 'charles iv']):
        specific_analysis['ruler_reign'].append({
            'source': result['title'],
            'text_sample': text_content[:500]
        })
    
    # Collect potential works by Abel Hugo
    if 'abel hugo' in text_content and 'title' in result:
        specific_analysis['potential_works'].add(result['title'])

# Convert set to list for JSON serialization
specific_analysis['potential_works'] = list(specific_analysis['potential_works'])

# Save comprehensive results
final_data = {
    'search_date': datetime.now().isoformat(),
    'search_summary': {
        'total_results': len(all_results),
        'total_errors': len(search_errors),
        'wikipedia_pages_searched': len(wikipedia_urls),
        'google_books_queries': len(book_queries)
    },
    'research_focus': "Abel Hugo - Victor Hugo's brother, military service during Napoleonic Wars, time in Madrid, maps of Napoleon's camps in Spain, Hospital de Saint Louis connection, publications and timeframe",
    'all_results': all_results,
    'specific_analysis': specific_analysis,
    'search_errors': search_errors
}

output_file = 'workspace/abel_hugo_research_results.json'
with open(output_file, 'w') as f:
    json.dump(final_data, f, indent=2)

print(f"\nResults saved to: {output_file}")

# Display top results
print("\n=== TOP RELEVANT RESULTS ===")
for i, result in enumerate(all_results[:5], 1):
    print(f"\nResult {i} (Relevance Score: {result['relevance_score']})")
    print(f"Title: {result['title']}")
    print(f"Source: {result['source']}")
    if 'authors' in result:
        print(f"Authors: {', '.join(result['authors'])}")
    print(f"Keywords found: {', '.join(result['keywords_found'])}")
    
    # Display content preview
    if 'content' in result:
        print(f"Content preview: {result['content'][:300]}...")
    elif 'description' in result:
        print(f"Description: {result['description'][:300]}...")

# Display specific findings
print("\n=== SPECIFIC FINDINGS FROM ABEL HUGO RESEARCH ===")
for finding_type, findings in specific_analysis.items():
    if findings and finding_type != 'potential_works':
        print(f"\n{finding_type.replace('_', ' ').title()}: {len(findings)} mentions")
        for finding in findings[:2]:  # Show first 2
            print(f"  Source: {finding['source']}")
            print(f"  Text sample: {finding['text_sample'][:200]}...")

if specific_analysis['potential_works']:
    print(f"\nPotential Works by Abel Hugo: {len(specific_analysis['potential_works'])}")
    for work in specific_analysis['potential_works'][:8]:  # Show first 8
        print(f"  - {work}")

print(f"\n=== SEARCH SUMMARY ===")
print(f"Wikipedia pages searched: {len(wikipedia_urls)}")
print(f"Google Books queries: {len(book_queries)}")
print(f"Total relevant results: {len(all_results)}")
print(f"Search errors: {len(search_errors)}")

if search_errors:
    print("\nErrors encountered:")
    for error in search_errors[:3]:  # Show first 3 errors
        print(f"  - {error}")

print("\nResearch on Abel Hugo completed! Analysis will follow in next step.")
```