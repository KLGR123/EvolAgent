### Development Step 12: Extract winners of Survivor (US) seasons 1–44 and save to a JSON file

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- TV fan site automation for dynamically updating the Survivor winners list on the homepage after each new season
- Reality TV analytics dashboard for network producers to visualize winner demographics and vote margins across seasons 1–44
- Mobile trivia app backend pipeline that fetches Survivor winners data to generate fresh quiz questions weekly
- Academic research project collecting Survivor winners’ bios and season data for a sociological study on competition shows
- Social media marketing scheduler pulling season winners to auto-create winner-spotlight posts on Instagram and Twitter
- Data journalism feature extracting long-term Survivor winners data to map trends in contestant backgrounds and game strategies
- Machine learning dataset preparation by scraping Survivor winners for training models that predict future reality show outcomes
- SEO content optimization tool syncing website metadata with the latest Survivor winner names to boost search rankings

```
import os
import requests
from bs4 import BeautifulSoup

# 1) Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# 2) Define URL and realistic User-Agent header
url = 'https://en.wikipedia.org/wiki/Survivor_(American_TV_series)'
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/114.0.5735.199 Safari/537.36'
    )
}
print(f"Fetching page with User-Agent header: {headers['User-Agent']}")

# 3) Fetch page and save raw HTML
response = requests.get(url, headers=headers)
response.raise_for_status()
print(f"Page fetched successfully (status code: {response.status_code})")

html_path = os.path.join(workspace_dir, 'survivor_page.html')
with open(html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"Saved raw HTML to: {html_path}")

# 4) Parse HTML and extract all <span class='mw-headline'> elements
soup = BeautifulSoup(response.text, 'html.parser')
headlines = soup.find_all('span', class_='mw-headline')
print(f"Found {len(headlines)} section headlines on the page.")

# 5) Write section headlines (id and text) to file for inspection
headlines_path = os.path.join(workspace_dir, 'section_headlines.txt')
with open(headlines_path, 'w', encoding='utf-8') as f:
    f.write("# <span class='mw-headline'> elements on Survivor page\n")
    f.write("# Format: id=<span id>\ttext=<headline text>\n\n")
    for span in headlines:
        span_id = span.get('id', '')
        text = span.get_text(strip=True)
        f.write(f"id={span_id}\ttext={text}\n")

print(f"Section headlines written to: {headlines_path}")
print("Preview of first 10 entries:")
with open(headlines_path, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        print(line.strip())
        if i >= 9:
            break

```