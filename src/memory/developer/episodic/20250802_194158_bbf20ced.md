### Development Step 13: Locate Hreidmarâ€™s Son Journal and Emily Midkiffâ€™s June 2014 Dragon Depictions Article

**Description**: Search for information about Emily Midkiff's June 2014 article, focusing on identifying the journal named after one of Hreidmar's sons from Norse mythology. Research Hreidmar's sons (Fafnir, Regin, and Otr) to determine which one has a journal named after them, then locate Midkiff's specific article from June 2014 that discusses dragon depictions and contains quotes from two different authors expressing distaste.

**Use Cases**:
- Academic library metadata harvesting and automated descriptor creation for digital humanities collections in Norse mythology studies
- Folklore research automation for tracking thematic elements and dragon depictions across specialized mythological journals
- Editorial verification workflow for publishing houses to confirm author citations and publication dates in fantasy research articles
- Marketing intelligence gathering on emerging trends in the fantasy genre by scraping keyword frequency from niche academic periodicals
- Plagiarism detection and authorship verification in scholarly publishing through automated retrieval and context extraction of journal content
- Museum exhibition planning support by extracting relevant scholarly commentary on mythological artifacts from open-access journals
- AI-driven chatbot training for specialized Q&A on Nordic mythology literature using curated extracts from academic journal archives

```
import os
import json
import requests
from bs4 import BeautifulSoup

print('=== FAFNIR JOURNAL RESEARCH - CLEAN IMPLEMENTATION ===')
print('Objective: Find Emily Midkiff June 2014 article about dragon depictions')
print('Strategy: Clean syntax, systematic navigation, avoid complex regex\n')

# Ensure workspace exists
workspace = 'workspace'
os.makedirs(workspace, exist_ok=True)

# The Fafnir journal URL
fafnir_journal_url = 'https://fafnir.journal.fi/'
print(f'Target URL: {fafnir_journal_url}')
print('âœ“ CONFIRMED: Fafnir - Nordic Journal of Science Fiction and Fantasy Research')
print('âœ“ Named after Fafnir, the dragon son of Hreidmar from Norse mythology')
print('âœ“ Hreidmar\'s sons: Fafnir (dragon), Regin, Otr - Fafnir has journal named after him\n')

# Set up headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

print('=== STEP 1: ACCESSING FAFNIR JOURNAL HOMEPAGE ===')
print()

try:
    print(f'Making request to: {fafnir_journal_url}')
    response = requests.get(fafnir_journal_url, headers=headers, timeout=30)
    
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    
    if response.status_code == 200:
        # Save the homepage
        homepage_path = os.path.join(workspace, 'fafnir_journal_homepage.html')
        with open(homepage_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f'âœ“ Homepage saved to: {homepage_path}')
        
        # Parse the homepage
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Get page title
        page_title = soup.find('title')
        if page_title:
            print(f'\nPage title: {page_title.get_text().strip()}')
        
        print('\n=== STEP 2: SEARCHING HOMEPAGE FOR KEY TERMS ===')
        print()
        
        # Search homepage text for key terms
        homepage_text = soup.get_text().lower()
        
        key_terms = {
            '2014': homepage_text.count('2014'),
            'midkiff': homepage_text.count('midkiff'),
            'emily': homepage_text.count('emily'),
            'june': homepage_text.count('june'),
            'dragon': homepage_text.count('dragon')
        }
        
        print('Key term occurrences on homepage:')
        for term, count in key_terms.items():
            if count > 0:
                print(f'  {term}: {count} occurrences')
        
        print('\n=== STEP 3: FINDING NAVIGATION LINKS ===')
        print()
        
        # Find all links on the page
        all_links = soup.find_all('a', href=True)
        print(f'Found {len(all_links)} total links on homepage')
        
        # Look for archive/navigation patterns
        navigation_keywords = ['archive', 'archives', 'issues', 'volumes', '2014', 'browse', 'past', 'current']
        
        relevant_links = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                full_url = f'https://fafnir.journal.fi{href}'
            elif not href.startswith('http'):
                full_url = f'https://fafnir.journal.fi/{href}'
            else:
                full_url = href
            
            # Check if link is relevant
            text_lower = text.lower()
            href_lower = href.lower()
            
            relevance_score = 0
            matched_keywords = []
            
            for keyword in navigation_keywords:
                if keyword in text_lower:
                    relevance_score += 2
                    matched_keywords.append(f'text:{keyword}')
                if keyword in href_lower:
                    relevance_score += 1
                    matched_keywords.append(f'url:{keyword}')
            
            if relevance_score > 0 and len(text) > 2:
                relevant_links.append({
                    'url': full_url,
                    'text': text[:100],
                    'score': relevance_score,
                    'keywords': matched_keywords,
                    'has_2014': '2014' in text_lower or '2014' in href_lower
                })
        
        # Sort by relevance score
        relevant_links.sort(key=lambda x: x['score'], reverse=True)
        
        print(f'Found {len(relevant_links)} relevant navigation links:')
        for i, link in enumerate(relevant_links[:10], 1):
            print(f'  {i}. Score {link["score"]}: "{link["text"]}"')
            print(f'     URL: {link["url"]}')
            print(f'     Keywords: {link["keywords"]}')
            if link['has_2014']:
                print('     *** CONTAINS 2014 REFERENCE ***')
            print()
        
        # Try to access the most promising link
        if relevant_links:
            # Prioritize links with 2014 references
            priority_links = [link for link in relevant_links if link['has_2014']]
            if not priority_links:
                priority_links = relevant_links[:3]  # Take top 3 by score
            
            target_link = priority_links[0]
            
            print(f'=== STEP 4: ACCESSING PRIORITY NAVIGATION LINK ===')
            print()
            print(f'Target: "{target_link["text"]}" (Score: {target_link["score"]})')
            print(f'URL: {target_link["url"]}')
            print(f'Keywords matched: {target_link["keywords"]}')
            
            try:
                print('\nMaking request to navigation page...')
                nav_response = requests.get(target_link['url'], headers=headers, timeout=30)
                print(f'Navigation page status: {nav_response.status_code}')
                
                if nav_response.status_code == 200:
                    # Save navigation page
                    nav_path = os.path.join(workspace, 'fafnir_navigation_page.html')
                    with open(nav_path, 'w', encoding='utf-8') as f:
                        f.write(nav_response.text)
                    print(f'âœ“ Navigation page saved to: {nav_path}')
                    
                    # Parse navigation page
                    nav_soup = BeautifulSoup(nav_response.content, 'html.parser')
                    nav_text = nav_soup.get_text().lower()
                    
                    print('\n=== STEP 5: ANALYZING NAVIGATION PAGE ===')
                    print()
                    
                    # Count key terms on navigation page
                    nav_key_terms = {
                        '2014': nav_text.count('2014'),
                        'midkiff': nav_text.count('midkiff'),
                        'emily': nav_text.count('emily'),
                        'june': nav_text.count('june'),
                        'dragon': nav_text.count('dragon')
                    }
                    
                    print('Key terms on navigation page:')
                    for term, count in nav_key_terms.items():
                        if count > 0:
                            print(f'  {term}: {count} occurrences')
                    
                    # Look for 2014 and Midkiff related links
                    nav_links = nav_soup.find_all('a', href=True)
                    target_links = []
                    
                    for link in nav_links:
                        href = link.get('href', '')
                        text = link.get_text().strip()
                        
                        # Convert to absolute URL
                        if href.startswith('/'):
                            full_url = f'https://fafnir.journal.fi{href}'
                        elif not href.startswith('http'):
                            full_url = f'https://fafnir.journal.fi/{href}'
                        else:
                            full_url = href
                        
                        # Check for target terms
                        text_lower = text.lower()
                        href_lower = href.lower()
                        
                        target_terms = ['2014', 'june', 'midkiff', 'emily', 'dragon']
                        matched_terms = []
                        
                        for term in target_terms:
                            if term in text_lower or term in href_lower:
                                matched_terms.append(term)
                        
                        if matched_terms and len(text) > 3:
                            target_links.append({
                                'url': full_url,
                                'text': text[:150],
                                'matched_terms': matched_terms,
                                'has_midkiff': 'midkiff' in matched_terms,
                                'has_2014': '2014' in matched_terms,
                                'has_june': 'june' in matched_terms
                            })
                    
                    if target_links:
                        print(f'\n*** FOUND {len(target_links)} TARGET LINKS ***')
                        for i, link in enumerate(target_links[:5], 1):
                            print(f'  {i}. "{link["text"]}"')
                            print(f'     URL: {link["url"]}')
                            print(f'     Matched terms: {link["matched_terms"]}')
                            if link['has_midkiff']:
                                print('     *** CONTAINS MIDKIFF ***')
                            if link['has_june'] and link['has_2014']:
                                print('     *** CONTAINS JUNE 2014 ***')
                            print()
                        
                        # Prioritize links with Midkiff, then June+2014, then 2014
                        best_link = None
                        for link in target_links:
                            if link['has_midkiff']:
                                best_link = link
                                break
                        
                        if not best_link:
                            for link in target_links:
                                if link['has_june'] and link['has_2014']:
                                    best_link = link
                                    break
                        
                        if not best_link:
                            for link in target_links:
                                if link['has_2014']:
                                    best_link = link
                                    break
                        
                        if not best_link:
                            best_link = target_links[0]
                        
                        print(f'=== STEP 6: ACCESSING BEST TARGET LINK ===')
                        print()
                        print(f'Selected: "{best_link["text"]}"')
                        print(f'URL: {best_link["url"]}')
                        print(f'Matched terms: {best_link["matched_terms"]}')
                        
                        try:
                            print('\nMaking request to target page...')
                            target_response = requests.get(best_link['url'], headers=headers, timeout=30)
                            print(f'Target page status: {target_response.status_code}')
                            
                            if target_response.status_code == 200:
                                # Save target page
                                target_path = os.path.join(workspace, 'fafnir_target_page.html')
                                with open(target_path, 'w', encoding='utf-8') as f:
                                    f.write(target_response.text)
                                print(f'âœ“ Target page saved to: {target_path}')
                                
                                # Parse target page
                                target_soup = BeautifulSoup(target_response.content, 'html.parser')
                                target_text = target_soup.get_text().lower()
                                
                                print('\n=== STEP 7: SEARCHING TARGET PAGE FOR EMILY MIDKIFF ===')
                                print()
                                
                                # Count key terms on target page
                                target_key_terms = {
                                    'midkiff': target_text.count('midkiff'),
                                    'emily': target_text.count('emily'),
                                    'june': target_text.count('june'),
                                    'dragon': target_text.count('dragon'),
                                    'depiction': target_text.count('depiction'),
                                    'distaste': target_text.count('distaste')
                                }
                                
                                print('Key terms on target page:')
                                for term, count in target_key_terms.items():
                                    if count > 0:
                                        print(f'  {term}: {count} occurrences')
                                
                                if target_key_terms['midkiff'] > 0:
                                    print('\nðŸŽ¯ *** EMILY MIDKIFF FOUND ON TARGET PAGE! ***')
                                    
                                    # Extract contexts around Midkiff mentions
                                    full_target_text = target_soup.get_text()
                                    midkiff_contexts = []
                                    
                                    # Find all positions of 'midkiff' in the text
                                    search_text = full_target_text.lower()
                                    start_pos = 0
                                    
                                    while True:
                                        pos = search_text.find('midkiff', start_pos)
                                        if pos == -1:
                                            break
                                        
                                        # Extract context around the match
                                        context_start = max(0, pos - 300)
                                        context_end = min(len(full_target_text), pos + 400)
                                        context = full_target_text[context_start:context_end].strip()
                                        midkiff_contexts.append(context)
                                        start_pos = pos + 1
                                    
                                    print(f'Found {len(midkiff_contexts)} Midkiff contexts:')
                                    for i, context in enumerate(midkiff_contexts, 1):
                                        print(f'\n{i}. Context around Midkiff mention:')
                                        print(f'   ...{context[:200]}...')
                                    
                                    # Look for quotes in the text (simple approach)
                                    if target_key_terms['dragon'] > 0:
                                        print('\n*** DRAGON CONTENT FOUND - SEARCHING FOR QUOTES ***')
                                        
                                        # Simple quote extraction - look for text between quote marks
                                        text_content = target_response.text
                                        
                                        # Find quoted text (simple patterns)
                                        quote_candidates = []
                                        
                                        # Look for text between double quotes
                                        import re
                                        double_quote_pattern = r'"([^"]{30,200})"'
                                        matches = re.findall(double_quote_pattern, text_content)
                                        quote_candidates.extend(matches)
                                        
                                        # Look for text between smart quotes
                                        smart_quote_pattern = r'"([^"]{30,200})
```