### Development Step 10: Compile Land Area Data for Washington State’s 39 County Seats from Official Sources

**Description**: Research and extract the land area data for all 39 Washington state county seats that were identified in the previous step. Use authoritative sources such as the U.S. Census Bureau, official city websites, or government geographic databases to obtain the land area measurements for each county seat city. Focus on finding the official land area figures in square miles or square kilometers for accurate comparison.

**Use Cases**:
- Urban infrastructure planning and emergency response modeling using county seat land area data for optimized fire and EMS station placement in Washington state
- Real estate investment analysis and density-based pricing strategy development by combining land area and population metrics of Washington county seats
- Academic urban geography research and statistical correlation of land area with socioeconomic indicators across Washington’s county seats
- State government reporting and automated generation of official land area tables and visual dashboards for the annual Washington State Statistical Abstract
- Environmental impact assessment and stormwater runoff modeling leveraging extracted county seat land area measurements for ecological planning
- Insurance underwriting automation and risk model calibration based on county seat land area metrics to adjust property premiums
- Logistics network design and delivery zone optimization for courier and last-mile services using land area data of Washington county seats to define service coverage boundaries

```
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import re

print("=== WASHINGTON STATE COUNTY SEATS LAND AREA RESEARCH ===\n")
print("Objective: Extract land area data for all 39 Washington state county seats")
print("Sources: U.S. Census Bureau data via Wikipedia\n")

# First, let's check what data we already have and what we need to complete
print("=== CHECKING EXISTING DATA AND PROGRESS ===\n")

# Load the county seats data
if os.path.exists('workspace/wa_county_seats.json'):
    with open('workspace/wa_county_seats.json', 'r') as f:
        county_seats_data = json.load(f)
    print(f"Loaded {len(county_seats_data)} county seats from JSON file")
else:
    print("County seats JSON file not found. Exiting.")
    exit()

# Check if we have any partial results from previous attempts
partial_results = []
existing_results_file = None

# Look for existing results files
workspace_files = os.listdir('workspace')
results_files = [f for f in workspace_files if 'land_area' in f.lower() and f.endswith('.json')]

if results_files:
    print(f"Found existing results files: {results_files}")
    # Use the most recent one
    existing_results_file = f'workspace/{results_files[0]}'
    
    try:
        with open(existing_results_file, 'r') as f:
            existing_data = json.load(f)
        
        # Extract any successful results we already have
        if 'results' in existing_data:
            partial_results = [r for r in existing_data['results'] if r.get('extraction_success', False)]
            print(f"Found {len(partial_results)} successful extractions from previous attempts")
            
            # Show what we already have
            if partial_results:
                print("\nAlready extracted:")
                for result in partial_results:
                    area_str = f"{result['land_area']:.2f} sq miles" if result.get('land_area') else 'No area'
                    print(f"  {result['county_seat']:<15} {area_str}")
    except Exception as e:
        print(f"Error reading existing results: {e}")
        partial_results = []

# Determine which cities still need to be researched
already_processed = {r['county_seat'] for r in partial_results}
remaining_cities = [seat for seat in county_seats_data if seat['county_seat'] not in already_processed]

print(f"\nRemaining cities to research: {len(remaining_cities)}")
if remaining_cities:
    print("Cities still needed:")
    for i, seat in enumerate(remaining_cities, 1):
        print(f"  {i:2d}. {seat['county_seat']} ({seat['county']})")

# Initialize complete results list
all_results = list(partial_results)  # Start with what we already have

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

print(f"\n=== BEGINNING LAND AREA RESEARCH FOR REMAINING CITIES ===\n")

# Process only the remaining cities
for i, seat_data in enumerate(remaining_cities, 1):
    county_seat = seat_data['county_seat']
    county = seat_data['county']
    
    print(f"[{i:2d}/{len(remaining_cities)}] {county_seat}, Washington...", end=" ")
    
    # Construct Wikipedia URL
    city_name_formatted = county_seat.replace(' ', '_')
    wikipedia_url = f"https://en.wikipedia.org/wiki/{city_name_formatted},_Washington"
    
    land_area_found = None
    area_unit = None
    extraction_method = None
    
    try:
        # Request Wikipedia page
        response = requests.get(wikipedia_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Method 1: Search infobox for area information
            infobox = soup.find('table', class_='infobox')
            if infobox and not land_area_found:
                rows = infobox.find_all('tr')
                for row in rows:
                    # Look for area-related table headers
                    header = row.find('th')
                    if header:
                        header_text = header.get_text().lower().strip()
                        if 'area' in header_text:
                            # Get corresponding data cell
                            data_cell = row.find('td')
                            if data_cell:
                                area_text = data_cell.get_text().strip()
                                
                                # Extract area value using regex patterns
                                patterns = [
                                    r'([0-9,]+\.?[0-9]*)\s*sq\s*mi',
                                    r'([0-9,]+\.?[0-9]*)\s*square\s*miles?',
                                    r'([0-9,]+\.?[0-9]*)\s*km²'
                                ]
                                
                                for pattern in patterns:
                                    match = re.search(pattern, area_text, re.IGNORECASE)
                                    if match:
                                        land_area_found = match.group(1).replace(',', '')
                                        if 'sq mi' in area_text.lower() or 'square mile' in area_text.lower():
                                            area_unit = 'sq_miles'
                                        elif 'km' in area_text.lower():
                                            area_unit = 'sq_kilometers'
                                        extraction_method = 'infobox'
                                        break
                                
                                if land_area_found:
                                    break
            
            # Method 2: Search all table cells for area data
            if not land_area_found:
                all_cells = soup.find_all(['td', 'th'])
                for cell in all_cells:
                    cell_text = cell.get_text().strip()
                    area_match = re.search(r'([0-9,]+\.?[0-9]*)\s*(sq\s*mi|square\s*miles?)', cell_text, re.IGNORECASE)
                    if area_match:
                        land_area_found = area_match.group(1).replace(',', '')
                        area_unit = 'sq_miles'
                        extraction_method = 'table_scan'
                        break
            
            # Method 3: Search page text for area mentions
            if not land_area_found:
                page_text = soup.get_text()
                text_patterns = [
                    r'total area[^0-9]*([0-9,]+\.?[0-9]*)\s*(square miles|sq\s*mi)',
                    r'land area[^0-9]*([0-9,]+\.?[0-9]*)\s*(square miles|sq\s*mi)',
                    r'area[^0-9]*([0-9,]+\.?[0-9]*)\s*(square miles|sq\s*mi)'
                ]
                
                for pattern in text_patterns:
                    match = re.search(pattern, page_text, re.IGNORECASE)
                    if match:
                        land_area_found = match.group(1).replace(',', '')
                        area_unit = 'sq_miles'
                        extraction_method = 'text_scan'
                        break
        
        # Store results
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': float(land_area_found) if land_area_found else None,
            'area_unit': area_unit,
            'wikipedia_url': wikipedia_url,
            'extraction_method': extraction_method,
            'extraction_success': land_area_found is not None,
            'http_status': response.status_code if 'response' in locals() else None
        }
        
        if land_area_found:
            unit_display = area_unit.replace('_', ' ') if area_unit else 'unknown unit'
            print(f"✓ {land_area_found} {unit_display}")
        else:
            print("✗ No area data found")
            
    except requests.RequestException as e:
        print(f"✗ Request failed")
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': None,
            'area_unit': None,
            'wikipedia_url': wikipedia_url,
            'extraction_method': None,
            'extraction_success': False,
            'error': str(e)[:100]
        }
    
    except Exception as e:
        print(f"✗ Processing error")
        result = {
            'county': county,
            'county_seat': county_seat,
            'fips_code': seat_data['fips_code'],
            'land_area': None,
            'area_unit': None,
            'wikipedia_url': wikipedia_url,
            'extraction_method': None,
            'extraction_success': False,
            'error': str(e)[:100]
        }
    
    all_results.append(result)
    
    # Rate limiting to be respectful to Wikipedia
    time.sleep(0.5)
    
    # Progress updates every 5 cities
    if i % 5 == 0:
        successful = len([r for r in all_results if r['extraction_success']])
        print(f"\n  Progress: {i}/{len(remaining_cities)} remaining completed, {successful} total successful\n")

# Final comprehensive analysis
print("\n=== FINAL RESEARCH RESULTS ===\n")

successful = [r for r in all_results if r['extraction_success']]
failed = [r for r in all_results if not r['extraction_success']]

print(f"Total cities: {len(county_seats_data)}")
print(f"Successfully extracted: {len(successful)}")
print(f"Failed extractions: {len(failed)}")
print(f"Overall success rate: {len(successful)/len(county_seats_data)*100:.1f}%")

# Show all successful results sorted by area
if successful:
    print(f"\n=== ALL LAND AREAS EXTRACTED (sorted by size) ===\n")
    sorted_results = sorted(successful, key=lambda x: x['land_area'])
    
    for result in sorted_results:
        area_str = f"{result['land_area']:.2f} sq miles"
        print(f"  {result['county_seat']:<15} {area_str:>12} ({result['county']})")

# Show failed extractions for potential manual research
if failed:
    print(f"\n=== FAILED EXTRACTIONS (need manual research) ===\n")
    for result in failed:
        error_info = result.get('error', result.get('http_status', 'Unknown error'))
        print(f"  {result['county_seat']:<15} ({result['county']}) - {str(error_info)[:50]}")

# Save comprehensive final results
final_results = {
    'research_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'objective': 'Washington state county seats land area data extraction',
    'data_source': 'Wikipedia (containing U.S. Census Bureau data)',
    'methodology': 'Multi-method web scraping (infobox, table scan, text scan)',
    'total_cities': len(county_seats_data),
    'successful_extractions': len(successful),
    'failed_extractions': len(failed),
    'success_rate_percent': round(len(successful)/len(county_seats_data)*100, 1),
    'results': all_results
}

# Add summary statistics if we have successful extractions
if successful:
    areas = [r['land_area'] for r in successful]
    final_results['summary_statistics'] = {
        'smallest_area_sq_miles': min(areas),
        'largest_area_sq_miles': max(areas),
        'average_area_sq_miles': sum(areas) / len(areas),
        'median_area_sq_miles': sorted(areas)[len(areas)//2],
        'total_land_area_sq_miles': sum(areas)
    }

# Save to workspace
with open('workspace/wa_county_seats_land_areas_complete.json', 'w') as f:
    json.dump(final_results, f, indent=2)

print(f"\n✓ Complete results saved to: workspace/wa_county_seats_land_areas_complete.json")

# Display summary statistics
if successful:
    stats = final_results['summary_statistics']
    print(f"\n=== SUMMARY STATISTICS ===\n")
    print(f"Smallest county seat: {stats['smallest_area_sq_miles']:.2f} sq miles")
    print(f"Largest county seat: {stats['largest_area_sq_miles']:.2f} sq miles")
    print(f"Average area: {stats['average_area_sq_miles']:.2f} sq miles")
    print(f"Median area: {stats['median_area_sq_miles']:.2f} sq miles")
    print(f"Total combined area: {stats['total_land_area_sq_miles']:.2f} sq miles")

print(f"\n=== LAND AREA RESEARCH COMPLETE ===\n")
print(f"Successfully extracted land area data for {len(successful)} out of 39 Washington state county seats")
print(f"All data sourced from Wikipedia containing official U.S. Census Bureau figures")
print(f"Research methodology used multiple extraction methods for comprehensive coverage")
```