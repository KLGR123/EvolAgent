### Development Step 8: Locate and Verify Quoted Passage in Greetham’s “Uncoupled” (2008) via DOI

**Description**: Search for the academic article 'Uncoupled: OR, How I Lost My Author(s)' by David Greetham published in Textual Cultures: Texts, Contexts, Interpretation, vol. 3 no. 1, 2008, pages 45-46. Use the provided DOI 10.2979/tex.2008.3.1.44 to locate the article on Project MUSE or through DOI resolution. Extract the complete text from pages 45-46 to verify if the quoted passage 'obscured not by a "cloak of print" but by the veil of scribal confusion and mis-transmission' appears exactly as cited in the bibliography.

**Use Cases**:
- Automated citation verification for university libraries, resolving DOIs to fetch article pages and confirm exact quote usage in student theses
- Digital humanities research pipeline, scraping multiple academic articles via DOI resolution to extract metadata and full-text for textual variant analysis
- Journal editorial workflow automation, using DOI lookups and HTML parsing to validate article titles, authors, and quoted passages before final publication
- Legal research compliance tool, retrieving law review articles through DOI redirects to verify precise quotations and bibliographic details in firm memoranda
- Library catalog enrichment, harvesting journal metadata and abstracts from DOI-linked HTML for bulk import into institutional repositories
- Scholarly annotation platform integration, fetching target pages by DOI and injecting verified quotes with context into online commentaries
- Meta-analysis data aggregator, programmatically extracting methodological sections from DOI-referenced studies to populate comparative research databases
- AI training dataset preparation, resolving DOIs to collect clean academic text and verified quotations for domain-specific natural language models

```
import os
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import time

print('=== ACCESSING GREETHAM ARTICLE - FINAL CORRECTED VERSION ===')
print('Title: Uncoupled: OR, How I Lost My Author(s)')
print('Author: David Greetham')
print('Journal: Textual Cultures: Texts, Contexts, Interpretation')
print('Volume: 3, Issue: 1, Year: 2008, Pages: 45-46')
print('DOI: 10.2979/tex.2008.3.1.44')
print('Target Quote: "obscured not by a "cloak of print" but by the veil of scribal confusion and mis-transmission"')
print('\n' + '='*100 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# First, let's check if we already have the raw HTML from previous attempts
raw_response_path = 'workspace/doi_response_raw.html'
if os.path.exists(raw_response_path):
    print('=== USING EXISTING RAW HTML RESPONSE ===')
    print(f'Found existing raw response: {raw_response_path}')
    
    with open(raw_response_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    print(f'✓ Loaded existing HTML content: {len(html_content):,} characters')
    
    # Parse the existing content
    soup = BeautifulSoup(html_content, 'html.parser')
    
else:
    print('=== FETCHING FRESH CONTENT FROM DOI ===')
    # Construct the DOI URL
    doi_url = 'https://doi.org/10.2979/tex.2008.3.1.44'
    print(f'DOI URL to resolve: {doi_url}')
    
    # Headers to mimic a real browser request
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }
    
    try:
        # Make the DOI request
        print(f'Requesting: {doi_url}')
        doi_response = requests.get(doi_url, headers=headers, timeout=30, allow_redirects=True)
        
        print(f'✓ Request completed')
        print(f'Status code: {doi_response.status_code}')
        print(f'Final URL after redirects: {doi_response.url}')
        print(f'Content length: {len(doi_response.content):,} bytes')
        
        if doi_response.status_code == 200:
            # Save the response
            with open(raw_response_path, 'w', encoding='utf-8') as f:
                f.write(doi_response.text)
            print(f'✓ Raw response saved to: {raw_response_path}')
            
            # Parse the content
            soup = BeautifulSoup(doi_response.content, 'html.parser')
        else:
            print(f'❌ DOI resolution failed with status {doi_response.status_code}')
            exit()
            
    except Exception as e:
        print(f'❌ Error during DOI resolution: {str(e)}')
        exit()

# Now we have soup object - let's properly extract all information
print('\n=== STEP 1: EXTRACTING PAGE CONTENT AND METADATA ===')

# CRITICAL FIX: Define page_text immediately after soup creation
page_text = soup.get_text()
page_text_lower = page_text.lower()
print(f'✓ Page text extracted: {len(page_text):,} characters')

# Get page title
page_title = soup.find('title')
if page_title:
    title_text = page_title.get_text().strip()
    print(f'Page title: {title_text}')
    
    # Verify this is the correct article
    if 'uncoupled' in title_text.lower() or 'greetham' in title_text.lower():
        print('✓ Confirmed: This appears to be the correct Greetham article')
    else:
        print('⚠ Warning: Page title does not clearly match expected article')
else:
    print('⚠ No page title found')

# Extract article metadata
print('\n--- EXTRACTING ARTICLE METADATA ---')

# Search for article title
title_selectors = ['h1', 'h1.title', '.article-title', '.citation_title', '.title']
article_title = None
for selector in title_selectors:
    title_elem = soup.select_one(selector)
    if title_elem:
        title_text = title_elem.get_text().strip()
        if len(title_text) > 10:
            article_title = title_text
            print(f'Article title found: {title_text}')
            break

if not article_title:
    print('Article title not found with standard selectors')

# Search for author information
author_selectors = ['.author', '.citation_author', '.article-author', '[data-author]']
article_author = None
for selector in author_selectors:
    author_elem = soup.select_one(selector)
    if author_elem:
        author_text = author_elem.get_text().strip()
        if 'greetham' in author_text.lower():
            article_author = author_text
            print(f'Author found: {author_text}')
            break

if not article_author:
    print('Author not found with standard selectors')
    # Now page_text_lower is properly defined
    if 'greetham' in page_text_lower:
        print('✓ Author name "Greetham" found in page text')

# Search for journal information
journal_selectors = ['.journal-title', '.citation_journal_title', '.source-title']
journal_title = None
for selector in journal_selectors:
    journal_elem = soup.select_one(selector)
    if journal_elem:
        journal_text = journal_elem.get_text().strip()
        if 'textual' in journal_text.lower():
            journal_title = journal_text
            print(f'Journal found: {journal_text}')
            break

if not journal_title:
    print('Journal title not found with standard selectors')
    if 'textual cultures' in page_text_lower:
        print('✓ Journal name "Textual Cultures" found in page text')

print('\n=== STEP 2: SEARCHING FOR TARGET QUOTE ON MAIN PAGE ===')

# Define the target quote and variations
target_quote = 'obscured not by a "cloak of print" but by the veil of scribal confusion and mis-transmission'

# Define quote variations with proper string handling
quote_variations = [
    target_quote,
    target_quote.replace('"', "'"),  # Replace double quotes with single quotes
    target_quote.replace('"', "
```