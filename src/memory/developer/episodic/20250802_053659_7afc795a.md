### Development Step 13: Download University of Leicester paper on Hiccup’s fish supply for dragon diet

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated retrieval of the latest regulatory compliance PDFs (e.g., GDPR guidelines) from official government domains for a legal team
- Pharmaceutical R&D workflows to scrape and download clinical trial protocol PDFs from university and hospital websites for systematic reviews
- Patent analytics processes that inspect search-engine result snapshots to extract direct links to USPTO patent PDFs for IP portfolio monitoring
- Supply chain safety operations automating the collection of material safety data sheet (MSDS) PDFs from supplier portals to ensure up-to-date hazard information
- Investment research pipelines gathering and parsing SEC EDGAR HTML search results to download 10-K and 8-K PDF filings for quantitative financial modeling
- Academic library digitization projects scanning search-engine HTML archives to locate and ingest university thesis and dissertation PDFs into institutional repositories
- Investigative journalism teams extracting leaked or publicly posted report PDFs from archived web search dumps for source document verification
- IT security audit scripts analyzing archived search result pages to find and download vendor software documentation PDFs for vulnerability assessment

```
import os
import sys
from urllib.parse import urlparse, parse_qs, unquote
from bs4 import BeautifulSoup

# Step 1: Ensure workspace directory exists
workspace = 'workspace'
if not os.path.exists(workspace):
    print(f"ERROR: Workspace directory '{workspace}' does not exist.")
    print("Please run the search scripts first to generate HTML files.")
    sys.exit(1)
print(f"[INFO] Using workspace: {workspace}\n")

# Step 2: Define paths to saved search HTML files
ddg_html = os.path.join(workspace, 'site_le_duckduckgo_search.html')
bing_html = os.path.join(workspace, 'site_le_bing_search.html')

# Step 3: Verify HTML files exist
for path in (ddg_html, bing_html):
    if not os.path.exists(path):
        print(f"ERROR: Missing required file: {path}")
        print("Run the site-specific search step first to save HTML files.")
        sys.exit(1)
print(f"[INFO] Found HTML files:\n - {ddg_html}\n - {bing_html}\n")

# Container for detected Leicester links
leicester_links = []

# Helper to extract and print Leicester-related URLs
def inspect_file(file_path, label):
    print(f"[INSPECT] Searching for 'le.ac.uk' links in {label}...\n")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    soup = BeautifulSoup(content, 'html.parser')
    # Check all <a> hrefs
    for a in soup.find_all('a', href=True):
        href = a['href']
        # Direct Leicester links
        if 'le.ac.uk' in href:
            print(f"  [DIRECT] {label} --> {href}")
            leicester_links.append(href)
        # DuckDuckGo uddg redirect links
        if '/l/?uddg=' in href:
            parsed = urlparse(href)
            q = parse_qs(parsed.query).get('uddg', [])
            if q:
                real = unquote(q[0])
                if 'le.ac.uk' in real:
                    print(f"  [REDIRECT] {label} uddg decoded --> {real}")
                    leicester_links.append(real)
    # Fallback: raw text search for any 'le.ac.uk' occurrences
    lines = content.splitlines()
    for i, line in enumerate(lines):
        if 'le.ac.uk' in line and '<a ' not in line:
            # Print context window
            start = max(0, i-2)
            end = min(len(lines), i+3)
            print(f"  [TEXT] Context around line {i+1}:")
            for j in range(start, end):
                print(f"    {j+1}: {lines[j].strip()}")
            print()

# Inspect DuckDuckGo HTML
try:
    inspect_file(ddg_html, 'DuckDuckGo HTML')
except Exception as e:
    print(f"ERROR while inspecting DuckDuckGo file: {e}")

print("\n" + "-"*60 + "\n")

# Inspect Bing HTML
try:
    inspect_file(bing_html, 'Bing HTML')
except Exception as e:
    print(f"ERROR while inspecting Bing file: {e}")

# Deduplicate Leicester links
unique_leicester = []
for url in leicester_links:
    if url not in unique_leicester:
        unique_leicester.append(url)

# Save inspection results to a local text file
output_path = os.path.join(workspace, 'leicester_link_inspection.txt')
with open(output_path, 'w', encoding='utf-8') as out_f:
    out_f.write('Detected Leicester domain links and contexts:\n\n')
    for url in unique_leicester:
        out_f.write(f"{url}\n")

print(f"[DONE] Inspection complete. Found {len(unique_leicester)} unique Leicester links.")
print(f"[INFO] Detailed report saved to: {output_path}")
print("Next: Review 'leicester_link_inspection.txt' to identify the correct PDF URL pattern on 'le.ac.uk'.")
```