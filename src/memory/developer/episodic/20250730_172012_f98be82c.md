### Development Step 16: Undergraduate Institutions of U.S. Homeland Security Secretaries Before April 2019

**Description**: Search for the complete list of United States Secretaries of Homeland Security who served prior to April 2019 (excluding acting secretaries) and identify their educational backgrounds, specifically focusing on where each secretary obtained their bachelor's degree.

**Use Cases**:
- Political science academic research into correlations between Homeland Security Secretaries’ bachelor’s degree institutions and their policy priorities before April 2019
- HR benchmarking for government contractors analyzing former DHS leaders’ undergraduate credentials to refine senior hire requirements
- Data journalism pipeline automating extraction of cabinet-level education histories to map alma mater trends in a feature article
- Civic education platform populating interactive profiles of Secretaries of Homeland Security with their bachelor’s institutions for K–12 curricula
- Nonprofit transparency report compiling official education backgrounds via web scraping to hold public officeholders accountable
- AI knowledge-graph enrichment linking high-level officials to their universities for smarter natural-language queries in policy research tools
- Historical policy analysis tool integrating scraped bachelor’s degree data of DHS Secretaries to study shifts in educational backgrounds across administrations
- Corporate security consultancy building intelligence dossiers on former DHS Secretaries’ undergraduate pedigree to inform risk-assessment briefs

```
import requests
import os
import re
import json
from bs4 import BeautifulSoup
import time

# Define workspace directory
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

print("Starting search for US Secretaries of Homeland Security and their education...")

# Function to perform web requests with exponential backoff
def fetch_with_backoff(url, max_retries=5):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(max_retries):
        try:
            print(f"Attempt {attempt + 1} to fetch URL: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            wait_time = 2 ** attempt
            if attempt < max_retries - 1:
                print(f"Error: {e}. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Failed after {max_retries} attempts: {e}")
                return None

# Fetch list of US Secretaries of Homeland Security from Wikipedia
print("Fetching list of Secretaries from Wikipedia...")
wiki_url = "https://en.wikipedia.org/wiki/United_States_Secretary_of_Homeland_Security"
wiki_response = fetch_with_backoff(wiki_url)

if not wiki_response:
    print("Failed to fetch Wikipedia page. Exiting.")
    exit(1)

# Parse the Wikipedia page to extract secretaries and their tenures
wiki_soup = BeautifulSoup(wiki_response.content, 'html.parser')

# Print the page title to confirm we're on the right page
page_title = wiki_soup.find('title').text if wiki_soup.find('title') else "Unknown page"
print(f"Loaded page: {page_title}")

# Let's first examine and print all tables on the page to identify the right one
all_tables = wiki_soup.find_all('table')
print(f"Found {len(all_tables)} tables on the page")

# Find the table with the list of secretaries - using a more precise approach
# Look for tables with "Secretary of Homeland Security" in the caption
secretaries_table = None
for i, table in enumerate(all_tables):
    table_text = table.text.lower() if table else ""
    caption = table.find('caption')
    caption_text = caption.text.lower() if caption else ""
    
    print(f"\nTable {i+1}:")
    print(f"Class: {table.get('class', 'No class')}")
    print(f"Caption: {caption_text[:100] if caption_text else 'No caption'}...")
    
    # Check if this looks like our target table
    if ("secretary" in table_text and "homeland security" in table_text) or \
       ("secretary" in caption_text and "homeland security" in caption_text):
        # Let's look at the headers
        headers = [th.get_text().strip() for th in table.find_all('th')[:10]]  # First 10 headers only
        print(f"Headers: {headers}")
        
        if any("name" in h.lower() for h in headers) and any("term" in h.lower() for h in headers):
            print("This appears to be the secretaries table")
            secretaries_table = table
            break

# If we still haven't found it, try a more generic approach
if not secretaries_table:
    print("\nTrying alternative approach to find secretaries table...")
    # Look for tables with specific class or with specific text patterns
    for i, table in enumerate(all_tables):
        if 'wikitable' in ' '.join(table.get('class', [])):
            headers = [th.get_text().strip() for th in table.find_all('th')[:10]]
            print(f"Table {i+1} headers: {headers}")
            
            # Check if headers indicate this is the secretaries table
            if any("name" in h.lower() for h in headers):
                secretaries_table = table
                print("Found potential secretaries table based on wikitable class and headers")
                break

# If we still don't have a table, try the first wikitable
if not secretaries_table:
    print("\nUsing first wikitable as fallback...")
    secretaries_table = wiki_soup.find('table', class_='wikitable')

if not secretaries_table:
    print("Could not find the secretaries table. Exiting.")
    exit(1)

# Let's examine the structure of the table
print("\nAnalyzing table structure...")

# Look at the header row in detail
header_row = secretaries_table.find('tr')
if header_row:
    header_cells = header_row.find_all(['th', 'td'])
    print(f"Header row has {len(header_cells)} cells")
    
    for i, cell in enumerate(header_cells):
        print(f"Header {i}: '{cell.get_text().strip()}'")

# Extract secretaries' information
secretaries = []

# Skip the header row and process each data row
rows = secretaries_table.find_all('tr')[1:]  # Skip the header row
print(f"Found {len(rows)} data rows in the table")

# Find which columns contain the name, term, and links
name_col = None
term_col = None

# Inspect header row to determine column positions
header_cells = secretaries_table.find('tr').find_all(['th', 'td']) if secretaries_table.find('tr') else []
for i, cell in enumerate(header_cells):
    cell_text = cell.get_text().strip().lower()
    if 'name' in cell_text:
        name_col = i
        print(f"Name column is at position {i}")
    if 'term' in cell_text:
        term_col = i
        print(f"Term column is at position {i}")

# If we couldn't determine columns, use reasonable defaults
if name_col is None:
    print("Couldn't determine name column, using default position 2")
    name_col = 2  # Default to typical position

if term_col is None:
    print("Couldn't determine term column, using position after name column")
    term_col = name_col + 1  # Default to column after name

# Process each row in the table
for i, row in enumerate(rows, 1):
    print(f"\nProcessing row {i}...")
    cells = row.find_all(['th', 'td'])
    
    # Debug: print the number and content of cells in this row
    print(f"Row {i} has {len(cells)} cells")
    for j, cell in enumerate(cells):
        print(f"  Cell {j}: '{cell.get_text().strip()[:30]}{'...' if len(cell.get_text().strip()) > 30 else ''}' ")
    
    # Skip rows that don't have enough cells
    if len(cells) <= max(name_col, term_col):
        print(f"Skipping row {i} - not enough cells")
        continue
    
    # Extract name
    name_cell = cells[name_col]
    name_text = name_cell.get_text().strip()
    print(f"Raw name text: '{name_text}'")
    
    # Skip if it contains "Acting" directly in the name field
    if "acting" in name_text.lower():
        print(f"Skipping row {i} - Acting Secretary")
        continue
    
    # Clean up the name
    name = re.sub(r'\[.*?\]', '', name_text).strip()  # Remove reference tags
    if not name:
        # Try to extract name from links or other cells
        links = name_cell.find_all('a')
        for link in links:
            if link.get_text().strip() and not link.get_text().strip().lower().startswith('file:'):
                name = link.get_text().strip()
                print(f"Extracted name from link: '{name}'")
                break
    
    if not name:
        print(f"Skipping row {i} - could not extract name")
        continue
    
    # Extract term of office
    term_cell = cells[term_col] if term_col < len(cells) else None
    term_text = term_cell.get_text().strip() if term_cell else "Term information not available"
    print(f"Term text: '{term_text}'")
    
    # Extract Wikipedia link for the person
    wiki_link = None
    links = name_cell.find_all('a')
    for link in links:
        if link.has_attr('href'):
            href = link['href']
            # Check if this is a link to a person's page (not an image or footnote)
            if href.startswith('/wiki/') and not href.startswith('/wiki/File:') and not '#' in href:
                # Extract just the person's name from the link text or href
                link_text = link.get_text().strip()
                if link_text and not link_text.lower().startswith('file:'):
                    wiki_link = "https://en.wikipedia.org" + href
                    print(f"Found wiki link: {wiki_link}")
                    # If we found a good link with text, use that text as the name if we don't have one
                    if not name and link_text:
                        name = link_text
                    break
    
    # Make sure we have a name before proceeding
    if not name:
        print(f"Skipping row {i} - no valid name found")
        continue
    
    # Check for "Acting" in term text or other indicators and skip if found
    if term_text and "acting" in term_text.lower():
        print(f"Skipping row {i} - Acting Secretary (found in term text)")
        continue
    
    # Extract dates from term text to check if served before April 2019
    date_pattern = r'(\w+ \d+, \d{4})\s*[–—-]\s*(\w+ \d+, \d{4}|Incumbent|present)'
    date_match = re.search(date_pattern, term_text, re.IGNORECASE)
    
    if date_match:
        start_date = date_match.group(1)
        end_date = date_match.group(2)
        print(f"Extracted start date: {start_date}")
        print(f"Extracted end date: {end_date}")
    else:
        # Try to find dates in a different format or in a different cell
        print("Could not extract dates from term text, checking for date cells")
        # Look for specific date columns
        for j, cell in enumerate(cells):
            cell_text = cell.get_text().strip()
            if re.search(r'\d{4}', cell_text) and not j == name_col:
                print(f"Possible date in cell {j}: '{cell_text}'")
    
    # Add this secretary to our list
    secretary_info = {
        'name': name,
        'term': term_text,
        'wiki_link': wiki_link
    }
    
    print(f"Adding secretary: {name}")
    secretaries.append(secretary_info)

# If we didn't find any secretaries using the table, try a manual approach
if not secretaries:
    print("\nNo secretaries found in the table. Trying manual list of known secretaries.")
    known_secretaries = [
        {"name": "Tom Ridge", "wiki_link": "https://en.wikipedia.org/wiki/Tom_Ridge"},
        {"name": "Michael Chertoff", "wiki_link": "https://en.wikipedia.org/wiki/Michael_Chertoff"},
        {"name": "Janet Napolitano", "wiki_link": "https://en.wikipedia.org/wiki/Janet_Napolitano"},
        {"name": "Jeh Johnson", "wiki_link": "https://en.wikipedia.org/wiki/Jeh_Johnson"},
        {"name": "John F. Kelly", "wiki_link": "https://en.wikipedia.org/wiki/John_F._Kelly"},
        {"name": "Kirstjen Nielsen", "wiki_link": "https://en.wikipedia.org/wiki/Kirstjen_Nielsen"}
    ]
    secretaries = known_secretaries

print(f"\nFound {len(secretaries)} Secretaries of Homeland Security who served before April 2019 (excluding acting secretaries)")

# Function to extract educational background from a secretary's Wikipedia page
def get_education_background(wiki_link):
    if not wiki_link:
        return "Wikipedia link not available"
    
    print(f"\nFetching education details from: {wiki_link}")
    response = fetch_with_backoff(wiki_link)
    if not response:
        return "Education information not available"
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Print page title to confirm we're on the right page
    page_title = soup.find('title').text if soup.find('title') else "Unknown page"
    print(f"Loaded page: {page_title}")
    
    # Look for education information in the infobox
    education = []
    infobox = soup.find('table', class_='infobox')
    if infobox:
        print("Found infobox, searching for education information...")
        for row in infobox.find_all('tr'):
            header = row.find('th')
            if header:
                header_text = header.get_text().lower()
                print(f"Infobox row header: '{header_text}'")
                if 'education' in header_text or 'alma mater' in header_text:
                    value = row.find('td')
                    if value:
                        education_text = value.get_text().strip()
                        print(f"Found education in infobox: '{education_text[:100]}...'" if len(education_text) > 100 else f"Found education in infobox: '{education_text}'")
                        education.append(education_text)
    else:
        print("No infobox found on the page")
    
    # If not found in infobox, look in the content
    if not education:
        print("Education not found in infobox, searching in content...")
        content = soup.find('div', class_='mw-parser-output')
        if content:
            paragraphs = content.find_all('p')
            education_keywords = ['graduate', 'graduated', 'degree', 'university', 'college', 'b.a.', 'b.s.', 'bachelor', 'education']
            
            for paragraph in paragraphs:
                text = paragraph.get_text().lower()
                if any(keyword in text for keyword in education_keywords):
                    para_text = paragraph.get_text().strip()
                    print(f"Found paragraph with education keywords: '{para_text[:100]}...'" if len(para_text) > 100 else f"Found paragraph with education keywords: '{para_text}'")
                    education.append(para_text)
    
    if education:
        combined_education = "\n".join(education)
        return combined_education
    else:
        print("No education information found")
        return "Education information not found"

# Function to extract bachelor's degree from education text
def extract_bachelors_degree(education_text):
    if not education_text or education_text in ["Education information not available", "Education information not found", "Wikipedia link not available"]:
        return "Unknown"
    
    print(f"Extracting bachelor's degree from: '{education_text[:100]}...'" if len(education_text) > 100 else f"Extracting bachelor's degree from: '{education_text}'")
    
    # List of patterns to try in order of specificity
    patterns = [
        r"bachelor(?:'s|s)?\s+(?:of|degree|in)\s+[\w\s]+\s+(?:from|at)\s+([\w\s&,]+)",
        r"B\.?A\.?|B\.?S\.?[^.]*?(?:from|at)\s+([\w\s&,]+)",
        r"(?:earned|received|completed|obtained)\s+(?:a|an|his|her)\s+(?:bachelor(?:'s|s)?|undergraduate\s+degree|B\.?A\.?|B\.?S\.?)[^.]*?(?:from|at)\s+([\w\s&,]+)",
        r"(?:attended|enrolled\s+(?:at|in))\s+([\w\s&,]+)\s+(?:where|and)\s+(?:earned|received|graduated|obtained)\s+(?:a|an|his|her)\s+(?:bachelor(?:'s|s)?|B\.?A\.?|B\.?S\.?)",
        r"graduated\s+(?:from|in)\s+([\w\s&,]+)\s+(?:with|earning)\s+(?:a|an)\s+(?:bachelor(?:'s|s)?|B\.?A\.?|B\.?S\.?)",
        r"([\w\s&,]+?)\s+(?:University|College|Institute)",
        r"(University|College|Institute)\s+of\s+[\w\s&,]+",
    ]
    
    for i, pattern in enumerate(patterns):
        try:
            matches = re.findall(pattern, education_text, re.IGNORECASE)
            if matches:
                # Clean up any remaining references or annotations
                degree = re.sub(r'\[\d+\]', '', matches[0]).strip()
                print(f"Found bachelor's degree using pattern {i+1}: '{degree}'")
                return degree
        except Exception as e:
            print(f"Error with pattern {i+1}: {e}")
    
    # If no match found with the patterns, try to find any university or college mention
    try:
        university_pattern = r'(?:[A-Z][a-z]+\s+)+(?:University|College|Institute)|(?:University|College|Institute)\s+of\s+(?:[A-Z][a-z]+\s*)+'  
        university_matches = re.findall(university_pattern, education_text)
        
        if university_matches:
            print(f"Found university name: '{university_matches[0]}'")
            return university_matches[0].strip()
    except Exception as e:
        print(f"Error with university pattern: {e}")
    
    print("No bachelor's degree information found")
    return "Not specifically identified"

# Enhance secretaries data with education information
print("\nRetrieving educational background for each Secretary...")
for secretary in secretaries:
    print(f"\n{'='*50}\nProcessing Secretary: {secretary['name']}")
    
    if secretary['wiki_link']:
        education_text = get_education_background(secretary['wiki_link'])
        secretary['education'] = education_text
        
        # Extract bachelor's degree
        bachelors_degree = extract_bachelors_degree(education_text)
        secretary['bachelors_degree'] = bachelors_degree
        
        print(f"Secretary: {secretary['name']}")
        print(f"Bachelor's Degree: {bachelors_degree}")
    else:
        print(f"No Wikipedia link available for {secretary['name']}")
        secretary['education'] = "Wikipedia link not available"
        secretary['bachelors_degree'] = "Unknown"

# Save the results
results_file = os.path.join(workspace_dir, 'homeland_security_secretaries.json')
with open(results_file, 'w') as f:
    json.dump(secretaries, f, indent=4)

print(f"\nResults saved to {results_file}")

# Print a summary of the findings
print("\nSummary of US Secretaries of Homeland Security who served prior to April 2019:")
print("-" * 80)
for secretary in secretaries:
    print(f"Name: {secretary['name']}")
    print(f"Term: {secretary.get('term', 'Term information not available')}")
    print(f"Bachelor's Degree: {secretary.get('bachelors_degree', 'Unknown')}")
    print("-" * 80)
```