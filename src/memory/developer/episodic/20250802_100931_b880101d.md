### Development Step 1: US Presidents Birthplaces: City, State, and Geographic Coordinates in JSON

**Description**: Collect a complete list of all United States presidents with their official birth city names and state names, then obtain the geographic coordinates (longitude and latitude) for each birth city using authoritative sources such as official presidential biographies or Wikipedia infoboxes. Save the results in a structured format (e.g., CSV or JSON) at workspace/presidential_birthplaces.json.

**Use Cases**:
- Interactive educational map for K–12 history lessons, plotting each president’s birthplace coordinates to engage students with geospatial timelines
- Travel itinerary planner for presidential heritage tourism, generating optimized routes to visit the birth cities of U.S. presidents
- Political science research dashboard analyzing regional birth trends of presidents to study correlations with voting patterns and demographic shifts
- Automated fact-checking service for newsrooms, validating reported presidential birthplace details against a structured, coordinate-based dataset
- GIS integration for university geography departments, overlaying presidential birthplaces onto thematic maps for spatial analysis projects
- Digital archives enrichment workflow, tagging presidential biography records with precise latitude/longitude metadata for improved search and retrieval
- Voice assistant skill development, enabling conversational queries like “Where was Abraham Lincoln born?” by returning exact city coordinates from the JSON dataset

```
import os
import requests
from bs4 import BeautifulSoup

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# URL for list of US presidents by birthplace
url = 'https://en.wikipedia.org/wiki/List_of_United_States_presidents_by_birthplace'
print(f"Fetching Wikipedia page: {url}")

# Fetch the page content
try:
    response = requests.get(url)
    response.raise_for_status()
    html_content = response.text
    # Save raw HTML for inspection
    html_path = os.path.join(workspace_dir, 'presidents_by_birthplace.html')
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"Saved raw HTML to {html_path}")
except Exception as e:
    print(f"Error fetching or saving HTML page: {e}")
    raise

# Parse the HTML to inspect structure
soup = BeautifulSoup(html_content, 'html.parser')
content_div = soup.find('div', class_='mw-parser-output')
if not content_div:
    print("Error: Could not find main content div 'mw-parser-output'")
    exit(1)

print("Inspecting page structure: State headings and number of list items under each...")

# We'll look for h3 headings (states) and the following ul lists
count = 0
for header in content_div.find_all(['h2', 'h3']):
    # Only consider state-level headings after the intro
    span = header.find('span', class_='mw-headline')
    if span and header.name in ['h2', 'h3']:
        state_name = span.get_text().strip()
        # Next sibling may be a div, but the list of presidents is in next ul
        next_node = header.find_next_sibling()
        # Find the first ul following this header
        ul = None
        while next_node:
            if next_node.name == 'ul':
                ul = next_node
                break
            next_node = next_node.find_next_sibling()
        if not ul:
            continue
        items = ul.find_all('li')
        print(f"Section: {state_name} -> {len(items)} entries")
        # Print the first 3 items for inspection
        for li in items[:3]:
            # Each li contains a link to president page and text for birthplace
            text = li.get_text().strip()
            print(f"  - {text}")
        print('-' * 60)
        count += 1
        if count >= 5:
            # Only show first 5 sections for brevity
            break

print("Inspection complete. You can review the saved HTML file or printed sections above to understand the structure.")
```