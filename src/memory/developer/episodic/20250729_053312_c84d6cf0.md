### Development Step 1: Analyzing Legume Wikipedia 2022 Content Violations and Defining “R” in Core Policies

**Description**: Search for information about the Legume Wikipedia page and identify what content violations occurred in its public logs during 2022 before December. Focus on finding the specific type of content that was violated and the associated three core policies, particularly identifying what 'R' stands for in those policies.

**Use Cases**:
- Academic research on editorial behavior: automating the extraction and analysis of 2022 edit logs from subject-specific Wikipedia pages (like “Legume”) to study how often and why contributors trigger content-policy violations.
- Corporate brand protection: monitoring a company’s Wikipedia entries for unauthorized changes, copyright infringements, or defamatory edits in public logs so legal teams can respond quickly.
- Data journalism investigation: scraping talk pages and edit histories to uncover patterns of biased content or hidden conflicts of interest, then visualizing when each policy breach occurred in 2022.
- Compliance auditing for NGOs: auditing Wikipedia’s public logs for policy violations (e.g., copyright, vandalism) to ensure reference materials used in grant proposals adhere to verifiable sourcing standards.
- Community moderation automation: building a bot that detects new edit history entries with policy indicators (reverts, deletions) in real time and flags them for human review, reducing response times on high-traffic articles.
- Educational digital literacy tool: creating a classroom application that tracks actual content disputes on Wikipedia, highlights three core policies (including what “R” stands for), and teaches students how to evaluate source reliability.
- Legal evidence collection: compiling timestamped records of reverted or deleted edits from 2022 public logs to support defamation or intellectual property cases with documented policy violations.

```
import requests
from bs4 import BeautifulSoup
import json
import time
import os
import re

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')

print("=== LEGUME WIKIPEDIA CONTENT VIOLATIONS RESEARCH ===\n")
print("Target: Content violations on Legume Wikipedia page during 2022 (before December)")
print("Focus: Identify violation type and three core policies, especially what 'R' stands for\n")

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Connection': 'keep-alive'
}

print("Step 1: Accessing Legume Wikipedia page and related resources...\n")

# Target URLs to investigate
target_urls = [
    "https://en.wikipedia.org/wiki/Legume",
    "https://en.wikipedia.org/wiki/Talk:Legume",  # Talk page may contain policy discussions
    "https://en.wikipedia.org/w/index.php?title=Legume&action=history",  # Edit history
]

successful_sources = []
failed_sources = []

for url in target_urls:
    print(f"Accessing: {url}")
    try:
        response = requests.get(url, headers=headers, timeout=20)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            print(f"✓ Successfully accessed {url}")
            
            # Save the content for analysis
            filename = url.replace('https://', '').replace('http://', '').replace('/', '_').replace('?', '_').replace('=', '_').replace('&', '_').replace(':', '_') + '.html'
            filepath = f'workspace/{filename}'
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # Quick content analysis
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find('title')
            title_text = title.get_text().strip() if title else 'No title found'
            
            # Look for content related to violations, policies, or 2022 activity
            content_text = response.text.lower()
            policy_indicators = [
                'violation', 'policy', 'content policy', 'copyright', 'vandalism',
                'revert', 'removal', 'deleted', '2022', 'december', 'november',
                'october', 'september', 'august', 'july', 'june', 'may', 'april',
                'march', 'february', 'january', 'warning', 'blocked', 'protection'
            ]
            
            found_indicators = [indicator for indicator in policy_indicators if indicator in content_text]
            
            successful_sources.append({
                'url': url,
                'title': title_text,
                'filename': filepath,
                'found_policy_indicators': found_indicators,
                'content_length': len(response.text),
                'has_2022_content': '2022' in content_text
            })
            
            print(f"  Title: {title_text}")
            print(f"  Content length: {len(response.text)} characters")
            print(f"  Contains 2022 references: {'2022' in content_text}")
            print(f"  Policy indicators found: {found_indicators[:5]}...")  # Show first 5
            
        else:
            failed_sources.append({'url': url, 'status': response.status_code})
            print(f"✗ Failed - Status: {response.status_code}")
            
    except Exception as e:
        failed_sources.append({'url': url, 'error': str(e)})
        print(f"✗ Error: {str(e)}")
    
    print()  # Empty line for readability
    time.sleep(2)  # Be respectful to servers

print(f"=== INITIAL ACCESS RESULTS ===\n")
print(f"Successfully accessed: {len(successful_sources)} sources")
print(f"Failed to access: {len(failed_sources)} sources\n")

# Analyze successful sources for content violations and policy information
if successful_sources:
    print("=== ANALYZING SOURCES FOR CONTENT VIOLATIONS ===\n")
    
    priority_sources = []
    
    for i, source in enumerate(successful_sources, 1):
        print(f"{i}. {source['url']}")
        print(f"   File: {source['filename']}")
        print(f"   Has 2022 content: {source['has_2022_content']}")
        print(f"   Policy indicators: {len(source['found_policy_indicators'])} found")
        
        # Prioritize sources with 2022 content and policy indicators
        if source['has_2022_content'] or len(source['found_policy_indicators']) > 3:
            priority_sources.append(source)
            print(f"   *** PRIORITY SOURCE - Contains relevant information ***")
        
        print()
    
    print(f"Priority sources identified: {len(priority_sources)}\n")
    
    # Save initial analysis results
    initial_analysis = {
        'research_target': 'Legume Wikipedia content violations 2022',
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'sources_accessed': len(successful_sources),
        'priority_sources': len(priority_sources),
        'sources_data': successful_sources,
        'failed_sources': failed_sources,
        'search_focus': [
            'Content violations in 2022 before December',
            'Three core policies involved',
            'What R stands for in policies',
            'Public logs and edit history'
        ]
    }
    
    with open('workspace/legume_initial_analysis.json', 'w') as f:
        json.dump(initial_analysis, f, indent=2)
    
    print(f"Initial analysis saved to: workspace/legume_initial_analysis.json")
    
else:
    print("No sources successfully accessed. Need alternative approach.")

print(f"\n=== INITIAL RESEARCH PHASE COMPLETE ===\n")
print(f"Files saved in workspace/ directory for detailed analysis")
print(f"Next: Parse HTML content for specific violation details and policy information")
```