### Development Step 3: Identify Organization and Advocate for S√£o Francisco Basin Environmental Plan and Sobradinho Dam Displacement

**Description**: Conduct a comprehensive web search to identify the organization that launched the 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco' covering 505 municipalities and collaborates with Minist√©rios P√∫blicos. Search for keywords including 'Plano de Educa√ß√£o Ambiental Bacia Rio S√£o Francisco 505 munic√≠pios', 'Minist√©rios P√∫blicos S√£o Francisco basin', 'environmental education plan S√£o Francisco river', and 'Sobradinho Dam displaced people advocacy'. Focus on identifying the specific organization and then finding which individual within that organization advocated for people displaced by the Sobradinho Dam construction.

**Use Cases**:
- Government environmental policy monitoring and automated extraction of stakeholder involvement in large-scale river basin education programs
- NGO advocacy research to identify and track individuals and organizations responsible for Sobradinho Dam‚Äìdisplaced population support and resettlement efforts
- Academic literature review automation for compiling and summarizing environmental education plans across 505 municipalities in the S√£o Francisco basin
- Corporate social responsibility due diligence by scraping Minist√©rio P√∫blico collaborations and government agency mentions in regional environmental impact initiatives
- Legal case preparation by mapping Minist√©rio P√∫blico engagements and environmental-plan evidence for water-rights and resettlement litigation
- Data journalism content aggregation and narrative analysis on Sobradinho Dam displacement from official reports and media sources
- Market intelligence gathering on potential partners (CHESF, CODEVASF, IBAMA, ANA) for environmental service contracts and funding opportunities in northeastern Brazil

```
import requests
import json
import os
from datetime import datetime
import time
from bs4 import BeautifulSoup

print("Conducting comprehensive web search to identify the organization behind 'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco'...")

# Create workspace directory if it doesn't exist
if not os.path.exists('workspace'):
    os.makedirs('workspace')
    print("Created workspace directory")

# Define search keywords in Portuguese and English
search_keywords = [
    'Plano de Educa√ß√£o Ambiental da Bacia do Rio S√£o Francisco',
    'Plano Educa√ß√£o Ambiental S√£o Francisco',
    'PEABRIOSF',
    'Bacia Rio S√£o Francisco 505 munic√≠pios',
    'Minist√©rios P√∫blicos S√£o Francisco',
    'environmental education plan S√£o Francisco river',
    'Sobradinho Dam displaced people',
    'Barragem Sobradinho deslocados',
    'S√£o Francisco basin environmental education',
    'CHESF environmental education',
    'IBAMA S√£o Francisco',
    'ANA S√£o Francisco basin',
    'CODEVASF environmental',
    'Comit√™ Bacia Hidrogr√°fica S√£o Francisco'
]

print(f"Search will focus on {len(search_keywords)} key terms related to S√£o Francisco basin environmental education")

# FIXED: Define headers BEFORE the function that uses them
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'
}

def search_web_for_sao_francisco_plan():
    """Search web sources for S√£o Francisco environmental education plan information"""
    
    # Initial URLs to search - focusing on Brazilian environmental and government sites
    initial_urls = [
        "https://pt.wikipedia.org/wiki/Rio_S%C3%A3o_Francisco",
        "https://pt.wikipedia.org/wiki/Bacia_hidrogr%C3%A1fica_do_rio_S%C3%A3o_Francisco",
        "https://pt.wikipedia.org/wiki/Usina_Hidrel%C3%A9trica_de_Sobradinho",
        "https://pt.wikipedia.org/wiki/CHESF",
        "https://pt.wikipedia.org/wiki/CODEVASF",
        "https://en.wikipedia.org/wiki/S%C3%A3o_Francisco_River",
        "https://en.wikipedia.org/wiki/Sobradinho_Dam"
    ]
    
    search_results = {}
    analysis_results = {}
    
    print(f"\nStarting web search of {len(initial_urls)} initial sources...")
    
    for i, url in enumerate(initial_urls, 1):
        page_name = url.split('/')[-1].replace('%C3%A3', 'a').replace('%C3%A9', 'e').replace('%20', '_')
        print(f"\n[{i}/{len(initial_urls)}] Fetching: {page_name}")
        print(f"URL: {url}")
        
        try:
            # Now headers is properly defined and accessible
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            content = response.text
            
            # Parse with BeautifulSoup to extract clean text
            soup = BeautifulSoup(content, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Get text content
            text_content = soup.get_text()
            lines = (line.strip() for line in text_content.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            clean_text = ' '.join(chunk for chunk in chunks if chunk)
            
            search_results[page_name] = {
                'url': url,
                'content_length': len(clean_text),
                'content': clean_text[:20000],  # Store first 20000 characters
                'fetch_time': datetime.now().isoformat(),
                'status': 'success'
            }
            
            print(f"‚úì Successfully retrieved {len(clean_text):,} characters")
            
            # Analyze content for keywords immediately
            found_keywords = []
            relevant_sections = []
            
            content_lower = clean_text.lower()
            
            for keyword in search_keywords:
                if keyword.lower() in content_lower:
                    found_keywords.append(keyword)
                    
                    # Find sections around keyword
                    start_pos = 0
                    keyword_lower = keyword.lower()
                    
                    while True:
                        pos = content_lower.find(keyword_lower, start_pos)
                        if pos == -1:
                            break
                        
                        # Extract context around keyword
                        section_start = max(0, pos - 800)
                        section_end = min(len(clean_text), pos + 800)
                        section = clean_text[section_start:section_end]
                        
                        relevant_sections.append({
                            'keyword': keyword,
                            'section': section,
                            'position': pos,
                            'occurrence': len([s for s in relevant_sections if s['keyword'] == keyword]) + 1
                        })
                        
                        start_pos = pos + 1
                        
                        # Limit to 3 occurrences per keyword per page
                        if len([s for s in relevant_sections if s['keyword'] == keyword]) >= 3:
                            break
            
            analysis_results[page_name] = {
                'url': url,
                'found_keywords': found_keywords,
                'relevant_sections': relevant_sections,
                'keyword_count': len(found_keywords),
                'section_count': len(relevant_sections)
            }
            
            print(f"‚úì Found {len(found_keywords)} keywords, {len(relevant_sections)} relevant sections")
            if found_keywords:
                print(f"Keywords: {', '.join(found_keywords[:5])}{'...' if len(found_keywords) > 5 else ''}")
                
        except Exception as e:
            print(f"‚úó Error fetching {url}: {str(e)}")
            search_results[page_name] = {
                'url': url,
                'error': str(e),
                'content_length': 0,
                'content': '',
                'fetch_time': datetime.now().isoformat(),
                'status': 'error'
            }
            analysis_results[page_name] = {
                'url': url,
                'found_keywords': [],
                'relevant_sections': [],
                'keyword_count': 0,
                'section_count': 0,
                'error': str(e)
            }
        
        # Add delay between requests to be respectful
        time.sleep(2)
    
    return search_results, analysis_results

# Execute the initial search
print("Starting comprehensive web search for S√£o Francisco environmental education plan...")
search_results, analysis_results = search_web_for_sao_francisco_plan()

print(f"\n{'='*80}")
print("INITIAL SEARCH COMPLETED")
print(f"{'='*80}")

# Save initial search results
initial_output_file = "workspace/sao_francisco_initial_search.json"
with open(initial_output_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)
print(f"\nInitial search results saved to {initial_output_file}")

# Generate search summary
search_summary = {
    'search_date': datetime.now().isoformat(),
    'urls_searched': len(search_results),
    'successful_fetches': len([r for r in search_results.values() if r.get('status') == 'success']),
    'failed_fetches': len([r for r in search_results.values() if r.get('status') == 'error']),
    'total_keywords_found': sum(r.get('keyword_count', 0) for r in analysis_results.values()),
    'total_sections_found': sum(r.get('section_count', 0) for r in analysis_results.values())
}

print(f"\nINITIAL SEARCH SUMMARY:")
print(f"URLs searched: {search_summary['urls_searched']}")
print(f"Successful fetches: {search_summary['successful_fetches']}")
print(f"Failed fetches: {search_summary['failed_fetches']}")
print(f"Total keywords found: {search_summary['total_keywords_found']}")
print(f"Total relevant sections: {search_summary['total_sections_found']}")

# Display results by page
print(f"\n{'='*80}")
print("INITIAL SEARCH RESULTS BY PAGE")
print(f"{'='*80}")

for page_name, results in analysis_results.items():
    if results.get('keyword_count', 0) > 0:
        print(f"\nüìÑ {page_name}")
        print(f"   URL: {results['url']}")
        print(f"   Keywords found ({results['keyword_count']}): {', '.join(results['found_keywords'])}")
        print(f"   Relevant sections: {results['section_count']}")
    elif 'error' in results:
        print(f"\n‚ùå {page_name} - Error: {results['error']}")
    else:
        print(f"\n‚ö™ {page_name} - No relevant keywords found")

print(f"\n{'='*80}")
print("ANALYZING CONTENT FOR SPECIFIC ORGANIZATIONS AND INDIVIDUALS")
print(f"{'='*80}")

# Look for specific patterns related to environmental education plans and organizations
organization_evidence = []
plan_details = []
sobradinho_advocacy = []
ministery_collaboration = []

for page_name, results in analysis_results.items():
    for section in results.get('relevant_sections', []):
        section_text = section['section'].lower()
        section_content = section['section']
        
        # Look for organization names and environmental education plans
        org_indicators = ['chesf', 'codevasf', 'ibama', 'ana', 'comit√™', 'minist√©rio p√∫blico', 'funda√ß√£o', 'instituto']
        if any(indicator in section_text for indicator in org_indicators) and ('educa√ß√£o ambiental' in section_text or 'environmental education' in section_text):
            organization_evidence.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for specific plan details
        if ('plano' in section_text or 'plan' in section_text) and ('educa√ß√£o ambiental' in section_text or 'environmental education' in section_text):
            plan_details.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for Sobradinho Dam and displaced people
        if 'sobradinho' in section_text and ('deslocad' in section_text or 'displaced' in section_text or 'reassent' in section_text):
            sobradinho_advocacy.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })
        
        # Look for Ministry collaboration
        if 'minist√©rio' in section_text and ('p√∫blico' in section_text or 'colabora' in section_text):
            ministery_collaboration.append({
                'source': page_name,
                'section': section_content,
                'keyword': section['keyword'],
                'url': results['url']
            })

# Display findings
print(f"\nüè¢ ORGANIZATION EVIDENCE: {len(organization_evidence)}")
for i, evidence in enumerate(organization_evidence[:3], 1):
    print(f"\n{i}. From {evidence['source']}:")
    print(f"   {evidence['section'][:500]}...")

print(f"\nüìã PLAN DETAILS: {len(plan_details)}")
for i, detail in enumerate(plan_details[:3], 1):
    print(f"\n{i}. From {detail['source']}:")
    print(f"   {detail['section'][:500]}...")

print(f"\nüèóÔ∏è SOBRADINHO ADVOCACY: {len(sobradinho_advocacy)}")
for i, advocacy in enumerate(sobradinho_advocacy[:3], 1):
    print(f"\n{i}. From {advocacy['source']}:")
    print(f"   {advocacy['section'][:500]}...")

print(f"\nü§ù MINISTRY COLLABORATION: {len(ministery_collaboration)}")
for i, collab in enumerate(ministery_collaboration[:3], 1):
    print(f"\n{i}. From {collab['source']}:")
    print(f"   {collab['section'][:500]}...")

# Save comprehensive findings
comprehensive_findings = {
    'search_date': datetime.now().isoformat(),
    'search_summary': search_summary,
    'organization_evidence': organization_evidence,
    'plan_details': plan_details,
    'sobradinho_advocacy': sobradinho_advocacy,
    'ministry_collaboration': ministery_collaboration,
    'search_keywords': search_keywords,
    'pages_analyzed': list(analysis_results.keys())
}

findings_file = "workspace/sao_francisco_comprehensive_findings.json"
with open(findings_file, 'w', encoding='utf-8') as f:
    json.dump(comprehensive_findings, f, indent=2, ensure_ascii=False)
print(f"\nComprehensive findings saved to {findings_file}")

print(f"\n{'='*80}")
print("INITIAL SEARCH PHASE COMPLETED - PREPARING FOR TARGETED SEARCH")
print(f"{'='*80}")
print(f"\nüìä Initial Search Statistics:")
print(f"   ‚Ä¢ Pages searched: {len(analysis_results)}")
print(f"   ‚Ä¢ Organization evidence found: {len(organization_evidence)}")
print(f"   ‚Ä¢ Plan details found: {len(plan_details)}")
print(f"   ‚Ä¢ Sobradinho advocacy references: {len(sobradinho_advocacy)}")
print(f"   ‚Ä¢ Ministry collaboration mentions: {len(ministery_collaboration)}")

print(f"\nüéØ NEXT STEPS:")
print(f"   ‚Ä¢ Conduct targeted search based on initial findings")
print(f"   ‚Ä¢ Focus on specific organizations identified (CHESF, CODEVASF, etc.)")
print(f"   ‚Ä¢ Search for specific individuals involved in Sobradinho Dam advocacy")
print(f"   ‚Ä¢ Look for official documents and reports")

print(f"\n‚úÖ INITIAL SEARCH COMPLETED SUCCESSFULLY!")
```