### Development Step 4: Identifying the Federal Rules of Evidence Article with Most 'Witnesses' in Rule Titles and Analyzing Amendments

**Description**: Access the Federal Rules of Evidence section on Cornell Law School's Legal Information Institute website and systematically analyze all rules to identify which article has 'witnesses' appearing in the most rule titles as of 2021. Count the occurrences of 'witnesses' in titles across all Federal Rules of Evidence articles, then identify the article with the highest count for further analysis of its first rule's amendment history.

**Use Cases**:
- Legal research automation for law firms seeking to quickly identify which Federal Rules of Evidence articles most frequently address witnesses, streamlining case preparation involving witness testimony.
- Academic analysis for legal scholars studying trends in evidentiary law, enabling systematic review of how witness-related rules are distributed and amended across different articles.
- Compliance auditing for corporate legal departments needing to ensure internal policies align with the most frequently amended or referenced witness rules in federal evidence law.
- Legal publishing workflow enhancement, allowing editors to prioritize updates and commentary for articles with the highest concentration of witness-related rules and recent amendments.
- Judicial training program development, helping instructors focus on the most relevant Federal Rules of Evidence articles for witness examination in continuing education courses for judges and attorneys.

```
import requests
from bs4 import BeautifulSoup
import os
import json
from datetime import datetime
import re
from collections import defaultdict

print('=== FEDERAL RULES OF EVIDENCE - WITNESSES ANALYSIS ===')
print('Objective: Access Federal Rules of Evidence and find which article has most "witnesses" in rule titles')
print('Based on previous analysis: Federal Rules of Evidence is the 5th category alphabetically\n')

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# Set up headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# Federal Rules of Evidence URL from previous analysis
fre_url = 'https://www.law.cornell.edu/rules/fre'
print(f'Accessing Federal Rules of Evidence: {fre_url}')

print('\n=== STEP 1: ACCESSING FEDERAL RULES OF EVIDENCE PAGE ===')

try:
    response = requests.get(fre_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f'Successfully accessed Federal Rules of Evidence page')
    print(f'Status code: {response.status_code}')
    print(f'Content length: {len(response.content):,} bytes')
    
    # Parse the page
    soup = BeautifulSoup(response.content, 'html.parser')
    
    page_title = soup.find('title').get_text() if soup.find('title') else 'No title found'
    print(f'Page title: {page_title}')
    
    # Save the raw HTML for inspection
    html_file = os.path.join(workspace_dir, 'federal_rules_evidence_page.html')
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(response.text)
    print(f'Raw HTML saved to: {html_file}')
    
    print('\n=== STEP 2: ANALYZING PAGE STRUCTURE FOR ARTICLES AND RULES ===')
    
    # Look for the main content area and rule structure
    print('Searching for rule structure and articles...')
    
    # Strategy 1: Look for article headings and rule lists
    article_sections = soup.find_all(['h1', 'h2', 'h3', 'h4'], string=re.compile(r'Article|ARTICLE', re.I))
    print(f'Found {len(article_sections)} potential article headings')
    
    for i, section in enumerate(article_sections[:10], 1):  # Show first 10
        print(f'{i}. {section.name}: "{section.get_text().strip()}"')
    
    # Strategy 2: Look for rule links and structure
    print('\nSearching for rule links and numbering...')
    
    # Find all links that might be rules (typically numbered)
    rule_links = []
    all_links = soup.find_all('a', href=True)
    
    for link in all_links:
        href = link.get('href')
        text = link.get_text().strip()
        
        # Look for rule patterns (Rule XXX, Rule X-XXX, etc.)
        if (re.search(r'rule\s*\d+', text, re.I) or 
            re.search(r'/rules/fre/rule-\d+', href) or
            re.search(r'\bfre\b.*\d+', href)):
            
            rule_links.append({
                'text': text,
                'href': href,
                'full_url': href if href.startswith('http') else 'https://www.law.cornell.edu' + href
            })
    
    print(f'Found {len(rule_links)} potential rule links')
    
    # Display first 10 rule links to understand structure
    for i, link in enumerate(rule_links[:10], 1):
        print(f'{i}. "{link["text"]}" -> {link["href"]}')
    
    if len(rule_links) > 10:
        print(f'... and {len(rule_links) - 10} more rule links')
    
    # Strategy 3: Look for table of contents or navigation structure
    print('\nSearching for table of contents or navigation structure...')
    
    # Look for elements that might contain the full structure
    nav_elements = soup.find_all(['nav', 'div', 'ul'], class_=re.compile(r'toc|nav|menu|content|index', re.I))
    print(f'Found {len(nav_elements)} potential navigation elements')
    
    # Check each navigation element for rule structure
    for i, nav in enumerate(nav_elements[:5], 1):  # Check first 5
        nav_text = nav.get_text()
        if 'rule' in nav_text.lower() and len(nav_text) > 100:  # Substantial content
            print(f'\nNavigation element {i} contains rule content:')
            print(f'Length: {len(nav_text)} characters')
            print(f'Preview: {nav_text[:300]}...')
            
            # Look for article structure within this navigation
            nav_links = nav.find_all('a', href=True)
            print(f'Contains {len(nav_links)} links')
    
    # Strategy 4: Look for the main content area with all rules
    print('\nSearching for main content area with complete rule listing...')
    
    # Look for main content containers
    main_content = soup.find_all(['div', 'main', 'section'], class_=re.compile(r'content|main|body', re.I))
    print(f'Found {len(main_content)} main content areas')
    
    # Find the most comprehensive content area
    best_content = None
    max_rule_count = 0
    
    for content in main_content:
        content_text = content.get_text().lower()
        rule_count = content_text.count('rule')
        if rule_count > max_rule_count:
            max_rule_count = rule_count
            best_content = content
    
    if best_content:
        print(f'Best content area has {max_rule_count} "rule" mentions')
        
        # Extract all text and look for structure
        content_text = best_content.get_text()
        
        # Save content for detailed analysis
        content_file = os.path.join(workspace_dir, 'fre_main_content.txt')
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(content_text)
        print(f'Main content saved to: {content_file}')
    
    # Save initial analysis
    initial_analysis = {
        'analysis_date': datetime.now().isoformat(),
        'source_url': fre_url,
        'page_title': page_title,
        'article_headings_found': len(article_sections),
        'rule_links_found': len(rule_links),
        'navigation_elements_found': len(nav_elements),
        'main_content_areas_found': len(main_content),
        'max_rule_mentions_in_content': max_rule_count,
        'rule_links_sample': rule_links[:20],  # First 20 for reference
        'article_headings_sample': [section.get_text().strip() for section in article_sections[:10]]
    }
    
    analysis_file = os.path.join(workspace_dir, 'fre_initial_structure_analysis.json')
    with open(analysis_file, 'w') as f:
        json.dump(initial_analysis, f, indent=2)
    
    print(f'\nInitial structure analysis saved to: {analysis_file}')
    
except requests.exceptions.RequestException as e:
    print(f'Error accessing Federal Rules of Evidence page: {e}')
except Exception as e:
    print(f'Unexpected error during analysis: {e}')
    import traceback
    traceback.print_exc()

print('\n=== INITIAL FEDERAL RULES OF EVIDENCE ANALYSIS COMPLETE ===')
print('Next step: Parse the content to identify articles and count "witnesses" in rule titles')
```