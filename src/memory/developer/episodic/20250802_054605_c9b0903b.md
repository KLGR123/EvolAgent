### Development Step 45: Download University of Leicester PDF: "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?"

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Legal researchers automating retrieval of PDF court opinions from public judiciary archives for case citation analysis
- Market intelligence teams collecting competitor whitepapers in PDF format from industry association websites for strategic planning
- Financial auditors downloading PDF annual reports from regulatory filing portals for compliance verification
- Academic librarians aggregating PDF research articles from university repositories to curate digital reading lists
- Healthcare data analysts fetching PDF clinical guideline documents from hospital intranet for outcome benchmarking
- Product managers automating download of PDF specification sheets from supplier websites for BOM updates
- Policy analysts retrieving PDF government legislation drafts from official ministry domains for impact assessment
- Supply chain coordinators gathering PDF shipping manifests from logistics providers’ online archives for delivery tracking

```
import os
import sys
import re
import requests
from bs4 import BeautifulSoup

# --- Configuration ---
workspace = 'workspace'
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# The exact paper title to search for
title = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet?"
# DuckDuckGo HTML search endpoint
ddg_url = 'https://html.duckduckgo.com/html/'
# Browser-like headers to avoid being blocked by simple filters
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# Three successive queries: restrict to Leicester archive, then broader, then global PDF search
queries = [
    f'filetype:pdf "{title}" site:lra.le.ac.uk',
    f'filetype:pdf "{title}" site:le.ac.uk',
    f'filetype:pdf "{title}"'
]

# Paths for outputs
ddg_html = os.path.join(workspace, 'ddg_search.html')
pdf_list = os.path.join(workspace, 'ddg_pdf_urls.txt')
pdf_out = os.path.join(workspace, 'dragon_diet_fish_bag.pdf')

all_candidates = []

# Iterate through search queries until we find PDF links
for q in queries:
    print(f"[INFO] DuckDuckGo search query: {q}")
    try:
        resp = requests.post(ddg_url, data={'q': q}, headers=headers, timeout=30)
    except Exception as e:
        print(f"[WARN] Request error for query '{q}': {e}")
        continue

    status = resp.status_code
    print(f"[INFO] Response status: {status}")
    # Accept any 2xx status as success
    if status < 200 or status >= 300:
        print(f"[WARN] Non-2xx status for query '{q}', skipping.\n")
        continue

    # Save the first successful HTML page for manual inspection
    if not os.path.exists(ddg_html):
        with open(ddg_html, 'w', encoding='utf-8') as f:
            f.write(resp.text)
        print(f"[SAVE] DuckDuckGo HTML → {ddg_html}")

    html = resp.text
    soup = BeautifulSoup(html, 'html.parser')
    candidates = set()

    # 1) Extract <a> tags with href containing '.pdf'
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '.pdf' in href.lower():
            candidates.add(href)
            print(f"    [FOUND] PDF link in anchor: {href}")

    # 2) Regex fallback to catch raw PDF URLs
    # Properly quoted regex string to avoid unmatched bracket errors
    pattern = r"https?://[^\s\"']+?\.pdf(?:\?[^\"'\s]*)?"
    for match in re.findall(pattern, html, flags=re.IGNORECASE):
        candidates.add(match)
        print(f"    [FOUND] PDF via regex: {match}")

    # Decode any DuckDuckGo redirect wrappers (uddg=)
    for link in candidates:
        if 'uddg=' in link:
            m = re.search(r'uddg=([^&]+)', link)
            url = requests.utils.unquote(m.group(1)) if m else link
        else:
            url = link
        all_candidates.append(url)

    if all_candidates:
        print(f"[INFO] Found {len(all_candidates)} PDF candidate(s). Stopping further queries.")
        break
    else:
        print(f"[INFO] No PDF links for query '{q}'. Trying next.\n")

# If still no candidates, exit with error
if not all_candidates:
    print("[ERROR] No PDF candidates found after all queries.")
    print(f"Inspect '{ddg_html}' and refine search or parsing logic.")
    sys.exit(1)

# Deduplicate and save candidate list
unique = []
for u in all_candidates:
    if u not in unique:
        unique.append(u)
with open(pdf_list, 'w', encoding='utf-8') as f:
    for u in unique:
        f.write(u + '\n')
print(f"[SAVE] Candidate PDF URLs → {pdf_list}")

# Function to download a PDF
def download_pdf(url, out_path):
    print(f"[INFO] Downloading PDF from: {url}")
    try:
        dl = requests.get(url, headers=headers, timeout=60)
        print(f"[INFO] Download status: {dl.status_code}")
    except Exception as e:
        print(f"[ERROR] Download failed: {e}")
        return False
    if dl.status_code == 200 and len(dl.content) > 10_000:
        with open(out_path, 'wb') as f:
            f.write(dl.content)
        size = os.path.getsize(out_path)
        print(f"[SUCCESS] Saved PDF → {out_path} ({size} bytes)")
        return True
    else:
        print(f"[ERROR] Download returned status {dl.status_code} or file too small ({len(dl.content)} bytes)")
        return False

# Attempt download of the first candidate
first_url = unique[0]
if not download_pdf(first_url, pdf_out):
    print(f"[ERROR] Could not download from first URL. Try another link from {pdf_list}.")
    sys.exit(1)

```