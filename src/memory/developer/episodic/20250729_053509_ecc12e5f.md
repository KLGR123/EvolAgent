### Development Step 5: Analyze 2022 Legume Wikipedia Logs for Content Violations and Decode ‘R’ Policy Meaning

**Description**: Search for information about the Legume Wikipedia page and identify what content violations occurred in its public logs during 2022 before December. Focus on finding the specific type of content that was violated and the associated three core policies, particularly identifying what 'R' stands for in those policies.

**Use Cases**:
- Wikipedia community management: Auditing the edit history of high-traffic plant taxonomy pages (like “Legume”) to flag all vandalism, spam, or unsourced content removals before December 2022 and ensure policy compliance.
- Academic research on collaborative editing: Extracting detailed rollback summaries and policy references from the Legume page’s 2022 logs to analyze how “No Original Research” (NOR) and “Verifiability” (V) issues evolve in botanical articles.
- Data journalism investigation: Mining the public history of articles on crop nutrition to identify when and why nutritional claims were removed or reverted, helping reporters document misinformation trends in legumes.
- Legal and copyright compliance review: Systematically collecting all entries marked “deleted,” “removed,” or “rollback” from the Legume page to verify that no copyrighted text was inadvertently retained in public edits during 2022.
- Non-profit conservation monitoring: Automating a monthly scan of endangered plant species pages (including legumes) to detect policy violations and generate alerts for curators when unsourced or promotional content appears.
- Bot development for policy enforcement: Feeding the parsed 2022 edit summaries into a custom moderation bot that recognizes “RV,” “SPAM,” and “VAND” tags to automatically revert or flag noncompliant edits in ecology-related articles.
- Digital literacy education: Leveraging real flagged entries from the Legume page to teach students about the three core Wikipedia policies (NPOV, V, NOR) and how to spot and document content violations in collaborative platforms.
- SEO and brand management: Reviewing promotional content removals on corporate-sponsored legume pages to understand why certain claims were reverted, guiding marketing teams to craft compliance-ready copy.

```
import os
import json
from bs4 import BeautifulSoup
import re

print("=== DEEP DIVE INTO EDIT HISTORY FOR CONTENT VIOLATIONS ===\n")
print("Objective: Extract detailed edit summaries and administrative actions from 2022\n")

# Locate workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("No workspace directory found.")
    exit()

workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# Load the edit history HTML file directly for detailed parsing
history_file = None
for file in os.listdir(workspace_dir):
    if 'history' in file.lower() and file.endswith('.html'):
        history_file = os.path.join(workspace_dir, file)
        break

if not history_file or not os.path.exists(history_file):
    print("Edit history file not found.")
    exit()

print(f"Parsing edit history file: {os.path.basename(history_file)}")
file_size = os.path.getsize(history_file)
print(f"File size: {file_size:,} bytes\n")

# Parse the HTML with focus on edit summaries and administrative actions
with open(history_file, 'r', encoding='utf-8') as f:
    html_content = f.read()

soup = BeautifulSoup(html_content, 'html.parser')

print("=== SEARCHING FOR EDIT ENTRIES WITH SUMMARIES ===\n")

# Look for edit history entries - Wikipedia edit history typically uses specific CSS classes
edit_entries = []

# Try multiple selectors for edit history entries
selectors_to_try = [
    'li.mw-history-line',  # Common class for edit history lines
    'li[class*="history"]',  # Any li with 'history' in class name
    'li[id*="mw-history"]',  # Any li with history in id
    'tr[class*="history"]',  # Table rows with history class
    'tr[class*="mw-history"]'  # MW history table rows
]

for selector in selectors_to_try:
    entries = soup.select(selector)
    if entries:
        print(f"Found {len(entries)} edit entries using selector: {selector}")
        edit_entries = entries
        break

if not edit_entries:
    print("No edit entries found with standard selectors. Trying broader search...\n")
    # Try to find any list items or table rows containing dates
    all_lis = soup.find_all('li')
    all_trs = soup.find_all('tr')
    
    # Look for elements containing 2022 dates
    potential_entries = []
    for element in all_lis + all_trs:
        element_text = element.get_text()
        if '2022' in element_text and any(month in element_text.lower() for month in ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november']):
            potential_entries.append(element)
    
    edit_entries = potential_entries
    print(f"Found {len(edit_entries)} potential edit entries containing 2022 dates\n")

print("=== ANALYZING EDIT ENTRIES FOR VIOLATIONS ===\n")

violation_entries = []
content_violations = []
policy_references = []

for i, entry in enumerate(edit_entries[:20], 1):  # Analyze first 20 entries
    entry_text = entry.get_text().strip()
    entry_html = str(entry)
    
    print(f"Edit Entry {i}:")
    print(f"Text: {entry_text[:300]}...")  # First 300 characters
    
    # Look for 2022 dates specifically before December
    before_december_2022 = False
    date_patterns = [
        r'\b(January|February|March|April|May|June|July|August|September|October|November)\s+\d{1,2},?\s+2022\b',
        r'\b2022[-/]\d{1,2}[-/]\d{1,2}\b',
        r'\b\d{1,2}\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov)\s+2022\b'
    ]
    
    for pattern in date_patterns:
        if re.search(pattern, entry_text, re.IGNORECASE):
            before_december_2022 = True
            break
    
    # Look for violation indicators in edit summaries
    violation_keywords = [
        'revert', 'reverted', 'undid', 'rollback', 'removed', 'deleted', 'blocked',
        'vandalism', 'spam', 'violation', 'policy', 'warning', 'copyright',
        'inappropriate', 'unsourced', 'unreliable', 'promotional', 'pov'
    ]
    
    found_violations = [keyword for keyword in violation_keywords if keyword.lower() in entry_text.lower()]
    
    # Look for policy abbreviations that might include 'R'
    policy_abbreviations = re.findall(r'\b[A-Z]{2,}\b', entry_text)
    r_policies = [abbrev for abbrev in policy_abbreviations if 'R' in abbrev]
    
    # Look for "three" with policy context
    three_policy_context = 'three' in entry_text.lower() and any(word in entry_text.lower() for word in ['policy', 'policies', 'rule', 'rules'])
    
    print(f"  Before December 2022: {before_december_2022}")
    print(f"  Violation keywords found: {found_violations}")
    print(f"  Policy abbreviations: {policy_abbreviations}")
    print(f"  R-containing policies: {r_policies}")
    print(f"  Three+policy context: {three_policy_context}")
    
    # Store significant entries
    if before_december_2022 and (found_violations or r_policies or three_policy_context):
        violation_entry = {
            'entry_number': i,
            'full_text': entry_text,
            'html': entry_html,
            'violation_keywords': found_violations,
            'policy_abbreviations': policy_abbreviations,
            'r_policies': r_policies,
            'three_policy_context': three_policy_context,
            'before_december_2022': before_december_2022
        }
        violation_entries.append(violation_entry)
        print(f"  *** FLAGGED AS POTENTIAL VIOLATION ENTRY ***")
    
    print()

print(f"=== FLAGGED VIOLATION ENTRIES: {len(violation_entries)} ===\n")

# Detailed analysis of flagged entries
for i, entry in enumerate(violation_entries, 1):
    print(f"VIOLATION ENTRY {i}:")
    print(f"Full text: {entry['full_text']}")
    print(f"Keywords: {entry['violation_keywords']}")
    print(f"Policy abbreviations: {entry['policy_abbreviations']}")
    print(f"R-containing policies: {entry['r_policies']}")
    print()

# Search for specific Wikipedia policy abbreviations
print("=== SEARCHING FOR COMMON WIKIPEDIA POLICY ABBREVIATIONS ===\n")

common_policies = {
    'BLP': 'Biographies of Living Persons',
    'COI': 'Conflict of Interest', 
    'NPOV': 'Neutral Point of View',
    'NOR': 'No Original Research',
    'RS': 'Reliable Sources',
    'V': 'Verifiability',
    'WEIGHT': 'Due Weight',
    'SPAM': 'Spam',
    'VAND': 'Vandalism',
    'RV': 'Revert',
    'CSD': 'Criteria for Speedy Deletion'
}

all_text = soup.get_text().lower()
found_policies = {}

for abbrev, full_name in common_policies.items():
    if abbrev.lower() in all_text or full_name.lower() in all_text:
        # Count occurrences and find context
        abbrev_count = all_text.count(abbrev.lower())
        name_count = all_text.count(full_name.lower())
        
        found_policies[abbrev] = {
            'full_name': full_name,
            'abbrev_count': abbrev_count,
            'name_count': name_count,
            'total_mentions': abbrev_count + name_count,
            'contains_r': 'R' in abbrev
        }
        
        print(f"{abbrev} ({full_name}): {abbrev_count + name_count} mentions (Contains R: {'R' in abbrev})")

print(f"\nTotal unique policies found: {len(found_policies)}")

# Look for "three core policies" specifically
print("\n=== SEARCHING FOR THREE CORE POLICIES REFERENCE ===\n")

# Wikipedia's three core content policies are typically:
# 1. Neutral Point of View (NPOV)
# 2. Verifiability (V)
# 3. No Original Research (NOR)

core_policies_patterns = [
    r'three\s+core\s+polic[iy]',
    r'three\s+content\s+polic[iy]', 
    r'three\s+fundamental\s+polic[iy]',
    r'three\s+main\s+polic[iy]',
    r'npov.*verif.*research',
    r'verif.*npov.*research',
    r'neutral.*verif.*original'
]

for pattern in core_policies_patterns:
    matches = re.finditer(pattern, all_text, re.IGNORECASE | re.DOTALL)
    for match in matches:
        start_pos = max(0, match.start() - 300)
        end_pos = min(len(all_text), match.end() + 300)
        context = all_text[start_pos:end_pos]
        
        print(f"Three core policies reference found:")
        print(f"Pattern: {pattern}")
        print(f"Match: {match.group()}")
        print(f"Context: ...{context}...")
        print()

# Save comprehensive results
final_results = {
    'research_target': 'Legume Wikipedia content violations 2022 (before December)',
    'analysis_method': 'Deep edit history parsing',
    'edit_entries_analyzed': len(edit_entries),
    'violation_entries_found': len(violation_entries),
    'flagged_violations': violation_entries,
    'policy_abbreviations_found': found_policies,
    'r_containing_policies': {k: v for k, v in found_policies.items() if 'R' in k},
    'summary': {
        'total_policies_identified': len(found_policies),
        'policies_with_r': len([k for k in found_policies.keys() if 'R' in k]),
        'potential_violation_entries': len(violation_entries)
    }
}

with open(f'{workspace_dir}/legume_deep_violation_analysis.json', 'w') as f:
    json.dump(final_results, f, indent=2, default=str)

print(f"\n=== DEEP ANALYSIS COMPLETE ===\n")
print(f"Results saved to: {workspace_dir}/legume_deep_violation_analysis.json")
print(f"Violation entries identified: {len(violation_entries)}")
print(f"Policy abbreviations found: {len(found_policies)}")
print(f"Policies containing 'R': {[k for k in found_policies.keys() if 'R' in k]}")
print(f"\nKey findings will be summarized in next analysis step.")
```