### Development Step 34: Find and Download University of Leicester "Dragon’s Diet Fish Supply" Paper PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Academic researcher automating retrieval of niche University of Leicester PDFs to integrate specialized dragon‐diet fish supply data into comparative zoology studies
- Environmental consultancy harvesting campus‐hosted fishery impact assessments for sustainable aquaculture project proposals
- University librarian streamlining bulk download of institution‐archived theses and technical reports for digital preservation workflows
- Patent attorney extracting relevant technical documentation from le.ac.uk to validate prior art in biotechnical dragon‐diet patent applications
- Public health analyst collecting domain‐specific PDF guidelines on aquatic animal nutrition from academic repositories for policy development
- Corporate compliance officer monitoring emerging research papers on novel feed supplies hosted on university sites to update internal risk reports
- Graduate student compiling and annotating lecture readings by automatically fetching full-text PDFs of department-published papers into a reference manager
- Digital humanities scholar aggregating historical animal husbandry dissertations from Leicester’s online archive for text-mining and trend analysis

```
import os
import sys
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

# --- Configuration ---
workspace = 'workspace'
bing_html = os.path.join(workspace, 'bing_search.html')
candidates_out = os.path.join(workspace, 'found_pdf_urls.txt')

# --- Ensure workspace folder exists ---
if not os.path.isdir(workspace):
    os.makedirs(workspace, exist_ok=True)

# --- Step 1: Perform Bing search ---
query = "Can Hiccup Supply Enough Fish to Maintain a Dragon's Diet? filetype:pdf site:le.ac.uk"
bing_url = 'https://www.bing.com/search'
params = {'q': query}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"[INFO] Searching Bing for PDF with query:\n      {query}\n")
resp = requests.get(bing_url, params=params, headers=headers, timeout=30)
print(f"[INFO] Bing returned status: {resp.status_code}")
if resp.status_code != 200:
    print("ERROR: Bing search failed; exiting.")
    sys.exit(1)
# Save raw HTML
with open(bing_html, 'w', encoding='utf-8') as f:
    f.write(resp.text)
print(f"[SAVE] Search results → {bing_html}\n")

# --- Step 2: Parse HTML and extract .pdf URLs ---
print("[INFO] Parsing saved HTML for PDF links...")
with open(bing_html, 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html, 'html.parser')

pdf_urls = set()
# 2a) direct <a href> containing .pdf
print("[STEP] scanning <a> tags for .pdf hrefs...")
for a in soup.find_all('a', href=True):
    href = a['href']
    h = href.lower()
    if h.endswith('.pdf') or '.pdf?' in h:
        pdf_urls.add(href)
print(f"    found {len(pdf_urls)} literal .pdf href(s)")

# 2b) handle Bing redirects (/url?q=...)
print("[STEP] parsing Bing redirect links (/url?q=...)")
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.startswith('/url?') or 'bing.com/url?' in href:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        for key in ('q','u','url'):
            if key in qs:
                real = unquote(qs[key][0])
                lr = real.lower()
                if lr.endswith('.pdf') or '.pdf?' in lr:
                    pdf_urls.add(real)
                break
print(f"    total after redirect parsing: {len(pdf_urls)} url(s)")

# 2c) regex fallback over raw HTML
print("[STEP] running regex fallback for http(s)://...pdf patterns...")
# Corrected regex: double-quoted raw-string, no unescaped single-quote inside class
pattern = r"https?://[^\s"']+?\.pdf(?:\?[^"']*)?"
matches = re.findall(pattern, html, flags=re.IGNORECASE)
for m in matches:
    pdf_urls.add(m)
print(f"    total after regex fallback: {len(pdf_urls)} url(s)")

# 2d) filter for Leicester domains
print("[STEP] filtering for Leicester domains (le.ac.uk, lra.le.ac.uk, core.ac.uk, hdl.handle.net)...")
domains = ('le.ac.uk','lra.le.ac.uk','core.ac.uk','hdl.handle.net')
le_urls = [u for u in pdf_urls if any(d in u.lower() for d in domains)]
if le_urls:
    print(f"    found {len(le_urls)} Leicester-specific PDF url(s):")
    for u in le_urls:
        print(f"      - {u}")
else:
    print("    no domain-filtered URLs; listing all candidates:")
    for u in sorted(pdf_urls):
        print(f"      - {u}")

# Save candidates to file
with open(candidates_out, 'w', encoding='utf-8') as f:
    for u in sorted(pdf_urls):
        f.write(u + '\n')
print(f"\n[SAVE] all PDF candidates → {candidates_out}")
print("[COMPLETE] extraction phase. Inspect the list to choose the correct PDF link.")
```