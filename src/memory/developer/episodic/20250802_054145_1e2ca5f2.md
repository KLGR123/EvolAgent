### Development Step 29: Search and download Leicester's "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" PDF

**Description**: Conduct a web search for the University of Leicester paper titled "Can Hiccup Supply Enough Fish to Maintain a Dragon’s Diet?" and download the full PDF of that paper into workspace/dragon_diet_fish_bag.pdf.

**Use Cases**:
- Automated retrieval of pharmacology research PDFs for R&D literature reviews in drug discovery
- Extraction of patent PDF documents from search results for technology intelligence in a semiconductor startup
- Batch downloading of court opinion PDFs from search engine listings for a law firm’s intellectual property case database
- Collection of product datasheet PDFs from manufacturer sites for an e-commerce catalog’s technical specification updates
- Systematic harvesting of environmental impact report PDFs for sustainability consultants assessing corporate disclosures
- Scraping government regulation PDF documents from search results for a compliance team enforcing data privacy standards
- Aggregation of conference proceedings and thesis PDFs for a university library expanding its open-access repository
- Extraction of policy whitepaper PDFs for an NGO analyzing healthcare legislative proposals

```
import os
import sys
from bs4 import BeautifulSoup

# Configuration
workspace = 'workspace'
html_file = os.path.join(workspace, 'bing_search.html')
algo_links_file = os.path.join(workspace, 'bing_algo_links.txt')
all_links_file = os.path.join(workspace, 'bing_all_links.txt')

# Ensure prerequisites
if not os.path.isdir(workspace):
    print(f"ERROR: Workspace directory '{workspace}' not found. Please run the search script first.")
    sys.exit(1)
if not os.path.isfile(html_file):
    print(f"ERROR: HTML file not found at {html_file}. Please generate it with the debug script.")
    sys.exit(1)

print(f"[INFO] Loading Bing search HTML from: {html_file}\n")
with open(html_file, 'r', encoding='utf-8') as f:
    html = f.read()

# Parse HTML
soup = BeautifulSoup(html, 'html.parser')

# 1) Extract main search result links (li.b_algo > h2 > a)
algo_links = []
for li in soup.find_all('li', class_='b_algo'):
    a = li.find('h2') and li.find('h2').find('a')
    if a and a.has_attr('href'):
        title = a.get_text(strip=True)
        href = a['href']
        algo_links.append((title, href))

# 2) Extract all anchor links as fallback
all_links = []
for a in soup.find_all('a', href=True):
    text = a.get_text(strip=True)
    href = a['href']
    all_links.append((text, href))

# Report counts
print(f"[PARSE] Found {len(algo_links)} main result links (li.b_algo > h2 > a)")
print(f"[PARSE] Found {len(all_links)} total <a> tags with href\n")

# Print first 10 main result links for inspection
print("[SAMPLE] First 10 main search result links:")
for idx, (title, href) in enumerate(algo_links[:10], 1):
    print(f" {idx}. {title}\n    -> {href}")

# Print first 10 anchor links containing 'pdf' substring
pdf_in_anchors = [(t, u) for (t, u) in all_links if '.pdf' in u.lower()]
print(f"\n[PARSE] Found {len(pdf_in_anchors)} anchor hrefs containing '.pdf'; listing first 10:")
for idx, (text, href) in enumerate(pdf_in_anchors[:10], 1):
    print(f" {idx}. Text: '{text}'\n    -> {href}")

# Save outputs for manual review
with open(algo_links_file, 'w', encoding='utf-8') as f:
    for title, href in algo_links:
        f.write(f"{title}\t{href}\n")
print(f"\n[SAVE] Main algorithmic links written to: {algo_links_file}")

with open(all_links_file, 'w', encoding='utf-8') as f:
    for text, href in all_links:
        f.write(f"{text}\t{href}\n")
print(f"[SAVE] All anchor links written to: {all_links_file}")
```