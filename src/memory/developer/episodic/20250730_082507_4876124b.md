### Development Step 3: Locate 2018 Chinstrap Penguin Population Data on Wikipedia and Document Source, Units, and Context

**Description**: Search for and access the Wikipedia page about penguin species populations to locate the specific chinstrap penguin population data as of the end of 2018. Extract the population figure, noting whether it's given in breeding pairs or individual penguins, and document the exact source and context of this information.

**Use Cases**:
- Automated biodiversity monitoring dashboard for an environmental NGO to track and compare chinstrap penguin populations year over year
- Research pipeline for a polar ecology lab to integrate the 2018 IUCN population figures into species trend models
- Content management system for an educational website that auto-updates wildlife fact sheets with the latest penguin population data
- Data validation workflow for a conservation funding agency that cross-checks grant applications against official IUCN population estimates
- Media production tool for a nature documentary team that pulls precise penguin statistics and citations directly into scripts and graphics
- Policy briefing generator for government wildlife agencies that compiles exact 2018 chinstrap penguin numbers with source references
- Mobile field app for Antarctic researchers that preloads species population baselines to plan tagging and surveying efforts
- Data journalism toolkit for environmental reporters to source, verify, and visualize authoritative penguin population figures in news articles

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
import re

print("=== CHINSTRAP PENGUIN POPULATION RESEARCH - STEP 3 ===\n")
print("Objective: Fetch and parse Wikipedia pages to extract exact 2018 population data")
print("Focus: Get precise IUCN population estimate with full context and citation\n")

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# First, load the pages to fetch data
pages_file = 'workspace/wikipedia_pages_to_fetch.json'
if not os.path.exists(pages_file):
    print(f"❌ Pages file not found: {pages_file}")
    exit()

with open(pages_file, 'r') as f:
    pages_data = json.load(f)

print(f"Target pages to fetch: {len(pages_data.get('target_pages', []))}")

# Also identify additional high-value pages from our search analysis
high_priority_pages = [
    'https://en.wikipedia.org/wiki/Chinstrap_penguin',
    'https://en.wikipedia.org/wiki/List_of_Sphenisciformes_by_population'
]

print(f"High priority pages: {len(high_priority_pages)}")

all_extracted_data = []

# Fetch each Wikipedia page
for page_url in high_priority_pages:
    page_name = page_url.split('/')[-1].replace('_', ' ')
    print(f"\n--- FETCHING: {page_name} ---")
    print(f"URL: {page_url}")
    
    try:
        response = requests.get(page_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Save the HTML content
        safe_filename = page_url.split('/')[-1].lower()
        html_file = f'workspace/{safe_filename}.html'
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"✓ HTML saved to: {html_file}")
        
        # Extract key information
        page_title = soup.find('h1', class_='firstHeading')
        page_title_text = page_title.get_text().strip() if page_title else 'No title found'
        
        print(f"Page title: {page_title_text}")
        
        # Look for population data in the content
        page_text = soup.get_text()
        
        # Search for 2018 IUCN population data
        print(f"\n=== SEARCHING FOR 2018 POPULATION DATA ===")
        
        # Look for specific patterns mentioning 2018 and population
        population_patterns = [
            r'(In 2018[^.]*?population[^.]*?[0-9,]+[^.]*)\.',
            r'(2018[^.]*?IUCN[^.]*?population[^.]*?[0-9,]+[^.]*)\.',
            r'(IUCN[^.]*?2018[^.]*?population[^.]*?[0-9,]+[^.]*)\.',
            r'(population[^.]*?2018[^.]*?[0-9,]+[^.]*)\.',
            r'([0-9,]+ million[^.]*?chinstrap[^.]*)\.',
            r'(chinstrap[^.]*?[0-9,]+ million[^.]*)\.',
            r'([0-9,]+[^.]*?specimens[^.]*?chinstrap[^.]*)\.',
            r'(chinstrap[^.]*?[0-9,]+[^.]*?specimens[^.]*)\.',
        ]
        
        found_population_data = []
        
        for pattern in population_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                cleaned_match = ' '.join(match.split())  # Clean up whitespace
                if len(cleaned_match) > 20 and ('2018' in cleaned_match or '8' in cleaned_match):  # Filter meaningful matches
                    found_population_data.append({
                        'pattern_used': pattern,
                        'matched_text': cleaned_match,
                        'source_page': page_name
                    })
        
        print(f"Population data matches found: {len(found_population_data)}")
        
        for i, data in enumerate(found_population_data[:5], 1):  # Show top 5 matches
            print(f"  {i}. {data['matched_text'][:120]}...")
        
        # Look specifically for IUCN conservation status sections
        print(f"\n=== SEARCHING FOR CONSERVATION STATUS SECTIONS ===")
        
        # Find conservation status or similar sections
        conservation_sections = soup.find_all(['h2', 'h3', 'h4'], string=re.compile(r'conservation|status|population', re.IGNORECASE))
        
        print(f"Conservation-related sections found: {len(conservation_sections)}")
        
        conservation_content = []
        
        for section in conservation_sections:
            print(f"  Section: {section.get_text().strip()}")
            
            # Get the content following this heading
            next_elements = []
            current = section
            
            # Look for the next few paragraphs after the heading
            for _ in range(5):
                current = current.find_next_sibling()
                if current is None:
                    break
                if current.name in ['p', 'div', 'ul']:
                    text_content = current.get_text().strip()
                    if text_content and len(text_content) > 20:
                        next_elements.append(text_content)
                elif current.name in ['h1', 'h2', 'h3', 'h4']:  # Hit next section
                    break
            
            if next_elements:
                section_content = ' '.join(next_elements)
                if '2018' in section_content or 'population' in section_content.lower():
                    conservation_content.append({
                        'section_title': section.get_text().strip(),
                        'content': section_content[:500] + '...' if len(section_content) > 500 else section_content
                    })
        
        # Look for infobox data (often contains population figures)
        print(f"\n=== SEARCHING FOR INFOBOX DATA ===")
        
        infoboxes = soup.find_all(['table'], class_=re.compile(r'infobox', re.IGNORECASE))
        print(f"Infoboxes found: {len(infoboxes)}")
        
        infobox_data = []
        for infobox in infoboxes:
            rows = infobox.find_all('tr')
            for row in rows:
                row_text = row.get_text()
                if ('population' in row_text.lower() or '2018' in row_text or 
                    'conservation' in row_text.lower() or 'status' in row_text.lower()):
                    cleaned_text = ' '.join(row_text.split())
                    if len(cleaned_text) > 10:
                        infobox_data.append(cleaned_text)
        
        print(f"Relevant infobox entries: {len(infobox_data)}")
        for entry in infobox_data[:3]:  # Show first 3
            print(f"  - {entry[:100]}...")
        
        # Store all extracted data for this page
        page_extraction = {
            'page_name': page_name,
            'page_url': page_url,
            'page_title': page_title_text,
            'html_file': html_file,
            'population_matches': found_population_data,
            'conservation_sections': conservation_content,
            'infobox_entries': infobox_data,
            'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        all_extracted_data.append(page_extraction)
        
    except Exception as e:
        print(f"❌ Error fetching {page_name}: {str(e)}")
        
        # Still record the attempt
        all_extracted_data.append({
            'page_name': page_name,
            'page_url': page_url,
            'error': str(e),
            'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    
    time.sleep(1)  # Be respectful to Wikipedia

# Save all extracted data
final_extraction = {
    'research_objective': 'Extract chinstrap penguin population data as of end 2018',
    'extraction_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'pages_processed': len(all_extracted_data),
    'pages_data': all_extracted_data
}

with open('workspace/chinstrap_penguin_population_extraction.json', 'w') as f:
    json.dump(final_extraction, f, indent=2)

print(f"\n=== EXTRACTION SUMMARY ===\n")
print(f"Pages processed: {final_extraction['pages_processed']}")
print(f"Extraction data saved to: workspace/chinstrap_penguin_population_extraction.json")

# Analyze the most promising population data found
print(f"\n=== ANALYZING MOST PROMISING POPULATION DATA ===\n")

all_population_matches = []
for page_data in all_extracted_data:
    if 'population_matches' in page_data:
        for match in page_data['population_matches']:
            all_population_matches.append({
                'source_page': page_data['page_name'],
                'text': match['matched_text']
            })

print(f"Total population data matches across all pages: {len(all_population_matches)}")

# Filter for the most relevant matches (containing 2018 and specific numbers)
best_matches = []
for match in all_population_matches:
    text = match['text'].lower()
    if ('2018' in text and 'iucn' in text and 
        ('8 million' in text or '8,000,000' in text or 'million' in text)):
        best_matches.append(match)

print(f"\nBest matches (2018 + IUCN + population figures): {len(best_matches)}")

for i, match in enumerate(best_matches, 1):
    print(f"\n{i}. Source: {match['source_page']}")
    print(f"   Text: {match['text']}")

print(f"\n✓ Wikipedia pages fetched and parsed successfully")
print(f"✓ Population data extraction complete")
print(f"\nNext step: Analyze extracted data to identify the exact 2018 IUCN population figure")
```