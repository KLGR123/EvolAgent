### Development Step 18: Locate 2011 Bulgarian Census Data on Tertiary Education Completion by Gender

**Description**: Search for the 2011 Bulgarian census data on tertiary education completion by gender. Look for official census results from Bulgaria's National Statistical Institute or other authoritative sources that provide the specific breakdown of men and women who completed tertiary education in 2011.

**Use Cases**:
- Government education ministry automating the retrieval of 2011 census tertiary education data by gender to shape targeted higher-education policies and budget allocations
- Academic demography team extracting male vs. female university completion rates from official NSI reports for a comparative study on Balkan education trends
- NGO program analysts integrating Bulgaria’s 2011 gender-disaggregated tertiary attainment figures into monitoring dashboards for women’s empowerment initiatives
- Data journalism unit scraping and visualizing 2011 census tables on higher education by gender to publish an interactive story on Bulgaria’s education gap
- HR consulting firm analyzing regional tertiary education completion rates by gender to advise multinational clients on talent sourcing strategies in Bulgaria
- EdTech platform automating ingestion of 2011 census gender-specific higher-education statistics to tailor scholarship matching and mentorship programs
- International development agency pulling historical gender-disaggregated tertiary data to assess progress on UNESCO and UN SDG education targets
- Business intelligence team embedding NSI-sourced tertiary education metrics by gender into workforce planning tools for strategic market expansion

```
import os
import json
from bs4 import BeautifulSoup
import requests
import time

print("=== BULGARIAN 2011 CENSUS TERTIARY EDUCATION BY GENDER - DIRECT SEARCH ===")
print("Strategy: Direct web search for official Bulgarian census data\n")

# First, let's check what we have in workspace and understand the structure
workspace_path = 'workspace'
if os.path.exists(workspace_path):
    files = os.listdir(workspace_path)
    print(f"Workspace contains {len(files)} files")
    
    # Look for our analysis results to understand what we found
    analysis_files = [f for f in files if 'analysis' in f.lower() and f.endswith('.json')]
    print(f"Analysis files found: {len(analysis_files)}")
    
    if analysis_files:
        # Inspect the most recent analysis file
        latest_analysis = analysis_files[-1]
        print(f"\nInspecting: {latest_analysis}")
        
        with open(os.path.join(workspace_path, latest_analysis), 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        print(f"Analysis file keys: {list(analysis_data.keys())}")
        if 'detailed_findings' in analysis_data:
            print(f"Files with findings: {len(analysis_data['detailed_findings'])}")
            
            # Show what we found
            for finding in analysis_data['detailed_findings']:
                print(f"  - {finding['filename']}: {finding['relevance_score']}/6 relevance")
                print(f"    Sentences: {len(finding.get('good_sentences', []))}, Tables: {len(finding.get('education_tables', []))}")
else:
    os.makedirs(workspace_path, exist_ok=True)

print("\n=== SEARCHING FOR OFFICIAL BULGARIAN 2011 CENSUS DATA ===")
print("Targeting Bulgarian National Statistical Institute (NSI) and official census reports\n")

# Search for official Bulgarian census data using SerpAPI
api_key = os.getenv("SERPAPI_API_KEY")

if not api_key:
    print("No SerpAPI key available. Using direct URL approach for Bulgarian NSI.")
    
    # Direct URLs to Bulgarian National Statistical Institute census data
    official_urls = [
        "https://www.nsi.bg/en/content/6710/population-education",
        "https://www.nsi.bg/en/content/6704/population-census-2011", 
        "https://www.nsi.bg/en/content/2011-population-census-main-results",
        "https://www.nsi.bg/sites/default/files/files/data/timeseries/Education_1.1.xls",
        "https://www.nsi.bg/sites/default/files/files/publications/Census2011final_en.pdf"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    census_data_found = []
    
    for url in official_urls:
        print(f"Accessing: {url}")
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            print(f"  Status: {response.status_code}")
            
            if response.status_code == 200:
                # Save the content
                filename = url.split('/')[-1]
                if not filename or '.' not in filename:
                    filename = url.replace('https://', '').replace('/', '_') + '.html'
                
                filepath = os.path.join(workspace_path, f"nsi_official_{filename}")
                
                # Handle different content types
                if url.endswith('.pdf'):
                    with open(filepath.replace('.html', '.pdf'), 'wb') as f:
                        f.write(response.content)
                    print(f"  Saved PDF: {filepath.replace('.html', '.pdf')}")
                elif url.endswith('.xls'):
                    with open(filepath.replace('.html', '.xls'), 'wb') as f:
                        f.write(response.content)
                    print(f"  Saved Excel: {filepath.replace('.html', '.xls')}")
                else:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    print(f"  Saved HTML: {filepath}")
                    
                    # Quick analysis for HTML content
                    soup = BeautifulSoup(response.text, 'html.parser')
                    text_content = soup.get_text().lower()
                    
                    # Check for tertiary education and gender data
                    has_tertiary = 'tertiary' in text_content or 'higher education' in text_content
                    has_gender = any(term in text_content for term in ['men', 'women', 'male', 'female'])
                    has_2011 = '2011' in text_content
                    has_education = 'education' in text_content
                    
                    relevance_score = sum([has_tertiary, has_gender, has_2011, has_education])
                    print(f"  Content relevance: {relevance_score}/4 (Tertiary: {has_tertiary}, Gender: {has_gender}, 2011: {has_2011}, Education: {has_education})")
                    
                    if relevance_score >= 2:
                        census_data_found.append({
                            'url': url,
                            'filename': filepath,
                            'relevance': relevance_score,
                            'content_length': len(response.text)
                        })
            else:
                print(f"  Failed to access: {response.status_code}")
                
        except Exception as e:
            print(f"  Error: {str(e)}")
        
        time.sleep(2)  # Be respectful to the server
        print()
else:
    print("Using SerpAPI to search for Bulgarian 2011 census tertiary education data")
    
    # Search queries targeting specific Bulgarian census data
    search_queries = [
        'Bulgaria 2011 census tertiary education men women statistics site:nsi.bg',
        '"Bulgarian 2011 census" "tertiary education" "male" "female" filetype:pdf',
        'Bulgaria census 2011 higher education completion gender statistics',
        '"National Statistical Institute Bulgaria" 2011 census education attainment gender'
    ]
    
    census_data_found = []
    
    for query in search_queries:
        print(f"Searching: {query}")
        
        params = {
            "q": query,
            "api_key": api_key,
            "engine": "google",
            "num": 10
        }
        
        try:
            response = requests.get("https://serpapi.com/search.json", params=params)
            
            if response.status_code == 200:
                results = response.json()
                
                if results.get("organic_results"):
                    print(f"  Found {len(results['organic_results'])} results")
                    
                    for i, result in enumerate(results['organic_results'][:3], 1):
                        print(f"  {i}. {result.get('title', 'No title')}")
                        print(f"     URL: {result.get('link', 'No URL')}")
                        print(f"     Snippet: {result.get('snippet', 'No snippet')[:150]}...")
                        
                        # Try to fetch the actual content
                        if result.get('link'):
                            try:
                                content_response = requests.get(result['link'], headers=headers, timeout=10)
                                if content_response.status_code == 200:
                                    filename = f"search_result_{i}_{result['link'].split('/')[-1]}.html"
                                    filepath = os.path.join(workspace_path, filename)
                                    
                                    with open(filepath, 'w', encoding='utf-8') as f:
                                        f.write(content_response.text)
                                    
                                    census_data_found.append({
                                        'query': query,
                                        'title': result.get('title'),
                                        'url': result.get('link'),
                                        'filename': filepath,
                                        'snippet': result.get('snippet')
                                    })
                                    
                                    print(f"     Saved: {filepath}")
                            except Exception as e:
                                print(f"     Error fetching content: {str(e)}")
                        print()
                else:
                    print(f"  No results found for: {query}")
            else:
                print(f"  Search API error: {response.status_code}")
                
        except Exception as e:
            print(f"  Search error: {str(e)}")
        
        time.sleep(3)  # Rate limiting
        print()

# Analyze any new census data we found
if census_data_found:
    print(f"\n=== ANALYZING {len(census_data_found)} NEW CENSUS SOURCES ===")
    
    tertiary_education_findings = []
    
    for source in census_data_found:
        print(f"\nAnalyzing: {source.get('filename', source.get('url'))}")
        
        if source.get('filename') and os.path.exists(source['filename']):
            try:
                with open(source['filename'], 'r', encoding='utf-8') as f:
                    content = f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                text_content = soup.get_text()
                
                print(f"  Content length: {len(text_content):,} characters")
                
                # Search for specific tertiary education statistics by gender
                patterns = [
                    r'tertiary education[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                    r'(?:men|women|male|female)[^.]*?tertiary education[^.]*?(\d+[.,]?\d*\s*%?)',
                    r'higher education[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                    r'university[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)',
                    r'2011[^.]*?(?:tertiary|higher)[^.]*?education[^.]*?(?:men|women|male|female)[^.]*?(\d+[.,]?\d*\s*%?)'
                ]
                
                matches_found = []
                for pattern in patterns:
                    matches = re.finditer(pattern, text_content, re.IGNORECASE)
                    for match in matches:
                        context_start = max(0, match.start() - 300)
                        context_end = min(len(text_content), match.end() + 300)
                        context = text_content[context_start:context_end]
                        
                        if 'bulgaria' in context.lower():
                            matches_found.append({
                                'match': match.group(),
                                'context': context,
                                'pattern': pattern
                            })
                
                print(f"  Statistical matches found: {len(matches_found)}")
                
                if matches_found:
                    for i, match in enumerate(matches_found[:3], 1):
                        print(f"  Match {i}: {match['match']}")
                        print(f"  Context: {match['context'][:200]}...")
                        print()
                    
                    tertiary_education_findings.extend(matches_found)
                
                # Look for tables with education data
                tables = soup.find_all('table')
                education_tables = []
                
                for table in tables:
                    table_text = table.get_text().lower()
                    if ('education' in table_text and 
                        any(term in table_text for term in ['men', 'women', 'male', 'female']) and
                        '2011' in table_text):
                        
                        headers = [th.get_text().strip() for th in table.find_all('th')]
                        rows = table.find_all('tr')
                        
                        education_tables.append({
                            'headers': headers[:10],  # First 10 headers
                            'row_count': len(rows)
                        })
                
                if education_tables:
                    print(f"  Education tables found: {len(education_tables)}")
                    for i, table in enumerate(education_tables[:2], 1):
                        print(f"  Table {i} headers: {table['headers'][:5]}")
                        print(f"  Table {i} rows: {table['row_count']}")
                
            except Exception as e:
                print(f"  Error analyzing {source.get('filename')}: {str(e)}")
    
    # Save comprehensive findings
    final_results = {
        'search_objective': 'Bulgarian 2011 census tertiary education completion by gender',
        'search_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'sources_analyzed': len(census_data_found),
        'statistical_matches_found': len(tertiary_education_findings),
        'new_sources': census_data_found,
        'tertiary_education_findings': tertiary_education_findings
    }
    
    results_file = os.path.join(workspace_path, 'bulgarian_2011_census_final_search_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n=== FINAL RESULTS SUMMARY ===")
    print(f"Sources analyzed: {len(census_data_found)}")
    print(f"Statistical matches found: {len(tertiary_education_findings)}")
    print(f"Results saved to: {results_file}")
    
    if tertiary_education_findings:
        print("\n=== KEY FINDINGS ===")
        for i, finding in enumerate(tertiary_education_findings[:5], 1):
            print(f"{i}. {finding['match']}")
            print(f"   Context: {finding['context'][:150]}...")
            print()
else:
    print("\n=== NO NEW CENSUS DATA FOUND ===")
    print("Unable to access new official Bulgarian census sources.")
    print("The previous analysis of saved sources is the most comprehensive available.")

print("\n=== SEARCH COMPLETE ===")
print("Bulgarian 2011 census tertiary education by gender search completed")
```