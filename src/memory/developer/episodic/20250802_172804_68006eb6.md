### Development Step 3: Find Dinosaur Wikipedia Featured Articles Promoted in November 2016 with Nomination Details

**Description**: Search for Featured Articles on English Wikipedia that were promoted in November 2016, specifically focusing on dinosaur-related articles. Use targeted web searches with queries like 'Wikipedia Featured Articles November 2016 dinosaur', 'site:en.wikipedia.org Featured Article candidates November 2016 dinosaur', and 'Wikipedia FAC promoted November 2016 paleontology'. Look for the Wikipedia Featured Article log, archives, or candidate pages that show articles promoted during that specific month. Extract information about any dinosaur articles that achieved Featured Article status in November 2016, including the article title and nomination details.

**Use Cases**:
- Automated museum exhibit updater that scans Wikipedia Featured Article logs for newly promoted dinosaur pages in November 2016 and integrates updated species descriptions and images into the digital display system
- Paleontology research trend analyzer that parses historical FA nominations for dinosaur-related terms to quantify public interest peaks in Cretaceous vs. Jurassic species over time
- Educational content planner for a science blog that automatically extracts titles and nomination details of November 2016 dinosaur Featured Articles to schedule themed blog posts and social media campaigns
- Digital archive curator for a university library that identifies authoritative dinosaur Wikipedia Featured Articles from November 2016 and imports metadata into the libraryâ€™s digital repository catalog
- SEO timing tool for a natural history publisher that monitors the promotion dates of dinosaur articles to optimize blog releases, backlinks, and keyword bids around newly â€œfeaturedâ€ species
- Paleontology news alert system that scrapes FA log archives for dinosaur promotions and sends real-time notifications to newsletter subscribers and press offices
- Data pipeline for a paleontology educational app that periodically fetches and parses Wikipedia FA logs to update its in-app dinosaur encyclopedia entries with the latest featured content

```
import os
import json
from datetime import datetime

print("=== INSPECTING EXISTING WORKSPACE DATA ===\n")
print("Objective: Analyze saved Wikipedia FA data and continue systematic search\n")

# Find existing workspace directories
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace_')]
print(f"Found {len(workspace_dirs)} workspace directories:")
for i, ws_dir in enumerate(workspace_dirs, 1):
    print(f"  {i}. {ws_dir}")
    if os.path.exists(ws_dir):
        files = os.listdir(ws_dir)
        print(f"     Files: {files}")
        for file in files:
            file_path = os.path.join(ws_dir, file)
            if os.path.isfile(file_path):
                size = os.path.getsize(file_path)
                print(f"       - {file}: {size:,} bytes")
    print()

# Use the most recent workspace or create new one
if workspace_dirs:
    # Sort by creation time and use the most recent
    workspace_dirs.sort()
    workspace_dir = workspace_dirs[-1]
    print(f"Using most recent workspace: {workspace_dir}\n")
else:
    workspace_dir = f"workspace_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(workspace_dir, exist_ok=True)
    print(f"Created new workspace: {workspace_dir}\n")

# Check if we have the FA log HTML file from previous execution
fa_log_file = os.path.join(workspace_dir, 'fa_log_1.html')
if os.path.exists(fa_log_file):
    print(f"=== ANALYZING SAVED FA LOG DATA ===\n")
    print(f"Found saved FA log file: {os.path.basename(fa_log_file)}")
    print(f"File size: {os.path.getsize(fa_log_file):,} bytes")
    
    # Read and parse the HTML content
    with open(fa_log_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    print(f"HTML content loaded: {len(html_content):,} characters")
    
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Extract the page title
    title = soup.find('title')
    if title:
        print(f"Page title: {title.get_text().strip()}")
    
    # Look for November 2016 specific content
    page_text = soup.get_text().lower()
    print(f"\n=== SEARCHING FOR DINOSAUR FEATURED ARTICLES IN NOVEMBER 2016 ===\n")
    
    # More comprehensive dinosaur-related terms
    dinosaur_terms = [
        'dinosaur', 'paleontology', 'fossil', 'cretaceous', 'jurassic', 'triassic',
        'prehistoric', 'extinct', 'reptile', 'paleontologist', 'mesozoic',
        'allosaurus', 'tyrannosaurus', 'triceratops', 'stegosaurus', 'diplodocus',
        'velociraptor', 'spinosaurus', 'carnotaurus', 'therizinosaurus', 'parasaurolophus'
    ]
    
    found_terms = []
    for term in dinosaur_terms:
        if term in page_text:
            # Count occurrences
            count = page_text.count(term)
            found_terms.append((term, count))
            print(f"ğŸ¦• Found '{term}': {count} occurrences")
    
    print(f"\nTotal dinosaur-related terms found: {len(found_terms)}")
    
    # Now let's look for specific article titles and promotion information
    print(f"\n=== EXTRACTING FEATURED ARTICLE PROMOTIONS FROM NOVEMBER 2016 ===\n")
    
    # Look for promoted articles section
    promoted_articles = []
    
    # Find all links to Wikipedia articles
    article_links = []
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        if href.startswith('/wiki/') and ':' not in href.split('/')[-1]:
            link_text = link.get_text().strip()
            if link_text and len(link_text) > 2:  # Filter out very short links
                article_links.append({
                    'title': link_text,
                    'href': href,
                    'url': f'https://en.wikipedia.org{href}'
                })
    
    print(f"Found {len(article_links)} total Wikipedia article links")
    
    # Filter for potential dinosaur articles
    potential_dinosaur_articles = []
    for link in article_links:
        link_title_lower = link['title'].lower()
        if any(term in link_title_lower for term in dinosaur_terms):
            potential_dinosaur_articles.append(link)
            print(f"ğŸ¦• Potential dinosaur article: {link['title']}")
    
    print(f"\nFound {len(potential_dinosaur_articles)} potential dinosaur articles")
    
    # Look for specific patterns that indicate article promotions
    print(f"\n=== SEARCHING FOR PROMOTION PATTERNS ===\n")
    
    # Split content into lines and look for promotion announcements
    lines = html_content.split('\n')
    promotion_lines = []
    
    for i, line in enumerate(lines):
        line_lower = line.lower()
        # Look for lines that might indicate article promotions
        if any(pattern in line_lower for pattern in ['promoted', 'featured', 'november', '2016']):
            # Check if this line also contains dinosaur terms
            if any(term in line_lower for term in dinosaur_terms):
                promotion_lines.append({
                    'line_number': i + 1,
                    'content': line.strip(),
                    'contains_dinosaur_terms': [term for term in dinosaur_terms if term in line_lower]
                })
                print(f"ğŸ¯ Line {i+1}: {line.strip()[:200]}...")
                print(f"    Dinosaur terms: {[term for term in dinosaur_terms if term in line_lower]}")
    
    print(f"\nFound {len(promotion_lines)} lines with promotion + dinosaur content")
    
    # Look for table rows or list items that might contain the actual promoted articles
    print(f"\n=== ANALYZING TABLES AND LISTS FOR PROMOTED ARTICLES ===\n")
    
    # Find tables that might contain the promoted articles list
    tables = soup.find_all('table')
    relevant_tables = []
    
    for i, table in enumerate(tables):
        table_text = table.get_text().lower()
        if 'november' in table_text and '2016' in table_text:
            # Check if this table contains dinosaur terms
            dinosaur_terms_in_table = [term for term in dinosaur_terms if term in table_text]
            if dinosaur_terms_in_table:
                relevant_tables.append({
                    'table_index': i,
                    'dinosaur_terms': dinosaur_terms_in_table,
                    'text_preview': table_text[:500]
                })
                print(f"ğŸ“Š Table {i}: Contains {dinosaur_terms_in_table}")
                print(f"    Preview: {table_text[:200]}...\n")
    
    print(f"Found {len(relevant_tables)} tables with November 2016 + dinosaur content")
    
    # Look for unordered lists that might contain promoted articles
    lists = soup.find_all(['ul', 'ol'])
    relevant_lists = []
    
    for i, list_elem in enumerate(lists):
        list_text = list_elem.get_text().lower()
        if any(term in list_text for term in dinosaur_terms):
            relevant_lists.append({
                'list_index': i,
                'list_type': list_elem.name,
                'dinosaur_terms': [term for term in dinosaur_terms if term in list_text],
                'text_preview': list_text[:300]
            })
            print(f"ğŸ“‹ {list_elem.name.upper()} {i}: Contains dinosaur terms")
            print(f"    Terms: {[term for term in dinosaur_terms if term in list_text]}")
            print(f"    Preview: {list_text[:150]}...\n")
    
    print(f"Found {len(relevant_lists)} lists with dinosaur content")
    
    # Compile comprehensive analysis results
    analysis_results = {
        'analysis_metadata': {
            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'source_file': os.path.basename(fa_log_file),
            'source_size_bytes': os.path.getsize(fa_log_file),
            'html_content_length': len(html_content)
        },
        'dinosaur_terms_analysis': {
            'terms_searched': dinosaur_terms,
            'terms_found': found_terms,
            'total_terms_found': len(found_terms)
        },
        'article_links_analysis': {
            'total_article_links': len(article_links),
            'potential_dinosaur_articles': potential_dinosaur_articles,
            'dinosaur_articles_count': len(potential_dinosaur_articles)
        },
        'promotion_patterns': {
            'promotion_lines_found': promotion_lines,
            'promotion_lines_count': len(promotion_lines)
        },
        'structural_analysis': {
            'relevant_tables': relevant_tables,
            'relevant_lists': relevant_lists,
            'tables_with_dinosaur_content': len(relevant_tables),
            'lists_with_dinosaur_content': len(relevant_lists)
        }
    }
    
    # Save detailed analysis
    analysis_file = os.path.join(workspace_dir, 'fa_november_2016_dinosaur_analysis.json')
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“ Detailed analysis saved to: {os.path.basename(analysis_file)}")
    print(f"File size: {os.path.getsize(analysis_file):,} bytes")
    
else:
    print("âŒ No saved FA log HTML file found from previous execution")
    print("Need to re-run the Wikipedia search")

print(f"\n=== ANALYSIS SUMMARY ===\n")
if 'analysis_results' in locals():
    print(f"âœ… Successfully analyzed saved Wikipedia FA log data")
    print(f"ğŸ¦• Dinosaur terms found: {analysis_results['dinosaur_terms_analysis']['total_terms_found']}")
    print(f"ğŸ”— Potential dinosaur articles: {analysis_results['article_links_analysis']['dinosaur_articles_count']}")
    print(f"ğŸ“Š Tables with dinosaur content: {analysis_results['structural_analysis']['tables_with_dinosaur_content']}")
    print(f"ğŸ“‹ Lists with dinosaur content: {analysis_results['structural_analysis']['lists_with_dinosaur_content']}")
    print(f"ğŸ¯ Promotion pattern matches: {analysis_results['promotion_patterns']['promotion_lines_count']}")
    
    if analysis_results['article_links_analysis']['dinosaur_articles_count'] > 0:
        print(f"\nğŸ‰ POTENTIAL DINOSAUR FEATURED ARTICLES FOUND:")
        for article in analysis_results['article_links_analysis']['potential_dinosaur_articles']:
            print(f"  - {article['title']} ({article['url']})")
    
    if analysis_results['promotion_patterns']['promotion_lines_count'] > 0:
        print(f"\nğŸ¯ PROMOTION PATTERN MATCHES:")
        for pattern in analysis_results['promotion_patterns']['promotion_lines_found'][:3]:  # Show first 3
            print(f"  - Line {pattern['line_number']}: {pattern['content'][:100]}...")
            print(f"    Dinosaur terms: {pattern['contains_dinosaur_terms']}")
else:
    print("âŒ No analysis results available")

print(f"\nâœ… Analysis completed. Ready for next steps or manual verification.")
```