### Development Step 57: Extract Survivor US Seasons 1–44 Winners from Wikipedia into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Automating a fan‐run Survivor trivia mobile app: fetch the latest season–winner JSON nightly and update in‐app quiz questions without manual edits
- Powering an Alexa skill or Google Assistant action: query the workspace/survivor_winners_list.json to answer “Who won Survivor Season 17?” in real time
- Generating dynamic content for a reality TV blog: pull winner data to auto‐populate “On This Day” posts celebrating past champions
- Feeding an entertainment analytics dashboard: ingest the JSON into a BI tool to chart demographic or voting trends across all 44 seasons
- Building a Slack or Discord bot for TV show communities: respond to user commands like “!winner 10” by reading from the scraped winners list
- Supporting academic research on media representation: supply a structured dataset of Survivor winners for statistical analysis of gender, age, or ethnicity patterns
- Automating social media anniversary tweets: schedule daily posts that reference the winner from the corresponding season’s original air date
- Informing casting strategy for competing networks: maintain up‐to‐date winner records to analyze the types of contestants who resonate most with audiences

```
import os
import sys
import json
import re
import requests
from bs4 import BeautifulSoup

# 1) Identify the active workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not workspace_dirs:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
workspace_dir = max(workspace_dirs, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the Survivor page
url = "https://en.wikipedia.org/wiki/Survivor_(American_TV_series)"
print(f"Fetching Survivor page...\nURL: {url}\n")
resp = requests.get(url, headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Language': 'en-US'
})
try:
    resp.raise_for_status()
    print(f"→ HTTP Status: {resp.status_code} OK\n")
except Exception as e:
    print(f"❌ Failed to fetch page: {e}")
    sys.exit(1)

# 3) Parse HTML
soup = BeautifulSoup(resp.text, 'html.parser')

# 4) Locate the table containing Season and Winner columns
wikitables = soup.find_all('table', class_='wikitable')
print(f"Found {len(wikitables)} wikitables on the page. Scanning for header-based match...\n")
target_table = None
for idx, tbl in enumerate(wikitables, start=1):
    # Inspect header row
    header = tbl.find('tr')
    if not header:
        continue
    headers = [cell.get_text(strip=True).lower() for cell in header.find_all(['th','td'])]
    print(f"Table {idx} headers = {headers[:7]}")
    if 'season' in headers and any('winner' in h for h in headers):
        target_table = tbl
        print(f"→ Matched table {idx} by headers ('season' & 'winner').\n")
        break

if not target_table:
    print("❌ Could not find a table with both 'Season' and 'Winner' headers. Exiting.")
    sys.exit(1)

# 5) Determine column indices dynamically
header_cells = target_table.find('tr').find_all(['th','td'])
col_texts = [c.get_text(strip=True) for c in header_cells]
col_lower = [t.lower() for t in col_texts]
print(f"Detected columns: {col_texts}\n")
# season column index
try:
    season_idx = col_lower.index('season')
except ValueError:
    season_idx = next((i for i,t in enumerate(col_lower) if 'season' in t), None)
# winner column index (first containing 'winner')
winner_idx = next((i for i,t in enumerate(col_lower) if 'winner' in t), None)
if season_idx is None or winner_idx is None:
    print("❌ Could not determine 'season' or 'winner' column indices. Exiting.")
    sys.exit(1)
print(f"Season column at index {season_idx}, Winner column at index {winner_idx}\n")

# 6) Extract rows for seasons 1–44, with fallback for numeric vote counts
numeric_pattern = re.compile(r'^[\d–\-\s]+$')
data = []
rows = target_table.find_all('tr')[1:]  # skip header row
for i, row in enumerate(rows, start=1):
    cells = row.find_all(['th','td'])
    if len(cells) <= max(season_idx, winner_idx):
        continue
    # parse season number
    season_text = cells[season_idx].get_text(' ', strip=True).split()[0]
    try:
        season_num = int(season_text)
    except ValueError:
        continue
    if not (1 <= season_num <= 44):
        continue
    # primary winner text
    primary = cells[winner_idx].get_text(' ', strip=True)
    # fallback if primary is empty or just vote count
    if not primary or numeric_pattern.fullmatch(primary):
        fallback = None
        for j, cell in enumerate(cells):
            if j == season_idx:
                continue
            link = cell.find('a')
            candidate = link.get_text(strip=True) if link else cell.get_text(' ', strip=True)
            if any(ch.isalpha() for ch in candidate):
                fallback = candidate
                print(f"  Season {season_num} fallback -> cell {j}: '{fallback}' (primary was '{primary}')")
                break
        winner_name = fallback or primary
    else:
        winner_name = primary
    data.append({'season': season_num, 'winner': winner_name})
    print(f"Parsed Season {season_num}: {winner_name}")

# 7) Validate count and save JSON
if len(data) != 44:
    print(f"⚠️ Warning: Parsed {len(data)} seasons (expected 44). Please verify.")
sorted_data = sorted(data, key=lambda x: x['season'])
out_file = os.path.join(workspace_dir, 'survivor_winners_list.json')
with open(out_file, 'w', encoding='utf-8') as f:
    json.dump(sorted_data, f, indent=2, ensure_ascii=False)
print(f"\n✅ Wrote {len(sorted_data)} season–winner entries to: {out_file}")
```