# Developer Plan 02

## Plan
Search for the specific book title written by James Beard Award winners Jane and Michael Stern that contains their recommendation of Papa Felipe's Mexican Restaurant. Focus on identifying their 2009 publication or other books where they recommended this Albuquerque restaurant that Ali Khan visited on 'Cheap Eats'. Extract the complete title of the book, ensuring any numbers in the title are written in plain text format as requested.

## Description
This is the necessary next step because: (1) The previous research successfully identified that Papa Felipe's Mexican Restaurant in Albuquerque was the restaurant Ali Khan visited on his 2015 'Cheap Eats' show and that James Beard Award winners Jane and Michael Stern recommended it in a 2009 publication, (2) We now need to find the exact complete title of the book containing their recommendation, (3) Expected outcome is to obtain the specific book title with any numbers written in plain text format, (4) This directly addresses the core TASK requirement of identifying the complete book title by the two James Beard Award winners

## Episodic Examples
### Development Step 18: Find 2009 Reissue Publisher of Martineau & Atkinsonâ€™s Letters on Manâ€™s Nature and Development

**Description**: Verify the specific publisher that reissued 'Letters on the Laws of Man's Nature and Development' by Harriet Martineau and Henry George Atkinson in 2009. The previous search confirmed this is the correct 1851 co-authored book on atheistic naturalism, phrenology, and mesmerism, but we need to identify the exact 2009 reissue publisher. Search for '2009 reissue Letters on the Laws of Man's Nature and Development publisher', 'Martineau Atkinson 2009 republication', and check academic publishers, university presses, or specialty reprint publishers that may have reissued this historical work in 2009.

**Use Cases**:
- University library catalog metadata reconciliation: automatically parsing JSON and HTML records in the libraryâ€™s digital repository to confirm and correct the 2009 reissue publisher of nineteenth-century works (e.g., Martineau & Atkinson) for accurate OPAC entries
- Academic research group bibliographic verification: deploying regex-driven scans across downloaded archive files and search-result HTML to validate specialized reprint publishers of historical treatises before inclusion in scholarly databases
- Rare books dealer inventory validation: running workspace directory inspections and content-analysis scripts to detect 2009 reissues from specialty presses (such as Nabu Press or Kessinger Publishing) and tag inventory records with precise publisher information
- Digital humanities project metadata extraction: integrating BeautifulSoup-based HTML parsing and JSON analysis workflows to harvest publisher details from a corpus of 2009 reprinted nineteenth-century texts for an online exhibitâ€™s catalog
- Publishing house competitive monitoring: automating web-crawled HTML and local JSON file evaluation to track market entries of specialty reprint editions of classic philosophical works and log their publishers for strategy reports
- Course syllabus compilation for literature programs: scanning PDF syllabi and JSON course package files with regex patterns to ensure the correct 2009 edition and publisher name appear in required reading lists
- National library legal deposit compliance auditing: using scripted directory scans and publisher-pattern extraction to verify that electronically deposited reissued editions include accurate 2009 publisher metadata for archival requirements
- Reference management system enrichment: applying automated analysis of downloaded citation JSON and associated HTML sources to populate and correct the publisher field for a 2009 reissue in researchersâ€™ EndNote or Zotero libraries

```
import os
import json
from bs4 import BeautifulSoup
import re

print('=== MANUAL ANALYSIS OF WORKSPACE FILES FOR 2009 MARTINEAU-ATKINSON PUBLISHER ===')
print('Objective: Extract 2009 reissue publisher from existing search files')
print('Book: Letters on the Laws of Man\'s Nature and Development')
print('Authors: Harriet Martineau and Henry George Atkinson')
print('\n' + '='*100 + '\n')

# First, let's inspect what files we actually have in workspace
print('=== STEP 1: INSPECTING WORKSPACE DIRECTORY STRUCTURE ===')

if not os.path.exists('workspace'):
    print('âŒ Workspace directory does not exist')
else:
    workspace_files = os.listdir('workspace')
    print(f'Total files in workspace: {len(workspace_files)}')
    
    # Categorize files - FIXED: Define file_lower properly
    json_files = []
    html_files = []
    txt_files = []
    other_files = []
    
    for file in workspace_files:
        if file.endswith('.json'):
            json_files.append(file)
        elif file.endswith('.html'):
            html_files.append(file)
        elif file.endswith('.txt'):
            txt_files.append(file)
        else:
            other_files.append(file)
    
    print(f'\nFile breakdown:')
    print(f'  JSON files: {len(json_files)}')
    print(f'  HTML files: {len(html_files)}')
    print(f'  TXT files: {len(txt_files)}')
    print(f'  Other files: {len(other_files)}')
    
    # Show recent files that might contain relevant information
    print('\nRecent JSON analysis files:')
    for json_file in sorted(json_files)[-5:]:  # Last 5 JSON files
        file_path = os.path.join('workspace', json_file)
        file_size = os.path.getsize(file_path)
        print(f'  - {json_file} ({file_size:,} bytes)')
    
    # Look for files that might contain book/publisher information - FIXED
    relevant_files = []
    for file in workspace_files:
        file_lower = file.lower()  # Define file_lower for each iteration
        if any(term in file_lower for term in ['martineau', 'atkinson', 'letters', 'book', 'publisher', '2009']):
            relevant_files.append(file)
    
    print(f'\nFiles with relevant keywords: {len(relevant_files)}')
    for file in relevant_files[:10]:  # Show first 10
        print(f'  - {file}')

print('\n=== STEP 2: ANALYZING SPECIFIC MARTINEAU-ATKINSON JSON FILES ===')

# Focus on the most promising JSON files first
margineau_files = [f for f in json_files if 'martineau' in f.lower() or 'atkinson' in f.lower() or '2009' in f.lower()]
print(f'\nFound {len(margineau_files)} Martineau/Atkinson-related JSON files:')
for file in margineau_files:
    print(f'  - {file}')

book_related_findings = []

# Analyze each Martineau-related JSON file
for json_file in margineau_files:
    print(f'\n--- DETAILED ANALYSIS: {json_file} ---')
    
    try:
        file_path = os.path.join('workspace', json_file)
        
        # First inspect the raw content
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()
        
        print(f'File size: {len(raw_content):,} characters')
        
        # Check for key terms in raw content
        content_lower = raw_content.lower()
        count_2009 = content_lower.count('2009')
        count_martineau = content_lower.count('martineau')
        count_atkinson = content_lower.count('atkinson')
        count_publisher = content_lower.count('publisher')
        
        print(f'Key term counts:')
        print(f'  2009: {count_2009}')
        print(f'  Martineau: {count_martineau}')
        print(f'  Atkinson: {count_atkinson}')
        print(f'  Publisher: {count_publisher}')
        
        # If this file has good term counts, analyze the JSON structure
        if count_2009 > 0 and (count_martineau > 0 or count_atkinson > 0):
            print('âœ“ HIGH RELEVANCE: Contains both 2009 and author references')
            
            try:
                # Parse JSON safely
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                print('\nJSON structure inspection:')
                if isinstance(data, dict):
                    print(f'  Root level keys: {len(data.keys())}')
                    for key in list(data.keys())[:8]:  # Show first 8 keys
                        value = data[key]
                        if isinstance(value, dict):
                            print(f'    {key}: dict with {len(value)} keys')
                        elif isinstance(value, list):
                            print(f'    {key}: list with {len(value)} items')
                        else:
                            preview = str(value)[:80]
                            print(f'    {key}: {type(value).__name__} = {preview}...')
                    
                    if len(data.keys()) > 8:
                        print(f'    ... and {len(data.keys()) - 8} more keys')
                    
                    # Look for specific publisher-related information
                    print('\nSearching for publisher information in JSON structure...')
                    
                    def search_json_for_publishers(obj, path=''):
                        """Recursively search JSON for publisher information"""
                        findings = []
                        
                        if isinstance(obj, dict):
                            for key, value in obj.items():
                                current_path = f'{path}.{key}' if path else key
                                
                                # Check if key relates to publishers
                                if any(term in key.lower() for term in ['publisher', 'press', 'publishing']):
                                    findings.append({
                                        'path': current_path,
                                        'key': key,
                                        'value': value,
                                        'type': 'publisher_key'
                                    })
                                    print(f'    ðŸ“š Publisher key: {current_path} = {value}')
                                
                                # Recursively search nested objects
                                findings.extend(search_json_for_publishers(value, current_path))
                        
                        elif isinstance(obj, list):
                            for i, item in enumerate(obj[:10]):  # Check first 10 items
                                current_path = f'{path}[{i}]'
                                findings.extend(search_json_for_publishers(item, current_path))
                        
                        elif isinstance(obj, str):
                            # Check if string contains publisher information and 2009
                            obj_lower = obj.lower()
                            if '2009' in obj_lower and any(term in obj_lower for term in ['publisher', 'press', 'publishing', 'books']):
                                findings.append({
                                    'path': path,
                                    'content': obj,
                                    'type': 'publisher_string'
                                })
                                print(f'    ðŸŽ¯ Publisher string: {path} = {obj[:150]}...')
                        
                        return findings
                    
                    # Search the entire JSON structure
                    json_findings = search_json_for_publishers(data)
                    
                    if json_findings:
                        print(f'\nâœ“ Found {len(json_findings)} publisher-related items in JSON structure')
                        book_related_findings.extend([{**finding, 'file': json_file, 'source': 'json_structure'} for finding in json_findings])
                    else:
                        print('\n- No publisher information found in JSON structure')
                        
                        # If no structured publisher info, look for text content with publishers
                        print('\nSearching raw content for publisher patterns...')
                        
                        # Look for lines containing both 2009 and publisher terms
                        lines = raw_content.split('\n')
                        publisher_lines = []
                        
                        for line in lines:
                            line_lower = line.lower().strip()
                            if ('2009' in line_lower and 
                                any(term in line_lower for term in ['publisher', 'published', 'press', 'publishing', 'books']) and
                                len(line.strip()) > 15):
                                
                                publisher_lines.append(line.strip())
                        
                        if publisher_lines:
                            print(f'    Found {len(publisher_lines)} lines with 2009 + publisher terms:')
                            for i, line in enumerate(publisher_lines[:3], 1):  # Show first 3
                                print(f'      {i}. {line[:200]}...')
                                book_related_findings.append({
                                    'file': json_file,
                                    'content': line,
                                    'type': 'publisher_line',
                                    'source': 'raw_content'
                                })
                        else:
                            print('    No publisher lines found')
                
            except json.JSONDecodeError as e:
                print(f'âŒ JSON parsing error: {str(e)}')
                print('Treating as text file and searching for publisher patterns...')
                
                # If JSON is malformed, search as text - FIXED regex patterns
                publisher_patterns = [
                    r'"publisher"\s*:\s*"([^"]+)"',
                    r'published by ([^\n,]{10,50})',
                    r'([A-Z][a-z]+ (?:Press|Publishing|Books))'
                ]
                
                for pattern in publisher_patterns:
                    matches = re.findall(pattern, raw_content, re.IGNORECASE)
                    for match in matches:
                        if isinstance(match, tuple):
                            match = match[0] if match[0] else match[1] if len(match) > 1 else ''
                        
                        match = match.strip()
                        if len(match) > 3 and '2009' not in match:
                            print(f'    ðŸ“š Pattern match: {match}')
                            book_related_findings.append({
                                'file': json_file,
                                'content': match,
                                'type': 'regex_pattern',
                                'source': 'text_analysis'
                            })
        else:
            print('- Low relevance: Missing key terms')
            
    except Exception as e:
        print(f'âŒ Error analyzing {json_file}: {str(e)}')

print('\n=== STEP 3: ANALYZING MOST RELEVANT HTML FILES ===')

# Look for HTML files that might contain search results with 2009 publisher info
html_findings = []

# Focus on HTML files that might contain relevant search results
relevant_html = [f for f in html_files if any(term in f.lower() for term in ['search', 'martineau', 'atkinson', 'book', '2009'])]
print(f'\nFound {len(relevant_html)} potentially relevant HTML files')

# Analyze the most promising HTML files
for html_file in relevant_html[:8]:  # Analyze first 8 relevant HTML files
    print(f'\n--- Analyzing {html_file} ---')
    
    try:
        file_path = os.path.join('workspace', html_file)
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f'File size: {len(html_content):,} characters')
        
        # Parse HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for element in soup(['script', 'style']):
            element.decompose()
        
        # Get text content
        text_content = soup.get_text()
        text_lower = text_content.lower()
        
        # Check for our key terms
        has_2009 = '2009' in text_lower
        has_martineau = 'martineau' in text_lower
        has_atkinson = 'atkinson' in text_lower
        has_letters = 'letters' in text_lower
        has_publisher = any(term in text_lower for term in ['publisher', 'published', 'press', 'publishing'])
        
        relevance_score = sum([has_2009, has_martineau, has_atkinson, has_letters, has_publisher])
        print(f'Relevance score: {relevance_score}/5 (2009={has_2009}, Martineau={has_martineau}, Atkinson={has_atkinson}, Letters={has_letters}, Publisher={has_publisher})')
        
        if relevance_score >= 3:  # At least 3 matching terms
            print('âœ“ High relevance content found')
            
            # Look for specific publisher patterns - FIXED regex patterns
            publisher_patterns = [
                r'published by ([^\n,]{5,60})',
                r'publisher[:\s]+([^\n,]{5,60})',
                r'([A-Z][a-z]+ (?:Press|Publishing|Books))',
                r'(\b(?:Nabu|Kessinger|Forgotten Books|BiblioLife|Palala|Wentworth|Franklin Classics|Cambridge|Oxford|Harvard|Yale|Princeton|Routledge|Palgrave|Springer)\b[^\n]{0,40})',
                r'reprinted by ([^\n,]{5,60})',
                r'reissued by ([^\n,]{5,60})'
            ]
            
            pattern_matches = []
            for pattern in publisher_patterns:
                matches = re.findall(pattern, text_content, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0] if match[0] else match[1] if len(match) > 1 else ''
                    
                    match = match.strip()
                    if len(match) > 4 and match not in pattern_matches:
                        pattern_matches.append(match)
            
            if pattern_matches:
                print(f'  ðŸ“š Publisher patterns found: {len(pattern_matches)}')
                for i, match in enumerate(pattern_matches[:5], 1):
                    print(f'    {i}. {match}')
                    html_findings.append({
                        'file': html_file,
                        'content': match,
                        'type': 'publisher_pattern',
                        'source': 'html_analysis'
                    })
            
            # Look for text around 2009 mentions
            if has_2009:
                print('  ðŸŽ¯ Analyzing context around 2009 mentions...')
                
                # Find positions of "2009" in text
                positions = []
                start = 0
                while True:
                    pos = text_lower.find('2009', start)
                    if pos == -1:
                        break
                    positions.append(pos)
                    start = pos + 1
                
                print(f'    Found {len(positions)} instances of "2009"')
                
                for i, pos in enumerate(positions[:3], 1):  # Analyze first 3 instances
                    # Extract context around this position
                    context_start = max(0, pos - 200)
                    context_end = min(len(text_content), pos + 300)
                    context = text_content[context_start:context_end]
                    
                    # Check if context contains publisher information
                    context_lower = context.lower()
                    if any(term in context_lower for term in ['publisher', 'published', 'press', 'publishing', 'books']):
                        print(f'    Context {i} (contains publisher info):')
                        print(f'      {context[:150]}...')
                        
                        html_findings.append({
                            'file': html_file,
                            'content': context,
                            'type': '2009_context',
                            'source': 'html_context_analysis'
                        })
        else:
            print('- Low relevance content')
            
    except Exception as e:
        print(f'âŒ Error analyzing {html_file}: {str(e)}')

print('\n=== STEP 4: CONSOLIDATING AND ANALYZING ALL FINDINGS ===')

all_findings = book_related_findings + html_findings
print(f'Total findings collected: {len(all_findings)}')
print(f'  From JSON files: {len(book_related_findings)}')
print(f'  From HTML files: {len(html_findings)}')

if all_findings:
    print('\n--- DETAILED FINDINGS ANALYSIS ---')
    
    # Group findings by type
    by_type = {}
    for finding in all_findings:
        finding_type = finding['type']
        if finding_type not in by_type:
            by_type[finding_type] = []
        by_type[finding_type].append(finding)
    
    print('\nFindings by type:')
    for finding_type, findings in by_type.items():
        print(f'  {finding_type.replace("_", " ").title()}: {len(findings)} findings')
    
    # Extract and analyze publisher names from all findings
    print('\n--- PUBLISHER IDENTIFICATION ANALYSIS ---')
    
    known_publishers = [
        'Cambridge University Press', 'Oxford University Press', 'Harvard University Press',
        'Yale University Press', 'Princeton University Press', 'University of Chicago Press',
        'Routledge', 'Palgrave Macmillan', 'Springer', 'Brill', 'Ashgate', 'Continuum',
        'Thoemmes Press', 'Pickering & Chatto', 'Nabu Press', 'Kessinger Publishing',
        'Forgotten Books', 'BiblioLife', 'Gale ECCO', 'Making of Modern Law',
        'Elibron Classics', 'Palala Press', 'Wentworth Press', 'Franklin Classics',
        'CreateSpace', 'Lightning Source', 'BookSurge'
    ]
    
    publisher_mentions = {}
    
    for finding in all_findings:
        # Get all text content from the finding
        content_parts = []
        if 'content' in finding:
            content_parts.append(str(finding['content']))
        if 'value' in finding:
            content_parts.append(str(finding['value']))
        
        full_content = ' '.join(content_parts)
        content_lower = full_content.lower()
        
        # Check against known publishers
        for publisher in known_publishers:
            if publisher.lower() in content_lower:
                if publisher not in publisher_mentions:
                    publisher_mentions[publisher] = []
                publisher_mentions[publisher].append(finding)
    
    if publisher_mentions:
        print(f'\nðŸŽ¯ PUBLISHER IDENTIFICATION RESULTS:')
        print(f'Found {len(publisher_mentions)} unique publishers mentioned')
        
        # Sort by frequency
        sorted_publishers = sorted(publisher_mentions.items(), key=lambda x: len(x[1]), reverse=True)
        
        for publisher, mentions in sorted_publishers:
            print(f'\nðŸ“š {publisher}: {len(mentions)} mention(s)')
            
            for i, mention in enumerate(mentions, 1):
                print(f'  {i}. File: {mention["file"]} (Type: {mention["type"]})')
                content = str(mention.get('content', mention.get('value', '')))
                print(f'     Evidence: {content[:120]}...' if len(content) > 120 else f'     Evidence: {content}')
        
        # Identify most likely 2009 publisher
        top_publisher = sorted_publishers[0][0]
        top_count = len(sorted_publishers[0][1])
        
        print(f'\nðŸ† MOST LIKELY 2009 PUBLISHER: {top_publisher}')
        print(f'Evidence strength: {top_count} mention(s) across multiple sources')
        
        # Check if it's a specialty reprint publisher
        specialty_publishers = [
            'Nabu Press', 'Kessinger Publishing', 'Forgotten Books', 'BiblioLife', 
            'Palala Press', 'Wentworth Press', 'Franklin Classics', 'Elibron Classics'
        ]
        
        if top_publisher in specialty_publishers:
            print(f'âœ“ CONFIRMED: {top_publisher} is a known specialty reprint publisher')
            print('This strongly supports the 2009 reissue identification')
        else:
            print(f'ðŸ“ NOTE: {top_publisher} - academic/commercial publisher')
            print('Less common for historical reprints but possible for scholarly reissues')
        
        # Save the final answer
        final_answer = {
            'question': 'What publisher reissued "Letters on the Laws of Man\'s Nature and Development" by Harriet Martineau and Henry George Atkinson in 2009?',
            'answer': top_publisher,
            'evidence_strength': top_count,
            'evidence_sources': [mention['file'] for mention in sorted_publishers[0][1]],
            'publisher_type': 'specialty_reprint' if top_publisher in specialty_publishers else 'academic_commercial',
            'analysis_timestamp': '2025-01-21 15:30:00'
        }
        
        answer_file = 'workspace/2009_publisher_final_answer.json'
        with open(answer_file, 'w', encoding='utf-8') as f:
            json.dump(final_answer, f, indent=2, ensure_ascii=False)
        
        print(f'\nâœ“ Final answer saved to: {answer_file}')
        
    else:
        print('\nâš  No specific known publishers identified in the findings')
        print('The publisher may be mentioned but not in our known publisher list')
        
        # Show all findings for manual review
        print('\nAll findings for manual review:')
        for i, finding in enumerate(all_findings[:10], 1):
            print(f'\n{i}. File: {finding["file"]} (Type: {finding["type"]})')
            content = str(finding.get('content', finding.get('value', '')))
            print(f'   Content: {content[:200]}...' if len(content) > 200 else f'   Content: {content}')
else:
    print('\nâš  No relevant findings extracted from workspace files')
    print('The 2009 publisher information may not be present in current files')

# Save comprehensive analysis
analysis_summary = {
    'analysis_objective': 'Extract 2009 reissue publisher from workspace files',
    'book_details': {
        'title': 'Letters on the Laws of Man\'s Nature and Development',
        'authors': ['Harriet Martineau', 'Henry George Atkinson'],
        'original_year': 1851,
        'target_reissue_year': 2009
    },
    'workspace_analysis': {
        'total_files': len(workspace_files),
        'json_files_analyzed': len(margineau_files) if 'margineau_files' in locals() else 0,
        'html_files_analyzed': len(relevant_html[:8]) if 'relevant_html' in locals() else 0,
        'total_findings': len(all_findings)
    },
    'publisher_analysis': {
        'publishers_mentioned': list(publisher_mentions.keys()) if 'publisher_mentions' in locals() and publisher_mentions else [],
        'most_likely_publisher': sorted_publishers[0][0] if 'sorted_publishers' in locals() and sorted_publishers else None,
        'evidence_strength': len(sorted_publishers[0][1]) if 'sorted_publishers' in locals() and sorted_publishers else 0
    },
    'all_findings': all_findings,
    'analysis_timestamp': '2025-01-21 15:30:00'
}

analysis_file = 'workspace/comprehensive_2009_publisher_analysis.json'
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_summary, f, indent=2, ensure_ascii=False)

print(f'\n*** COMPREHENSIVE WORKSPACE ANALYSIS COMPLETE ***')
print(f'âœ“ Analyzed {len(workspace_files)} total files in workspace')
print(f'âœ“ Focused on {len(margineau_files) if "margineau_files" in locals() else 0} Martineau-related JSON files')
print(f'âœ“ Analyzed {len(relevant_html[:8]) if "relevant_html" in locals() else 0} relevant HTML files')
print(f'âœ“ Extracted {len(all_findings)} relevant findings')
print(f'âœ“ Comprehensive analysis saved to: {analysis_file}')

if 'sorted_publishers' in locals() and sorted_publishers:
    print(f'\nðŸŽ¯ FINAL CONCLUSION: The 2009 reissue publisher is {sorted_publishers[0][0]}')
    print(f'This conclusion is based on {len(sorted_publishers[0][1])} pieces of evidence from workspace files')
else:
    print('\nâš  CONCLUSION: 2009 publisher not definitively identified from workspace analysis')
    print('The information may require additional targeted searches or manual review of specific files')

print('\n=== WORKSPACE ANALYSIS SUMMARY ===')
print(f'Total workspace files: {len(workspace_files)}')
print(f'JSON files: {len(json_files)} (analyzed: {len(margineau_files) if "margineau_files" in locals() else 0})')
print(f'HTML files: {len(html_files)} (analyzed: {len(relevant_html[:8]) if "relevant_html" in locals() else 0})')
print(f'Relevant findings extracted: {len(all_findings)}')
print('\nAll analysis files saved to workspace/ directory for further review')
```

### Development Step 12: 2009 Reissue Publisher of Martineau & Atkinson's Letters on Laws of Man's Nature and Development

**Description**: Verify the specific publisher that reissued 'Letters on the Laws of Man's Nature and Development' by Harriet Martineau and Henry George Atkinson in 2009. The previous search confirmed this is the correct 1851 co-authored book on atheistic naturalism, phrenology, and mesmerism, but we need to identify the exact 2009 reissue publisher. Search for '2009 reissue Letters on the Laws of Man's Nature and Development publisher', 'Martineau Atkinson 2009 republication', and check academic publishers, university presses, or specialty reprint publishers that may have reissued this historical work in 2009.

**Use Cases**:
- Library cataloging of historical texts to verify the exact 2009 reissue publisher for accurate OPAC metadata
- Rare book dealer inventory validation to confirm the printing house behind the 2009 republication before acquisition
- Digital humanities project metadata cleansing by identifying the correct 2009 edition publisher of Martineauâ€“Atkinson letters
- Academic researcher citation audit for a journal article, ensuring the referenced 2009 reissue is attributed to the correct publisher
- Ebook aggregator licensing review to cross-check the 2009 reprint publisher before negotiating digital distribution rights
- Metadata librarian updating WorldCat and institutional repository entries with precise 2009 edition publisher details
- University press editorial fact-checking when preparing a critical introduction to Martineauâ€™s work and citing the 2009 republication
- Book collector automated alert system to detect and log specialty reprint publishers (e.g., Nabu Press, Forgotten Books) for the 2009 edition

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import quote_plus, urljoin

print('=== FIXED: SEARCHING FOR 2009 REISSUE PUBLISHER OF MARTINEAU-ATKINSON LETTERS ===')
print('Title: Letters on the Laws of Man\'s Nature and Development')
print('Authors: Harriet Martineau and Henry George Atkinson')
print('Original: 1851')
print('Target: 2009 reissue publisher identification')
print('\n' + '='*100 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# Headers for web requests
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache'
}

# Define comprehensive search queries for 2009 reissue
search_queries = [
    '"Letters on the Laws of Man\'s Nature and Development" 2009 publisher',
    'Martineau Atkinson "Letters Laws" 2009 reissue',
    'Harriet Martineau Henry Atkinson 2009 republication',
    '"Letters on the Laws of Man\'s Nature and Development" 2009 reprint',
    'Martineau Atkinson 2009 edition publisher',
    '"Laws of Man\'s Nature and Development" 2009 reissue',
    'Harriet Martineau 2009 Letters Laws publisher',
    'Henry George Atkinson 2009 reprint publisher',
    '"Letters on the Laws" Martineau Atkinson 2009',
    'Martineau Atkinson correspondence 2009 publisher'
]

print('=== STEP 1: CONDUCTING TARGETED PUBLISHER SEARCHES ===')
print(f'Total search queries: {len(search_queries)}')
print('\nSearch queries:')
for i, query in enumerate(search_queries, 1):
    print(f'  {i:2d}. {query}')

search_results = {}

# Function to perform search and analyze results - FIXED VARIABLE SCOPE
def perform_search(query, search_index):
    # Define search_base_url inside function to fix scope issue
    search_base_url = 'https://html.duckduckgo.com/html/'
    
    print(f'\n--- SEARCH {search_index}: {query} ---')
    try:
        params = {'q': query}
        response = requests.get(search_base_url, params=params, headers=headers, timeout=30)
        print(f'Status: {response.status_code}')
        
        if response.status_code == 200:
            # Save raw HTML for analysis
            filename = f'search_{search_index:02d}_{query.replace(" ", "_").replace("\'", "").replace('"', "")[:40]}.html'
            filepath = os.path.join('workspace', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f'Saved: {filepath}')
            
            # Parse for relevant results
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for result links with publisher information
            result_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text().strip()
                
                # Filter for highly relevant results
                if href and text and len(text) > 15:
                    text_lower = text.lower()
                    relevance_score = 0
                    
                    # High-value terms for 2009 reissue identification
                    high_value_terms = [
                        ('2009', 3),
                        ('martineau', 2),
                        ('atkinson', 2),
                        ('letters', 1),
                        ('laws', 1),
                        ('nature', 1),
                        ('development', 1),
                        ('publisher', 2),
                        ('reissue', 2),
                        ('reprint', 2),
                        ('edition', 1),
                        ('republication', 2)
                    ]
                    
                    # Publisher-specific terms
                    publisher_terms = [
                        ('cambridge university press', 4),
                        ('oxford university press', 4),
                        ('harvard university press', 4),
                        ('yale university press', 4),
                        ('princeton university press', 4),
                        ('university of chicago press', 4),
                        ('routledge', 3),
                        ('palgrave', 3),
                        ('macmillan', 3),
                        ('springer', 3),
                        ('brill', 3),
                        ('ashgate', 3),
                        ('continuum', 3),
                        ('thoemmes', 3),
                        ('pickering', 3),
                        ('nabu press', 2),
                        ('kessinger', 2),
                        ('forgotten books', 2),
                        ('bibliolife', 2),
                        ('gale ecco', 2),
                        ('making of modern law', 2)
                    ]
                    
                    # Calculate relevance score
                    for term, score in high_value_terms + publisher_terms:
                        if term in text_lower:
                            relevance_score += score
                    
                    # Additional scoring for URL domains
                    if href:
                        href_lower = href.lower()
                        if any(domain in href_lower for domain in ['cambridge.org', 'oup.com', 'harvard.edu', 'yale.edu', 'routledge.com', 'palgrave.com']):
                            relevance_score += 3
                        elif any(domain in href_lower for domain in ['amazon.com', 'worldcat.org', 'goodreads.com', 'abebooks.com']):
                            relevance_score += 2
                    
                    if relevance_score >= 3:  # Only include highly relevant results
                        result_links.append({
                            'url': href,
                            'text': text[:300],  # Longer text for better analysis
                            'relevance_score': relevance_score
                        })
            
            # Sort by relevance score
            result_links.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            search_results[query] = {
                'html_file': filepath,
                'status_code': response.status_code,
                'relevant_links': result_links[:15],  # Top 15 most relevant
                'total_links_found': len(result_links)
            }
            
            print(f'Found {len(result_links)} highly relevant links')
            if result_links:
                print('Top results:')
                for i, link in enumerate(result_links[:5], 1):
                    print(f'  {i}. Score {link["relevance_score"]}: {link["text"][:120]}...')
                    print(f'     URL: {link["url"]}')
            
            time.sleep(2)  # Rate limiting
            return True
        else:
            print(f'Failed: HTTP {response.status_code}')
            return False
            
    except Exception as e:
        print(f'Error: {str(e)}')
        return False

# Perform all searches
print('\n=== EXECUTING SEARCHES ===')
successful_searches = 0

for i, query in enumerate(search_queries, 1):
    if perform_search(query, i):
        successful_searches += 1
    
    # Brief pause between searches
    if i < len(search_queries):
        time.sleep(1)

print(f'\n=== STEP 2: ANALYZING SEARCH RESULTS ===')
print(f'Successful searches: {successful_searches}/{len(search_queries)}')

# Compile and analyze all findings
high_priority_findings = []
all_publishers_mentioned = set()
publisher_frequency = {}

# Known academic and reprint publishers to watch for
known_publishers = [
    'Cambridge University Press', 'Oxford University Press', 'Harvard University Press',
    'Yale University Press', 'Princeton University Press', 'University of Chicago Press',
    'Routledge', 'Palgrave Macmillan', 'Springer', 'Brill', 'Ashgate', 'Continuum',
    'Thoemmes Press', 'Pickering & Chatto', 'Nabu Press', 'Kessinger Publishing',
    'Forgotten Books', 'BiblioLife', 'Gale ECCO', 'Making of Modern Law',
    'Elibron Classics', 'Palala Press', 'Wentworth Press', 'Franklin Classics'
]

print('\n--- ANALYZING ALL SEARCH RESULTS FOR PUBLISHER PATTERNS ---')

for query, results in search_results.items():
    print(f'\nQuery: "{query}"')
    print(f'  Relevant links: {results["total_links_found"]}')
    
    for link in results['relevant_links']:
        # Check for 2009 and publisher combinations
        text_lower = link['text'].lower()
        
        if '2009' in text_lower and any(pub.lower() in text_lower for pub in known_publishers):
            # This is a high-priority finding
            matching_publishers = [pub for pub in known_publishers if pub.lower() in text_lower]
            
            high_priority_findings.append({
                'query': query,
                'text': link['text'],
                'url': link['url'],
                'score': link['relevance_score'],
                'publishers_mentioned': matching_publishers,
                'priority': 'HIGH - Contains 2009 + Publisher'
            })
            
            # Track publisher frequency
            for pub in matching_publishers:
                publisher_frequency[pub] = publisher_frequency.get(pub, 0) + 1
                all_publishers_mentioned.add(pub)
            
            print(f'  ðŸŽ¯ HIGH PRIORITY: {matching_publishers} mentioned with 2009')
        
        elif '2009' in text_lower:
            # Contains 2009 but may have publisher info we need to extract
            high_priority_findings.append({
                'query': query,
                'text': link['text'],
                'url': link['url'],
                'score': link['relevance_score'],
                'publishers_mentioned': [],
                'priority': 'MEDIUM - Contains 2009'
            })
            
            print(f'  ðŸ“ MEDIUM: Contains 2009, checking for publisher info')
        
        # Track any publisher mentions regardless of year
        for pub in known_publishers:
            if pub.lower() in text_lower:
                all_publishers_mentioned.add(pub)
                publisher_frequency[pub] = publisher_frequency.get(pub, 0) + 1

print(f'\n=== STEP 3: PUBLISHER FREQUENCY ANALYSIS ===')
print(f'Total unique publishers mentioned: {len(all_publishers_mentioned)}')
print(f'High-priority findings (2009 + publisher): {len([f for f in high_priority_findings if f["priority"].startswith("HIGH")])}')

if publisher_frequency:
    print('\nPublisher mention frequency:')
    sorted_publishers = sorted(publisher_frequency.items(), key=lambda x: x[1], reverse=True)
    for pub, count in sorted_publishers:
        print(f'  {pub}: {count} mentions')
else:
    print('\nNo specific publishers identified in search results')

print(f'\n=== STEP 4: DETAILED ANALYSIS OF HIGH-PRIORITY FINDINGS ===')

if high_priority_findings:
    # Sort by priority and score
    high_priority_findings.sort(key=lambda x: (x['priority'] == 'HIGH - Contains 2009 + Publisher', x['score']), reverse=True)
    
    print(f'\nAnalyzing {len(high_priority_findings)} high-priority findings:')
    
    for i, finding in enumerate(high_priority_findings[:10], 1):  # Top 10 findings
        print(f'\nðŸ” FINDING {i} - {finding["priority"]} (Score: {finding["score"]})')
        print(f'Query: {finding["query"]}')
        print(f'Publishers: {finding["publishers_mentioned"] if finding["publishers_mentioned"] else "None explicitly identified"}')
        print(f'URL: {finding["url"]}')
        print(f'Text: {finding["text"][:400]}...')
        print('-' * 120)
        
        # If this is a high-priority finding with a specific URL, we should investigate further
        if finding['priority'].startswith('HIGH') and finding['url']:
            print(f'  â­ RECOMMENDED FOR DETAILED INVESTIGATION: {finding["url"]}')
else:
    print('\nâš  No high-priority findings identified')
    print('This suggests the 2009 reissue may be from a smaller or specialized publisher')

# Check for specific reprint/specialty publishers
print(f'\n=== STEP 5: CHECKING FOR SPECIALTY REPRINT PUBLISHERS ===')

specialty_publishers = [
    'Nabu Press', 'Kessinger Publishing', 'Forgotten Books', 'BiblioLife',
    'Palala Press', 'Wentworth Press', 'Franklin Classics', 'Elibron Classics',
    'Gale ECCO', 'Making of Modern Law', 'Thoemmes Press', 'Pickering & Chatto'
]

specialty_findings = []
for query, results in search_results.items():
    for link in results['relevant_links']:
        text_lower = link['text'].lower()
        
        for specialty_pub in specialty_publishers:
            if specialty_pub.lower() in text_lower:
                specialty_findings.append({
                    'publisher': specialty_pub,
                    'query': query,
                    'text': link['text'],
                    'url': link['url'],
                    'has_2009': '2009' in text_lower
                })

if specialty_findings:
    print(f'Found {len(specialty_findings)} specialty publisher mentions:')
    
    # Group by publisher
    by_publisher = {}
    for finding in specialty_findings:
        pub = finding['publisher']
        if pub not in by_publisher:
            by_publisher[pub] = []
        by_publisher[pub].append(finding)
    
    for pub, findings in by_publisher.items():
        print(f'\n{pub}: {len(findings)} mentions')
        for finding in findings[:2]:  # Show top 2 per publisher
            status = 'âœ“ WITH 2009' if finding['has_2009'] else '- without 2009'
            print(f'  {status}: {finding["text"][:150]}...')
            print(f'    URL: {finding["url"]}')
else:
    print('No specialty reprint publishers clearly identified')

# Additional search for specific reprint publisher patterns
print(f'\n=== STEP 6: ANALYZING FOR REPRINT PUBLISHER PATTERNS ===')

# Look for common reprint publisher indicators in all search results
reprint_indicators = [
    'reprint', 'reprinted', 'reproduction', 'facsimile', 'digitally printed',
    'print on demand', 'pod', 'classic reprint', 'historical reproduction',
    'nabu', 'kessinger', 'forgotten books', 'bibliolife', 'palala',
    'wentworth', 'franklin classics', 'elibron', 'gale ecco'
]

reprint_pattern_findings = []
for query, results in search_results.items():
    for link in results['relevant_links']:
        text_lower = link['text'].lower()
        
        # Check for reprint indicators
        found_indicators = []
        for indicator in reprint_indicators:
            if indicator in text_lower:
                found_indicators.append(indicator)
        
        if found_indicators and '2009' in text_lower:
            reprint_pattern_findings.append({
                'query': query,
                'text': link['text'],
                'url': link['url'],
                'indicators': found_indicators,
                'score': link['relevance_score']
            })

if reprint_pattern_findings:
    print(f'Found {len(reprint_pattern_findings)} results with 2009 + reprint indicators:')
    
    # Sort by number of indicators and score
    reprint_pattern_findings.sort(key=lambda x: (len(x['indicators']), x['score']), reverse=True)
    
    for i, finding in enumerate(reprint_pattern_findings[:5], 1):
        print(f'\nðŸŽ¯ REPRINT PATTERN {i}:')
        print(f'Indicators: {finding["indicators"]}')
        print(f'Score: {finding["score"]}')
        print(f'URL: {finding["url"]}')
        print(f'Text: {finding["text"][:200]}...')
        print('-' * 80)
else:
    print('No clear reprint patterns with 2009 found')

# Save comprehensive analysis
analysis_results = {
    'search_objective': 'Identify 2009 reissue publisher for Martineau-Atkinson Letters',
    'book_details': {
        'title': 'Letters on the Laws of Man\'s Nature and Development',
        'authors': ['Harriet Martineau', 'Henry George Atkinson'],
        'original_year': 1851,
        'target_reissue_year': 2009
    },
    'search_summary': {
        'total_queries': len(search_queries),
        'successful_searches': successful_searches,
        'total_relevant_links': sum(len(r['relevant_links']) for r in search_results.values())
    },
    'publisher_analysis': {
        'publishers_mentioned': list(all_publishers_mentioned),
        'publisher_frequency': publisher_frequency,
        'high_priority_findings_count': len([f for f in high_priority_findings if f['priority'].startswith('HIGH')]),
        'specialty_publisher_findings': len(specialty_findings),
        'reprint_pattern_findings': len(reprint_pattern_findings)
    },
    'high_priority_findings': high_priority_findings[:10],  # Top 10 findings
    'specialty_findings': specialty_findings,
    'reprint_pattern_findings': reprint_pattern_findings[:5],  # Top 5 reprint patterns
    'search_queries_used': search_queries,
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
}

analysis_file = 'workspace/martineau_atkinson_2009_publisher_analysis_fixed.json'
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)

print(f'\n*** PUBLISHER SEARCH ANALYSIS COMPLETE ***')
print(f'âœ“ Conducted {successful_searches} successful searches')
print(f'âœ“ Analyzed {sum(len(r["relevant_links"]) for r in search_results.values())} relevant results')
print(f'âœ“ Identified {len(all_publishers_mentioned)} unique publishers mentioned')
print(f'âœ“ Found {len(high_priority_findings)} high-priority findings')
print(f'âœ“ Found {len(reprint_pattern_findings)} reprint pattern matches')
print(f'âœ“ Comprehensive analysis saved to: {analysis_file}')

# Summary of most promising leads
if high_priority_findings:
    top_finding = high_priority_findings[0]
    print(f'\nðŸŽ¯ MOST PROMISING LEAD:')
    print(f'Priority: {top_finding["priority"]}')
    print(f'Publishers mentioned: {top_finding["publishers_mentioned"]}')
    print(f'Score: {top_finding["score"]}')
    print(f'URL: {top_finding["url"]}')
elif reprint_pattern_findings:
    top_reprint = reprint_pattern_findings[0]
    print(f'\nðŸŽ¯ MOST PROMISING REPRINT LEAD:')
    print(f'Reprint indicators: {top_reprint["indicators"]}')
    print(f'Score: {top_reprint["score"]}')
    print(f'URL: {top_reprint["url"]}')
else:
    print(f'\nâš  No definitive 2009 publisher identified in initial search')
    print('Recommend manual investigation of saved search results')

print('\n=== NEXT STEPS ===')
print('1. Review high-priority findings for definitive publisher identification')
print('2. Investigate URLs from top-scoring results')
print('3. Check specialty reprint publishers for 2009 editions')
print('4. Cross-reference with library catalogs (WorldCat, etc.)')
print('5. Examine reprint pattern findings for publisher clues')
```

### Development Step 16: Identify 2009 Reissuing Publisher of Martineau & Atkinsonâ€™s â€œLetters on the Laws of Manâ€™s Natureâ€

**Description**: Verify the specific publisher that reissued 'Letters on the Laws of Man's Nature and Development' by Harriet Martineau and Henry George Atkinson in 2009. The previous search confirmed this is the correct 1851 co-authored book on atheistic naturalism, phrenology, and mesmerism, but we need to identify the exact 2009 reissue publisher. Search for '2009 reissue Letters on the Laws of Man's Nature and Development publisher', 'Martineau Atkinson 2009 republication', and check academic publishers, university presses, or specialty reprint publishers that may have reissued this historical work in 2009.

**Use Cases**:
- Academic library cataloging workflow for verifying the 2009 reissue publisher of a 19th-century philosophical text by parsing locally stored JSON metadata dumps and HTML search results.
- Digital humanities research pipeline that automates extraction of publisher and edition details from mixed-format scratch files (JSON, HTML, TXT) to analyze historical reprint trends.
- Rare-book dealer inventory system that scans downloaded search logs and webpage snippets to confirm specialty reprint publishers before listing 2009 reissues online.
- Archival digitization project that enriches OCRâ€™d manuscript records by programmatically extracting â€œpublished byâ€ lines and publisher names from archived HTML and JSON files.
- Publishing rights compliance team tool that parses legal repository JSON exports and webpage archives to cross-check reissue publisher information for copyright clearances.
- Competitive intelligence monitoring setup for book publishers that automatically sifts through scraped web search results and JSON feeds to detect new reprints and their issuing presses.
- Content management system integration script that ingests varied workspace files to update book metadata fields with accurate publisher and edition data for an online library catalog.

```
import os
import json
from bs4 import BeautifulSoup
import re

print('=== MANUAL ANALYSIS OF WORKSPACE FILES FOR 2009 MARTINEAU-ATKINSON PUBLISHER ===')
print('Objective: Extract 2009 reissue publisher from existing search files')
print('Book: Letters on the Laws of Man\'s Nature and Development')
print('Authors: Harriet Martineau and Henry George Atkinson')
print('\n' + '='*100 + '\n')

# First, let's inspect what files we actually have in workspace
print('=== STEP 1: INSPECTING WORKSPACE DIRECTORY STRUCTURE ===')

if not os.path.exists('workspace'):
    print('âŒ Workspace directory does not exist')
else:
    workspace_files = os.listdir('workspace')
    print(f'Total files in workspace: {len(workspace_files)}')
    
    # Categorize files
    json_files = [f for f in workspace_files if f.endswith('.json')]
    html_files = [f for f in workspace_files if f.endswith('.html')]
    txt_files = [f for f in workspace_files if f.endswith('.txt')]
    other_files = [f for f in workspace_files if not f.endswith(('.json', '.html', '.txt'))]
    
    print(f'\nFile breakdown:')
    print(f'  JSON files: {len(json_files)}')
    print(f'  HTML files: {len(html_files)}')
    print(f'  TXT files: {len(txt_files)}')
    print(f'  Other files: {len(other_files)}')
    
    # Show recent files that might contain relevant information
    print('\nRecent JSON analysis files:')
    for json_file in sorted(json_files)[-5:]:  # Last 5 JSON files
        file_path = os.path.join('workspace', json_file)
        file_size = os.path.getsize(file_path)
        print(f'  - {json_file} ({file_size:,} bytes)')
    
    # Look for files that might contain book/publisher information
    relevant_files = []
    for file in workspace_files:
        file_lower = file.lower()
        if any(term in file_lower for term in ['martineau', 'atkinson', 'letters', 'book', 'publisher', '2009']):
            relevant_files.append(file)
    
    print(f'\nFiles with relevant keywords: {len(relevant_files)}')
    for file in relevant_files[:10]:  # Show first 10
        print(f'  - {file}')

print('\n=== STEP 2: ANALYZING JSON FILES FOR BOOK INFORMATION ===')

# Let's inspect the structure of JSON files before trying to parse them
book_related_findings = []

for json_file in json_files:
    if any(term in json_file.lower() for term in ['book', 'martineau', 'analysis', 'search']):
        print(f'\n--- Inspecting {json_file} ---')
        
        try:
            file_path = os.path.join('workspace', json_file)
            with open(file_path, 'r', encoding='utf-8') as f:
                # First, let's see what the structure looks like
                content = f.read()
                print(f'File size: {len(content):,} characters')
                
                # Try to parse as JSON
                f.seek(0)  # Reset file pointer
                data = json.load(f)
                
                print('JSON structure:')
                if isinstance(data, dict):
                    for key in list(data.keys())[:10]:  # Show first 10 keys
                        value_type = type(data[key]).__name__
                        if isinstance(data[key], (list, dict)):
                            length = len(data[key])
                            print(f'  {key}: {value_type} (length: {length})')
                        else:
                            preview = str(data[key])[:100]
                            print(f'  {key}: {value_type} = {preview}...')
                    
                    if len(data.keys()) > 10:
                        print(f'  ... and {len(data.keys()) - 10} more keys')
                
                # Look for specific book-related information
                content_lower = content.lower()
                
                # Check for 2009 references
                if '2009' in content_lower:
                    count_2009 = content_lower.count('2009')
                    print(f'âœ“ Contains "2009": {count_2009} occurrences')
                    
                    # Look for publisher information near 2009
                    lines = content.split('\n')
                    for i, line in enumerate(lines):
                        line_lower = line.lower()
                        if '2009' in line_lower and any(term in line_lower for term in ['publisher', 'press', 'publishing', 'books']):
                            print(f'  ðŸ“š Publisher line: {line.strip()[:150]}...')
                            book_related_findings.append({
                                'file': json_file,
                                'type': 'publisher_line',
                                'content': line.strip(),
                                'context': 'JSON file analysis'
                            })
                
                # Check for author names
                has_martineau = 'martineau' in content_lower
                has_atkinson = 'atkinson' in content_lower
                
                if has_martineau or has_atkinson:
                    authors_found = []
                    if has_martineau:
                        authors_found.append('Martineau')
                    if has_atkinson:
                        authors_found.append('Atkinson')
                    print(f'âœ“ Contains authors: {authors_found}')
                
                # Check for title keywords
                title_keywords = ['letters', 'laws', 'nature', 'development']
                title_matches = [kw for kw in title_keywords if kw in content_lower]
                if title_matches:
                    print(f'âœ“ Contains title keywords: {title_matches}')
                
                # If this file has both 2009 and author info, it's highly relevant
                if '2009' in content_lower and (has_martineau or has_atkinson):
                    print('ðŸŽ¯ HIGH RELEVANCE: Contains both 2009 and author information')
                    
                    # Extract specific findings
                    if isinstance(data, dict):
                        # Look for publisher-related keys
                        for key, value in data.items():
                            key_lower = key.lower()
                            if any(pub_term in key_lower for pub_term in ['publisher', 'press', 'publishing']):
                                print(f'  ðŸ“š Publisher key found: {key} = {value}')
                                book_related_findings.append({
                                    'file': json_file,
                                    'type': 'publisher_key',
                                    'key': key,
                                    'value': value,
                                    'context': 'JSON structure analysis'
                                })
                        
                        # Look for results or findings arrays
                        for key, value in data.items():
                            if isinstance(value, list) and len(value) > 0:
                                key_lower = key.lower()
                                if any(term in key_lower for term in ['result', 'finding', 'book', 'search']):
                                    print(f'  ðŸ“‹ Results array: {key} ({len(value)} items)')
                                    
                                    # Sample first few items
                                    for i, item in enumerate(value[:3]):
                                        if isinstance(item, dict):
                                            item_str = str(item)
                                            if '2009' in item_str.lower():
                                                print(f'    Item {i+1} (contains 2009): {item_str[:200]}...')
                                                book_related_findings.append({
                                                    'file': json_file,
                                                    'type': 'results_item',
                                                    'content': item,
                                                    'context': f'{key} array item {i+1}'
                                                })
                
        except json.JSONDecodeError as e:
            print(f'âŒ JSON parsing error: {str(e)}')
        except Exception as e:
            print(f'âŒ Error analyzing {json_file}: {str(e)}')

print('\n=== STEP 3: ANALYZING HTML FILES FOR SEARCH RESULTS ===')

# Look at HTML files that might contain search results
html_findings = []

for html_file in html_files[:10]:  # Analyze first 10 HTML files
    if any(term in html_file.lower() for term in ['search', 'martineau', 'atkinson', 'book']):
        print(f'\n--- Analyzing {html_file} ---')
        
        try:
            file_path = os.path.join('workspace', html_file)
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            print(f'File size: {len(html_content):,} characters')
            
            # Parse HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove script and style elements
            for element in soup(['script', 'style']):
                element.decompose()
            
            # Get text content
            text_content = soup.get_text()
            text_lower = text_content.lower()
            
            # Check for our key terms
            has_2009 = '2009' in text_lower
            has_martineau = 'martineau' in text_lower
            has_atkinson = 'atkinson' in text_lower
            has_letters = 'letters' in text_lower
            
            relevance_score = sum([has_2009, has_martineau, has_atkinson, has_letters])
            print(f'Relevance score: {relevance_score}/4 (2009={has_2009}, Martineau={has_martineau}, Atkinson={has_atkinson}, Letters={has_letters})')
            
            if relevance_score >= 2:  # At least 2 matching terms
                print('âœ“ Relevant content found')
                
                # Look for publisher information
                publisher_patterns = [
                    r'published by ([^\n,]{10,50})',
                    r'publisher[:\s]+([^\n,]{10,50})',
                    r'([A-Z][a-z]+ (?:Press|Publishing|Books))',
                    r'(\b(?:Nabu|Kessinger|Forgotten Books|BiblioLife|Palala|Wentworth)\b[^\n]{0,30})',
                    r'reprinted by ([^\n,]{10,50})'
                ]
                
                for pattern in publisher_patterns:
                    matches = re.findall(pattern, text_content, re.IGNORECASE)
                    for match in matches:
                        if isinstance(match, tuple):
                            match = match[0] if match[0] else match[1] if len(match) > 1 else ''
                        
                        match = match.strip()
                        if len(match) > 5:
                            print(f'  ðŸ“š Publisher pattern match: {match}')
                            html_findings.append({
                                'file': html_file,
                                'type': 'publisher_pattern',
                                'content': match,
                                'context': 'HTML pattern matching'
                            })
                
                # Look for lines containing both 2009 and publisher-related terms
                lines = text_content.split('\n')
                for line in lines:
                    line_lower = line.lower().strip()
                    if ('2009' in line_lower and 
                        any(pub_term in line_lower for pub_term in ['publisher', 'published', 'press', 'publishing', 'books']) and
                        len(line.strip()) > 20):
                        
                        print(f'  ðŸŽ¯ 2009 + Publisher line: {line.strip()[:150]}...')
                        html_findings.append({
                            'file': html_file,
                            'type': '2009_publisher_line',
                            'content': line.strip(),
                            'context': 'HTML line analysis'
                        })
            
        except Exception as e:
            print(f'âŒ Error analyzing {html_file}: {str(e)}')

print('\n=== STEP 4: CONSOLIDATING ALL FINDINGS ===')

all_findings = book_related_findings + html_findings
print(f'Total findings: {len(all_findings)}')

if all_findings:
    print('\n--- DETAILED FINDINGS ANALYSIS ---')
    
    # Group findings by type
    by_type = {}
    for finding in all_findings:
        finding_type = finding['type']
        if finding_type not in by_type:
            by_type[finding_type] = []
        by_type[finding_type].append(finding)
    
    for finding_type, findings in by_type.items():
        print(f'\n{finding_type.replace("_", " ").title()}: {len(findings)} findings')
        
        for i, finding in enumerate(findings[:3], 1):  # Show first 3 of each type
            print(f'  {i}. File: {finding["file"]}')
            if 'content' in finding:
                content = str(finding['content'])
                print(f'     Content: {content[:200]}...' if len(content) > 200 else f'     Content: {content}')
            if 'key' in finding and 'value' in finding:
                print(f'     Key: {finding["key"]} = {finding["value"]}')
    
    # Look for publisher names in all findings
    known_publishers = [
        'Cambridge University Press', 'Oxford University Press', 'Harvard University Press',
        'Yale University Press', 'Princeton University Press', 'University of Chicago Press',
        'Routledge', 'Palgrave Macmillan', 'Springer', 'Brill', 'Ashgate', 'Continuum',
        'Thoemmes Press', 'Pickering & Chatto', 'Nabu Press', 'Kessinger Publishing',
        'Forgotten Books', 'BiblioLife', 'Gale ECCO', 'Making of Modern Law',
        'Elibron Classics', 'Palala Press', 'Wentworth Press', 'Franklin Classics'
    ]
    
    publisher_mentions = {}
    
    for finding in all_findings:
        content = str(finding.get('content', '')) + ' ' + str(finding.get('value', ''))
        content_lower = content.lower()
        
        for publisher in known_publishers:
            if publisher.lower() in content_lower:
                if publisher not in publisher_mentions:
                    publisher_mentions[publisher] = []
                publisher_mentions[publisher].append(finding)
    
    if publisher_mentions:
        print('\n=== PUBLISHER IDENTIFICATION RESULTS ===')
        
        # Sort by frequency
        sorted_publishers = sorted(publisher_mentions.items(), key=lambda x: len(x[1]), reverse=True)
        
        for publisher, mentions in sorted_publishers:
            print(f'\nðŸ† {publisher}: {len(mentions)} mention(s)')
            
            for i, mention in enumerate(mentions, 1):
                print(f'  {i}. File: {mention["file"]}')
                print(f'     Type: {mention["type"]}')
                content = str(mention.get('content', mention.get('value', '')))
                print(f'     Evidence: {content[:150]}...' if len(content) > 150 else f'     Evidence: {content}')
        
        # Identify most likely 2009 publisher
        top_publisher = sorted_publishers[0][0]
        top_count = len(sorted_publishers[0][1])
        
        print(f'\nðŸŽ¯ CONCLUSION: Most likely 2009 publisher is {top_publisher}')
        print(f'Evidence strength: {top_count} mention(s) across multiple files')
        
        # Check if it's a specialty reprint publisher
        specialty_publishers = ['Nabu Press', 'Kessinger Publishing', 'Forgotten Books', 'BiblioLife', 'Palala Press', 'Wentworth Press', 'Franklin Classics']
        
        if top_publisher in specialty_publishers:
            print(f'âœ“ CONFIRMED: {top_publisher} is a known specialty reprint publisher')
            print('This strongly supports the 2009 reissue identification')
    else:
        print('\nâš  No specific known publishers identified in the findings')
        print('The publisher may be mentioned but not recognized by our known publisher list')
else:
    print('\nâš  No relevant findings extracted from workspace files')
    print('The 2009 publisher information may not be present in current files')

# Save analysis results
analysis_summary = {
    'analysis_objective': 'Extract 2009 reissue publisher from workspace files',
    'book_details': {
        'title': 'Letters on the Laws of Man\'s Nature and Development',
        'authors': ['Harriet Martineau', 'Henry George Atkinson'],
        'original_year': 1851,
        'target_reissue_year': 2009
    },
    'workspace_analysis': {
        'total_files': len(workspace_files) if 'workspace_files' in locals() else 0,
        'json_files_analyzed': len(json_files) if 'json_files' in locals() else 0,
        'html_files_analyzed': min(10, len(html_files)) if 'html_files' in locals() else 0,
        'total_findings': len(all_findings),
        'findings_by_type': {k: len(v) for k, v in by_type.items()} if 'by_type' in locals() else {}
    },
    'publisher_analysis': {
        'publishers_mentioned': list(publisher_mentions.keys()) if 'publisher_mentions' in locals() and publisher_mentions else [],
        'most_likely_publisher': sorted_publishers[0][0] if 'sorted_publishers' in locals() and sorted_publishers else None,
        'evidence_strength': len(sorted_publishers[0][1]) if 'sorted_publishers' in locals() and sorted_publishers else 0
    },
    'all_findings': all_findings,
    'analysis_timestamp': '2025-01-21 15:00:00'
}

analysis_file = 'workspace/final_2009_publisher_analysis.json'
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_summary, f, indent=2, ensure_ascii=False)

print(f'\n*** COMPREHENSIVE WORKSPACE ANALYSIS COMPLETE ***')
print(f'âœ“ Analyzed {len(workspace_files) if "workspace_files" in locals() else 0} total files')
print(f'âœ“ Extracted {len(all_findings)} relevant findings')
print(f'âœ“ Analysis saved to: {analysis_file}')

if 'sorted_publishers' in locals() and sorted_publishers:
    print(f'\nðŸ† FINAL ANSWER: The 2009 reissue publisher is most likely {sorted_publishers[0][0]}')
    print(f'This conclusion is based on {len(sorted_publishers[0][1])} pieces of evidence from the workspace files')
else:
    print('\nâš  CONCLUSION: 2009 publisher not definitively identified from workspace analysis')
    print('The information may require additional targeted searches or library catalog consultation')

print('\n=== WORKSPACE FILES SUMMARY ===')
if 'workspace_files' in locals():
    for file_type, files in [('JSON', json_files), ('HTML', html_files), ('TXT', txt_files)]:
        if files:
            print(f'{file_type} files ({len(files)}): Available for manual review')
else:
    print('Workspace directory structure not available')
```

### Development Step 14: Identify 2009 Publisher of Martineau & Atkinsonâ€™s Letters on Laws of Manâ€™s Nature and Development

**Description**: Verify the specific publisher that reissued 'Letters on the Laws of Man's Nature and Development' by Harriet Martineau and Henry George Atkinson in 2009. The previous search confirmed this is the correct 1851 co-authored book on atheistic naturalism, phrenology, and mesmerism, but we need to identify the exact 2009 reissue publisher. Search for '2009 reissue Letters on the Laws of Man's Nature and Development publisher', 'Martineau Atkinson 2009 republication', and check academic publishers, university presses, or specialty reprint publishers that may have reissued this historical work in 2009.

**Use Cases**:
- Academic library metadata extraction from archived WorldCat and Google Books HTML to automatically enrich catalog records with the 2009 reissue publisher for historical texts
- Digital humanities workflow for verifying 19th-century philosophical treatise reprint details by parsing saved search result HTML and JSON analysis files
- Rare-book dealer inventory verification tool that scans supplier-provided HTML catalogs to confirm reissue publisher and edition year for resale accuracy
- Publisher compliance auditing script to extract rights-holder information from reprinted editions by analyzing legacy web-scraped files and applying regex patterns
- Used bookstore chain automation for bulk-extracting 2009 reissue publisher data from multiple supplier webpages to streamline procurement and cataloguing
- Bibliometric research pipeline aggregating publisher mentions from existing HTML archives to analyze reprint frequencies of classic naturalism works in 2009
- Museum collection cataloging assistant that parses internal HTML and JSON reports to verify publisher and edition details for exhibit labeling of book artifacts
- Academic citation management integration that auto-populates the publisher field for reissued works by analyzing saved HTML search dumps and analysis summaries

```
import os
import json
from bs4 import BeautifulSoup
import re

print('=== ANALYZING EXISTING WORKSPACE FILES FOR 2009 PUBLISHER CLUES ===')
print('Objective: Extract 2009 reissue publisher from previously saved search results')
print('Focus: Martineau-Atkinson "Letters on the Laws of Man\'s Nature and Development"')
print('\n' + '='*100 + '\n')

# First, let's inspect the structure of existing analysis files
print('=== STEP 1: INSPECTING EXISTING ANALYSIS FILES ===')

workspace_files = os.listdir('workspace')
json_files = [f for f in workspace_files if f.endswith('.json')]
html_files = [f for f in workspace_files if f.endswith('.html')]

print(f'Found {len(json_files)} JSON analysis files and {len(html_files)} HTML search files')
print('\nJSON files:')
for json_file in json_files:
    file_size = os.path.getsize(os.path.join('workspace', json_file))
    print(f'  - {json_file} ({file_size:,} bytes)')

# Let's inspect the most recent comprehensive analysis file
analysis_files = [f for f in json_files if 'analysis' in f.lower() or 'complete' in f.lower()]
if analysis_files:
    latest_analysis = analysis_files[-1]  # Get the most recent
    print(f'\nInspecting latest analysis file: {latest_analysis}')
    
    try:
        with open(os.path.join('workspace', latest_analysis), 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        print('\nAnalysis file structure:')
        for key in analysis_data.keys():
            if isinstance(analysis_data[key], dict):
                print(f'  {key}: dict with {len(analysis_data[key])} keys')
                for subkey in list(analysis_data[key].keys())[:5]:  # Show first 5 subkeys
                    print(f'    - {subkey}')
                if len(analysis_data[key]) > 5:
                    print(f'    ... and {len(analysis_data[key]) - 5} more')
            elif isinstance(analysis_data[key], list):
                print(f'  {key}: list with {len(analysis_data[key])} items')
            else:
                print(f'  {key}: {type(analysis_data[key]).__name__}')
    except Exception as e:
        print(f'Error reading analysis file: {str(e)}')

print('\n=== STEP 2: ANALYZING HTML FILES WITH 2009 REFERENCES ===')

# Focus on the 3 files identified as having 2009 + author references
target_files = [
    'reissue_search_2_Martineau_Atkinson_Letters_1851_200.html',
    'reissue_search_2_Martineau_Atkinson_Letters_200.html',
    'reissue_search_4_atheistic_naturalism_1851_book_2009.html'
]

# Known publishers to search for
known_publishers = [
    'Cambridge University Press', 'Oxford University Press', 'Harvard University Press',
    'Yale University Press', 'Princeton University Press', 'University of Chicago Press',
    'Routledge', 'Palgrave Macmillan', 'Springer', 'Brill', 'Ashgate', 'Continuum',
    'Thoemmes Press', 'Pickering & Chatto', 'Nabu Press', 'Kessinger Publishing',
    'Forgotten Books', 'BiblioLife', 'Gale ECCO', 'Making of Modern Law',
    'Elibron Classics', 'Palala Press', 'Wentworth Press', 'Franklin Classics',
    'CreateSpace', 'Lightning Source', 'BookSurge'
]

specialty_publishers = [
    'Nabu Press', 'Kessinger Publishing', 'Forgotten Books', 'BiblioLife',
    'Palala Press', 'Wentworth Press', 'Franklin Classics', 'Elibron Classics',
    'Gale ECCO', 'Making of Modern Law', 'Thoemmes Press', 'Pickering & Chatto'
]

publisher_findings = []

for filename in target_files:
    filepath = os.path.join('workspace', filename)
    
    if not os.path.exists(filepath):
        print(f'âš  File not found: {filename}')
        continue
    
    print(f'\n--- ANALYZING {filename} ---')
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        file_size = len(html_content)
        print(f'File size: {file_size:,} characters')
        
        # Parse HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text content
        text_content = soup.get_text()
        text_lower = text_content.lower()
        
        # Count key terms
        count_2009 = text_lower.count('2009')
        count_martineau = text_lower.count('martineau')
        count_atkinson = text_lower.count('atkinson')
        
        print(f'Key term counts: 2009={count_2009}, Martineau={count_martineau}, Atkinson={count_atkinson}')
        
        # Look for lines containing 2009 AND (Martineau OR Atkinson)
        relevant_lines = []
        lines = text_content.split('\n')
        
        for line in lines:
            line_lower = line.lower().strip()
            if '2009' in line_lower and ('martineau' in line_lower or 'atkinson' in line_lower):
                if len(line.strip()) > 20:  # Ignore very short lines
                    relevant_lines.append(line.strip())
        
        print(f'Found {len(relevant_lines)} lines with 2009 + author references')
        
        if relevant_lines:
            print('\nRelevant lines:')
            for i, line in enumerate(relevant_lines[:5], 1):  # Show first 5
                print(f'  {i}. {line[:200]}...' if len(line) > 200 else f'  {i}. {line}')
                
                # Check each line for publisher names
                line_lower = line.lower()
                for pub in known_publishers:
                    if pub.lower() in line_lower:
                        publisher_findings.append({
                            'filename': filename,
                            'publisher': pub,
                            'line': line,
                            'type': 'academic' if pub not in specialty_publishers else 'specialty'
                        })
                        print(f'    ðŸŽ¯ PUBLISHER FOUND: {pub}')
        
        # Also search for common reprint/publisher patterns
        reprint_patterns = [
            r'published by ([^,\n]+)',
            r'publisher[:\s]+([^,\n]+)',
            r'([A-Z][a-z]+ Press)',
            r'(\w+ Publishing)',
            r'(\w+ Books)',
            r'reprinted by ([^,\n]+)',
            r'reissued by ([^,\n]+)'
        ]
        
        print('\nSearching for publisher patterns...')
        pattern_matches = []
        
        for pattern in reprint_patterns:
            matches = re.findall(pattern, text_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1] if len(match) > 1 else ''
                
                match = match.strip()
                if len(match) > 3 and '2009' not in match:  # Avoid matching the year itself
                    pattern_matches.append({
                        'pattern': pattern,
                        'match': match,
                        'filename': filename
                    })
        
        if pattern_matches:
            print(f'Found {len(pattern_matches)} publisher pattern matches:')
            unique_matches = list(set([m['match'] for m in pattern_matches]))
            for match in unique_matches[:10]:  # Show first 10 unique matches
                print(f'  - {match}')
                
                # Check if this matches any known publishers
                for pub in known_publishers:
                    if pub.lower() in match.lower() or match.lower() in pub.lower():
                        publisher_findings.append({
                            'filename': filename,
                            'publisher': pub,
                            'line': f'Pattern match: {match}',
                            'type': 'pattern_match'
                        })
                        print(f'    ðŸŽ¯ MATCHES KNOWN PUBLISHER: {pub}')
        
        # Look specifically for text around 2009 mentions
        print('\nAnalyzing context around 2009 mentions...')
        
        # Find all positions of '2009' in the text
        text_positions = []
        start = 0
        while True:
            pos = text_lower.find('2009', start)
            if pos == -1:
                break
            text_positions.append(pos)
            start = pos + 1
        
        print(f'Found {len(text_positions)} instances of "2009"')
        
        for i, pos in enumerate(text_positions[:5], 1):  # Analyze first 5 instances
            # Extract context around this position
            context_start = max(0, pos - 300)
            context_end = min(len(text_content), pos + 300)
            context = text_content[context_start:context_end]
            
            print(f'\n2009 Context {i}:')
            print('=' * 80)
            print(context)
            print('=' * 80)
            
            # Check this context for publishers
            context_lower = context.lower()
            context_publishers = []
            
            for pub in known_publishers:
                if pub.lower() in context_lower:
                    context_publishers.append(pub)
            
            if context_publishers:
                print(f'ðŸŽ¯ PUBLISHERS IN CONTEXT: {context_publishers}')
                for pub in context_publishers:
                    publisher_findings.append({
                        'filename': filename,
                        'publisher': pub,
                        'line': f'Context around 2009: {context[:100]}...',
                        'type': 'context_analysis'
                    })
            
            # Also check for Martineau/Atkinson in this context
            has_martineau = 'martineau' in context_lower
            has_atkinson = 'atkinson' in context_lower
            
            if has_martineau or has_atkinson:
                authors_in_context = []
                if has_martineau:
                    authors_in_context.append('Martineau')
                if has_atkinson:
                    authors_in_context.append('Atkinson')
                
                print(f'âœ“ AUTHORS IN CONTEXT: {authors_in_context}')
        
    except Exception as e:
        print(f'âŒ Error analyzing {filename}: {str(e)}')

print('\n=== STEP 3: COMPREHENSIVE PUBLISHER FINDINGS ANALYSIS ===')

if publisher_findings:
    print(f'\nðŸŽ¯ TOTAL PUBLISHER FINDINGS: {len(publisher_findings)}')
    
    # Group by publisher
    by_publisher = {}
    for finding in publisher_findings:
        pub = finding['publisher']
        if pub not in by_publisher:
            by_publisher[pub] = []
        by_publisher[pub].append(finding)
    
    print(f'\nUnique publishers mentioned: {len(by_publisher)}')
    
    # Sort publishers by frequency
    sorted_publishers = sorted(by_publisher.items(), key=lambda x: len(x[1]), reverse=True)
    
    print('\nPublisher frequency analysis:')
    for pub, findings in sorted_publishers:
        print(f'\n{pub}: {len(findings)} mentions')
        
        # Show details for each mention
        for i, finding in enumerate(findings, 1):
            print(f'  {i}. File: {finding["filename"]}')  
            print(f'     Type: {finding["type"]}')
            print(f'     Context: {finding["line"][:150]}...' if len(finding["line"]) > 150 else f'     Context: {finding["line"]}')
    
    # Identify the most likely 2009 publisher
    print('\n=== MOST LIKELY 2009 PUBLISHER IDENTIFICATION ===')
    
    if sorted_publishers:
        top_publisher, top_findings = sorted_publishers[0]
        print(f'\nðŸ† MOST FREQUENTLY MENTIONED: {top_publisher}')
        print(f'Total mentions: {len(top_findings)}')
        print('Evidence:')
        for i, finding in enumerate(top_findings, 1):
            print(f'  {i}. {finding["type"].replace("_", " ").title()}: {finding["line"][:100]}...')
        
        # Check if this publisher is known for reprints
        if top_publisher in specialty_publishers:
            print(f'\nâœ“ CONFIRMATION: {top_publisher} is a known specialty reprint publisher')
            print('This strongly suggests they published the 2009 reissue')
        else:
            print(f'\nðŸ“š NOTE: {top_publisher} is a major academic publisher')
            print('Less common for reprints, but possible for scholarly reissues')
else:
    print('\nâš  No specific publisher findings in the analyzed files')
    print('The 2009 reissue publisher may not be clearly identified in current search results')

# Save comprehensive findings
findings_summary = {
    'analysis_objective': 'Extract 2009 reissue publisher from existing search files',
    'book_details': {
        'title': 'Letters on the Laws of Man\'s Nature and Development',
        'authors': ['Harriet Martineau', 'Henry George Atkinson'],
        'original_year': 1851,
        'target_reissue_year': 2009
    },
    'files_analyzed': target_files,
    'publisher_findings': publisher_findings,
    'publisher_frequency': {pub: len(findings) for pub, findings in (by_publisher.items() if publisher_findings else [])},
    'most_likely_publisher': sorted_publishers[0][0] if publisher_findings and sorted_publishers else None,
    'analysis_timestamp': '2025-01-21 14:30:00'
}

findings_file = 'workspace/2009_publisher_findings_analysis.json'
with open(findings_file, 'w', encoding='utf-8') as f:
    json.dump(findings_summary, f, indent=2, ensure_ascii=False)

print(f'\n*** ANALYSIS COMPLETE ***')
print(f'âœ“ Analyzed {len(target_files)} HTML files with 2009 references')
print(f'âœ“ Found {len(publisher_findings)} publisher-related findings')
print(f'âœ“ Identified {len(by_publisher) if publisher_findings else 0} unique publishers')
print(f'âœ“ Analysis saved to: {findings_file}')

if publisher_findings and sorted_publishers:
    top_publisher = sorted_publishers[0][0]
    print(f'\nðŸŽ¯ CONCLUSION: Most likely 2009 publisher is {top_publisher}')
    print(f'Based on {len(sorted_publishers[0][1])} mentions across analyzed files')
else:
    print('\nâš  CONCLUSION: 2009 publisher not definitively identified')
    print('Recommend additional searches or library catalog consultation')

print('\n=== NEXT STEPS ===')
print('1. Verify the identified publisher through library catalogs')
print('2. Cross-reference with WorldCat or Google Books')
print('3. Check publisher\'s catalog for 2009 reissues')
print('4. Confirm ISBN or other publication details')
```

### Development Step 13: 2009 Reissue Publisher of â€œLetters on the Laws of Manâ€™s Nature and Developmentâ€

**Description**: Verify the specific publisher that reissued 'Letters on the Laws of Man's Nature and Development' by Harriet Martineau and Henry George Atkinson in 2009. The previous search confirmed this is the correct 1851 co-authored book on atheistic naturalism, phrenology, and mesmerism, but we need to identify the exact 2009 reissue publisher. Search for '2009 reissue Letters on the Laws of Man's Nature and Development publisher', 'Martineau Atkinson 2009 republication', and check academic publishers, university presses, or specialty reprint publishers that may have reissued this historical work in 2009.

**Use Cases**:
- Library catalog metadata enhancement: automating the identification of the 2009 reissue publisher for historical titles like Martineau & Atkinsonâ€™s Letters to update and standardize library records.
- Antiquarian book dealership due diligence: verifying the authenticity and exact publisher of a 2009 reprint of a rare 1851 naturalism/phrenology work before listing it for sale.
- Academic bibliography validation: enabling researchers to confirm the correct publisher and edition year when citing the 2009 reissue of a 19th-century correspondence volume.
- Digital humanities project metadata curation: scraping and analyzing online sources to fill missing publisher fields for digitized archives of classical texts reissued in 2009.
- Online book retailer inventory verification: automatically cross-checking product listings to ensure the 2009 edition of Martineau & Atkinsonâ€™s work is attributed to the correct reprint publisher.
- Publishing market intelligence gathering: detecting specialty reprint publishers and tracking their 2009 release patterns for competitive analysis in the academic book market.
- Library acquisition workflow automation: integrating web-scraping scripts to auto-populate publisher details for newly acquired historical reprints in the year 2009.
- Reference management tool integration: building a plugin that fetches edition and publisher information for specified historical works (e.g., 2009 reissues) directly into citation software.

```
import os
import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import quote_plus, urljoin

print('=== FULLY FIXED: SEARCHING FOR 2009 REISSUE PUBLISHER OF MARTINEAU-ATKINSON LETTERS ===')
print('Title: Letters on the Laws of Man\'s Nature and Development')
print('Authors: Harriet Martineau and Henry George Atkinson')
print('Original: 1851')
print('Target: 2009 reissue publisher identification')
print('\n' + '='*100 + '\n')

# Ensure workspace directory exists
os.makedirs('workspace', exist_ok=True)

# Define comprehensive search queries for 2009 reissue
search_queries = [
    '"Letters on the Laws of Man\'s Nature and Development" 2009 publisher',
    'Martineau Atkinson "Letters Laws" 2009 reissue',
    'Harriet Martineau Henry Atkinson 2009 republication',
    '"Letters on the Laws of Man\'s Nature and Development" 2009 reprint',
    'Martineau Atkinson 2009 edition publisher',
    '"Laws of Man\'s Nature and Development" 2009 reissue',
    'Harriet Martineau 2009 Letters Laws publisher',
    'Henry George Atkinson 2009 reprint publisher',
    '"Letters on the Laws" Martineau Atkinson 2009',
    'Martineau Atkinson correspondence 2009 publisher'
]

print('=== STEP 1: CONDUCTING TARGETED PUBLISHER SEARCHES ===')
print(f'Total search queries: {len(search_queries)}')
print('\nSearch queries:')
for i, query in enumerate(search_queries, 1):
    print(f'  {i:2d}. {query}')

search_results = {}

# Function to perform search and analyze results - FIXED ALL VARIABLE SCOPE ISSUES
def perform_search(query, search_index):
    # Define ALL required variables inside function to fix scope issues
    search_base_url = 'https://html.duckduckgo.com/html/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }
    
    print(f'\n--- SEARCH {search_index}: {query} ---')
    try:
        params = {'q': query}
        response = requests.get(search_base_url, params=params, headers=headers, timeout=30)
        print(f'Status: {response.status_code}')
        
        if response.status_code == 200:
            # Save raw HTML for analysis
            filename = f'search_{search_index:02d}_{query.replace(" ", "_").replace("\'", "").replace('"', "")[:40]}.html'
            filepath = os.path.join('workspace', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f'Saved: {filepath}')
            
            # Parse for relevant results
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for result links with publisher information
            result_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text().strip()
                
                # Filter for highly relevant results
                if href and text and len(text) > 15:
                    text_lower = text.lower()
                    relevance_score = 0
                    
                    # High-value terms for 2009 reissue identification
                    high_value_terms = [
                        ('2009', 3),
                        ('martineau', 2),
                        ('atkinson', 2),
                        ('letters', 1),
                        ('laws', 1),
                        ('nature', 1),
                        ('development', 1),
                        ('publisher', 2),
                        ('reissue', 2),
                        ('reprint', 2),
                        ('edition', 1),
                        ('republication', 2)
                    ]
                    
                    # Publisher-specific terms
                    publisher_terms = [
                        ('cambridge university press', 4),
                        ('oxford university press', 4),
                        ('harvard university press', 4),
                        ('yale university press', 4),
                        ('princeton university press', 4),
                        ('university of chicago press', 4),
                        ('routledge', 3),
                        ('palgrave', 3),
                        ('macmillan', 3),
                        ('springer', 3),
                        ('brill', 3),
                        ('ashgate', 3),
                        ('continuum', 3),
                        ('thoemmes', 3),
                        ('pickering', 3),
                        ('nabu press', 2),
                        ('kessinger', 2),
                        ('forgotten books', 2),
                        ('bibliolife', 2),
                        ('gale ecco', 2),
                        ('making of modern law', 2)
                    ]
                    
                    # Calculate relevance score
                    for term, score in high_value_terms + publisher_terms:
                        if term in text_lower:
                            relevance_score += score
                    
                    # Additional scoring for URL domains
                    if href:
                        href_lower = href.lower()
                        if any(domain in href_lower for domain in ['cambridge.org', 'oup.com', 'harvard.edu', 'yale.edu', 'routledge.com', 'palgrave.com']):
                            relevance_score += 3
                        elif any(domain in href_lower for domain in ['amazon.com', 'worldcat.org', 'goodreads.com', 'abebooks.com']):
                            relevance_score += 2
                    
                    if relevance_score >= 3:  # Only include highly relevant results
                        result_links.append({
                            'url': href,
                            'text': text[:300],  # Longer text for better analysis
                            'relevance_score': relevance_score
                        })
            
            # Sort by relevance score
            result_links.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            search_results[query] = {
                'html_file': filepath,
                'status_code': response.status_code,
                'relevant_links': result_links[:15],  # Top 15 most relevant
                'total_links_found': len(result_links)
            }
            
            print(f'Found {len(result_links)} highly relevant links')
            if result_links:
                print('Top results:')
                for i, link in enumerate(result_links[:5], 1):
                    print(f'  {i}. Score {link["relevance_score"]}: {link["text"][:120]}...')
                    print(f'     URL: {link["url"]}')
            
            time.sleep(2)  # Rate limiting
            return True
        else:
            print(f'Failed: HTTP {response.status_code}')
            return False
            
    except Exception as e:
        print(f'Error: {str(e)}')
        return False

# Perform all searches
print('\n=== EXECUTING SEARCHES ===')
successful_searches = 0

for i, query in enumerate(search_queries, 1):
    if perform_search(query, i):
        successful_searches += 1
    
    # Brief pause between searches
    if i < len(search_queries):
        time.sleep(1)

print(f'\n=== STEP 2: ANALYZING SEARCH RESULTS ===')
print(f'Successful searches: {successful_searches}/{len(search_queries)}')

# Compile and analyze all findings
high_priority_findings = []
all_publishers_mentioned = set()
publisher_frequency = {}

# Known academic and reprint publishers to watch for
known_publishers = [
    'Cambridge University Press', 'Oxford University Press', 'Harvard University Press',
    'Yale University Press', 'Princeton University Press', 'University of Chicago Press',
    'Routledge', 'Palgrave Macmillan', 'Springer', 'Brill', 'Ashgate', 'Continuum',
    'Thoemmes Press', 'Pickering & Chatto', 'Nabu Press', 'Kessinger Publishing',
    'Forgotten Books', 'BiblioLife', 'Gale ECCO', 'Making of Modern Law',
    'Elibron Classics', 'Palala Press', 'Wentworth Press', 'Franklin Classics'
]

print('\n--- ANALYZING ALL SEARCH RESULTS FOR PUBLISHER PATTERNS ---')

for query, results in search_results.items():
    print(f'\nQuery: "{query}"')
    print(f'  Relevant links: {results["total_links_found"]}')
    
    for link in results['relevant_links']:
        # Check for 2009 and publisher combinations
        text_lower = link['text'].lower()
        
        if '2009' in text_lower and any(pub.lower() in text_lower for pub in known_publishers):
            # This is a high-priority finding
            matching_publishers = [pub for pub in known_publishers if pub.lower() in text_lower]
            
            high_priority_findings.append({
                'query': query,
                'text': link['text'],
                'url': link['url'],
                'score': link['relevance_score'],
                'publishers_mentioned': matching_publishers,
                'priority': 'HIGH - Contains 2009 + Publisher'
            })
            
            # Track publisher frequency
            for pub in matching_publishers:
                publisher_frequency[pub] = publisher_frequency.get(pub, 0) + 1
                all_publishers_mentioned.add(pub)
            
            print(f'  ðŸŽ¯ HIGH PRIORITY: {matching_publishers} mentioned with 2009')
        
        elif '2009' in text_lower:
            # Contains 2009 but may have publisher info we need to extract
            high_priority_findings.append({
                'query': query,
                'text': link['text'],
                'url': link['url'],
                'score': link['relevance_score'],
                'publishers_mentioned': [],
                'priority': 'MEDIUM - Contains 2009'
            })
            
            print(f'  ðŸ“ MEDIUM: Contains 2009, checking for publisher info')
        
        # Track any publisher mentions regardless of year
        for pub in known_publishers:
            if pub.lower() in text_lower:
                all_publishers_mentioned.add(pub)
                publisher_frequency[pub] = publisher_frequency.get(pub, 0) + 1

print(f'\n=== STEP 3: PUBLISHER FREQUENCY ANALYSIS ===')
print(f'Total unique publishers mentioned: {len(all_publishers_mentioned)}')
print(f'High-priority findings (2009 + publisher): {len([f for f in high_priority_findings if f["priority"].startswith("HIGH")])}')

if publisher_frequency:
    print('\nPublisher mention frequency:')
    sorted_publishers = sorted(publisher_frequency.items(), key=lambda x: x[1], reverse=True)
    for pub, count in sorted_publishers:
        print(f'  {pub}: {count} mentions')
else:
    print('\nNo specific publishers identified in search results')

print(f'\n=== STEP 4: DETAILED ANALYSIS OF HIGH-PRIORITY FINDINGS ===')

if high_priority_findings:
    # Sort by priority and score
    high_priority_findings.sort(key=lambda x: (x['priority'] == 'HIGH - Contains 2009 + Publisher', x['score']), reverse=True)
    
    print(f'\nAnalyzing {len(high_priority_findings)} high-priority findings:')
    
    for i, finding in enumerate(high_priority_findings[:10], 1):  # Top 10 findings
        print(f'\nðŸ” FINDING {i} - {finding["priority"]} (Score: {finding["score"]})')
        print(f'Query: {finding["query"]}')
        print(f'Publishers: {finding["publishers_mentioned"] if finding["publishers_mentioned"] else "None explicitly identified"}')
        print(f'URL: {finding["url"]}')
        print(f'Text: {finding["text"][:400]}...')
        print('-' * 120)
        
        # If this is a high-priority finding with a specific URL, we should investigate further
        if finding['priority'].startswith('HIGH') and finding['url']:
            print(f'  â­ RECOMMENDED FOR DETAILED INVESTIGATION: {finding["url"]}')
else:
    print('\nâš  No high-priority findings identified')
    print('This suggests the 2009 reissue may be from a smaller or specialized publisher')

# Check for specific reprint/specialty publishers
print(f'\n=== STEP 5: CHECKING FOR SPECIALTY REPRINT PUBLISHERS ===')

specialty_publishers = [
    'Nabu Press', 'Kessinger Publishing', 'Forgotten Books', 'BiblioLife',
    'Palala Press', 'Wentworth Press', 'Franklin Classics', 'Elibron Classics',
    'Gale ECCO', 'Making of Modern Law', 'Thoemmes Press', 'Pickering & Chatto'
]

specialty_findings = []
for query, results in search_results.items():
    for link in results['relevant_links']:
        text_lower = link['text'].lower()
        
        for specialty_pub in specialty_publishers:
            if specialty_pub.lower() in text_lower:
                specialty_findings.append({
                    'publisher': specialty_pub,
                    'query': query,
                    'text': link['text'],
                    'url': link['url'],
                    'has_2009': '2009' in text_lower
                })

if specialty_findings:
    print(f'Found {len(specialty_findings)} specialty publisher mentions:')
    
    # Group by publisher
    by_publisher = {}
    for finding in specialty_findings:
        pub = finding['publisher']
        if pub not in by_publisher:
            by_publisher[pub] = []
        by_publisher[pub].append(finding)
    
    for pub, findings in by_publisher.items():
        print(f'\n{pub}: {len(findings)} mentions')
        for finding in findings[:2]:  # Show top 2 per publisher
            status = 'âœ“ WITH 2009' if finding['has_2009'] else '- without 2009'
            print(f'  {status}: {finding["text"][:150]}...')
            print(f'    URL: {finding["url"]}')
else:
    print('No specialty reprint publishers clearly identified')

# Additional search for specific reprint publisher patterns
print(f'\n=== STEP 6: ANALYZING FOR REPRINT PUBLISHER PATTERNS ===')

# Look for common reprint publisher indicators in all search results
reprint_indicators = [
    'reprint', 'reprinted', 'reproduction', 'facsimile', 'digitally printed',
    'print on demand', 'pod', 'classic reprint', 'historical reproduction',
    'nabu', 'kessinger', 'forgotten books', 'bibliolife', 'palala',
    'wentworth', 'franklin classics', 'elibron', 'gale ecco'
]

reprint_pattern_findings = []
for query, results in search_results.items():
    for link in results['relevant_links']:
        text_lower = link['text'].lower()
        
        # Check for reprint indicators
        found_indicators = []
        for indicator in reprint_indicators:
            if indicator in text_lower:
                found_indicators.append(indicator)
        
        if found_indicators and '2009' in text_lower:
            reprint_pattern_findings.append({
                'query': query,
                'text': link['text'],
                'url': link['url'],
                'indicators': found_indicators,
                'score': link['relevance_score']
            })

if reprint_pattern_findings:
    print(f'Found {len(reprint_pattern_findings)} results with 2009 + reprint indicators:')
    
    # Sort by number of indicators and score
    reprint_pattern_findings.sort(key=lambda x: (len(x['indicators']), x['score']), reverse=True)
    
    for i, finding in enumerate(reprint_pattern_findings[:5], 1):
        print(f'\nðŸŽ¯ REPRINT PATTERN {i}:')
        print(f'Indicators: {finding["indicators"]}')
        print(f'Score: {finding["score"]}')
        print(f'URL: {finding["url"]}')
        print(f'Text: {finding["text"][:200]}...')
        print('-' * 80)
else:
    print('No clear reprint patterns with 2009 found')

# Save comprehensive analysis
analysis_results = {
    'search_objective': 'Identify 2009 reissue publisher for Martineau-Atkinson Letters',
    'book_details': {
        'title': 'Letters on the Laws of Man\'s Nature and Development',
        'authors': ['Harriet Martineau', 'Henry George Atkinson'],
        'original_year': 1851,
        'target_reissue_year': 2009
    },
    'search_summary': {
        'total_queries': len(search_queries),
        'successful_searches': successful_searches,
        'total_relevant_links': sum(len(r['relevant_links']) for r in search_results.values())
    },
    'publisher_analysis': {
        'publishers_mentioned': list(all_publishers_mentioned),
        'publisher_frequency': publisher_frequency,
        'high_priority_findings_count': len([f for f in high_priority_findings if f['priority'].startswith('HIGH')]),
        'specialty_publisher_findings': len(specialty_findings),
        'reprint_pattern_findings': len(reprint_pattern_findings)
    },
    'high_priority_findings': high_priority_findings[:10],  # Top 10 findings
    'specialty_findings': specialty_findings,
    'reprint_pattern_findings': reprint_pattern_findings[:5],  # Top 5 reprint patterns
    'search_queries_used': search_queries,
    'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
}

analysis_file = 'workspace/martineau_atkinson_2009_publisher_search_complete.json'
with open(analysis_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_results, f, indent=2, ensure_ascii=False)

print(f'\n*** PUBLISHER SEARCH ANALYSIS COMPLETE ***')
print(f'âœ“ Conducted {successful_searches} successful searches')
print(f'âœ“ Analyzed {sum(len(r["relevant_links"]) for r in search_results.values())} relevant results')
print(f'âœ“ Identified {len(all_publishers_mentioned)} unique publishers mentioned')
print(f'âœ“ Found {len(high_priority_findings)} high-priority findings')
print(f'âœ“ Found {len(reprint_pattern_findings)} reprint pattern matches')
print(f'âœ“ Comprehensive analysis saved to: {analysis_file}')

# Summary of most promising leads
if high_priority_findings:
    top_finding = high_priority_findings[0]
    print(f'\nðŸŽ¯ MOST PROMISING LEAD:')
    print(f'Priority: {top_finding["priority"]}')
    print(f'Publishers mentioned: {top_finding["publishers_mentioned"]}')
    print(f'Score: {top_finding["score"]}')
    print(f'URL: {top_finding["url"]}')
elif reprint_pattern_findings:
    top_reprint = reprint_pattern_findings[0]
    print(f'\nðŸŽ¯ MOST PROMISING REPRINT LEAD:')
    print(f'Reprint indicators: {top_reprint["indicators"]}')
    print(f'Score: {top_reprint["score"]}')
    print(f'URL: {top_reprint["url"]}')
else:
    print(f'\nâš  No definitive 2009 publisher identified in initial search')
    print('Recommend manual investigation of saved search results')

# Check existing search files for additional clues
print(f'\n=== STEP 7: CHECKING EXISTING SEARCH FILES FOR 2009 REFERENCES ===')

workspace_files = [f for f in os.listdir('workspace') if f.endswith('.html')]
print(f'Found {len(workspace_files)} existing HTML search files to analyze')

# Look for 2009 references in existing files
files_with_2009 = []
for filename in workspace_files[:20]:  # Check first 20 files to avoid overwhelming output
    filepath = os.path.join('workspace', filename)
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read().lower()
            if '2009' in content and ('martineau' in content or 'atkinson' in content):
                # Count occurrences
                count_2009 = content.count('2009')
                count_martineau = content.count('martineau')
                count_atkinson = content.count('atkinson')
                
                files_with_2009.append({
                    'filename': filename,
                    'count_2009': count_2009,
                    'count_martineau': count_martineau,
                    'count_atkinson': count_atkinson,
                    'total_relevance': count_2009 + count_martineau + count_atkinson
                })
    except Exception as e:
        print(f'Error reading {filename}: {str(e)}')

if files_with_2009:
    print(f'\nFound {len(files_with_2009)} existing files with 2009 + author references:')
    
    # Sort by relevance
    files_with_2009.sort(key=lambda x: x['total_relevance'], reverse=True)
    
    for i, file_info in enumerate(files_with_2009[:5], 1):
        print(f'{i}. {file_info["filename"]}')
        print(f'   2009: {file_info["count_2009"]}, Martineau: {file_info["count_martineau"]}, Atkinson: {file_info["count_atkinson"]}')
        
        # Try to extract specific publisher information from this file
        filepath = os.path.join('workspace', file_info['filename'])
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                soup = BeautifulSoup(content, 'html.parser')
                
                # Look for text containing both 2009 and publisher names
                text_content = soup.get_text().lower()
                
                # Find sentences/phrases containing 2009
                sentences_with_2009 = []
                lines = text_content.split('\n')
                for line in lines:
                    if '2009' in line and ('martineau' in line or 'atkinson' in line):
                        # Check if any known publishers are mentioned
                        for pub in known_publishers:
                            if pub.lower() in line:
                                sentences_with_2009.append({
                                    'text': line.strip()[:200],
                                    'publisher': pub
                                })
                                break
                        else:
                            # Check for specialty publishers
                            for specialty_pub in specialty_publishers:
                                if specialty_pub.lower() in line:
                                    sentences_with_2009.append({
                                        'text': line.strip()[:200],
                                        'publisher': specialty_pub
                                    })
                                    break
                
                if sentences_with_2009:
                    print(f'   ðŸŽ¯ PUBLISHER CLUES FOUND:')
                    for j, clue in enumerate(sentences_with_2009[:3], 1):
                        print(f'     {j}. {clue["publisher"]}: {clue["text"]}...')
                        
        except Exception as e:
            print(f'   Error analyzing {file_info["filename"]}: {str(e)}')
else:
    print('\nNo existing files found with 2009 + author references')

print('\n=== SEARCH COMPLETION SUMMARY ===')
print('âœ“ Executed comprehensive web searches for 2009 reissue publisher')
print('âœ“ Analyzed both new search results and existing workspace files')
print('âœ“ Applied advanced relevance scoring and pattern detection')
print('âœ“ Checked for both academic and specialty reprint publishers')
print('\nRecommendations for manual follow-up:')
print('1. Review high-priority findings URLs for definitive publisher identification')
print('2. Cross-reference findings with WorldCat or library catalogs')
print('3. Check Amazon or Google Books for 2009 edition details')
print('4. Contact academic libraries for specialized reprint publisher information')
```

## Created Time
2025-08-10 23:24:27
