# Developer Plan 02

## Plan
Search for and retrieve the full text of the research paper 'Trans fatty acid contents in chocolates and chocolate wafers in Turkey' to access its complete bibliography and reference list. Extract all numbered references from the paper's bibliography section, then systematically match each of the 6 table titles from the Excel file against the cited papers to determine which reference numbers correspond to each table's source paper.

## Description
This is the necessary next step because: (1) The developer has successfully extracted all 6 table titles from the Excel file and identified their content themes, (2) To determine the reference numbers, we need access to the original 'Trans fatty acid contents in chocolates and chocolate wafers in Turkey' paper and its numbered bibliography, (3) Expected outcome is to obtain the complete reference list and match each table title to its corresponding source paper in the bibliography, (4) This will enable us to provide the comma-separated list of reference numbers in the order they appear in the Excel file from top to bottom as requested in the TASK

## Episodic Examples
### Development Step 3: US Federal Minimum Butterfat Percentage Required for Ice Cream Classification (2020 Wikipedia Data)

**Description**: Research the US federal standards for butterfat content in ice cream as reported by Wikipedia in 2020. Search for the specific minimum butterfat percentage required by federal regulations for a product to be legally classified as ice cream in the United States. Extract the exact percentage value and any relevant context about these standards.

**Use Cases**:
- Regulatory compliance verification for food manufacturers ensuring their ice cream products meet US federal butterfat standards before market release
- Automated quality assurance checks in dairy production facilities to validate product labeling against legal ice cream definitions
- Food import/export documentation review for customs brokers to confirm imported ice cream products comply with US classification requirements
- Academic research on historical changes in food standards, using extracted butterfat regulations as part of a longitudinal analysis
- Consumer advocacy investigations to identify and report brands mislabeling frozen desserts as ice cream without meeting federal butterfat minimums
- Development of nutrition and ingredient databases for food delivery apps, ensuring accurate product categorization based on federal standards
- Legal case preparation for attorneys representing clients in food labeling disputes, using extracted Wikipedia data as supporting evidence
- Automated content curation for food bloggers or journalists reporting on industry trends and regulatory updates in the US ice cream market

```
import requests
from bs4 import BeautifulSoup
import os
import json
from datetime import datetime
import re

print("=== DIRECT ACCESS TO WIKIPEDIA ICE CREAM PAGE FOR BUTTERFAT STANDARDS ===")
print("Objective: Find US federal minimum butterfat percentage for ice cream classification")
print("Strategy: Direct Wikipedia page access using correct API endpoints\n")

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# First, let's try the correct Wikipedia API endpoint to get the Ice cream page
print("=== STEP 1: ACCESSING WIKIPEDIA ICE CREAM PAGE DIRECTLY ===")

try:
    # Use the correct Wikipedia API endpoint
    api_url = 'https://en.wikipedia.org/w/api.php'
    
    # Get the Ice cream page content
    params = {
        'action': 'query',
        'format': 'json',
        'titles': 'Ice cream',
        'prop': 'extracts',
        'exintro': False,  # Get full content
        'explaintext': True,  # Get plain text
        'exsectionformat': 'wiki'
    }
    
    print("Requesting Ice cream page from Wikipedia...")
    response = requests.get(api_url, params=params, timeout=30)
    response.raise_for_status()
    
    data = response.json()
    print(f"API response received (Status: {response.status_code})")
    
    # Extract page content
    if 'query' in data and 'pages' in data['query']:
        pages = data['query']['pages']
        
        for page_id, page_info in pages.items():
            if 'extract' in page_info:
                page_title = page_info.get('title', 'Unknown')
                full_text = page_info['extract']
                
                print(f"\nSuccessfully retrieved: '{page_title}'")
                print(f"Content length: {len(full_text):,} characters")
                
                # Save the full Wikipedia content for reference
                wiki_content_file = os.path.join(workspace_dir, 'wikipedia_ice_cream_full_content.txt')
                with open(wiki_content_file, 'w', encoding='utf-8') as f:
                    f.write(f"WIKIPEDIA ICE CREAM PAGE CONTENT\n")
                    f.write(f"Retrieved: {datetime.now().isoformat()}\n")
                    f.write(f"Page: {page_title}\n")
                    f.write(f"Content Length: {len(full_text):,} characters\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(full_text)
                
                print(f"Full content saved to: {wiki_content_file}")
                
                # Now search for butterfat content information
                print("\n=== STEP 2: ANALYZING CONTENT FOR BUTTERFAT STANDARDS ===")
                
                # Convert to lowercase for case-insensitive searching
                text_lower = full_text.lower()
                
                # Look for butterfat-related content
                if 'butterfat' in text_lower:
                    print("*** BUTTERFAT CONTENT FOUND ***")
                    
                    # Split into sentences for detailed analysis
                    sentences = full_text.split('.')
                    
                    butterfat_sentences = []
                    federal_standard_sentences = []
                    
                    for sentence in sentences:
                        sentence_clean = sentence.strip()
                        sentence_lower = sentence_clean.lower()
                        
                        # Look for sentences containing butterfat
                        if 'butterfat' in sentence_lower:
                            butterfat_sentences.append(sentence_clean)
                            
                            # Check if it mentions federal standards, FDA, or regulations
                            if any(keyword in sentence_lower for keyword in ['federal', 'fda', 'regulation', 'standard', 'minimum', 'require']):
                                federal_standard_sentences.append(sentence_clean)
                    
                    print(f"\nSentences mentioning butterfat: {len(butterfat_sentences)}")
                    print(f"Sentences about federal standards: {len(federal_standard_sentences)}")
                    
                    # Display butterfat sentences
                    if butterfat_sentences:
                        print("\n=== BUTTERFAT CONTENT ANALYSIS ===")
                        
                        for i, sentence in enumerate(butterfat_sentences, 1):
                            print(f"\n{i}. {sentence}")
                            
                            # Extract percentage values from each sentence
                            percentage_patterns = [
                                r'(\d+(?:\.\d+)?)\s*(?:percent|%)',
                                r'(\d+(?:\.\d+)?)\s*(?:per cent)',
                                r'(\d+(?:\.\d+)?)\s*(?:pct)'
                            ]
                            
                            found_percentages = []
                            for pattern in percentage_patterns:
                                matches = re.findall(pattern, sentence, re.IGNORECASE)
                                found_percentages.extend(matches)
                            
                            if found_percentages:
                                print(f"   Percentages found: {found_percentages}")
                                
                                # Check for context indicating minimum federal standard
                                if any(keyword in sentence.lower() for keyword in ['minimum', 'federal', 'fda', 'standard', 'regulation', 'require']):
                                    print(f"   *** POTENTIAL FEDERAL STANDARD: {found_percentages} ***")
                    
                    # Focus on federal standard sentences
                    if federal_standard_sentences:
                        print("\n=== FEDERAL STANDARD SENTENCES ===")
                        
                        federal_standards_found = []
                        
                        for i, sentence in enumerate(federal_standard_sentences, 1):
                            print(f"\n{i}. {sentence}")
                            
                            # Extract percentages from federal standard sentences
                            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*(?:percent|%)', sentence, re.IGNORECASE)
                            
                            if percentages:
                                print(f"   Federal standard percentages: {percentages}")
                                
                                federal_standards_found.append({
                                    'sentence': sentence,
                                    'percentages': percentages,
                                    'context': 'federal_standard'
                                })
                        
                        # Save federal standards analysis
                        if federal_standards_found:
                            standards_file = os.path.join(workspace_dir, 'federal_butterfat_standards.json')
                            with open(standards_file, 'w') as f:
                                json.dump({
                                    'analysis_date': datetime.now().isoformat(),
                                    'source': 'Wikipedia Ice cream page',
                                    'objective': 'US federal minimum butterfat percentage for ice cream',
                                    'federal_standards_found': len(federal_standards_found),
                                    'standards_data': federal_standards_found,
                                    'all_butterfat_sentences': butterfat_sentences
                                }, f, indent=2)
                            
                            print(f"\nFederal standards analysis saved to: {standards_file}")
                            
                            # Extract the most likely federal minimum percentage
                            print("\n=== FEDERAL MINIMUM BUTTERFAT PERCENTAGE EXTRACTION ===")
                            
                            all_federal_percentages = []
                            for standard in federal_standards_found:
                                all_federal_percentages.extend(standard['percentages'])
                            
                            if all_federal_percentages:
                                # Convert to float and find common values
                                percentage_values = []
                                for pct in all_federal_percentages:
                                    try:
                                        percentage_values.append(float(pct))
                                    except ValueError:
                                        continue
                                
                                if percentage_values:
                                    unique_percentages = list(set(percentage_values))
                                    print(f"Unique federal percentages found: {unique_percentages}")
                                    
                                    # Look for the most commonly mentioned percentage
                                    from collections import Counter
                                    percentage_counts = Counter(percentage_values)
                                    most_common = percentage_counts.most_common(1)
                                    
                                    if most_common:
                                        federal_minimum = most_common[0][0]
                                        frequency = most_common[0][1]
                                        
                                        print(f"\n*** FEDERAL MINIMUM BUTTERFAT PERCENTAGE: {federal_minimum}% ***")
                                        print(f"Mentioned {frequency} time(s) in federal standard contexts")
                                        
                                        # Find the specific sentence with this percentage
                                        for standard in federal_standards_found:
                                            if str(federal_minimum) in standard['percentages'] or str(int(federal_minimum)) in standard['percentages']:
                                                print(f"\nSource sentence: {standard['sentence']}")
                                                break
                                        
                                        # Save the final result
                                        result_file = os.path.join(workspace_dir, 'us_federal_ice_cream_butterfat_standard.json')
                                        with open(result_file, 'w') as f:
                                            json.dump({
                                                'analysis_date': datetime.now().isoformat(),
                                                'source': 'Wikipedia Ice cream page (2020 information)',
                                                'federal_minimum_butterfat_percentage': federal_minimum,
                                                'percentage_unit': 'percent',
                                                'context': 'US federal regulations for ice cream classification',
                                                'frequency_mentioned': frequency,
                                                'supporting_evidence': [s['sentence'] for s in federal_standards_found if str(federal_minimum) in s['percentages'] or str(int(federal_minimum)) in s['percentages']],
                                                'all_federal_percentages_found': unique_percentages
                                            }, f, indent=2)
                                        
                                        print(f"\nFinal result saved to: {result_file}")
                                        
                                        print(f"\n=== PLAN OBJECTIVE COMPLETED ===")
                                        print(f"US Federal Minimum Butterfat Content for Ice Cream: {federal_minimum}%")
                                        print(f"Source: Wikipedia (2020 information)")
                                        print(f"Context: Federal regulations for legal ice cream classification")
                
                else:
                    print("No butterfat content found in the Wikipedia Ice cream page.")
                    print("Searching for alternative terms...")
                    
                    # Search for alternative terms
                    alternative_terms = ['fat content', 'milk fat', 'dairy fat', 'cream content', 'fat percentage']
                    
                    for term in alternative_terms:
                        if term in text_lower:
                            print(f"Found alternative term: '{term}'")
                            
                            # Extract sentences with alternative terms
                            sentences = full_text.split('.')
                            relevant_sentences = []
                            
                            for sentence in sentences:
                                if term in sentence.lower():
                                    relevant_sentences.append(sentence.strip())
                            
                            if relevant_sentences:
                                print(f"Sentences with '{term}': {len(relevant_sentences)}")
                                for i, sentence in enumerate(relevant_sentences[:3], 1):  # Show first 3
                                    print(f"  {i}. {sentence[:200]}{'...' if len(sentence) > 200 else ''}")
            else:
                print("No content extract available from the Wikipedia page.")
    else:
        print("Error: Could not retrieve Wikipedia page data.")
        print(f"API response structure: {list(data.keys()) if isinstance(data, dict) else 'Not a dictionary'}")

except requests.exceptions.RequestException as e:
    print(f"Error accessing Wikipedia API: {e}")
    print("Will try alternative approach...")
    
    # Alternative approach: Direct HTML scraping
    print("\n=== ALTERNATIVE APPROACH: DIRECT HTML SCRAPING ===")
    
    try:
        # Direct access to Wikipedia Ice cream page
        wiki_url = 'https://en.wikipedia.org/wiki/Ice_cream'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(f"Accessing Wikipedia Ice cream page directly: {wiki_url}")
        response = requests.get(wiki_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print(f"Successfully accessed Wikipedia page (Status: {response.status_code})")
        print(f"Content length: {len(response.content):,} bytes")
        
        # Parse HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract text content
        page_text = soup.get_text()
        
        print(f"Extracted text length: {len(page_text):,} characters")
        
        # Search for butterfat content
        if 'butterfat' in page_text.lower():
            print("\n*** BUTTERFAT CONTENT FOUND VIA HTML SCRAPING ***")
            
            # Split into paragraphs for analysis
            paragraphs = page_text.split('\n')
            
            butterfat_paragraphs = []
            for paragraph in paragraphs:
                if 'butterfat' in paragraph.lower() and len(paragraph.strip()) > 20:
                    butterfat_paragraphs.append(paragraph.strip())
            
            print(f"Paragraphs mentioning butterfat: {len(butterfat_paragraphs)}")
            
            for i, paragraph in enumerate(butterfat_paragraphs, 1):
                print(f"\n{i}. {paragraph[:300]}{'...' if len(paragraph) > 300 else ''}")
                
                # Look for percentages
                percentages = re.findall(r'(\d+(?:\.\d+)?)\s*(?:percent|%)', paragraph, re.IGNORECASE)
                if percentages:
                    print(f"   Percentages: {percentages}")
                    
                    # Check for federal context
                    if any(keyword in paragraph.lower() for keyword in ['federal', 'fda', 'regulation', 'standard', 'minimum']):
                        print(f"   *** POTENTIAL FEDERAL STANDARD: {percentages} ***")
        
        else:
            print("No butterfat content found via HTML scraping either.")
            print("The Wikipedia page may not contain the specific federal butterfat standards.")
    
    except Exception as scraping_error:
        print(f"HTML scraping also failed: {scraping_error}")

except Exception as e:
    print(f"Unexpected error during Wikipedia analysis: {e}")
    import traceback
    traceback.print_exc()

print("\n=== WIKIPEDIA ANALYSIS COMPLETE ===")
```

### Development Step 8: Extract, Alphabetize EC 1.11.1.7;3.1.3.1 Chemicals from 2016 Wiley Sweet Potato Virus Paper

**Description**: Access the identified 2016 Wiley paper 'Effects of Sweet Potato Feathery Mottle Virus and Sweet Potato Chlorotic Stunt Virus' and extract the specific chemicals with EC numbers 1.11.1.7 and 3.1.3.1 used in the virus testing methods. Identify the chemical names corresponding to these EC numbers, alphabetize them, and format the EC numbers in the required semicolon-separated order.

**Use Cases**:
- Agricultural pathology workflows for automating the extraction of enzyme reagents (EC 1.11.1.7 and 3.1.3.1) from crop‐virus research to rapidly assemble diagnostic assay protocols
- Pharmaceutical R&D literature mining to identify peroxidase and alkaline phosphatase methods for high‐throughput drug screening assays
- Environmental compliance reporting by compiling a standardized database of enzyme‐based soil and water testing chemicals for regulatory submissions
- Food quality control in dairy and beverage industries to automate retrieval of phosphatase assay components from published validation studies
- Patent landscaping in biotechnology to extract and alphabetize EC number–linked chemicals from patent documents for competitor intelligence
- Contract research organization (CRO) proposal generation by auto‐populating materials and methods sections with enzymatic reagents from target protocols
- Academic systematic reviews of plant virology methods to standardize and format all EC 1.11.1.7;3.1.3.1 enzyme usages across multiple studies

```
import os
import json
from datetime import datetime

# First, let's properly inspect and load the search results
print("Locating and inspecting search results files...")
print("="*80)

# Check both workspace locations mentioned in the history
search_file_paths = [
    'workspace/sweet_potato_virus_paper_search_20250806_185041.json',
    'workspace_2a649bb1-795f-4a01-b3be-9a01868dae73/sweet_potato_virus_paper_search_20250806_185041.json'
]

search_data = None
used_path = None

for path in search_file_paths:
    if os.path.exists(path):
        print(f"Found search results file: {path}")
        used_path = path
        
        # First inspect the file structure
        print(f"\nInspecting file structure...")
        with open(path, 'r', encoding='utf-8') as f:
            search_data = json.load(f)
        
        print("Top-level keys:")
        for key in search_data.keys():
            if isinstance(search_data[key], list):
                print(f"  - {key}: list with {len(search_data[key])} items")
            elif isinstance(search_data[key], dict):
                print(f"  - {key}: dict with keys {list(search_data[key].keys())}")
            else:
                print(f"  - {key}: {search_data[key]}")
        
        break

if not search_data:
    print("No search results file found. Need to run search first.")
else:
    print(f"\nUsing search data from: {used_path}")
    print(f"Target: {search_data.get('target_paper', 'N/A')}")
    print(f"EC Numbers: {search_data.get('target_ec_numbers', 'N/A')}")
    
    # Now analyze the search results with proper variable scoping
    print("\n" + "="*80)
    print("ANALYZING SEARCH RESULTS FOR PAPER AND EC NUMBERS")
    print("="*80)
    
    paper_candidates = []
    ec_number_sources = []
    
    # Process each search query result set
    search_results = search_data.get('search_results', [])
    print(f"Processing {len(search_results)} search result sets...\n")
    
    for query_idx, query_result in enumerate(search_results, 1):
        query = query_result.get('query', 'Unknown query')
        results = query_result.get('results', [])
        
        print(f"Query {query_idx}: {query}")
        print(f"Results found: {len(results)}")
        print("-"*50)
        
        # Analyze each result in this query set
        for result_idx, result in enumerate(results[:8], 1):  # Top 8 results per query
            title = result.get('title', 'No title')
            link = result.get('link', 'No URL')
            snippet = result.get('snippet', 'No snippet')
            
            # Create combined text for analysis (fix the variable scoping issue)
            title_lower = title.lower()
            snippet_lower = snippet.lower()
            link_lower = link.lower()
            combined_text = f"{title_lower} {snippet_lower} {link_lower}"
            
            print(f"  {result_idx}. {title[:80]}...")
            print(f"      URL: {link}")
            
            # Score relevance for the target paper
            relevance_score = 0
            matching_indicators = []
            
            # Check for paper-specific terms
            if 'sweet potato feathery mottle virus' in combined_text:
                relevance_score += 10
                matching_indicators.append('SPFMV')
            if 'sweet potato chlorotic stunt virus' in combined_text:
                relevance_score += 10
                matching_indicators.append('SPCSV')
            if '2016' in combined_text:
                relevance_score += 5
                matching_indicators.append('2016')
            if 'wiley' in combined_text or 'onlinelibrary.wiley.com' in combined_text:
                relevance_score += 5
                matching_indicators.append('Wiley')
            if 'effects' in combined_text:
                relevance_score += 3
                matching_indicators.append('Effects')
            if 'uganda' in combined_text:
                relevance_score += 2
                matching_indicators.append('Uganda')
            
            # Check for EC numbers or enzyme-related content
            ec_indicators = []
            if '1.11.1.7' in combined_text:
                relevance_score += 8
                ec_indicators.append('EC 1.11.1.7')
            if '3.1.3.1' in combined_text:
                relevance_score += 8
                ec_indicators.append('EC 3.1.3.1')
            if any(term in combined_text for term in ['ec number', 'enzyme', 'alkaline phosphatase', 'peroxidase']):
                relevance_score += 4
                ec_indicators.append('Enzyme terms')
            
            if matching_indicators:
                print(f"      📊 Relevance Score: {relevance_score}")
                print(f"      🎯 Indicators: {', '.join(matching_indicators)}")
                if ec_indicators:
                    print(f"      🧪 EC/Enzyme: {', '.join(ec_indicators)}")
            
            # Store high-relevance paper candidates
            if relevance_score >= 15:
                paper_candidates.append({
                    'title': title,
                    'link': link,
                    'snippet': snippet,
                    'score': relevance_score,
                    'indicators': matching_indicators + ec_indicators,
                    'query': query,
                    'is_wiley_direct': 'onlinelibrary.wiley.com' in link_lower
                })
                print(f"      ⭐ HIGH RELEVANCE - Added to candidates")
            
            # Store EC number sources separately
            if any(ec in combined_text for ec in ['1.11.1.7', '3.1.3.1']):
                ec_number_sources.append({
                    'title': title,
                    'link': link,
                    'snippet': snippet,
                    'ec_numbers_found': [ec for ec in ['1.11.1.7', '3.1.3.1'] if ec in combined_text],
                    'query': query
                })
                print(f"      🔬 EC NUMBERS FOUND - Added to EC sources")
        
        print()  # Blank line between queries
    
    # Sort candidates by relevance score
    paper_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    print("="*80)
    print(f"ANALYSIS RESULTS SUMMARY")
    print("="*80)
    
    print(f"\n📚 PAPER CANDIDATES FOUND: {len(paper_candidates)}")
    if paper_candidates:
        print("\nTop candidates:")
        for i, candidate in enumerate(paper_candidates[:3], 1):
            print(f"\n{i}. SCORE: {candidate['score']}")
            print(f"   Title: {candidate['title']}")
            print(f"   URL: {candidate['link']}")
            print(f"   Indicators: {', '.join(candidate['indicators'])}")
            print(f"   Direct Wiley Access: {'✅ YES' if candidate['is_wiley_direct'] else '❌ NO'}")
            
            # Check if this is likely the target paper
            if (candidate['score'] >= 25 and 
                candidate['is_wiley_direct'] and 
                'effects' in candidate['title'].lower()):
                print(f"   🎯 THIS IS LIKELY THE TARGET PAPER!")
    
    print(f"\n🧪 EC NUMBER SOURCES FOUND: {len(ec_number_sources)}")
    if ec_number_sources:
        print("\nEC number sources:")
        for i, source in enumerate(ec_number_sources, 1):
            print(f"\n{i}. Title: {source['title']}")
            print(f"   URL: {source['link']}")
            print(f"   EC Numbers: {', '.join(source['ec_numbers_found'])}")
            print(f"   Snippet: {source['snippet'][:200]}...")
            
            # Look for chemical names in the snippet
            snippet_lower = source['snippet'].lower()
            chemical_hints = []
            if 'alkaline phosphatase' in snippet_lower:
                chemical_hints.append('Alkaline phosphatase (likely EC 3.1.3.1)')
            if 'peroxidase' in snippet_lower:
                chemical_hints.append('Peroxidase (likely EC 1.11.1.7)')
            if 'alkaline' in snippet_lower and 'phosphatase' not in snippet_lower:
                chemical_hints.append('Contains "alkaline" - may refer to alkaline phosphatase')
            
            if chemical_hints:
                print(f"   💡 Chemical hints: {'; '.join(chemical_hints)}")
    
    # Save comprehensive analysis
    analysis_results = {
        'analysis_timestamp': datetime.now().isoformat(),
        'target_paper': search_data.get('target_paper'),
        'target_ec_numbers': search_data.get('target_ec_numbers'),
        'paper_candidates': paper_candidates,
        'ec_number_sources': ec_number_sources,
        'top_candidate': paper_candidates[0] if paper_candidates else None,
        'analysis_summary': {
            'total_paper_candidates': len(paper_candidates),
            'total_ec_sources': len(ec_number_sources),
            'wiley_direct_access': len([c for c in paper_candidates if c['is_wiley_direct']]),
            'high_confidence_match': len([c for c in paper_candidates if c['score'] >= 25]) > 0
        }
    }
    
    analysis_file = 'workspace/comprehensive_paper_analysis.json'
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n📋 NEXT STEPS RECOMMENDATION:")
    if paper_candidates and paper_candidates[0]['score'] >= 25:
        print(f"✅ Target paper identified with high confidence")
        print(f"✅ Direct Wiley access available: {paper_candidates[0]['link']}")
        print(f"🔄 NEXT: Access paper content to extract EC number chemical names")
        
        # Based on the EC sources found, provide initial chemical identification
        print(f"\n🧪 PRELIMINARY EC NUMBER CHEMICAL IDENTIFICATION:")
        print(f"Based on search results analysis:")
        print(f"   EC 1.11.1.7 = Peroxidase (enzyme that catalyzes oxidation reactions)")
        print(f"   EC 3.1.3.1 = Alkaline phosphatase (enzyme that removes phosphate groups)")
        print(f"\n📝 ALPHABETICAL ORDER: Alkaline phosphatase, Peroxidase")
        print(f"📝 EC FORMAT: 3.1.3.1;1.11.1.7")
        
    else:
        print(f"⚠️ Need to access paper content directly for confirmation")
        print(f"⚠️ May need additional search strategies")
    
    print(f"\nAnalysis saved to: {analysis_file}")
    print(f"Ready for content extraction phase.")
```

### Development Step 9: Download Westerink’s "A Dark Trace" from Project MUSE and Extract Chapter 2’s Influential Author

**Description**: Access and download the full text of 'A Dark Trace: Sigmund Freud on the Sense of Guilt' by H. Westerink from Project MUSE using DOI 10.1353/book.24372. Since the book was confirmed to be open access, retrieve the complete text and save it to workspace/dark_trace_freud_book.pdf or appropriate format. Focus on locating and extracting Chapter 2 content to identify the author who influenced Freud's belief in 'endopsychic myths'. If the full book is not directly downloadable, extract Chapter 2 specifically or access the book's table of contents to determine the exact chapter title and content structure.

**Use Cases**:
- Legal due diligence in corporate mergers: automatically download open-access regulatory codes in PDF, search for “antitrust” and “competition” term variations, extract and summarize context to identify potential deal blockers.
- Pharmaceutical literature review automation: fetch clinical trial protocols via DOI, load full-text PDFs, search for “double-blind” and “placebo” mentions, and extract methodological passages along with author names for evidence synthesis.
- Patent portfolio analysis for semiconductor R&D: retrieve patent documents from public repositories, scan PDFs for “heterojunction” and “quantum well” variants, extract inventor citations and contextual explanations to map technology lineage.
- Historical philosophy research on Nietzsche and Kant: access digitized editions of 19th-century works, locate references to “categorical imperative” or “will to power,” and extract surrounding paragraphs to trace cross-author influences.
- Competitive intelligence from SEC filings: download publicly available 10-K and 10-Q reports, search for “risk factor,” “liquidity risk,” and “market volatility” variations, and pull relevant excerpts for financial analysis dashboards.
- Academic curriculum design from open textbooks: ingest complete PDF textbooks via DOIs, locate chapter summaries or “learning objectives” headings, extract and compile structured outlines for course syllabi.
- Investigative journalism document mining: import leaked policy PDFs, search for “whistleblower,” “confidential,” and “internal memo” terms, extract context with names and dates to support storytelling.
- Compliance monitoring in healthcare: load clinical guideline PDFs, scan for “contraindication,” “adverse effect,” and “off-label” variations, and extract detailed sections with authoring bodies for automated policy updates.

```
from langchain_community.document_loaders import PyPDFLoader
import os
import json

print('=== SEARCHING ENTIRE BOOK FOR "ENDOPSYCHIC MYTHS" REFERENCES ===')
print('Objective: Since Chapter 2 did not contain "endopsychic" references, search the complete book to locate this specific term and identify the influencing author\n')

# Load the PDF and search the entire document
workspace_files = os.listdir('workspace')
pdf_files = [f for f in workspace_files if f.endswith('.pdf')]

if pdf_files:
    pdf_path = os.path.join('workspace', pdf_files[0])
    print(f'Searching entire PDF: {pdf_path}')
    
    try:
        # Load the complete PDF
        loader = PyPDFLoader(pdf_path)
        pages = loader.load_and_split()
        
        print(f'✓ PDF loaded successfully')
        print(f'Total pages to search: {len(pages)}')
        
        # Combine all pages into full text
        full_text = '\n\n'.join([page.page_content for page in pages])
        print(f'Total document length: {len(full_text):,} characters')
        
        # Search for "endopsychic" variations
        endopsychic_variations = [
            'endopsychic myth',
            'endopsychic myths',
            'endopsychic',
            'endo-psychic',
            'endopsychical'
        ]
        
        print('\n=== SEARCHING FOR ENDOPSYCHIC VARIATIONS ===')
        
        found_endopsychic = False
        full_text_lower = full_text.lower()
        
        for variation in endopsychic_variations:
            count = full_text_lower.count(variation.lower())
            if count > 0:
                print(f'✓ Found "{variation}": {count} occurrences')
                found_endopsychic = True
                
                # Extract all positions for this variation
                positions = []
                start = 0
                while True:
                    pos = full_text_lower.find(variation.lower(), start)
                    if pos == -1:
                        break
                    positions.append(pos)
                    start = pos + 1
                
                print(f'\n--- EXTRACTING ALL "{variation.upper()}" REFERENCES ({len(positions)} found) ---')
                
                for i, pos in enumerate(positions, 1):
                    # Extract substantial context around each occurrence
                    context_start = max(0, pos - 1000)
                    context_end = min(len(full_text), pos + 1200)
                    context = full_text[context_start:context_end]
                    
                    # Determine which page this occurs on
                    char_count = 0
                    page_num = 0
                    for page_idx, page in enumerate(pages):
                        if char_count + len(page.page_content) >= pos:
                            page_num = page_idx + 1
                            break
                        char_count += len(page.page_content) + 2  # +2 for \n\n separator
                    
                    print(f'\n🎯 REFERENCE {i} - Position {pos} (Page ~{page_num}):')
                    print('='*120)
                    print(context)
                    print('='*120)
                    
                    # Analyze this passage for author influences
                    context_lower = context.lower()
                    potential_authors = [
                        'jung', 'carl jung', 'c.g. jung', 'c. g. jung',
                        'nietzsche', 'friedrich nietzsche', 'f. nietzsche',
                        'schopenhauer', 'arthur schopenhauer', 'a. schopenhauer',
                        'kant', 'immanuel kant', 'i. kant',
                        'darwin', 'charles darwin', 'c. darwin',
                        'hegel', 'georg hegel', 'g.w.f. hegel',
                        'goethe', 'johann wolfgang von goethe',
                        'lamarck', 'jean-baptiste lamarck'
                    ]
                    
                    mentioned_authors = []
                    for author in potential_authors:
                        if author in context_lower:
                            mentioned_authors.append(author)
                    
                    if mentioned_authors:
                        print(f'\n*** AUTHORS MENTIONED IN THIS PASSAGE: {[author.title() for author in mentioned_authors]} ***')
                        
                        # Look for specific influence language
                        influence_phrases = [
                            'influenced by', 'influence of', 'influenced freud',
                            'borrowed from', 'adopted from', 'derived from',
                            'took from', 'learned from', 'inspired by',
                            'following', 'based on', 'according to'
                        ]
                        
                        found_influence_language = []
                        for phrase in influence_phrases:
                            if phrase in context_lower:
                                found_influence_language.append(phrase)
                        
                        if found_influence_language:
                            print(f'🔍 INFLUENCE LANGUAGE DETECTED: {found_influence_language}')
                            print('\n🎯 THIS PASSAGE LIKELY CONTAINS THE ANSWER! 🎯')
                        
                        # Look for direct statements about endopsychic myths
                        myth_context_phrases = [
                            'concept of endopsychic', 'idea of endopsychic', 'notion of endopsychic',
                            'endopsychic concept', 'endopsychic idea', 'endopsychic notion',
                            'belief in endopsychic', 'theory of endopsychic'
                        ]
                        
                        found_myth_context = []
                        for phrase in myth_context_phrases:
                            if phrase in context_lower:
                                found_myth_context.append(phrase)
                        
                        if found_myth_context:
                            print(f'💡 ENDOPSYCHIC CONCEPT LANGUAGE: {found_myth_context}')
                    
                    else:
                        print('\nNo specific authors mentioned in this immediate passage')
                        print('Searching for author names in broader context...')
                        
                        # Expand search area for author names
                        expanded_start = max(0, pos - 2000)
                        expanded_end = min(len(full_text), pos + 2000)
                        expanded_context = full_text[expanded_start:expanded_end]
                        expanded_lower = expanded_context.lower()
                        
                        broader_authors = []
                        for author in potential_authors:
                            if author in expanded_lower:
                                broader_authors.append(author)
                        
                        if broader_authors:
                            print(f'Authors in broader context: {[author.title() for author in broader_authors]}')
                    
                    print(f'\n{"-"*120}\n')
            else:
                print(f'✗ "{variation}": Not found')
        
        if not found_endopsychic:
            print('\n⚠ No "endopsychic" variations found in the entire document')
            print('The term may be referenced differently or may not be the exact phrase used')
            
            # Search for related mythological concepts that might be the actual term
            print('\n=== SEARCHING FOR ALTERNATIVE MYTHOLOGICAL CONCEPTS ===')
            
            alternative_terms = [
                'unconscious myth',
                'psychic myth',
                'mental myth',
                'psychological myth',
                'inner myth',
                'primitive myth',
                'ancestral memory',
                'collective unconscious',
                'phylogenetic',
                'archaic heritage',
                'primal fantasies',
                'inherited memory'
            ]
            
            found_alternatives = []
            
            for term in alternative_terms:
                count = full_text_lower.count(term.lower())
                if count > 0:
                    found_alternatives.append((term, count))
                    print(f'✓ Found "{term}": {count} occurrences')
            
            if found_alternatives:
                print(f'\n=== EXAMINING TOP ALTERNATIVE CONCEPTS ===')
                
                # Focus on the most promising alternative (highest count)
                top_alternative = max(found_alternatives, key=lambda x: x[1])
                term, count = top_alternative
                
                print(f'\nExamining most frequent alternative: "{term}" ({count} occurrences)')
                
                positions = []
                start = 0
                while True:
                    pos = full_text_lower.find(term.lower(), start)
                    if pos == -1:
                        break
                    positions.append(pos)
                    start = pos + 1
                
                # Show first few occurrences
                for i, pos in enumerate(positions[:3], 1):
                    context_start = max(0, pos - 800)
                    context_end = min(len(full_text), pos + 1000)
                    context = full_text[context_start:context_end]
                    
                    # Determine page number
                    char_count = 0
                    page_num = 0
                    for page_idx, page in enumerate(pages):
                        if char_count + len(page.page_content) >= pos:
                            page_num = page_idx + 1
                            break
                        char_count += len(page.page_content) + 2
                    
                    print(f'\nAlternative Reference {i} - "{term}" (Page ~{page_num}):')
                    print('='*100)
                    print(context)
                    print('='*100)
                    
                    # Check for author influences
                    context_lower = context.lower()
                    mentioned_authors = []
                    for author in ['jung', 'nietzsche', 'schopenhauer', 'kant', 'darwin', 'lamarck']:
                        if author in context_lower:
                            mentioned_authors.append(author)
                    
                    if mentioned_authors:
                        print(f'\nAuthors mentioned: {[a.title() for a in mentioned_authors]}')
                    
                    print(f'\n{"-"*100}\n')
        
        # Also search for direct references to key authors with mythological context
        print('\n=== SEARCHING FOR AUTHORS WITH MYTHOLOGICAL/INHERITANCE CONTEXT ===')
        
        key_authors_with_context = [
            ('jung', ['myth', 'mythology', 'collective', 'archetype']),
            ('lamarck', ['inheritance', 'inherited', 'acquired', 'transmission']),
            ('darwin', ['inheritance', 'heredity', 'evolution', 'acquired']),
            ('nietzsche', ['myth', 'mythology', 'cultural', 'psychological'])
        ]
        
        for author, context_terms in key_authors_with_context:
            author_positions = []
            start = 0
            while True:
                pos = full_text_lower.find(author.lower(), start)
                if pos == -1:
                    break
                author_positions.append(pos)
                start = pos + 1
            
            if author_positions:
                print(f'\n--- {author.upper()} REFERENCES WITH MYTHOLOGICAL CONTEXT ---')
                
                relevant_passages = []
                for pos in author_positions:
                    context_start = max(0, pos - 500)
                    context_end = min(len(full_text), pos + 700)
                    context = full_text[context_start:context_end]
                    context_lower = context.lower()
                    
                    # Check if this passage contains relevant mythological context
                    has_context = any(term in context_lower for term in context_terms)
                    if has_context:
                        relevant_passages.append((pos, context))
                
                if relevant_passages:
                    print(f'Found {len(relevant_passages)} relevant passages for {author.title()}:')
                    
                    for i, (pos, context) in enumerate(relevant_passages[:2], 1):
                        # Determine page
                        char_count = 0
                        page_num = 0
                        for page_idx, page in enumerate(pages):
                            if char_count + len(page.page_content) >= pos:
                                page_num = page_idx + 1
                                break
                            char_count += len(page.page_content) + 2
                        
                        print(f'\n{author.title()} Passage {i} (Page ~{page_num}):')
                        print('='*90)
                        print(context)
                        print('='*90)
                else:
                    print(f'No mythological context found for {author.title()}')
        
        # Save comprehensive search results
        search_results = {
            'search_objective': 'Find author who influenced Freud\'s belief in "endopsychic myths"',
            'document_stats': {
                'total_pages': len(pages),
                'total_characters': len(full_text)
            },
            'endopsychic_search': {
                'variations_searched': endopsychic_variations,
                'found_endopsychic': found_endopsychic,
                'total_occurrences': sum(full_text_lower.count(v.lower()) for v in endopsychic_variations)
            },
            'alternative_terms_found': found_alternatives if 'found_alternatives' in locals() else [],
            'search_timestamp': '2025-01-21 13:00:00'
        }
        
        results_file = 'workspace/complete_book_endopsychic_search_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(search_results, f, indent=2, ensure_ascii=False)
        
        print(f'\n*** COMPLETE BOOK SEARCH RESULTS ***')
        print(f'✓ Searched {len(pages)} pages ({len(full_text):,} characters)')
        print(f'✓ Endopsychic references found: {found_endopsychic}')
        
        if found_endopsychic:
            print('🎯 SUCCESS: Found "endopsychic" references in the book!')
            print('The extracted passages above should reveal the author who influenced Freud')
        else:
            print('⚠ "Endopsychic" not found - the term may be referenced differently')
            if 'found_alternatives' in locals() and found_alternatives:
                print(f'Alternative concepts found: {[term for term, count in found_alternatives]}')
        
        print(f'✓ Complete search results saved to: {results_file}')
        
    except Exception as e:
        print(f'❌ Error during complete book search: {str(e)}')

else:
    print('❌ No PDF files found in workspace')

print('\n=== COMPLETE BOOK SEARCH FINISHED ===')
print('Objective: Locate the specific author who influenced Freud\'s concept of "endopsychic myths"')
print('Status: Comprehensive search of entire book completed')
```

### Development Step 2: US Federal Minimum Butterfat Percentage Required for Ice Cream Classification (2020 Wikipedia Reference)

**Description**: Research the US federal standards for butterfat content in ice cream as reported by Wikipedia in 2020. Search for the specific minimum butterfat percentage required by federal regulations for a product to be legally classified as ice cream in the United States. Extract the exact percentage value and any relevant context about these standards.

**Use Cases**:
- Regulatory compliance verification for food manufacturers ensuring their ice cream products meet US federal butterfat standards before distribution
- Automated quality control checks in dairy processing plants to validate product recipes against legal definitions of ice cream
- Market research analysis for food industry consultants comparing international ice cream standards for product localization
- Academic research projects in food science departments studying the evolution of US ice cream regulations over time
- Development of consumer-facing mobile apps that educate users about food labeling and legal definitions of dairy products
- Legal due diligence for import/export businesses verifying that imported frozen desserts comply with US classification standards
- Automated content generation for food bloggers or nutrition websites explaining regulatory requirements for ice cream labeling
- Internal auditing tools for large food brands to periodically scrape and update regulatory data for compliance documentation

```
import requests
from bs4 import BeautifulSoup
import os
import json
from datetime import datetime
import re

print("=== RESEARCHING US FEDERAL ICE CREAM BUTTERFAT STANDARDS FROM WIKIPEDIA 2020 ===")
print("Objective: Find minimum butterfat percentage required by federal regulations for ice cream classification")
print("Target: Wikipedia information as reported in 2020\n")

# Ensure workspace directory exists
workspace_dir = 'workspace'
os.makedirs(workspace_dir, exist_ok=True)

# Search strategy: Look for Wikipedia pages about ice cream, food standards, FDA regulations
search_targets = [
    'Ice cream',
    'Ice cream (United States)',
    'Food and Drug Administration',
    'FDA food standards',
    'Dairy product standards',
    'Frozen dessert standards'
]

print("=== STEP 1: SEARCHING WIKIPEDIA FOR ICE CREAM STANDARDS PAGES ===")

# Wikipedia search API to find relevant pages
wikipedia_search_results = []

for target in search_targets:
    print(f"\nSearching Wikipedia for: '{target}'")
    
    # Use Wikipedia search API
    search_url = 'https://en.wikipedia.org/api/rest_v1/page/search'
    params = {
        'q': target,
        'limit': 5
    }
    
    try:
        response = requests.get(search_url, params=params, timeout=10)
        response.raise_for_status()
        
        search_data = response.json()
        
        if 'pages' in search_data:
            print(f"Found {len(search_data['pages'])} results:")
            
            for page in search_data['pages']:
                title = page.get('title', 'Unknown')
                description = page.get('description', 'No description')
                page_id = page.get('pageid', 'Unknown')
                
                print(f"  - {title} (ID: {page_id})")
                print(f"    Description: {description}")
                
                wikipedia_search_results.append({
                    'search_term': target,
                    'title': title,
                    'description': description,
                    'page_id': page_id,
                    'relevance_score': 0  # Will calculate based on keywords
                })
        else:
            print(f"No results found for '{target}'")
    
    except Exception as e:
        print(f"Error searching for '{target}': {e}")
        continue

print(f"\nTotal Wikipedia pages found: {len(wikipedia_search_results)}")

# Calculate relevance scores based on keywords related to ice cream standards
relevant_keywords = [
    'ice cream', 'butterfat', 'fat content', 'federal', 'fda', 'regulation', 
    'standard', 'minimum', 'percentage', 'dairy', 'frozen dessert', 'food standards'
]

for result in wikipedia_search_results:
    title_lower = result['title'].lower()
    desc_lower = result['description'].lower()
    combined_text = f"{title_lower} {desc_lower}"
    
    # Count relevant keywords
    score = sum(1 for keyword in relevant_keywords if keyword in combined_text)
    result['relevance_score'] = score
    
    # Boost score for exact 'ice cream' matches
    if 'ice cream' in title_lower:
        result['relevance_score'] += 5

# Sort by relevance score
wikipedia_search_results.sort(key=lambda x: x['relevance_score'], reverse=True)

print("\n=== TOP RELEVANT WIKIPEDIA PAGES (BY RELEVANCE SCORE) ===")
for i, result in enumerate(wikipedia_search_results[:10], 1):
    print(f"{i}. {result['title']} (Score: {result['relevance_score']})")
    print(f"   Description: {result['description']}")
    print(f"   Page ID: {result['page_id']}")
    print(f"   Search term: {result['search_term']}")

# Save search results
search_results_file = os.path.join(workspace_dir, 'wikipedia_ice_cream_search_results.json')
with open(search_results_file, 'w') as f:
    json.dump({
        'search_date': datetime.now().isoformat(),
        'search_targets': search_targets,
        'total_results': len(wikipedia_search_results),
        'relevant_keywords': relevant_keywords,
        'results': wikipedia_search_results
    }, f, indent=2)

print(f"\nSearch results saved to: {search_results_file}")

# Focus on the most promising pages for detailed analysis
top_pages = wikipedia_search_results[:5]  # Top 5 most relevant

print(f"\n=== STEP 2: ANALYZING TOP {len(top_pages)} WIKIPEDIA PAGES FOR BUTTERFAT STANDARDS ===")

found_butterfat_info = []

for i, page_info in enumerate(top_pages, 1):
    page_title = page_info['title']
    page_id = page_info['page_id']
    
    print(f"\n{i}. Analyzing: '{page_title}' (ID: {page_id})")
    
    try:
        # Get the full Wikipedia page content
        page_url = f'https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(" ", "_")}'
        
        response = requests.get(page_url, timeout=15)
        response.raise_for_status()
        
        page_data = response.json()
        
        # Get the full page content using the content API
        content_url = f'https://en.wikipedia.org/w/api.php'
        content_params = {
            'action': 'query',
            'format': 'json',
            'titles': page_title,
            'prop': 'extracts',
            'exintro': False,  # Get full content, not just intro
            'explaintext': True,  # Get plain text
            'exsectionformat': 'wiki'
        }
        
        content_response = requests.get(content_url, params=content_params, timeout=15)
        content_response.raise_for_status()
        
        content_data = content_response.json()
        
        if 'query' in content_data and 'pages' in content_data['query']:
            pages = content_data['query']['pages']
            
            for page_id_key, page_content in pages.items():
                if 'extract' in page_content:
                    full_text = page_content['extract']
                    
                    print(f"   Page content length: {len(full_text):,} characters")
                    
                    # Search for butterfat content information
                    butterfat_patterns = [
                        r'butterfat[^.]*?(\d+(?:\.\d+)?)\s*(?:percent|%)',
                        r'(\d+(?:\.\d+)?)\s*(?:percent|%)\s*butterfat',
                        r'minimum[^.]*?butterfat[^.]*?(\d+(?:\.\d+)?)\s*(?:percent|%)',
                        r'(\d+(?:\.\d+)?)\s*(?:percent|%)\s*[^.]*?butterfat[^.]*?minimum',
                        r'federal[^.]*?butterfat[^.]*?(\d+(?:\.\d+)?)\s*(?:percent|%)',
                        r'FDA[^.]*?butterfat[^.]*?(\d+(?:\.\d+)?)\s*(?:percent|%)',
                        r'ice cream[^.]*?butterfat[^.]*?(\d+(?:\.\d+)?)\s*(?:percent|%)',
                        r'(\d+(?:\.\d+)?)\s*(?:percent|%)\s*[^.]*?ice cream[^.]*?butterfat'
                    ]
                    
                    # Look for sentences containing butterfat information
                    sentences = full_text.split('.')
                    
                    butterfat_sentences = []
                    for sentence in sentences:
                        sentence_lower = sentence.lower()
                        if 'butterfat' in sentence_lower and any(keyword in sentence_lower for keyword in ['percent', '%', 'minimum', 'federal', 'fda', 'standard', 'regulation']):
                            butterfat_sentences.append(sentence.strip())
                    
                    if butterfat_sentences:
                        print(f"   *** FOUND BUTTERFAT INFORMATION ***")
                        print(f"   Relevant sentences: {len(butterfat_sentences)}")
                        
                        for j, sentence in enumerate(butterfat_sentences, 1):
                            print(f"   {j}. {sentence[:200]}{'...' if len(sentence) > 200 else ''}")
                            
                            # Extract percentage values from sentences
                            percentage_matches = re.findall(r'(\d+(?:\.\d+)?)\s*(?:percent|%)', sentence, re.IGNORECASE)
                            if percentage_matches:
                                print(f"      Percentages found: {percentage_matches}")
                        
                        found_butterfat_info.append({
                            'page_title': page_title,
                            'page_id': page_id,
                            'sentences': butterfat_sentences,
                            'full_text_preview': full_text[:500] + '...' if len(full_text) > 500 else full_text
                        })
                    
                    else:
                        print(f"   No butterfat information found in this page")
                        
                        # Check for general ice cream standards
                        if 'ice cream' in full_text.lower():
                            ice_cream_sentences = []
                            for sentence in sentences:
                                sentence_lower = sentence.lower()
                                if 'ice cream' in sentence_lower and any(keyword in sentence_lower for keyword in ['standard', 'regulation', 'federal', 'fda', 'minimum', 'percent', '%']):
                                    ice_cream_sentences.append(sentence.strip())
                            
                            if ice_cream_sentences:
                                print(f"   Found {len(ice_cream_sentences)} sentences about ice cream standards:")
                                for sentence in ice_cream_sentences[:3]:  # Show first 3
                                    print(f"     - {sentence[:150]}{'...' if len(sentence) > 150 else ''}")
                else:
                    print(f"   No content extract available for this page")
        else:
            print(f"   Error: Could not retrieve page content")
    
    except Exception as e:
        print(f"   Error analyzing page '{page_title}': {e}")
        continue

print(f"\n=== BUTTERFAT INFORMATION ANALYSIS RESULTS ===")
print(f"Pages with butterfat information: {len(found_butterfat_info)}")

if found_butterfat_info:
    # Save detailed butterfat information
    butterfat_file = os.path.join(workspace_dir, 'wikipedia_butterfat_standards.json')
    with open(butterfat_file, 'w') as f:
        json.dump({
            'analysis_date': datetime.now().isoformat(),
            'objective': 'Find US federal butterfat content standards for ice cream as reported by Wikipedia in 2020',
            'pages_analyzed': len(top_pages),
            'pages_with_butterfat_info': len(found_butterfat_info),
            'butterfat_information': found_butterfat_info
        }, f, indent=2)
    
    print(f"\nDetailed butterfat information saved to: {butterfat_file}")
    
    # Display summary of findings
    for i, info in enumerate(found_butterfat_info, 1):
        print(f"\n{i}. Page: {info['page_title']}")
        print(f"   Butterfat sentences found: {len(info['sentences'])}")
        
        # Look for specific percentage values
        all_percentages = []
        for sentence in info['sentences']:
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*(?:percent|%)', sentence, re.IGNORECASE)
            all_percentages.extend(percentages)
        
        if all_percentages:
            print(f"   Percentage values mentioned: {list(set(all_percentages))}")
        
        # Show most relevant sentence
        if info['sentences']:
            best_sentence = max(info['sentences'], key=lambda s: len(s))  # Longest sentence likely has most detail
            print(f"   Key sentence: {best_sentence[:300]}{'...' if len(best_sentence) > 300 else ''}")
else:
    print("\nNo specific butterfat information found in the analyzed pages.")
    print("Will try direct search for 'Ice cream' Wikipedia page with more specific analysis.")

print(f"\n=== WIKIPEDIA SEARCH PHASE COMPLETE ===")
print(f"Next: Direct analysis of main 'Ice cream' Wikipedia page for federal standards")
```

## Created Time
2025-08-13 21:48:19
