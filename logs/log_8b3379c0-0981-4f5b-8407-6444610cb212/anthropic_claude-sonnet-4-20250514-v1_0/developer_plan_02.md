# Developer Plan 02

## Plan
Search the Monterey Bay Aquarium website specifically for information about Minke Whales (identified as #9 from the first National Geographic YouTube short) to find their documented maximum length measurement in meters. Focus on their marine life database, species profiles, or educational content sections that would contain scientific data about whale dimensions.

## Description
This is the necessary next step because: (1) The developer successfully identified that #9 refers to the Minke Whale from the first National Geographic short and provided a research-based estimate of 10.7 meters, (2) However, the TASK specifically requires the measurement 'according to the Monterey Bay Aquarium website' rather than general marine biology sources, (3) Expected outcome is to find the official Monterey Bay Aquarium documentation of Minke Whale maximum length to provide the authoritative answer, (4) This will complete the TASK by obtaining the specific measurement from the required source.

## Episodic Examples
### Development Step 8: Extract 2020 Chinstrap Penguin Population Count and Methodology from Nature.com Study for Comparison

**Description**: Search for and access the Nature.com article titled 'global population assessment of the Chinstrap penguin' from 2020. Extract the specific population figure from this study, noting whether it's provided as breeding pairs or individual penguins. Document the exact methodology and population count to enable accurate comparison with the Wikipedia 2018 data.

**Use Cases**:
- Conservation NGO population tracking dashboards and policy recommendation reports for Antarctic wildlife management
- Government environmental agency species status assessments and legislative drafting for marine protection
- Academic meta-analysis combining multi-decade seabird population studies for climate change impact research
- Automated update of biodiversity informatics platforms (e.g., GBIF) with standardized species counts and confidence intervals
- Ecotourism operator planning and risk assessment for Antarctic expeditions based on up-to-date penguin colony sizes
- Science education module development and interactive visualizations for university ecology courses
- Journalistic data-driven articles on wildlife biodiversity trends for environmental news outlets
- Environmental consultancy feasibility studies and impact assessments for proposed Antarctic infrastructure projects

```
import os
import json

print("=== FINAL EXTRACTION: 2020 NATURE STUDY POPULATION DATA ===\n")

# Locate workspace directory
workspace_dirs = [d for d in os.listdir('.') if d.startswith('workspace')]
if not workspace_dirs:
    print("No workspace directory found.")
    exit()

workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# First, inspect the analysis file structure before accessing it
analysis_file = os.path.join(workspace_dir, 'nature_2020_population_analysis.json')
if not os.path.exists(analysis_file):
    print(f"Analysis file not found: {analysis_file}")
    exit()

print("=== INSPECTING ANALYSIS FILE STRUCTURE ===\n")
print(f"Reading analysis file: {os.path.basename(analysis_file)}")

with open(analysis_file, 'r') as f:
    analysis_data = json.load(f)

print("Top-level keys in analysis data:")
for key, value in analysis_data.items():
    if isinstance(value, dict):
        print(f"  {key}: Dictionary with {len(value)} keys")
    elif isinstance(value, list):
        print(f"  {key}: List with {len(value)} items")
    else:
        print(f"  {key}: {type(value).__name__} - {str(value)[:100]}...")

print("\n=== EXTRACTING KEY POPULATION DATA FROM ANALYSIS ===\n")

# Extract the main population figure from abstract
if 'abstract_text' in analysis_data:
    abstract_text = analysis_data['abstract_text']
    print("Abstract text (key excerpt):")
    print(abstract_text[:500] + "...")
    
    # Extract the specific population figure mentioned
    print("\n*** KEY FINDING FROM ABSTRACT ***")
    print("Global population: 3.42 million breeding pairs")
    print("95th-percentile CI: [2.98, 4.00] million breeding pairs")
    print("Number of colonies: 375 extant colonies")
    print("Data collection year: 2020")
else:
    print("Abstract text not found in analysis")

# Extract methodology information
print("\n=== METHODOLOGY FROM 2020 STUDY ===\n")

if 'methods_text' in analysis_data:
    methods_text = analysis_data['methods_text']
    print("Methodology approach (from abstract and methods):")
    print("1. Satellite imagery analysis")
    print("2. Drone (UAS) imagery counts")
    print("3. Ground counting of individual occupied nests or chicks")
    print("4. Guano coverage area estimation from satellite imagery")
    print("5. High-resolution commercial imagery prioritized")
    
    print("\nMethodology details (excerpt from methods section):")
    print(methods_text[:800] + "...")
else:
    print("Methods text not available")

# Compare measurement types and provide summary
print("\n=== POPULATION MEASUREMENT SPECIFICATIONS ===\n")

print("2020 Nature Study Specifications:")
print("• Population Count: 3.42 million")
print("• Measurement Unit: BREEDING PAIRS")
print("• Confidence Interval: 95th-percentile CI [2.98, 4.00] million")
print("• Total Colonies Assessed: 375 extant colonies")
print("• Data Collection Methods: Satellite + Drone + Ground surveys")
print("• Survey Scope: Global comprehensive assessment")
print("• Publication Year: 2020")
print("• Data Collection Period: Multi-year (referenced from 1980s to 2020)")

# Create final comparison dataset
final_results = {
    "nature_2020_study": {
        "title": "A global population assessment of the Chinstrap penguin (Pygoscelis antarctica)",
        "publication_year": 2020,
        "journal": "Nature Scientific Reports",
        "population_figure": "3.42 million",
        "measurement_unit": "breeding pairs",
        "confidence_interval": "95th-percentile CI: [2.98, 4.00] million breeding pairs",
        "number_of_colonies": 375,
        "methodology": {
            "primary_methods": [
                "Satellite imagery analysis",
                "Drone (UAS) imagery", 
                "Ground counting of nests/chicks",
                "Guano coverage estimation"
            ],
            "imagery_sources": [
                "High-resolution commercial imagery",
                "Planet imagery",
                "Landsat imagery"
            ],
            "survey_approach": "Comprehensive global assessment",
            "data_prioritization": "Direct census methods prioritized over satellite estimates"
        },
        "key_findings": {
            "extirpated_colonies": 23,
            "new_colonies_identified": 5,
            "previously_unreported_colonies": 21,
            "colonies_with_historical_comparison": "65% of all colonies",
            "declining_colonies_percentage": "45% probably or certainly declined",
            "increasing_colonies_percentage": "18% probably or certainly increased"
        },
        "data_quality_notes": [
            "Limited historical data at 35% of colonies prohibit population change assessment",
            "Most historical comparisons made against 1980s baseline data",
            "Large colonies in South Sandwich Islands lack historical benchmark"
        ]
    },
    "comparison_ready": {
        "measurement_type": "breeding pairs",
        "population_value": 3.42,
        "population_unit": "million breeding pairs", 
        "confidence_range_low": 2.98,
        "confidence_range_high": 4.00,
        "year": 2020,
        "methodology_summary": "Multi-method global survey using satellite, drone, and ground-based counting",
        "coverage": "Comprehensive global assessment of 375 colonies"
    }
}

# Save final extraction results
final_file = os.path.join(workspace_dir, 'nature_2020_final_population_data.json')
with open(final_file, 'w') as f:
    json.dump(final_results, f, indent=2)

print(f"\n=== FINAL DATA EXTRACTION COMPLETE ===\n")
print(f"Comprehensive population data saved to: {final_file}")

print("\n=== SUMMARY FOR WIKIPEDIA COMPARISON ===\n")
print("Nature 2020 Study Results:")
print(f"• Global Population: 3.42 million breeding pairs")
print(f"• Confidence Interval: 2.98 - 4.00 million breeding pairs")
print(f"• Total Colonies: 375 extant breeding sites")
print(f"• Methodology: Satellite + Drone + Ground surveys (comprehensive global assessment)")
print(f"• Data Quality: High precision with 95th-percentile confidence intervals")
print(f"• Study Scope: First comprehensive global population assessment")

print("\nReady for comparison with Wikipedia 2018 data:")
print("✓ Population figure extracted: 3.42 million breeding pairs")
print("✓ Measurement unit identified: breeding pairs (not individuals)")
print("✓ Methodology documented: multi-source survey approach")
print("✓ Confidence intervals provided: [2.98, 4.00] million")
print("✓ Publication year confirmed: 2020")

print("\nThis 2020 Nature study provides the specific population assessment")
print("needed for comparison with Wikipedia's 2018 population data.")
```

### Development Step 2: Locate 2018 Chinstrap Penguin Population Data on Wikipedia and Document Figure, Units, and Source

**Description**: Search for and access the Wikipedia page about penguin species populations to locate the specific chinstrap penguin population data as of the end of 2018. Extract the population figure, noting whether it's given in breeding pairs or individual penguins, and document the exact source and context of this information.

**Use Cases**:
- Wildlife conservation reporting and automated population data extraction for environmental NGO impact assessments
- Academic ecological research and historical trend analysis of penguin species for climate change studies
- Zoo exhibit content management and real-time species statistics integration for educational displays
- Government policy briefing and reliable IUCN-based species population sourcing for conservation legislation
- Data journalism fact-checking and automated population figure verification for wildlife news stories
- Mobile app development and species metadata enrichment for citizen science biodiversity tracking
- Educational platform content generation and automated insertion of up-to-date species statistics for science curricula

```
import os
import json

print("=== CHINSTRAP PENGUIN POPULATION RESEARCH - STEP 2 ===\n")
print("Objective: Inspect workspace files and understand their structure")
print("Focus: Examine search results before fetching Wikipedia pages\n")

# First, let's check what files are available in the workspace
print("Files available in workspace:")
if os.path.exists('workspace'):
    workspace_files = os.listdir('workspace')
    for file in workspace_files:
        file_path = os.path.join('workspace', file)
        file_size = os.path.getsize(file_path)
        print(f"  - {file} ({file_size:,} bytes)")
else:
    print("  No workspace directory found")

# Inspect the search results JSON file structure
search_results_file = 'workspace/chinstrap_penguin_search_results.json'
if os.path.exists(search_results_file):
    print(f"\n=== INSPECTING SEARCH RESULTS FILE ===\n")
    print(f"File: {search_results_file}")
    
    with open(search_results_file, 'r') as f:
        search_data = json.load(f)
    
    print("\nTop-level keys in search results:")
    for key, value in search_data.items():
        if isinstance(value, dict):
            print(f"  {key}: Dictionary with {len(value)} keys")
        elif isinstance(value, list):
            print(f"  {key}: List with {len(value)} items")
        else:
            print(f"  {key}: {value}")
    
    # Examine the search results structure
    if 'search_results_by_query' in search_data:
        print(f"\nSearch results by query:")
        for i, query_result in enumerate(search_data['search_results_by_query']):
            query = query_result.get('query', 'Unknown query')
            results_count = query_result.get('results_count', 0)
            print(f"  Query {i+1}: {query}")
            print(f"    Results found: {results_count}")
            
            # Show sample result structure
            if 'results' in query_result and query_result['results']:
                sample_result = query_result['results'][0]
                print(f"    Sample result keys: {list(sample_result.keys())}")
                if 'snippet' in sample_result:
                    snippet = sample_result['snippet'][:100] + "..." if len(sample_result['snippet']) > 100 else sample_result['snippet']
                    print(f"    Sample snippet: {snippet}")

# Inspect the pages to fetch JSON file structure
pages_file = 'workspace/wikipedia_pages_to_fetch.json'
if os.path.exists(pages_file):
    print(f"\n=== INSPECTING PAGES TO FETCH FILE ===\n")
    print(f"File: {pages_file}")
    
    with open(pages_file, 'r') as f:
        pages_data = json.load(f)
    
    print("\nTop-level keys in pages to fetch:")
    for key, value in pages_data.items():
        if isinstance(value, dict):
            print(f"  {key}: Dictionary with {len(value)} keys")
        elif isinstance(value, list):
            print(f"  {key}: List with {len(value)} items")
        else:
            print(f"  {key}: {value}")
    
    # Show target pages
    if 'target_pages' in pages_data:
        print(f"\nTarget pages identified:")
        for i, page in enumerate(pages_data['target_pages'], 1):
            print(f"  Page {i}:")
            for key, value in page.items():
                if key == 'snippet':
                    snippet = value[:80] + "..." if len(value) > 80 else value
                    print(f"    {key}: {snippet}")
                else:
                    print(f"    {key}: {value}")

# Look for specific population-related content in the search snippets
print(f"\n=== ANALYZING SEARCH SNIPPETS FOR POPULATION DATA ===\n")

if os.path.exists(search_results_file):
    with open(search_results_file, 'r') as f:
        search_data = json.load(f)
    
    population_indicators = []
    
    for query_result in search_data.get('search_results_by_query', []):
        for result in query_result.get('results', []):
            snippet = result.get('snippet', '').lower()
            title = result.get('title', '')
            url = result.get('link', '')
            
            # Look for population-related keywords
            population_keywords = ['2018', 'population', 'breeding pairs', '8,000,000', '8 million', 'iucn', 'estimated']
            
            found_keywords = [kw for kw in population_keywords if kw in snippet]
            
            if found_keywords:
                population_indicators.append({
                    'title': title,
                    'url': url,
                    'snippet': result.get('snippet', ''),
                    'found_keywords': found_keywords,
                    'from_query': query_result.get('query', '')
                })
    
    print(f"Results with population indicators: {len(population_indicators)}")
    
    for i, indicator in enumerate(population_indicators, 1):
        print(f"\n{i}. {indicator['title']}")
        print(f"   URL: {indicator['url']}")
        print(f"   Keywords found: {indicator['found_keywords']}")
        print(f"   Snippet: {indicator['snippet'][:150]}...")
        print(f"   From query: {indicator['from_query']}")

print(f"\n=== WORKSPACE INSPECTION COMPLETE ===\n")
print("Key findings:")
print("1. Main target: 'Chinstrap penguin' Wikipedia page")
print("2. Promising snippets mention '2018, the IUCN estimated' and '8,000,000'")
print("3. 'List of Sphenisciformes by population' also contains relevant data")
print("\nNext step: Fetch and parse the Wikipedia pages to extract exact population figures")
```

### Development Step 5: Finding Nemo Clownfish: USGS Pre-2020 Nonnative Records with Zip Codes

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting firm mapping nonnative Amphiprion ocellaris occurrences by zip code to generate ecological impact assessments for proposed shoreline development projects
- State wildlife agency automating data extraction from the USGS NAS database to allocate removal and monitoring resources for invasive clownfish populations in vulnerable coastal zip codes
- Marine biology researchers compiling pre-2020 Amphiprion ocellaris sighting records with precise location data for a peer-reviewed study on tropical fish invasions in U.S. waters
- GIS specialists integrating USGS-derived nonnative fish distribution points into municipal planning systems to identify priority zones for habitat restoration and conservation grants
- Commercial aquarium industry analysts monitoring escaped clownfish specimen reports across zip codes to inform containment policy updates and minimize future nonnative introductions
- Eco-tourism operators creating interactive diving and snorkeling route maps based on validated nonnative clownfish sighting hotspots by zip code to boost guided tours and educational outreach
- Environmental insurance underwriters incorporating invasive species occurrence data into coastal property risk models to adjust coverage rates in counties with documented Amphiprion ocellaris invasions

```
print("=== FINAL FIX: PROPER VARIABLE SCOPING IN LINK PROCESSING ===\n")

# Fix the persistent NameError by ensuring all variables are defined in correct scope
import requests
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse

print("Target Species: Amphiprion ocellaris (Clown anemonefish)")
print("Database: USGS Nonindigenous Aquatic Species (NAS) Database")
print("Objective: Find location data with zip codes for nonnative occurrences before 2020\n")

# Primary USGS NAS species profile URL
species_profile_url = "https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243"

print(f"Accessing USGS NAS Species Profile: {species_profile_url}")

try:
    # Set headers to mimic browser request
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive'
    }
    
    # Access the species profile page
    response = requests.get(species_profile_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f"Successfully accessed USGS page (Status: {response.status_code})")
    print(f"Content length: {len(response.content):,} bytes\n")
    
    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract basic species information
    print("=== SPECIES PROFILE INFORMATION ===\n")
    
    title_element = soup.find('title')
    page_title = title_element.get_text().strip() if title_element else 'Unknown'
    print(f"Page Title: {page_title}")
    
    # Extract species information from page text
    page_text = soup.get_text()
    
    # Extract scientific name
    scientific_name_match = re.search(r'(Amphiprion\s+\w+)', page_text, re.IGNORECASE)
    scientific_name = scientific_name_match.group(1) if scientific_name_match else 'Amphiprion ocellaris'
    print(f"Scientific Name: {scientific_name}")
    
    # Extract common name
    common_name_patterns = [r'clown\s*anemonefish', r'clownfish', r'orange\s*clownfish']
    common_name = 'clown anemonefish'  # default
    for pattern in common_name_patterns:
        match = re.search(pattern, page_text, re.IGNORECASE)
        if match:
            common_name = match.group(0)
            break
    print(f"Common Name: {common_name}")
    
    print("\n=== SEARCHING FOR OCCURRENCE/LOCATION DATA LINKS ===\n")
    
    # Define location keywords outside the loop
    location_keywords = ['occurrence', 'sighting', 'location', 'distribution', 'point map', 'specimen', 'collection', 'data', 'records']
    
    # Find all links on the page
    all_links = soup.find_all('a', href=True)
    print(f"Found {len(all_links)} total links on the species profile page\n")
    
    print("Analyzing links for occurrence/location data...")
    
    occurrence_links = []
    
    for i, link in enumerate(all_links, 1):
        # Extract link information with proper variable scoping
        href = link.get('href', '')
        link_text = link.get_text().strip()  # Define link_text here
        link_text_lower = link_text.lower()  # Define link_text_lower after link_text
        href_lower = href.lower()  # Also create lowercase version of href
        
        # Check if link relates to occurrence/location data
        text_has_keywords = any(keyword in link_text_lower for keyword in location_keywords)
        href_has_keywords = any(keyword in href_lower for keyword in location_keywords)
        
        is_occurrence_related = text_has_keywords or href_has_keywords
        
        if is_occurrence_related:
            # Convert relative URLs to absolute URLs
            full_url = urljoin(species_profile_url, href)
            
            # Find which keywords matched
            matching_keywords = []
            for keyword in location_keywords:
                if keyword in link_text_lower or keyword in href_lower:
                    matching_keywords.append(keyword)
            
            occurrence_link = {
                'text': link_text,
                'url': full_url,
                'href': href,
                'keywords_found': matching_keywords
            }
            
            occurrence_links.append(occurrence_link)
            
            print(f"Occurrence Link {len(occurrence_links)}:")
            print(f"  Text: {link_text}")
            print(f"  URL: {full_url}")
            print(f"  Keywords: {matching_keywords}")
            print(f"  {'-'*60}")
        
        # Show progress for long link lists
        if i % 20 == 0:
            print(f"Processed {i}/{len(all_links)} links...")
    
    print(f"\nTotal occurrence-related links found: {len(occurrence_links)}")
    
    # Categorize the occurrence links by type
    print("\n=== CATEGORIZING LOCATION DATA SOURCES ===\n")
    
    point_map_links = []
    specimen_links = []
    distribution_links = []
    data_links = []
    
    for link in occurrence_links:
        # Safely access the text and URL with proper variable scoping
        link_text_lower = link['text'].lower()
        link_url_lower = link['url'].lower()
        
        # Categorize by content type
        if 'point' in link_text_lower and 'map' in link_text_lower:
            point_map_links.append(link)
            print(f"POINT MAP: {link['text']} -> {link['url']}")
        elif 'specimen' in link_text_lower or 'collection' in link_text_lower:
            specimen_links.append(link)
            print(f"SPECIMEN: {link['text']} -> {link['url']}")
        elif 'distribution' in link_text_lower or 'occurrence' in link_text_lower:
            distribution_links.append(link)
            print(f"DISTRIBUTION: {link['text']} -> {link['url']}")
        elif 'data' in link_text_lower or 'record' in link_text_lower:
            data_links.append(link)
            print(f"DATA/RECORDS: {link['text']} -> {link['url']}")
    
    print(f"\nCategorization Summary:")
    print(f"  Point Map Links: {len(point_map_links)}")
    print(f"  Specimen Links: {len(specimen_links)}")
    print(f"  Distribution Links: {len(distribution_links)}")
    print(f"  Data/Record Links: {len(data_links)}")
    
    # Look for query forms that might allow location-based searches
    print("\n=== SEARCHING FOR QUERY INTERFACES ===\n")
    
    forms = soup.find_all('form')
    query_interfaces = []
    
    print(f"Found {len(forms)} forms on the page")
    
    for form_idx, form in enumerate(forms, 1):
        form_action = form.get('action', '')
        if form_action:
            full_action_url = urljoin(species_profile_url, form_action)
            
            # Look for input fields that might relate to location queries
            inputs = form.find_all('input')
            selects = form.find_all('select')
            
            location_inputs = []
            location_indicators = ['state', 'zip', 'location', 'county', 'lat', 'lon', 'coord', 'address', 'region']
            
            for input_elem in inputs + selects:
                input_name = input_elem.get('name', '').lower()
                input_id = input_elem.get('id', '').lower()
                input_type = input_elem.get('type', '').lower()
                
                if any(indicator in input_name or indicator in input_id for indicator in location_indicators):
                    location_inputs.append({
                        'element_type': input_elem.name,
                        'name': input_elem.get('name', ''),
                        'id': input_elem.get('id', ''),
                        'type': input_type
                    })
            
            if location_inputs:
                query_interface = {
                    'form_action': full_action_url,
                    'location_inputs': location_inputs,
                    'form_method': form.get('method', 'GET').upper()
                }
                query_interfaces.append(query_interface)
                
                print(f"QUERY INTERFACE {len(query_interfaces)}:")
                print(f"  Form Action: {full_action_url}")
                print(f"  Method: {query_interface['form_method']}")
                print(f"  Location Inputs ({len(location_inputs)}):")
                for inp in location_inputs:
                    print(f"    {inp['element_type']}: name='{inp['name']}' id='{inp['id']}' type='{inp['type']}'")
                print(f"  {'-'*60}")
    
    # Save all extracted information
    usgs_data = {
        'species_profile': {
            'url': species_profile_url,
            'species_id': '3243',
            'scientific_name': scientific_name,
            'common_name': common_name,
            'page_title': page_title,
            'access_date': '2024',
            'content_length': len(response.content),
            'total_links_found': len(all_links)
        },
        'occurrence_data_sources': {
            'total_occurrence_links': len(occurrence_links),
            'point_map_links': point_map_links,
            'specimen_links': specimen_links,
            'distribution_links': distribution_links,
            'data_record_links': data_links,
            'all_occurrence_links': occurrence_links
        },
        'query_interfaces': query_interfaces,
        'analysis_summary': {
            'point_maps_available': len(point_map_links),
            'specimen_records_available': len(specimen_links),
            'distribution_data_available': len(distribution_links),
            'query_forms_available': len(query_interfaces)
        },
        'next_steps': [
            'Access point map links for coordinate data',
            'Query specimen databases for collection locations',
            'Use distribution links to find occurrence records',
            'Submit location-based queries through identified forms',
            'Filter all results for pre-2020 records with zip codes'
        ]
    }
    
    # Save the extracted USGS data
    with open('workspace/usgs_nas_clownfish_complete_data.json', 'w') as f:
        json.dump(usgs_data, f, indent=2)
    
    print(f"\n=== USGS DATABASE ANALYSIS COMPLETE ===\n")
    print(f"Species Profile: {species_profile_url}")
    print(f"Species: {scientific_name} ({common_name})")
    print(f"Total Links Analyzed: {len(all_links)}")
    print(f"Occurrence-related Links: {len(occurrence_links)}")
    print(f"Point Map Sources: {len(point_map_links)}")
    print(f"Specimen Record Sources: {len(specimen_links)}")
    print(f"Distribution Data Sources: {len(distribution_links)}")
    print(f"Query Interfaces: {len(query_interfaces)}")
    print(f"\nComplete data saved to: workspace/usgs_nas_clownfish_complete_data.json")
    
    # Identify the most promising sources for zip code data
    print(f"\n=== PRIORITY TARGETS FOR ZIP CODE LOCATION DATA ===\n")
    
    priority_targets = []
    
    # Point maps are highest priority (usually have coordinates)
    if point_map_links:
        priority_targets.extend(point_map_links)
        print(f"HIGH PRIORITY - Point Maps ({len(point_map_links)}):")
        for link in point_map_links:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Distribution and occurrence data are second priority
    if distribution_links:
        priority_targets.extend(distribution_links[:2])  # Top 2
        print(f"\nMEDIUM PRIORITY - Distribution Data ({len(distribution_links)}):")
        for link in distribution_links[:2]:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Data/record links are third priority
    if data_links:
        priority_targets.extend(data_links[:2])  # Top 2
        print(f"\nMEDIUM PRIORITY - Data Records ({len(data_links)}):")
        for link in data_links[:2]:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Query interfaces for custom searches
    if query_interfaces:
        print(f"\nCUSTOM QUERY OPTION - Query Interfaces ({len(query_interfaces)}):")
        for interface in query_interfaces:
            print(f"  • Form: {interface['form_action']}")
            print(f"    Location inputs: {len(interface['location_inputs'])}")
    
    # Save priority targets for next step
    priority_data = {
        'objective': 'Extract specific location data with zip codes for nonnative Amphiprion ocellaris before 2020',
        'priority_targets': priority_targets,
        'query_interfaces': query_interfaces,
        'search_strategy': [
            '1. Access point map links first (highest chance of coordinates)',
            '2. Check distribution/occurrence links for location records',
            '3. Use query interfaces for location-specific searches',
            '4. Filter all results for pre-2020 timeframe',
            '5. Extract zip codes or convert coordinates to zip codes'
        ],
        'next_action': 'Access the highest priority target URLs to extract detailed location data'
    }
    
    with open('workspace/priority_location_extraction_plan.json', 'w') as f:
        json.dump(priority_data, f, indent=2)
    
    print(f"\nTotal priority targets identified: {len(priority_targets)}")
    print(f"Priority extraction plan saved to: workspace/priority_location_extraction_plan.json")
    
    if priority_targets:
        print(f"\nReady to access specific occurrence/location data sources for zip code extraction.")
        print(f"\nNext step: Access the first priority target to extract detailed location records.")
    else:
        print(f"\nNo direct priority targets found. Will need to use query interfaces or alternative approaches.")
    
except requests.exceptions.RequestException as e:
    print(f"Error accessing USGS database: {e}")
    print("Will try alternative approaches to access USGS NAS data.")
    
except Exception as e:
    print(f"Unexpected error during USGS database access: {e}")
    print("Error details for debugging:")
    import traceback
    traceback.print_exc()
```

### Development Step 6: Locate USGS Data on Invasive Clownfish (Finding Nemo) by Zip Code Pre-2020

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting for coastal restoration projects by mapping zip code–level occurrences of nonnative Amphiprion ocellaris to assess habitat risk before 2020
- Academic research into aquarium‐released tropical fish by compiling USGS NAS records of clown anemonefish sightings by zip code for longitudinal population studies
- GIS dashboard development for state wildlife agencies to visualize historical point‐map data of nonnative clownfish distributions and identify invasion hotspots
- Aquarium trade compliance monitoring by automating extraction of pre-2020 nonnative clownfish records with location data to verify retailer adherence to regional regulations
- Citizen science engagement platform pre-loading local occurrence records of Amphiprion ocellaris by zip code so volunteers can validate and update invasive species sightings
- Legislative policy support by providing lawmakers with precise zip code–based invasion data of Finding Nemo’s clownfish to inform import bans and management zones
- Marine conservation NGO program evaluation by analyzing historical USGS distribution and specimen records to measure success of invasive species control efforts at the community level
- Educational GIS lab modules using real USGS occurrence data for Amphiprion ocellaris to teach students about invasive species tracking, data extraction, and spatial analysis

```
print("=== RESTRUCTURED LINK PROCESSING TO ELIMINATE VARIABLE SCOPING BUG ===\n")

# Complete restructure to avoid generator expression variable scoping issues
import requests
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse
import os

print("Target Species: Amphiprion ocellaris (Clown anemonefish)")
print("Database: USGS Nonindigenous Aquatic Species (NAS) Database")
print("Objective: Find location data with zip codes for nonnative occurrences before 2020\n")

# Ensure workspace directory exists
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Primary USGS NAS species profile URL
species_profile_url = "https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243"

print(f"Accessing USGS NAS Species Profile: {species_profile_url}")

try:
    # Set headers to mimic browser request
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive'
    }
    
    # Access the species profile page
    response = requests.get(species_profile_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f"Successfully accessed USGS page (Status: {response.status_code})")
    print(f"Content length: {len(response.content):,} bytes\n")
    
    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract basic species information
    print("=== SPECIES PROFILE INFORMATION ===\n")
    
    title_element = soup.find('title')
    page_title = title_element.get_text().strip() if title_element else 'Unknown'
    print(f"Page Title: {page_title}")
    
    # Extract species information from page text
    page_text = soup.get_text()
    
    # Extract scientific name
    scientific_name_match = re.search(r'(Amphiprion\s+\w+)', page_text, re.IGNORECASE)
    scientific_name = scientific_name_match.group(1) if scientific_name_match else 'Amphiprion ocellaris'
    print(f"Scientific Name: {scientific_name}")
    
    # Extract common name
    common_name_patterns = [r'clown\s*anemonefish', r'clownfish', r'orange\s*clownfish']
    common_name = 'clown anemonefish'  # default
    for pattern in common_name_patterns:
        match = re.search(pattern, page_text, re.IGNORECASE)
        if match:
            common_name = match.group(0)
            break
    print(f"Common Name: {common_name}")
    
    print("\n=== SEARCHING FOR OCCURRENCE/LOCATION DATA LINKS ===\n")
    
    # Define location keywords
    location_keywords = ['occurrence', 'sighting', 'location', 'distribution', 'point map', 'specimen', 'collection', 'data', 'records']
    
    # Find all links on the page
    all_links = soup.find_all('a', href=True)
    print(f"Found {len(all_links)} total links on the species profile page\n")
    
    print("Analyzing links for occurrence/location data...")
    
    occurrence_links = []
    
    # Process links one by one without generator expressions
    for i, link in enumerate(all_links, 1):
        # Extract link information safely
        href = link.get('href', '')
        link_text = link.get_text().strip()
        
        # Create lowercase versions for comparison
        link_text_lower = link_text.lower()
        href_lower = href.lower()
        
        # Check for location keywords using explicit loops instead of generator expressions
        text_has_keywords = False
        href_has_keywords = False
        matching_keywords = []
        
        # Check text for keywords
        for keyword in location_keywords:
            if keyword in link_text_lower:
                text_has_keywords = True
                matching_keywords.append(keyword)
        
        # Check href for keywords
        for keyword in location_keywords:
            if keyword in href_lower:
                href_has_keywords = True
                if keyword not in matching_keywords:
                    matching_keywords.append(keyword)
        
        is_occurrence_related = text_has_keywords or href_has_keywords
        
        if is_occurrence_related:
            # Convert relative URLs to absolute URLs
            full_url = urljoin(species_profile_url, href)
            
            occurrence_link = {
                'text': link_text,
                'url': full_url,
                'href': href,
                'keywords_found': matching_keywords
            }
            
            occurrence_links.append(occurrence_link)
            
            print(f"Occurrence Link {len(occurrence_links)}:")
            print(f"  Text: {link_text}")
            print(f"  URL: {full_url}")
            print(f"  Keywords: {matching_keywords}")
            print(f"  {'-'*60}")
        
        # Show progress for long link lists
        if i % 20 == 0:
            print(f"Processed {i}/{len(all_links)} links...")
    
    print(f"\nTotal occurrence-related links found: {len(occurrence_links)}")
    
    # Categorize the occurrence links by type
    print("\n=== CATEGORIZING LOCATION DATA SOURCES ===\n")
    
    point_map_links = []
    specimen_links = []
    distribution_links = []
    data_links = []
    
    for link in occurrence_links:
        # Safely access the text and URL
        link_text_lower = link['text'].lower()
        link_url_lower = link['url'].lower()
        
        # Categorize by content type using explicit checks
        if 'point' in link_text_lower and 'map' in link_text_lower:
            point_map_links.append(link)
            print(f"POINT MAP: {link['text']} -> {link['url']}")
        elif 'specimen' in link_text_lower or 'collection' in link_text_lower:
            specimen_links.append(link)
            print(f"SPECIMEN: {link['text']} -> {link['url']}")
        elif 'distribution' in link_text_lower or 'occurrence' in link_text_lower:
            distribution_links.append(link)
            print(f"DISTRIBUTION: {link['text']} -> {link['url']}")
        elif 'data' in link_text_lower or 'record' in link_text_lower:
            data_links.append(link)
            print(f"DATA/RECORDS: {link['text']} -> {link['url']}")
    
    print(f"\nCategorization Summary:")
    print(f"  Point Map Links: {len(point_map_links)}")
    print(f"  Specimen Links: {len(specimen_links)}")
    print(f"  Distribution Links: {len(distribution_links)}")
    print(f"  Data/Record Links: {len(data_links)}")
    
    # Look for query forms that might allow location-based searches
    print("\n=== SEARCHING FOR QUERY INTERFACES ===\n")
    
    forms = soup.find_all('form')
    query_interfaces = []
    
    print(f"Found {len(forms)} forms on the page")
    
    for form_idx, form in enumerate(forms, 1):
        form_action = form.get('action', '')
        if form_action:
            full_action_url = urljoin(species_profile_url, form_action)
            
            # Look for input fields that might relate to location queries
            inputs = form.find_all('input')
            selects = form.find_all('select')
            
            location_inputs = []
            location_indicators = ['state', 'zip', 'location', 'county', 'lat', 'lon', 'coord', 'address', 'region']
            
            for input_elem in inputs + selects:
                input_name = input_elem.get('name', '').lower()
                input_id = input_elem.get('id', '').lower()
                input_type = input_elem.get('type', '').lower()
                
                # Check for location indicators using explicit loops
                has_location_indicator = False
                for indicator in location_indicators:
                    if indicator in input_name or indicator in input_id:
                        has_location_indicator = True
                        break
                
                if has_location_indicator:
                    location_inputs.append({
                        'element_type': input_elem.name,
                        'name': input_elem.get('name', ''),
                        'id': input_elem.get('id', ''),
                        'type': input_type
                    })
            
            if location_inputs:
                query_interface = {
                    'form_action': full_action_url,
                    'location_inputs': location_inputs,
                    'form_method': form.get('method', 'GET').upper()
                }
                query_interfaces.append(query_interface)
                
                print(f"QUERY INTERFACE {len(query_interfaces)}:")
                print(f"  Form Action: {full_action_url}")
                print(f"  Method: {query_interface['form_method']}")
                print(f"  Location Inputs ({len(location_inputs)}):")
                for inp in location_inputs:
                    print(f"    {inp['element_type']}: name='{inp['name']}' id='{inp['id']}' type='{inp['type']}'")
                print(f"  {'-'*60}")
    
    # Save all extracted information
    usgs_data = {
        'species_profile': {
            'url': species_profile_url,
            'species_id': '3243',
            'scientific_name': scientific_name,
            'common_name': common_name,
            'page_title': page_title,
            'access_date': '2024',
            'content_length': len(response.content),
            'total_links_found': len(all_links)
        },
        'occurrence_data_sources': {
            'total_occurrence_links': len(occurrence_links),
            'point_map_links': point_map_links,
            'specimen_links': specimen_links,
            'distribution_links': distribution_links,
            'data_record_links': data_links,
            'all_occurrence_links': occurrence_links
        },
        'query_interfaces': query_interfaces,
        'analysis_summary': {
            'point_maps_available': len(point_map_links),
            'specimen_records_available': len(specimen_links),
            'distribution_data_available': len(distribution_links),
            'query_forms_available': len(query_interfaces)
        },
        'next_steps': [
            'Access point map links for coordinate data',
            'Query specimen databases for collection locations',
            'Use distribution links to find occurrence records',
            'Submit location-based queries through identified forms',
            'Filter all results for pre-2020 records with zip codes'
        ]
    }
    
    # Save the extracted USGS data
    with open('workspace/usgs_nas_clownfish_complete_data.json', 'w') as f:
        json.dump(usgs_data, f, indent=2)
    
    print(f"\n=== USGS DATABASE ANALYSIS COMPLETE ===\n")
    print(f"Species Profile: {species_profile_url}")
    print(f"Species: {scientific_name} ({common_name})")
    print(f"Total Links Analyzed: {len(all_links)}")
    print(f"Occurrence-related Links: {len(occurrence_links)}")
    print(f"Point Map Sources: {len(point_map_links)}")
    print(f"Specimen Record Sources: {len(specimen_links)}")
    print(f"Distribution Data Sources: {len(distribution_links)}")
    print(f"Query Interfaces: {len(query_interfaces)}")
    print(f"\nComplete data saved to: workspace/usgs_nas_clownfish_complete_data.json")
    
    # Identify the most promising sources for zip code data
    print(f"\n=== PRIORITY TARGETS FOR ZIP CODE LOCATION DATA ===\n")
    
    priority_targets = []
    
    # Point maps are highest priority (usually have coordinates)
    if point_map_links:
        priority_targets.extend(point_map_links)
        print(f"HIGH PRIORITY - Point Maps ({len(point_map_links)}):")
        for link in point_map_links:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Distribution and occurrence data are second priority
    if distribution_links:
        priority_targets.extend(distribution_links[:2])  # Top 2
        print(f"\nMEDIUM PRIORITY - Distribution Data ({len(distribution_links)}):")
        for link in distribution_links[:2]:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Data/record links are third priority
    if data_links:
        priority_targets.extend(data_links[:2])  # Top 2
        print(f"\nMEDIUM PRIORITY - Data Records ({len(data_links)}):")
        for link in data_links[:2]:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Query interfaces for custom searches
    if query_interfaces:
        print(f"\nCUSTOM QUERY OPTION - Query Interfaces ({len(query_interfaces)}):")
        for interface in query_interfaces:
            print(f"  • Form: {interface['form_action']}")
            print(f"    Location inputs: {len(interface['location_inputs'])}")
    
    # Save priority targets for next step
    priority_data = {
        'objective': 'Extract specific location data with zip codes for nonnative Amphiprion ocellaris before 2020',
        'priority_targets': priority_targets,
        'query_interfaces': query_interfaces,
        'search_strategy': [
            '1. Access point map links first (highest chance of coordinates)',
            '2. Check distribution/occurrence links for location records',
            '3. Use query interfaces for location-specific searches',
            '4. Filter all results for pre-2020 timeframe',
            '5. Extract zip codes or convert coordinates to zip codes'
        ],
        'next_action': 'Access the highest priority target URLs to extract detailed location data'
    }
    
    with open('workspace/priority_location_extraction_plan.json', 'w') as f:
        json.dump(priority_data, f, indent=2)
    
    print(f"\nTotal priority targets identified: {len(priority_targets)}")
    print(f"Priority extraction plan saved to: workspace/priority_location_extraction_plan.json")
    
    if priority_targets:
        print(f"\nReady to access specific occurrence/location data sources for zip code extraction.")
        print(f"\nNext step: Access the first priority target to extract detailed location records.")
    else:
        print(f"\nNo direct priority targets found. Will need to use query interfaces or alternative approaches.")
        print(f"\nSuggested next approach: Use general search or query forms to find occurrence records.")
    
except requests.exceptions.RequestException as e:
    print(f"Error accessing USGS database: {e}")
    print("Will try alternative approaches to access USGS NAS data.")
    
except Exception as e:
    print(f"Unexpected error during USGS database access: {e}")
    print("Error details for debugging:")
    import traceback
    traceback.print_exc()
```

## Created Time
2025-08-14 01:08:33
