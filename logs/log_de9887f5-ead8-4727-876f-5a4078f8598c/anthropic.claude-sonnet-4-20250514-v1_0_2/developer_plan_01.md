# Developer Plan #1 - anthropic.claude-sonnet-4-20250514-v1:0

**Task ID:** de9887f5-ead8-4727-876f-5a4078f8598c  
**Model:** anthropic.claude-sonnet-4-20250514-v1:0  
**Plan Index:** 1  
**Timestamp:** 2025-08-06 22:16:30

## Plan Content
```
Search for Omar Valencia-Mendez's 2017 research paper about harlequin shrimp to identify the total length measurements of the harlequin shrimp specimens recorded in that study.
```

## Plan Description
```
This is the optimal first step because: (1) We need to establish the baseline measurement from the 2017 Valencia-Mendez paper before we can calculate the percentage comparison, (2) No previous research has been conducted on either paper, (3) Expected outcome is to locate the paper and extract the harlequin shrimp length data, (4) This provides the denominator needed for the percentage calculation comparing sea star size from the Fiedler 2002 paper
```

## Retrieved Episodic Memory Examples
```
### Development Step 2: Locate Nedoshivina 2010 paper on Vietnamese specimens by Kuznetzov, including methods and repository info

**Description**: Search for Nedoshivina's 2010 scientific paper that includes descriptions of Vietnamese specimens by Kuznetzov. Focus on finding the complete publication with methods/materials sections or specimen repository information. Use these search approaches: (1) Academic databases like Google Scholar, PubMed, or ResearchGate using keywords 'Nedoshivina 2010 Vietnamese specimens Kuznetzov', (2) Taxonomic databases that might reference specimen deposits, (3) Journal archives that published the paper in 2010. Look specifically for sections mentioning specimen deposition, museum collections, or institutional repositories where the Vietnamese materials were stored.

**Use Cases**:
- Museum curators automatically extracting specimen deposition details from Nedoshivina’s 2010 catalogue to update and verify their Tortricidae collection database at ZISP
- Biodiversity informaticians harvesting Vietnamese Tortricidae origin and repository metadata from published PDFs to integrate into a global species occurrence platform
- Academic researchers conducting a meta‐analysis of Tortricidae diversity by programmatically discovering, downloading, and parsing type specimen catalogues from online archives
- Digital librarians automating the discovery and archiving of obscure entomological publications in Atalanta to ensure complete institutional repository holdings
- Grant administrators monitoring open‐access compliance by locating and validating full‐text taxonomic papers in Google Scholar, ResearchGate, and journal archives
- Conservation biologists retrieving museum accession numbers and collection localities from PDF methods sections to model habitat suitability for endangered moth species
- Systematics PhD students streamlining their literature reviews by scraping abstracts, methods/materials, and repository information from historical taxonomic papers
- Data journalists investigating transparency in specimen reporting by aggregating repository details from multiple scientific databases into an interactive dashboard

```
import os
import json
import requests

# First, let's inspect the search results file to understand its structure
search_results_file = 'workspace/nedoshivina_2010_search_results.json'

print("=== ANALYZING NEDOSHIVINA 2010 SEARCH RESULTS ===\n")

if os.path.exists(search_results_file):
    print("Inspecting search results file structure:")
    with open(search_results_file, 'r') as f:
        search_data = json.load(f)
    
    print(f"Main keys in search data: {list(search_data.keys())}")
    
    if 'search_results' in search_data:
        results = search_data['search_results']
        print(f"\nSearch query used: {search_data.get('query_used', 'Unknown')}")
        print(f"Total results found: {search_data.get('total_results', 0)}")
        print(f"Academic links identified: {search_data.get('academic_links_found', 0)}")
        
        if 'organic_results' in results:
            print(f"\n=== TOP RESULTS ANALYSIS ===\n")
            
            # Focus on the most relevant results (first few)
            top_results = results['organic_results'][:5]
            
            direct_paper_links = []
            
            for i, result in enumerate(top_results):
                title = result.get('title', 'No title')
                link = result.get('link', 'No link')
                snippet = result.get('snippet', 'No snippet')
                
                print(f"Result {i+1}:")
                print(f"Title: {title}")
                print(f"Link: {link}")
                print(f"Snippet: {snippet[:200]}..." if len(snippet) > 200 else f"Snippet: {snippet}")
                
                # Identify direct paper access links
                if any(indicator in link.lower() for indicator in ['.pdf', 'zobodat', 'archive.org']):
                    direct_paper_links.append({
                        'title': title,
                        'link': link,
                        'type': 'PDF' if '.pdf' in link else 'Archive'
                    })
                    print(f"*** DIRECT PAPER ACCESS IDENTIFIED ***")
                
                print("-" * 80)
            
            print(f"\n=== DIRECT PAPER ACCESS OPPORTUNITIES ===\n")
            
            if direct_paper_links:
                print(f"Found {len(direct_paper_links)} direct access links:")
                
                for i, paper_link in enumerate(direct_paper_links):
                    print(f"\n{i+1}. {paper_link['title']}")
                    print(f"   Link: {paper_link['link']}")
                    print(f"   Type: {paper_link['type']}")
                
                # Try to download the first PDF link
                pdf_link = next((link for link in direct_paper_links if link['type'] == 'PDF'), None)
                
                if pdf_link:
                    print(f"\n=== ATTEMPTING TO DOWNLOAD PAPER PDF ===\n")
                    print(f"Target: {pdf_link['title']}")
                    print(f"URL: {pdf_link['link']}")
                    
                    try:
                        # Set headers to mimic browser request
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                            'Accept': 'application/pdf,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                            'Accept-Language': 'en-US,en;q=0.5',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'Connection': 'keep-alive',
                            'Upgrade-Insecure-Requests': '1'
                        }
                        
                        print("Making request to download PDF...")
                        response = requests.get(pdf_link['link'], headers=headers, timeout=30)
                        response.raise_for_status()
                        
                        # Check if we got a PDF
                        content_type = response.headers.get('content-type', '').lower()
                        print(f"Content-Type: {content_type}")
                        print(f"Content-Length: {len(response.content)} bytes")
                        
                        if 'pdf' in content_type or len(response.content) > 10000:  # Assume PDF if large file
                            pdf_path = 'workspace/nedoshivina_2010_catalogue_tortricidae_vietnam.pdf'
                            
                            with open(pdf_path, 'wb') as pdf_file:
                                pdf_file.write(response.content)
                            
                            file_size = os.path.getsize(pdf_path)
                            print(f"\n*** PDF SUCCESSFULLY DOWNLOADED ***")
                            print(f"Saved to: {pdf_path}")
                            print(f"File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
                            
                            # Verify the file is readable
                            if file_size > 1000:
                                print("PDF download appears successful and file size is reasonable.")
                                print("Paper is now available for detailed analysis of specimen repository information.")
                            else:
                                print("Warning: Downloaded file is very small, may not be the actual PDF.")
                        
                        else:
                            print(f"Downloaded content doesn't appear to be a PDF (Content-Type: {content_type})")
                            print("Content preview:")
                            print(response.text[:500] + "..." if len(response.text) > 500 else response.text)
                    
                    except requests.exceptions.RequestException as e:
                        print(f"Error downloading PDF: {e}")
                        print("Will try alternative access methods.")
                    
                    except Exception as e:
                        print(f"Unexpected error during PDF download: {e}")
                
                else:
                    print("No direct PDF links found, but archive links are available.")
            
            else:
                print("No direct paper access links identified.")
            
            # Extract key information from search results
            print(f"\n=== KEY FINDINGS FROM SEARCH RESULTS ===\n")
            
            findings = {
                'paper_title': 'A catalogue of type specimens of the Tortricidae described by V.I. Kuznetzov from Vietnam and deposited in the Zoological Institute, St. Petersburg',
                'author': 'Nedoshivina, Svetlana (S.V.)',
                'publication_year': 2010,
                'journal': 'Atalanta',
                'volume_pages': '41: 335-347',
                'specimen_repository': 'Zoological Institute, St. Petersburg (ZISP)',
                'specimen_origin': 'Vietnamese Tortricidae collected by V.I. Kuznetzov',
                'repository_details': 'Main collection of ZISP (Zoological Institute of the Russian Academy of Sciences, St. Petersburg)',
                'paper_type': 'Type specimen catalogue',
                'access_links': [link['link'] for link in direct_paper_links]
            }
            
            for key, value in findings.items():
                if isinstance(value, list):
                    print(f"{key.replace('_', ' ').title()}: {len(value)} links available")
                    for link in value:
                        print(f"  - {link}")
                else:
                    print(f"{key.replace('_', ' ').title()}: {value}")
            
            # Save detailed findings
            with open('workspace/nedoshivina_paper_findings.json', 'w') as f:
                json.dump(findings, f, indent=2)
            print(f"\nDetailed findings saved to: workspace/nedoshivina_paper_findings.json")
            
else:
    print(f"Search results file not found: {search_results_file}")
    print("Available files in workspace:")
    if os.path.exists('workspace'):
        for file in os.listdir('workspace'):
            print(f"  - {file}")
    else:
        print("Workspace directory not found.")
```

### Development Step 1: Find Nedoshivina 2010 Publication Describing Kuznetzov’s Vietnamese Specimens, Including Methods and Repository Info

**Description**: Search for Nedoshivina's 2010 scientific paper that includes descriptions of Vietnamese specimens by Kuznetzov. Focus on finding the complete publication with methods/materials sections or specimen repository information. Use these search approaches: (1) Academic databases like Google Scholar, PubMed, or ResearchGate using keywords 'Nedoshivina 2010 Vietnamese specimens Kuznetzov', (2) Taxonomic databases that might reference specimen deposits, (3) Journal archives that published the paper in 2010. Look specifically for sections mentioning specimen deposition, museum collections, or institutional repositories where the Vietnamese materials were stored.

**Use Cases**:
- Entomology researcher retrieving Nedoshivina’s 2010 paper to verify Vietnamese veneer moth specimen depositories before describing a new related species
- Museum curator cross-referencing taxonomic literature to confirm the institutional repository and accession numbers of Kuznetzov’s Vietnam collection for database curation
- Graduate student compiling methods and material details from primary sources to support a molecular phylogeny of Southeast Asian Tortricidae
- Biodiversity informatics specialist automating DOI and PDF link extraction for taxonomic publications to integrate specimen metadata into an online species portal
- Conservation biologist mapping historical specimen localities in Vietnam by extracting georeferenced collection data from Nedoshivina’s publication
- Journal editor validating author-provided repository statements against original publications to ensure compliance with taxonomic code requirements
- Natural history collection manager updating digital catalogs by linking specimen records to their original taxonomic descriptions and museum holdings
- Academic librarian indexing obscure entomological articles in digital archives, using SerpAPI to locate full-text PDFs and DOIs for 2010 publications

```
import os
import re
import requests

# Search for Nedoshivina's 2010 scientific paper that includes descriptions of Vietnamese specimens by Kuznetzov
query = '"Nedoshivina" 2010 Vietnamese specimens Kuznetzov'
max_results = 15
type = "search"

# Get SerpAPI key from environment variables
api_key = os.getenv("SERPAPI_API_KEY")

if api_key is None:
    print("Error: Missing API key. Make sure you have SERPAPI_API_KEY in your environment variables.")
else:
    print(f"Searching for: {query}")
    print(f"API key available: {api_key[:10]}...")
    
    # Prepare API request parameters
    params = {
        "q": query,
        "api_key": api_key,
        "engine": "google",
        "google_domain": "google.com",
        "safe": "off",
        "num": max_results,
        "type": type,
    }
    
    print(f"Making request to SerpAPI with params: {params}")
    
    # Make API request to SerpAPI
    response = requests.get("https://serpapi.com/search.json", params=params)
    
    print(f"Response status code: {response.status_code}")
    
    if response.status_code == 200:
        results = response.json()
        
        # Check if we got organic results
        if not results.get("organic_results"):
            print(f"No results found for '{query}'. Response keys: {list(results.keys())}")
            if 'error' in results:
                print(f"API Error: {results['error']}")
                
            # Try a broader search
            print("\nTrying a broader search with just 'Nedoshivina 2010'...")
            broad_query = "Nedoshivina 2010"
            params["q"] = broad_query
            
            response2 = requests.get("https://serpapi.com/search.json", params=params)
            if response2.status_code == 200:
                results2 = response2.json()
                if results2.get("organic_results"):
                    results = results2
                    query = broad_query
                    print(f"Broader search successful with {len(results['organic_results'])} results")
                    
        if results.get("organic_results"):
            print(f"\nFound {len(results['organic_results'])} results for '{query}':")
            print("="*80)
            
            # Look for academic paper links (PDF, DOI, journal sites)
            academic_links = []
            relevant_results = []
            
            for i, result in enumerate(results["organic_results"]):
                title = result.get('title', 'No title')
                link = result.get('link', 'No link')
                snippet = result.get('snippet', 'No snippet')
                
                print(f"\nResult {i+1}:")
                print(f"Title: {title}")
                print(f"Link: {link}")
                print(f"Snippet: {snippet}")
                print("-" * 60)
                
                # Check for academic/scientific indicators
                academic_indicators = [
                    'doi.org', 'pubmed', 'researchgate', 'scholar.google',
                    '.pdf', 'journal', 'publication', 'research',
                    'specimens', 'taxonomy', 'species', 'museum',
                    'repository', 'collection', 'vietnam'
                ]
                
                is_academic = any(indicator in (title + link + snippet).lower() for indicator in academic_indicators)
                
                if is_academic:
                    academic_links.append(link)
                    relevant_results.append(result)
                    print(f"*** POTENTIALLY RELEVANT ACADEMIC SOURCE ***")
                    
                # Check specifically for Vietnamese/specimen content
                vietnamese_indicators = ['vietnam', 'specimen', 'kuznetzov', 'collection', 'museum', 'repository']
                has_vietnamese_content = any(indicator in (title + link + snippet).lower() for indicator in vietnamese_indicators)
                
                if has_vietnamese_content:
                    print(f"*** CONTAINS VIETNAMESE/SPECIMEN CONTENT ***")
            
            print(f"\nTotal potentially academic links found: {len(academic_links)}")
            for i, link in enumerate(academic_links[:5]):  # Show first 5
                print(f"Academic link {i+1}: {link}")
                
            # Save search results to workspace
            import json
            search_data = {
                'query_used': query,
                'total_results': len(results['organic_results']),
                'academic_links_found': len(academic_links),
                'search_results': results,
                'relevant_results': relevant_results
            }
            
            with open('workspace/nedoshivina_2010_search_results.json', 'w') as f:
                json.dump(search_data, f, indent=2)
            print(f"\nSearch results saved to workspace/nedoshivina_2010_search_results.json")
            
            # Try more specific searches if initial search wasn't very successful
            if len(academic_links) < 3:
                print("\n" + "="*80)
                print("CONDUCTING ADDITIONAL TARGETED SEARCHES")
                print("="*80)
                
                additional_queries = [
                    '"Nedoshivina" Vietnamese specimens taxonomy',
                    'Kuznetzov Vietnamese specimens 2010',
                    'Nedoshivina 2010 filetype:pdf',
                    '"Nedoshivina" museum collection Vietnam'
                ]
                
                all_additional_results = []
                
                for additional_query in additional_queries:
                    print(f"\nSearching: {additional_query}")
                    params["q"] = additional_query
                    params["num"] = 10  # Fewer results for additional searches
                    
                    add_response = requests.get("https://serpapi.com/search.json", params=params)
                    if add_response.status_code == 200:
                        add_results = add_response.json()
                        if add_results.get("organic_results"):
                            print(f"Found {len(add_results['organic_results'])} additional results")
                            all_additional_results.extend(add_results['organic_results'])
                            
                            # Show top results for this search
                            for j, result in enumerate(add_results['organic_results'][:3]):
                                title = result.get('title', 'No title')
                                link = result.get('link', 'No link')
                                print(f"  {j+1}. {title[:100]}...")
                                print(f"     {link}")
                        else:
                            print("No results for this additional query")
                    else:
                        print(f"Error in additional search: {add_response.status_code}")
                
                # Save all additional search results
                if all_additional_results:
                    additional_data = {
                        'additional_queries': additional_queries,
                        'total_additional_results': len(all_additional_results),
                        'additional_search_results': all_additional_results
                    }
                    
                    with open('workspace/nedoshivina_additional_searches.json', 'w') as f:
                        json.dump(additional_data, f, indent=2)
                    print(f"\nAdditional search results saved to workspace/nedoshivina_additional_searches.json")
                    print(f"Total additional results found: {len(all_additional_results)}")
                
    else:
        print(f"Error: API request failed with status {response.status_code}: {response.text}")
```

### Development Step 66: Extract Fish Bag Volume (cubic meters) from Dragon Diet PDF

**Description**: Search within workspace/dragon_diet_fish_bag.pdf for the section or table where the fish bag volume is calculated, locate the numeric value reported in cubic meters, and extract that volume number directly from the PDF.

**Use Cases**:
- Fisheries logistics automation and inventory forecasting for seafood distributors
- Aquaculture research data extraction and feed ratio calculation in marine biology labs
- Environmental compliance auditing and carbon footprint estimation for marine transport NGOs
- Food safety regulatory verification and container specification validation for health inspectors
- E-commerce product catalog enrichment and metadata population for online seafood retailers
- Aquarium operations scheduling and water quality management for public aquariums and zoos
- Academic publication support and experimental methodology documentation for marine science researchers
- ERP integration and packaging data synchronization for seafood manufacturing plants

```
import os
import re
from PyPDF2 import PdfReader

print("=== FISH BAG VOLUME EXTRACTION (FILTER BY 'bag' CONTEXT) ===")

# Step 1: Locate the workspace directory (handles random suffixes)
cwd = os.getcwd()
workspace_dirs = [d for d in os.listdir(cwd) if os.path.isdir(d) and d.startswith('workspace')]
if not workspace_dirs:
    print("Error: No directory starting with 'workspace' found in current working directory.")
    exit(1)
workspace_dir = workspace_dirs[0]
print(f"Using workspace directory: {workspace_dir}\n")

# Step 2: Locate the PDF file in workspace
files = os.listdir(workspace_dir)
print(f"Files in '{workspace_dir}': {files}\n")
pdf_name = None
for fname in files:
    if fname.lower().endswith('.pdf') and 'dragon_diet_fish_bag' in fname.lower():
        pdf_name = fname
        break
if not pdf_name:
    print("Error: 'dragon_diet_fish_bag.pdf' not found in workspace directory.")
    exit(1)
pdf_path = os.path.join(workspace_dir, pdf_name)
print(f"Found PDF: {pdf_path}\n")

# Step 3: Read PDF and scan pages for volume patterns
reader = PdfReader(pdf_path)
num_pages = len(reader.pages)
print(f"Total pages in PDF: {num_pages}\n")

# Regex to capture numbers + cubic meter units (e.g., '0.1777 m3', '0.1777 m³', '5 cubic meters')
volume_pattern = re.compile(r"(\d+(?:[.,]\d+)*)\s*(?:m\^?3|m³|cubic meters?)", re.IGNORECASE)

# We'll look for the term 'bag' in the regex match context to pick the fish-bag volume
found_volume = None
found_unit = None
found_page = None

for i, page in enumerate(reader.pages, start=1):
    text = page.extract_text() or ''
    # Print a short preview for debugging
    print(f"--- Page {i} Preview ---")
    preview = text.replace('\n', ' ')[:200]
    print(preview + ('...' if len(text) > 200 else ''))

    # Search for all volume matches on this page
    for match in volume_pattern.finditer(text):
        vol_str = match.group(1)
        full_match = match.group(0)
        unit_str = full_match[len(vol_str):].strip()
        start, end = match.start(), match.end()
        context = text[max(0, start-50):min(len(text), end+50)].replace('\n', ' ')

        # Debug each match
        print(f"Match on page {i}: '{full_match}' -> value: {vol_str}, unit: {unit_str}")
        print(f"Context snippet: ...{context.strip()}...\n")

        # Filter for the fish bag capacity by checking 'bag' in the context
        if 'bag' in context.lower():
            found_volume = vol_str
            found_unit = unit_str
            found_page = i
            print(f"--> Selected as fish-bag volume (contains 'bag'): {vol_str} {unit_str} (page {i})\n")
            break
    # If we found it, no need to scan further pages
    if found_volume:
        break

# Step 4: Report result and save to file
if not found_volume:
    print("\nNo fish-bag volume found in any page.")
else:
    result_line = f"{found_volume} {found_unit}"
    print(f"\n>>> Extracted fish bag volume: {result_line} (from page {found_page})")
    out_path = os.path.join(workspace_dir, 'fish_bag_volume.txt')
    with open(out_path, 'w', encoding='utf-8') as f:
        f.write(result_line + "\n")
    print(f"Volume saved to: {out_path}")
```

### Development Step 6: Locate USGS Data on Invasive Clownfish (Finding Nemo) by Zip Code Pre-2020

**Description**: Identify the main character fish species from the movie Finding Nemo and then search for USGS databases or reports documenting nonnative fish species occurrences in the United States before 2020. Focus on finding official USGS records that include specific location data with zip codes for where this species was found as an invasive or nonnative population.

**Use Cases**:
- Environmental consulting for coastal restoration projects by mapping zip code–level occurrences of nonnative Amphiprion ocellaris to assess habitat risk before 2020
- Academic research into aquarium‐released tropical fish by compiling USGS NAS records of clown anemonefish sightings by zip code for longitudinal population studies
- GIS dashboard development for state wildlife agencies to visualize historical point‐map data of nonnative clownfish distributions and identify invasion hotspots
- Aquarium trade compliance monitoring by automating extraction of pre-2020 nonnative clownfish records with location data to verify retailer adherence to regional regulations
- Citizen science engagement platform pre-loading local occurrence records of Amphiprion ocellaris by zip code so volunteers can validate and update invasive species sightings
- Legislative policy support by providing lawmakers with precise zip code–based invasion data of Finding Nemo’s clownfish to inform import bans and management zones
- Marine conservation NGO program evaluation by analyzing historical USGS distribution and specimen records to measure success of invasive species control efforts at the community level
- Educational GIS lab modules using real USGS occurrence data for Amphiprion ocellaris to teach students about invasive species tracking, data extraction, and spatial analysis

```
print("=== RESTRUCTURED LINK PROCESSING TO ELIMINATE VARIABLE SCOPING BUG ===\n")

# Complete restructure to avoid generator expression variable scoping issues
import requests
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse
import os

print("Target Species: Amphiprion ocellaris (Clown anemonefish)")
print("Database: USGS Nonindigenous Aquatic Species (NAS) Database")
print("Objective: Find location data with zip codes for nonnative occurrences before 2020\n")

# Ensure workspace directory exists
if not os.path.exists('workspace'):
    os.makedirs('workspace')

# Primary USGS NAS species profile URL
species_profile_url = "https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243"

print(f"Accessing USGS NAS Species Profile: {species_profile_url}")

try:
    # Set headers to mimic browser request
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive'
    }
    
    # Access the species profile page
    response = requests.get(species_profile_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    print(f"Successfully accessed USGS page (Status: {response.status_code})")
    print(f"Content length: {len(response.content):,} bytes\n")
    
    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract basic species information
    print("=== SPECIES PROFILE INFORMATION ===\n")
    
    title_element = soup.find('title')
    page_title = title_element.get_text().strip() if title_element else 'Unknown'
    print(f"Page Title: {page_title}")
    
    # Extract species information from page text
    page_text = soup.get_text()
    
    # Extract scientific name
    scientific_name_match = re.search(r'(Amphiprion\s+\w+)', page_text, re.IGNORECASE)
    scientific_name = scientific_name_match.group(1) if scientific_name_match else 'Amphiprion ocellaris'
    print(f"Scientific Name: {scientific_name}")
    
    # Extract common name
    common_name_patterns = [r'clown\s*anemonefish', r'clownfish', r'orange\s*clownfish']
    common_name = 'clown anemonefish'  # default
    for pattern in common_name_patterns:
        match = re.search(pattern, page_text, re.IGNORECASE)
        if match:
            common_name = match.group(0)
            break
    print(f"Common Name: {common_name}")
    
    print("\n=== SEARCHING FOR OCCURRENCE/LOCATION DATA LINKS ===\n")
    
    # Define location keywords
    location_keywords = ['occurrence', 'sighting', 'location', 'distribution', 'point map', 'specimen', 'collection', 'data', 'records']
    
    # Find all links on the page
    all_links = soup.find_all('a', href=True)
    print(f"Found {len(all_links)} total links on the species profile page\n")
    
    print("Analyzing links for occurrence/location data...")
    
    occurrence_links = []
    
    # Process links one by one without generator expressions
    for i, link in enumerate(all_links, 1):
        # Extract link information safely
        href = link.get('href', '')
        link_text = link.get_text().strip()
        
        # Create lowercase versions for comparison
        link_text_lower = link_text.lower()
        href_lower = href.lower()
        
        # Check for location keywords using explicit loops instead of generator expressions
        text_has_keywords = False
        href_has_keywords = False
        matching_keywords = []
        
        # Check text for keywords
        for keyword in location_keywords:
            if keyword in link_text_lower:
                text_has_keywords = True
                matching_keywords.append(keyword)
        
        # Check href for keywords
        for keyword in location_keywords:
            if keyword in href_lower:
                href_has_keywords = True
                if keyword not in matching_keywords:
                    matching_keywords.append(keyword)
        
        is_occurrence_related = text_has_keywords or href_has_keywords
        
        if is_occurrence_related:
            # Convert relative URLs to absolute URLs
            full_url = urljoin(species_profile_url, href)
            
            occurrence_link = {
                'text': link_text,
                'url': full_url,
                'href': href,
                'keywords_found': matching_keywords
            }
            
            occurrence_links.append(occurrence_link)
            
            print(f"Occurrence Link {len(occurrence_links)}:")
            print(f"  Text: {link_text}")
            print(f"  URL: {full_url}")
            print(f"  Keywords: {matching_keywords}")
            print(f"  {'-'*60}")
        
        # Show progress for long link lists
        if i % 20 == 0:
            print(f"Processed {i}/{len(all_links)} links...")
    
    print(f"\nTotal occurrence-related links found: {len(occurrence_links)}")
    
    # Categorize the occurrence links by type
    print("\n=== CATEGORIZING LOCATION DATA SOURCES ===\n")
    
    point_map_links = []
    specimen_links = []
    distribution_links = []
    data_links = []
    
    for link in occurrence_links:
        # Safely access the text and URL
        link_text_lower = link['text'].lower()
        link_url_lower = link['url'].lower()
        
        # Categorize by content type using explicit checks
        if 'point' in link_text_lower and 'map' in link_text_lower:
            point_map_links.append(link)
            print(f"POINT MAP: {link['text']} -> {link['url']}")
        elif 'specimen' in link_text_lower or 'collection' in link_text_lower:
            specimen_links.append(link)
            print(f"SPECIMEN: {link['text']} -> {link['url']}")
        elif 'distribution' in link_text_lower or 'occurrence' in link_text_lower:
            distribution_links.append(link)
            print(f"DISTRIBUTION: {link['text']} -> {link['url']}")
        elif 'data' in link_text_lower or 'record' in link_text_lower:
            data_links.append(link)
            print(f"DATA/RECORDS: {link['text']} -> {link['url']}")
    
    print(f"\nCategorization Summary:")
    print(f"  Point Map Links: {len(point_map_links)}")
    print(f"  Specimen Links: {len(specimen_links)}")
    print(f"  Distribution Links: {len(distribution_links)}")
    print(f"  Data/Record Links: {len(data_links)}")
    
    # Look for query forms that might allow location-based searches
    print("\n=== SEARCHING FOR QUERY INTERFACES ===\n")
    
    forms = soup.find_all('form')
    query_interfaces = []
    
    print(f"Found {len(forms)} forms on the page")
    
    for form_idx, form in enumerate(forms, 1):
        form_action = form.get('action', '')
        if form_action:
            full_action_url = urljoin(species_profile_url, form_action)
            
            # Look for input fields that might relate to location queries
            inputs = form.find_all('input')
            selects = form.find_all('select')
            
            location_inputs = []
            location_indicators = ['state', 'zip', 'location', 'county', 'lat', 'lon', 'coord', 'address', 'region']
            
            for input_elem in inputs + selects:
                input_name = input_elem.get('name', '').lower()
                input_id = input_elem.get('id', '').lower()
                input_type = input_elem.get('type', '').lower()
                
                # Check for location indicators using explicit loops
                has_location_indicator = False
                for indicator in location_indicators:
                    if indicator in input_name or indicator in input_id:
                        has_location_indicator = True
                        break
                
                if has_location_indicator:
                    location_inputs.append({
                        'element_type': input_elem.name,
                        'name': input_elem.get('name', ''),
                        'id': input_elem.get('id', ''),
                        'type': input_type
                    })
            
            if location_inputs:
                query_interface = {
                    'form_action': full_action_url,
                    'location_inputs': location_inputs,
                    'form_method': form.get('method', 'GET').upper()
                }
                query_interfaces.append(query_interface)
                
                print(f"QUERY INTERFACE {len(query_interfaces)}:")
                print(f"  Form Action: {full_action_url}")
                print(f"  Method: {query_interface['form_method']}")
                print(f"  Location Inputs ({len(location_inputs)}):")
                for inp in location_inputs:
                    print(f"    {inp['element_type']}: name='{inp['name']}' id='{inp['id']}' type='{inp['type']}'")
                print(f"  {'-'*60}")
    
    # Save all extracted information
    usgs_data = {
        'species_profile': {
            'url': species_profile_url,
            'species_id': '3243',
            'scientific_name': scientific_name,
            'common_name': common_name,
            'page_title': page_title,
            'access_date': '2024',
            'content_length': len(response.content),
            'total_links_found': len(all_links)
        },
        'occurrence_data_sources': {
            'total_occurrence_links': len(occurrence_links),
            'point_map_links': point_map_links,
            'specimen_links': specimen_links,
            'distribution_links': distribution_links,
            'data_record_links': data_links,
            'all_occurrence_links': occurrence_links
        },
        'query_interfaces': query_interfaces,
        'analysis_summary': {
            'point_maps_available': len(point_map_links),
            'specimen_records_available': len(specimen_links),
            'distribution_data_available': len(distribution_links),
            'query_forms_available': len(query_interfaces)
        },
        'next_steps': [
            'Access point map links for coordinate data',
            'Query specimen databases for collection locations',
            'Use distribution links to find occurrence records',
            'Submit location-based queries through identified forms',
            'Filter all results for pre-2020 records with zip codes'
        ]
    }
    
    # Save the extracted USGS data
    with open('workspace/usgs_nas_clownfish_complete_data.json', 'w') as f:
        json.dump(usgs_data, f, indent=2)
    
    print(f"\n=== USGS DATABASE ANALYSIS COMPLETE ===\n")
    print(f"Species Profile: {species_profile_url}")
    print(f"Species: {scientific_name} ({common_name})")
    print(f"Total Links Analyzed: {len(all_links)}")
    print(f"Occurrence-related Links: {len(occurrence_links)}")
    print(f"Point Map Sources: {len(point_map_links)}")
    print(f"Specimen Record Sources: {len(specimen_links)}")
    print(f"Distribution Data Sources: {len(distribution_links)}")
    print(f"Query Interfaces: {len(query_interfaces)}")
    print(f"\nComplete data saved to: workspace/usgs_nas_clownfish_complete_data.json")
    
    # Identify the most promising sources for zip code data
    print(f"\n=== PRIORITY TARGETS FOR ZIP CODE LOCATION DATA ===\n")
    
    priority_targets = []
    
    # Point maps are highest priority (usually have coordinates)
    if point_map_links:
        priority_targets.extend(point_map_links)
        print(f"HIGH PRIORITY - Point Maps ({len(point_map_links)}):")
        for link in point_map_links:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Distribution and occurrence data are second priority
    if distribution_links:
        priority_targets.extend(distribution_links[:2])  # Top 2
        print(f"\nMEDIUM PRIORITY - Distribution Data ({len(distribution_links)}):")
        for link in distribution_links[:2]:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Data/record links are third priority
    if data_links:
        priority_targets.extend(data_links[:2])  # Top 2
        print(f"\nMEDIUM PRIORITY - Data Records ({len(data_links)}):")
        for link in data_links[:2]:
            print(f"  • {link['text']} -> {link['url']}")
    
    # Query interfaces for custom searches
    if query_interfaces:
        print(f"\nCUSTOM QUERY OPTION - Query Interfaces ({len(query_interfaces)}):")
        for interface in query_interfaces:
            print(f"  • Form: {interface['form_action']}")
            print(f"    Location inputs: {len(interface['location_inputs'])}")
    
    # Save priority targets for next step
    priority_data = {
        'objective': 'Extract specific location data with zip codes for nonnative Amphiprion ocellaris before 2020',
        'priority_targets': priority_targets,
        'query_interfaces': query_interfaces,
        'search_strategy': [
            '1. Access point map links first (highest chance of coordinates)',
            '2. Check distribution/occurrence links for location records',
            '3. Use query interfaces for location-specific searches',
            '4. Filter all results for pre-2020 timeframe',
            '5. Extract zip codes or convert coordinates to zip codes'
        ],
        'next_action': 'Access the highest priority target URLs to extract detailed location data'
    }
    
    with open('workspace/priority_location_extraction_plan.json', 'w') as f:
        json.dump(priority_data, f, indent=2)
    
    print(f"\nTotal priority targets identified: {len(priority_targets)}")
    print(f"Priority extraction plan saved to: workspace/priority_location_extraction_plan.json")
    
    if priority_targets:
        print(f"\nReady to access specific occurrence/location data sources for zip code extraction.")
        print(f"\nNext step: Access the first priority target to extract detailed location records.")
    else:
        print(f"\nNo direct priority targets found. Will need to use query interfaces or alternative approaches.")
        print(f"\nSuggested next approach: Use general search or query forms to find occurrence records.")
    
except requests.exceptions.RequestException as e:
    print(f"Error accessing USGS database: {e}")
    print("Will try alternative approaches to access USGS NAS data.")
    
except Exception as e:
    print(f"Unexpected error during USGS database access: {e}")
    print("Error details for debugging:")
    import traceback
    traceback.print_exc()
```
```

## Development History
*(This section will be updated as development progresses)*

