# Developer Plan 02

## Plan
Execute the extracted Python script from workspace/extracted_python_script.py against the provided array of strings: ['_alg', 'ghi', 'C++', 'jkl', 'tps', '/Q', 'pqr', 'stu', ':', '//', 'rose', 'vwx', 'yz1', '234', 'tta', '567', '890', 'cod', 'e.', 'or', 'g/', 'wiki', '/', 'ing', 'sort', 'abc', 'or', 'it', 'hms', 'mno', 'uic', 'ksort', '#', 'ht']. The script should generate a URL containing C++ source code as output.

## Description
This is the logical next step because: (1) We have successfully extracted the Python script from the image and saved it to workspace/extracted_python_script.py, (2) The previous step completed the image analysis and code extraction, (3) Expected outcome is to obtain a URL that contains C++ source code by running the Python script against the specified string array, (4) This advances us toward the next phase where we'll need to compile and run the C++ code against the integer array [35, 12, 8, 99, 21, 5]

## Episodic Examples
### Development Step 1: Parse and analyze Python script to understand logic and compute its final numeric result.

**Description**: Parse and analyze the attached Python code file data/gaia/2023/validation/f918266a-b3e0-4914-865d-4faa564f1aef.py to understand its structure and logic. Read the complete code content, identify the main execution flow, trace through any calculations or operations, and determine what the final numeric output will be when this code is executed.

**Use Cases**:
- Regulatory compliance audit of Python-based trading algorithms by analyzing code structure, dependencies, and operations for financial institutions
- Automated pre-merge code review in a DevOps CI/CD pipeline to detect unauthorized imports and enforce coding standards for enterprise software teams
- Generation of summary reports on data processing scripts in a bioinformatics research lab to validate function usage, mathematical operations, and workflow consistency
- Static analysis of configuration and orchestration scripts in cloud infrastructure repositories to identify potential security vulnerabilities and resource misconfigurations
- Automated grading and feedback for computer science students by parsing submitted Python assignments, checking for required functions, loops, and print statements
- Quality assurance for educational content platforms by scanning tutorial code examples to ensure consistency, presence of docstrings, and accurate execution flow descriptions
- Code migration and refactoring planning for legacy systems by inventorying existing Python modules, mapping variable assignments and control structures, and estimating complexity
- Documentation generation for an open-source machine learning library by extracting function and class definitions, usage examples, and operation details from source files

```
import os

# First, examine the attached Python file to understand its structure and content
file_path = 'data/gaia/2023/validation/f918266a-b3e0-4914-865d-4faa564f1aef.py'

print("=== PYTHON CODE FILE ANALYSIS ===")
print(f"Analyzing file: {file_path}")

# Check if file exists
if os.path.exists(file_path):
    print(f"File exists: {file_path}")
    
    # Get file size
    file_size = os.path.getsize(file_path)
    print(f"File size: {file_size} bytes")
    
    # Read and display the entire content
    with open(file_path, 'r', encoding='utf-8') as f:
        code_content = f.read()
    
    print(f"\nFile content length: {len(code_content)} characters")
    print(f"Number of lines: {len(code_content.splitlines())}")
    
    print("\n=== FULL PYTHON CODE CONTENT ===")
    print(code_content)
    print("=== END OF CODE CONTENT ===")
    
    # Analyze the code structure
    lines = code_content.splitlines()
    print(f"\n=== CODE STRUCTURE ANALYSIS ===")
    print(f"Total lines: {len(lines)}")
    
    # Look for key Python elements
    import re
    
    print("\n--- IMPORTS ---")
    for i, line in enumerate(lines, 1):
        if re.match(r'^\s*(import|from)\s+', line):
            print(f"Line {i}: {line.strip()}")
    
    print("\n--- FUNCTION DEFINITIONS ---")
    for i, line in enumerate(lines, 1):
        if re.match(r'^\s*def\s+', line):
            print(f"Line {i}: {line.strip()}")
    
    print("\n--- CLASS DEFINITIONS ---")
    for i, line in enumerate(lines, 1):
        if re.match(r'^\s*class\s+', line):
            print(f"Line {i}: {line.strip()}")
    
    print("\n--- VARIABLE ASSIGNMENTS ---")
    for i, line in enumerate(lines, 1):
        if '=' in line and not re.match(r'^\s*#', line) and not 'def ' in line and not 'class ' in line:
            stripped_line = line.strip()
            if stripped_line:
                print(f"Line {i}: {stripped_line}")
    
    print("\n--- PRINT STATEMENTS ---")
    for i, line in enumerate(lines, 1):
        if 'print(' in line:
            print(f"Line {i}: {line.strip()}")
    
    print("\n--- RETURN STATEMENTS ---")
    for i, line in enumerate(lines, 1):
        if re.match(r'^\s*return\s+', line):
            print(f"Line {i}: {line.strip()}")
    
    print("\n--- MAIN EXECUTION FLOW ---")
    # Look for if __name__ == '__main__':
    main_found = False
    for i, line in enumerate(lines, 1):
        if '__main__' in line:
            print(f"Line {i}: {line.strip()}")
            main_found = True
    
    if not main_found:
        print("No __main__ block found - code executes directly")
    
    # Look for mathematical operations
    print("\n--- MATHEMATICAL OPERATIONS ---")
    math_operators = ['+', '-', '*', '/', '//', '%', '**']
    for i, line in enumerate(lines, 1):
        for op in math_operators:
            if op in line and not re.match(r'^\s*#', line):
                print(f"Line {i}: {line.strip()} (contains '{op}')")
                break
    
    # Look for loops
    print("\n--- LOOPS ---")
    for i, line in enumerate(lines, 1):
        if re.match(r'^\s*(for|while)\s+', line):
            print(f"Line {i}: {line.strip()}")
    
    # Look for conditional statements
    print("\n--- CONDITIONALS ---")
    for i, line in enumerate(lines, 1):
        if re.match(r'^\s*if\s+', line) or re.match(r'^\s*elif\s+', line) or re.match(r'^\s*else\s*:', line):
            print(f"Line {i}: {line.strip()}")
    
    # Save the code content to workspace for further analysis
    with open('workspace/target_python_code.py', 'w', encoding='utf-8') as f:
        f.write(code_content)
    
    print(f"\n=== CODE SAVED FOR ANALYSIS ===")
    print(f"Code content saved to: workspace/target_python_code.py")
    
    # Now let's try to trace through the execution manually
    print("\n=== EXECUTION FLOW ANALYSIS ===")
    print("Analyzing the code to predict its output...")
    
    # Display line-by-line breakdown with line numbers for easier tracing
    print("\n--- LINE-BY-LINE CODE BREAKDOWN ---")
    for i, line in enumerate(lines, 1):
        if line.strip():  # Only show non-empty lines
            print(f"{i:3d}: {line}")
        elif line:  # Show empty lines but don't number them
            print(f"    {line}")
    
else:
    print(f"Error: File not found at {file_path}")
    print("Available files in data directory:")
    if os.path.exists('data'):
        for root, dirs, files in os.walk('data'):
            for file in files:
                if file.endswith('.py'):
                    print(f"  - {os.path.join(root, file)}")
    else:
        print("  - data directory not found")
```

### Development Step 2: Analyze Python code file to understand logic, trace execution, and find its final numeric output.

**Description**: Parse and analyze the attached Python code file data/gaia/2023/validation/f918266a-b3e0-4914-865d-4faa564f1aef.py to understand its structure and logic. Read the complete code content, identify the main execution flow, trace through any calculations or operations, and determine what the final numeric output will be when this code is executed.

**Use Cases**:
- Automated security auditing of financial services Python scripts to detect unexpected exception paths and harmful random value usages before deployment
- Integration into a CI/CD pipeline for e-commerce microservices to trace code flow changes and prevent regressions in discount calculation logic
- Pre-release validation of healthcare analytics tools, ensuring that data sampling routines (randomized values) meet compliance criteria and don’t silently fail
- Educational platform feature that step-by-step parses student-submitted Python assignments, highlighting execution flow and exception handling for teaching programming concepts
- Quality assurance automation in IoT firmware code, analyzing logic paths that generate sensor threshold alerts to catch misconfigurations early
- Documentation generator for enterprise DevOps teams that reads source files, maps function and class structures, and produces human-readable execution overviews
- Research lab tool to validate stochastic simulation scripts by tracing random number generation criteria and logging first valid outcomes for reproducibility
- Government software compliance verifier that parses public sector codebases, traces exception-raising conditions, and produces audit-ready reports of code logic paths

```
import os

# Read the complete Python code from the saved file to get the untruncated version
workspace_file = 'workspace/target_python_code.py'
original_file = 'data/gaia/2023/validation/f918266a-b3e0-4914-865d-4faa564f1aef.py'

print("=== READING COMPLETE UNTRUNCATED CODE ===")

# Read from the original file to ensure we get the complete content
with open(original_file, 'r', encoding='utf-8') as f:
    complete_code = f.read()

print(f"Complete code content ({len(complete_code)} characters):")
print("=" * 60)
print(complete_code)
print("=" * 60)

# Now let's trace through the execution step by step
print("\n=== MANUAL EXECUTION TRACE ===")

# Parse the code line by line to understand the logic
lines = complete_code.splitlines()
for i, line in enumerate(lines, 1):
    print(f"{i:2d}: {line}")

print("\n=== EXECUTION LOGIC ANALYSIS ===")

# Let's analyze what this code does:
print("\n1. IMPORTS:")
print("   - from random import randint (for generating random integers)")
print("   - import time (for time.sleep)")

print("\n2. CLASS UhOh:")
print("   - Custom exception class")

print("\n3. CLASS Hmm:")
print("   - __init__: sets self.value = randint(-100, 100) (random integer between -100 and 100)")
print("   - Yeah() method: needs to be analyzed from the complete code")

print("\n4. FUNCTION Okay():")
print("   - Generator function that yields Hmm() objects indefinitely")

print("\n5. FUNCTION keep_trying():")
print("   - Gets next Hmm object from generator")
print("   - Calls maybe.Yeah() method")
print("   - If Yeah() returns True, returns maybe.value")
print("   - If UhOh exception occurs, sleeps 0.1 seconds and retries")

print("\n6. MAIN EXECUTION:")
print("   - Creates generator go = Okay()")
print("   - Calls keep_trying(go) and prints the result")

# Now let's see the complete Yeah() method
print("\n=== ANALYZING Yeah() METHOD FROM COMPLETE CODE ===")

# Extract the Yeah method from the complete code
import re

# Find the Yeah method definition
yeah_method_start = None
yeah_method_lines = []
in_yeah_method = False
indent_level = None

for i, line in enumerate(lines):
    if 'def Yeah(self):' in line:
        yeah_method_start = i
        in_yeah_method = True
        indent_level = len(line) - len(line.lstrip())
        yeah_method_lines.append(line)
        print(f"Found Yeah() method at line {i+1}: {line}")
    elif in_yeah_method:
        current_indent = len(line) - len(line.lstrip()) if line.strip() else indent_level + 4
        if line.strip() and current_indent <= indent_level:
            # We've reached the end of the method
            break
        yeah_method_lines.append(line)

print("\nComplete Yeah() method:")
for line in yeah_method_lines:
    print(f"    {line}")

# Let's manually trace what the Yeah() method does
print("\n=== TRACING Yeah() METHOD LOGIC ===")
for i, line in enumerate(yeah_method_lines):
    print(f"Yeah-{i+1}: {line}")
    if 'if' in line and 'self.value' in line:
        print(f"    → This checks a condition on self.value (random number between -100 and 100)")
    elif 'raise UhOh' in line:
        print(f"    → This raises the UhOh exception")
    elif 'return' in line:
        print(f"    → This returns a value")

print("\n=== PREDICTING THE FINAL OUTPUT ===")
print("Based on the code structure:")
print("1. The program will keep generating Hmm objects with random values (-100 to 100)")
print("2. For each Hmm object, it calls Yeah() method")
print("3. If Yeah() returns True, it prints that object's value")
print("4. If Yeah() raises UhOh exception, it retries with a new Hmm object")
print("5. The final output will be the first random integer that satisfies the condition in Yeah()")

# Save the complete analysis
with open('workspace/code_execution_analysis.txt', 'w') as f:
    f.write("COMPLETE CODE ANALYSIS\n")
    f.write("=" * 50 + "\n\n")
    f.write("ORIGINAL CODE:\n")
    f.write(complete_code)
    f.write("\n\n")
    f.write("YEAH() METHOD ANALYSIS:\n")
    for line in yeah_method_lines:
        f.write(f"{line}\n")

print(f"\nComplete analysis saved to: workspace/code_execution_analysis.txt")
print("\nNext step: Need to understand the exact condition in Yeah() method to predict the output")
```

### Development Step 8: Extract, Alphabetize EC 1.11.1.7;3.1.3.1 Chemicals from 2016 Wiley Sweet Potato Virus Paper

**Description**: Access the identified 2016 Wiley paper 'Effects of Sweet Potato Feathery Mottle Virus and Sweet Potato Chlorotic Stunt Virus' and extract the specific chemicals with EC numbers 1.11.1.7 and 3.1.3.1 used in the virus testing methods. Identify the chemical names corresponding to these EC numbers, alphabetize them, and format the EC numbers in the required semicolon-separated order.

**Use Cases**:
- Agricultural pathology workflows for automating the extraction of enzyme reagents (EC 1.11.1.7 and 3.1.3.1) from crop‐virus research to rapidly assemble diagnostic assay protocols
- Pharmaceutical R&D literature mining to identify peroxidase and alkaline phosphatase methods for high‐throughput drug screening assays
- Environmental compliance reporting by compiling a standardized database of enzyme‐based soil and water testing chemicals for regulatory submissions
- Food quality control in dairy and beverage industries to automate retrieval of phosphatase assay components from published validation studies
- Patent landscaping in biotechnology to extract and alphabetize EC number–linked chemicals from patent documents for competitor intelligence
- Contract research organization (CRO) proposal generation by auto‐populating materials and methods sections with enzymatic reagents from target protocols
- Academic systematic reviews of plant virology methods to standardize and format all EC 1.11.1.7;3.1.3.1 enzyme usages across multiple studies

```
import os
import json
from datetime import datetime

# First, let's properly inspect and load the search results
print("Locating and inspecting search results files...")
print("="*80)

# Check both workspace locations mentioned in the history
search_file_paths = [
    'workspace/sweet_potato_virus_paper_search_20250806_185041.json',
    'workspace_2a649bb1-795f-4a01-b3be-9a01868dae73/sweet_potato_virus_paper_search_20250806_185041.json'
]

search_data = None
used_path = None

for path in search_file_paths:
    if os.path.exists(path):
        print(f"Found search results file: {path}")
        used_path = path
        
        # First inspect the file structure
        print(f"\nInspecting file structure...")
        with open(path, 'r', encoding='utf-8') as f:
            search_data = json.load(f)
        
        print("Top-level keys:")
        for key in search_data.keys():
            if isinstance(search_data[key], list):
                print(f"  - {key}: list with {len(search_data[key])} items")
            elif isinstance(search_data[key], dict):
                print(f"  - {key}: dict with keys {list(search_data[key].keys())}")
            else:
                print(f"  - {key}: {search_data[key]}")
        
        break

if not search_data:
    print("No search results file found. Need to run search first.")
else:
    print(f"\nUsing search data from: {used_path}")
    print(f"Target: {search_data.get('target_paper', 'N/A')}")
    print(f"EC Numbers: {search_data.get('target_ec_numbers', 'N/A')}")
    
    # Now analyze the search results with proper variable scoping
    print("\n" + "="*80)
    print("ANALYZING SEARCH RESULTS FOR PAPER AND EC NUMBERS")
    print("="*80)
    
    paper_candidates = []
    ec_number_sources = []
    
    # Process each search query result set
    search_results = search_data.get('search_results', [])
    print(f"Processing {len(search_results)} search result sets...\n")
    
    for query_idx, query_result in enumerate(search_results, 1):
        query = query_result.get('query', 'Unknown query')
        results = query_result.get('results', [])
        
        print(f"Query {query_idx}: {query}")
        print(f"Results found: {len(results)}")
        print("-"*50)
        
        # Analyze each result in this query set
        for result_idx, result in enumerate(results[:8], 1):  # Top 8 results per query
            title = result.get('title', 'No title')
            link = result.get('link', 'No URL')
            snippet = result.get('snippet', 'No snippet')
            
            # Create combined text for analysis (fix the variable scoping issue)
            title_lower = title.lower()
            snippet_lower = snippet.lower()
            link_lower = link.lower()
            combined_text = f"{title_lower} {snippet_lower} {link_lower}"
            
            print(f"  {result_idx}. {title[:80]}...")
            print(f"      URL: {link}")
            
            # Score relevance for the target paper
            relevance_score = 0
            matching_indicators = []
            
            # Check for paper-specific terms
            if 'sweet potato feathery mottle virus' in combined_text:
                relevance_score += 10
                matching_indicators.append('SPFMV')
            if 'sweet potato chlorotic stunt virus' in combined_text:
                relevance_score += 10
                matching_indicators.append('SPCSV')
            if '2016' in combined_text:
                relevance_score += 5
                matching_indicators.append('2016')
            if 'wiley' in combined_text or 'onlinelibrary.wiley.com' in combined_text:
                relevance_score += 5
                matching_indicators.append('Wiley')
            if 'effects' in combined_text:
                relevance_score += 3
                matching_indicators.append('Effects')
            if 'uganda' in combined_text:
                relevance_score += 2
                matching_indicators.append('Uganda')
            
            # Check for EC numbers or enzyme-related content
            ec_indicators = []
            if '1.11.1.7' in combined_text:
                relevance_score += 8
                ec_indicators.append('EC 1.11.1.7')
            if '3.1.3.1' in combined_text:
                relevance_score += 8
                ec_indicators.append('EC 3.1.3.1')
            if any(term in combined_text for term in ['ec number', 'enzyme', 'alkaline phosphatase', 'peroxidase']):
                relevance_score += 4
                ec_indicators.append('Enzyme terms')
            
            if matching_indicators:
                print(f"      📊 Relevance Score: {relevance_score}")
                print(f"      🎯 Indicators: {', '.join(matching_indicators)}")
                if ec_indicators:
                    print(f"      🧪 EC/Enzyme: {', '.join(ec_indicators)}")
            
            # Store high-relevance paper candidates
            if relevance_score >= 15:
                paper_candidates.append({
                    'title': title,
                    'link': link,
                    'snippet': snippet,
                    'score': relevance_score,
                    'indicators': matching_indicators + ec_indicators,
                    'query': query,
                    'is_wiley_direct': 'onlinelibrary.wiley.com' in link_lower
                })
                print(f"      ⭐ HIGH RELEVANCE - Added to candidates")
            
            # Store EC number sources separately
            if any(ec in combined_text for ec in ['1.11.1.7', '3.1.3.1']):
                ec_number_sources.append({
                    'title': title,
                    'link': link,
                    'snippet': snippet,
                    'ec_numbers_found': [ec for ec in ['1.11.1.7', '3.1.3.1'] if ec in combined_text],
                    'query': query
                })
                print(f"      🔬 EC NUMBERS FOUND - Added to EC sources")
        
        print()  # Blank line between queries
    
    # Sort candidates by relevance score
    paper_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    print("="*80)
    print(f"ANALYSIS RESULTS SUMMARY")
    print("="*80)
    
    print(f"\n📚 PAPER CANDIDATES FOUND: {len(paper_candidates)}")
    if paper_candidates:
        print("\nTop candidates:")
        for i, candidate in enumerate(paper_candidates[:3], 1):
            print(f"\n{i}. SCORE: {candidate['score']}")
            print(f"   Title: {candidate['title']}")
            print(f"   URL: {candidate['link']}")
            print(f"   Indicators: {', '.join(candidate['indicators'])}")
            print(f"   Direct Wiley Access: {'✅ YES' if candidate['is_wiley_direct'] else '❌ NO'}")
            
            # Check if this is likely the target paper
            if (candidate['score'] >= 25 and 
                candidate['is_wiley_direct'] and 
                'effects' in candidate['title'].lower()):
                print(f"   🎯 THIS IS LIKELY THE TARGET PAPER!")
    
    print(f"\n🧪 EC NUMBER SOURCES FOUND: {len(ec_number_sources)}")
    if ec_number_sources:
        print("\nEC number sources:")
        for i, source in enumerate(ec_number_sources, 1):
            print(f"\n{i}. Title: {source['title']}")
            print(f"   URL: {source['link']}")
            print(f"   EC Numbers: {', '.join(source['ec_numbers_found'])}")
            print(f"   Snippet: {source['snippet'][:200]}...")
            
            # Look for chemical names in the snippet
            snippet_lower = source['snippet'].lower()
            chemical_hints = []
            if 'alkaline phosphatase' in snippet_lower:
                chemical_hints.append('Alkaline phosphatase (likely EC 3.1.3.1)')
            if 'peroxidase' in snippet_lower:
                chemical_hints.append('Peroxidase (likely EC 1.11.1.7)')
            if 'alkaline' in snippet_lower and 'phosphatase' not in snippet_lower:
                chemical_hints.append('Contains "alkaline" - may refer to alkaline phosphatase')
            
            if chemical_hints:
                print(f"   💡 Chemical hints: {'; '.join(chemical_hints)}")
    
    # Save comprehensive analysis
    analysis_results = {
        'analysis_timestamp': datetime.now().isoformat(),
        'target_paper': search_data.get('target_paper'),
        'target_ec_numbers': search_data.get('target_ec_numbers'),
        'paper_candidates': paper_candidates,
        'ec_number_sources': ec_number_sources,
        'top_candidate': paper_candidates[0] if paper_candidates else None,
        'analysis_summary': {
            'total_paper_candidates': len(paper_candidates),
            'total_ec_sources': len(ec_number_sources),
            'wiley_direct_access': len([c for c in paper_candidates if c['is_wiley_direct']]),
            'high_confidence_match': len([c for c in paper_candidates if c['score'] >= 25]) > 0
        }
    }
    
    analysis_file = 'workspace/comprehensive_paper_analysis.json'
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n📋 NEXT STEPS RECOMMENDATION:")
    if paper_candidates and paper_candidates[0]['score'] >= 25:
        print(f"✅ Target paper identified with high confidence")
        print(f"✅ Direct Wiley access available: {paper_candidates[0]['link']}")
        print(f"🔄 NEXT: Access paper content to extract EC number chemical names")
        
        # Based on the EC sources found, provide initial chemical identification
        print(f"\n🧪 PRELIMINARY EC NUMBER CHEMICAL IDENTIFICATION:")
        print(f"Based on search results analysis:")
        print(f"   EC 1.11.1.7 = Peroxidase (enzyme that catalyzes oxidation reactions)")
        print(f"   EC 3.1.3.1 = Alkaline phosphatase (enzyme that removes phosphate groups)")
        print(f"\n📝 ALPHABETICAL ORDER: Alkaline phosphatase, Peroxidase")
        print(f"📝 EC FORMAT: 3.1.3.1;1.11.1.7")
        
    else:
        print(f"⚠️ Need to access paper content directly for confirmation")
        print(f"⚠️ May need additional search strategies")
    
    print(f"\nAnalysis saved to: {analysis_file}")
    print(f"Ready for content extraction phase.")
```

### Development Step 14: Extract Survivor (American TV) Seasons 1–44 Winners into JSON File

**Description**: Access the Wikipedia page https://en.wikipedia.org/wiki/Survivor_(American_TV_series) and extract the table of winners for seasons 1 through 44, saving each season number with its corresponding winner name into workspace/survivor_winners_list.json.

**Use Cases**:
- Entertainment trivia game content generation for mobile apps: automatically update question banks with the latest Survivor winners for quiz rounds
- Data journalism timeline visualization of Survivor champions: extract season‐by‐season winners to fuel interactive news graphics and online articles
- Academic social science research on reality TV demographics: compile winner data for statistical analysis of age, gender, and background trends across 44 seasons
- Fantasy Survivor league platform synchronization: pull the official winners list to update scoring algorithms and leaderboards in real time
- Marketing sponsorship performance analytics: correlate brand sponsors’ exposure with Survivor winners to measure campaign ROI
- Educational programming tutorials and workshops: use the Survivor winners scraping script to teach web scraping, HTML parsing, and JSON serialization in coding bootcamps
- Quality assurance monitoring for Wikipedia content: schedule periodic scraping and diffing of the winners table to detect and log unauthorized edits or vandalism

```
import os
import sys
import requests
from bs4 import BeautifulSoup

# 1) Detect the workspace directory (handles dynamic suffixes)
candidates = [d for d in os.listdir('.') if d.startswith('workspace') and os.path.isdir(d)]
if not candidates:
    print("❌ No workspace directory found. Exiting.")
    sys.exit(1)
# If multiple, pick the one with the most recent modification time
workspace_dir = max(candidates, key=lambda d: os.path.getmtime(d))
print(f"Using workspace directory: {workspace_dir}\n")

# 2) Fetch the printable version of the Survivor page with full headers
base_title = "Survivor_(American_TV_series)"
url = f"https://en.wikipedia.org/w/index.php?title={base_title}&printable=yes"
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/114.0.5735.199 Safari/537.36'
    ),
    'Accept-Language': 'en-US,en;q=0.9'
}
print(f"Fetching printable page:\n  {url}\n  with Accept-Language: {headers['Accept-Language']}\n")
response = requests.get(url, headers=headers)
response.raise_for_status()
print(f"Page fetched successfully (status code: {response.status_code})\n")

# 3) Save the printable HTML for inspection
html_path = os.path.join(workspace_dir, 'survivor_page_printable.html')
with open(html_path, 'w', encoding='utf-8') as f:
    f.write(response.text)
print(f"Saved printable HTML to: {html_path}\n")

# 4) Parse the printable HTML and extract all <span class="mw-headline">
soup = BeautifulSoup(response.text, 'html.parser')
headlines = soup.select('span.mw-headline')
print(f"Found {len(headlines)} <span class="mw-headline"> elements in printable view.\n")

# 5) Write each headline's id and text to a debug file
debug_path = os.path.join(workspace_dir, 'printable_spans.txt')
with open(debug_path, 'w', encoding='utf-8') as out:
    out.write("# All <span class='mw-headline'> in printable Survivor page\n")
    out.write("# Format: id=<span id>\ttext=<headline text>\n\n")
    for span in headlines:
        sid = span.get('id', '')
        txt = span.get_text(strip=True)
        out.write(f"id={sid}\ttext={txt}\n")

print(f"Wrote headlines to: {debug_path}\n")
print("Preview of first 10 entries:")
with open(debug_path, 'r', encoding='utf-8') as out:
    for i, line in enumerate(out):
        print(line.rstrip())
        if i >= 9:
            break
```

## Created Time
2025-08-08 18:29:35
